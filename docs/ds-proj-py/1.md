

# 一、数据探索和清理

概观

在本章中，您将开始使用 Python 和 Jupyter 笔记本，这是数据科学家使用的一些最常用的工具。然后，您将首先查看案例研究项目的数据集，这将构成本书的核心。你将开始对质量保证检查有一种直觉，即数据需要在模型建立之前通过。在本章结束时，您将能够使用 pandas(Python 中处理表格数据的顶级包)进行探索性数据分析、质量保证和数据清理。

# 简介

大多数企业拥有大量关于其运营和客户的数据。以描述性图表、图形和表格的形式报告这些数据是了解业务当前状态的好方法。但是，为了对未来的业务战略和运营提供量化指导，有必要更进一步。这就是需要机器学习和预测建模实践的地方。在本书中，我们将展示如何使用预测模型，从描述性分析到未来运营的具体指导。

为了实现这个目标，我们将通过 Python 和它的许多包介绍一些最广泛使用的机器学习工具。你还将感受到执行成功项目所必需的实践技能:在检查数据和与客户沟通时的好奇心。花时间仔细查看数据集并严格检查它是否准确满足其预期目的是值得的。您将在这里学习几种评估数据质量的技术。

在本章中，在熟悉了数据浏览的基本工具之后，我们将讨论一些典型的工作场景，看看如何接收数据。然后，我们将开始对案例研究数据集进行彻底的探索，并帮助您了解如何发现可能的问题，以便当您准备好进行建模时，可以满怀信心地继续进行。

# Python 和 Anaconda 包管理系统

在本书中，我们将使用 Python 编程语言。Python 是数据科学的顶级语言，也是发展最快的编程语言之一。Python 受欢迎的一个常见原因是它易于学习。如果你有 Python 经验，那太好了；然而，如果你有使用其他语言的经验，比如 C、Matlab 或 R，那么使用 Python 应该不会有太大的困难。你应该熟悉计算机编程的一般结构，以便从本书中获得最大收益。这种结构的例子有引导程序的**控制流**的`for`循环和`if`语句。无论您使用哪种语言，您都可能熟悉这些结构，在 Python 中也可以找到它们。

Python 不同于其他一些语言的一个关键特性是它是零索引的；换句话说，有序集合的第一个元素有一个索引`0`。Python 还支持负索引，其中索引`-1`指的是有序集合的最后一个元素，负索引从末尾向后计数。切片操作符`:`可用于从一个范围内选择一个有序集合的多个元素，从集合的开头开始，或者到集合的结尾。

## 索引和切片操作符

在这里，我们演示了索引和切片操作符是如何工作的。为了建立索引，我们将创建一个**列表**，这是一个**可变**有序集合，可以包含任何类型的数据，包括数字和字符串类型。“可变”仅仅意味着列表中的元素在第一次赋值后可以被改变。为了为我们的列表创建数字，这将是连续的整数，我们将使用内置的`range()` Python 函数。从技术上讲，`range()`函数创建了一个**迭代器**，我们将使用`list()`函数将它转换成一个列表，尽管在这里您不需要关心这个细节。下面的屏幕截图显示了控制台上打印的前五个正整数的列表，以及一些索引操作，并将列表的第一项更改为不同数据类型的新值:

![Figure 1.1: List creation and indexing
](image/B16925_01_01.jpg)

图 1.1:列表创建和索引

关于*图 1.1* 需要注意的几点:区间的终点对于切片索引和`range()`函数都是开放的，而起点是封闭的。换句话说，注意当我们指定`range()`的开始和结束时，端点 6 不包括在结果中，但是起点 1 包括在结果中。类似地，当用片`[:3]`对列表进行索引时，这包括列表中索引不超过 3 的所有元素。

我们提到了有序集合，但是 Python 也包括无序集合。其中很重要的一本叫做**字典**。字典是**键:值**对的无序集合。不是通过整数索引来查找字典的值，而是通过键来查找，键可以是数字或字符串。可以使用花括号`{}`和逗号分隔的**键:值**对来创建字典。下面的截图是一个例子，展示了我们如何创建一个包含水果数量的字典——检查苹果的数量，然后添加一种新的水果类型及其数量:

![Figure 1.2: An example dictionary
](image/B16925_01_02.jpg)

图 1.2:字典示例

Python 还有许多其他与众不同的特性，我们只是想在这里让您感受一下，而不涉及太多的细节。事实上，您可能会使用诸如**熊猫** ( `pandas`)和 **NumPy** ( `numpy`)之类的包来处理 Python 中的大部分数据。NumPy 提供了对数组和矩阵的快速数值计算，而 pandas 提供了对称为 **DataFrames** 的数据表的大量数据争论和探索能力。然而，熟悉 Python 的一些基础知识是有好处的——这种语言是所有这些的基础。例如，NumPy 和 pandas 中的索引工作方式与 Python 中的相同。

Python 的优势之一是它是开源的，并且有一个活跃的开发人员社区来创建令人惊叹的工具。我们将在本书中使用其中的几个工具。拥有来自不同贡献者的开源包的一个潜在缺陷是不同包之间的依赖性。比如你要安装熊猫，它可能依赖于某个版本的 NumPy，你可能安装了，也可能没有安装。在这方面，包装管理系统使生活变得更容易。当您通过软件包管理系统安装一个新的软件包时，它将确保满足所有的依赖关系。如果没有，系统会提示您根据需要升级或安装新的软件包。

对于本书，我们将使用 **Anaconda** 包管理系统，您应该已经安装了该系统。虽然我们在这里只使用 Python，但是也可以用 Anaconda 运行 R。

注意:环境

建议为本书创建一个新的 Python 3.x 环境。环境就像单独安装的 Python，其中您安装的软件包集可能不同，Python 的版本也可能不同。环境对于开发需要在不同版本的 Python 中部署的项目非常有用，这些项目可能具有不同的依赖关系。有关这方面的一般信息，请参见[https://docs . conda . io/projects/conda/en/latest/user-guide/tasks/manage-environments . html](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)。在开始接下来的练习之前，请参见*前言*中关于为本书设置 Anaconda 环境的具体说明。

## 练习 1.01:研究 Anaconda 并熟悉 Python

在本练习中，您将检查 Anaconda 安装中的包，并练习一些基本的 Python 控制流和数据结构，包括一个`for`循环、`dict`和`list`。这将确认您已经完成了前言中的安装步骤，并向您展示 Python 语法和数据结构与您可能熟悉的其他编程语言有何不同。执行以下步骤来完成练习:

注意

在执行本章中的练习和活动之前，请确保您遵循了*前言*中提到的关于设置 Python 环境的说明。这个练习的代码文件可以在这里找到:【https://packt.link/N0RPT】T2。

1.  Open up Terminal, if you're using macOS or Linux, or a Command Prompt window in Windows. If you're using an environment, activate it using `conda activate <name_of_your_environment>`. Then type `conda` `list` at the command line. You should observe an output similar to the following:![Figure 1.3: Selection of packages from conda list
    ](image/B16925_01_03.jpg)

    图 1.3:从 conda 列表中选择包

    您可以看到您的环境中安装的所有软件包，包括我们将直接与之交互的软件包，以及它们运行所需的依赖项。管理包之间的依赖关系是包管理系统的主要优点之一。

    注意

    有关 Anaconda 和命令行交互的更多信息，请查看这个“备忘单”:[https://docs . conda . io/projects/conda/en/latest/_ downloads/843 d9e 0198 F2 a 193 a 3484886 fa 28163 c/conda-cheat sheet . pdf](https://docs.conda.io/projects/conda/en/latest/_downloads/843d9e0198f2a193a3484886fa28163c/conda-cheatsheet.pdf)。

2.  Type `python` in Terminal to open a command-line Python interpreter. You should obtain an output similar to the following:![Figure 1.4: Command-line Python
    ](image/B16925_01_04.jpg)

    图 1.4:命令行 Python

    您应该会看到一些关于您的 Python 版本的信息，以及 Python 命令提示符(`>>>`)。当您在该提示符后键入时，您正在编写 Python 代码。

    注意

    尽管我们将在本书中使用 Jupyter 笔记本，但本练习的目的之一是通过命令提示符完成编写和运行 Python 程序的基本步骤。

3.  Write a `for` loop at the Command Prompt to print values from 0 to 4 using the following code (note that the three dots at the beginning of the second and third lines appear automatically if you are writing code in the command-line Python interpreter; if you're instead writing in a Jupyter notebook, these won't appear):

    ```py
    for counter in range(5):
    ...    print(counter)
    ... 
    ```

    当您在提示符上看到`...`时，点击*输入*,您应该会获得以下输出:

    ![Figure 1.5: Output of a for loop at the command line
    ](image/B16925_01_05.jpg)

    图 1.5:命令行中 for 循环的输出

    注意，在 Python 中，`for`循环的开始后面是冒号，**循环体需要缩进**。通常使用四个空格来缩进一个代码块。在这里，`for`循环打印由`range()`迭代器返回的值，使用带有`in`关键字的`counter`变量重复访问这些值。

    注意

    关于 Python 代码约定的更多细节，请参考以下:[https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/)。

    现在，我们将回到字典的例子。这里的第一步是创建字典。

4.  使用下面的代码创建一个水果字典(`apples`、`oranges`和`bananas`):

    ```py
    example_dict = {'apples':5, 'oranges':8, 'bananas':13}
    ```

5.  Convert the dictionary to a list using the `list()` function, as shown in the following snippet:

    ```py
    dict_to_list = list(example_dict)
    dict_to_list
    ```

    运行上述代码后，您应该会获得以下输出:

    ```py
    ['apples', 'oranges', 'bananas']
    ```

    请注意，当完成此操作并检查内容时，列表中只捕获了字典的键。如果我们想要这些值，我们必须用列表的`.values()`方法来指定。另外，请注意，字典键的列表恰好与我们在创建字典时编写它们的顺序相同。然而，这并不能保证，因为字典是无序的集合类型。

    对列表可以做的一件方便的事情是用`+`操作符将其他列表附加到列表中。例如，在下一步中，我们将把现有的水果列表与一个只包含一种水果的列表合并，覆盖包含原始列表的变量，就像这样:`list(example_dict.values());`感兴趣的读者可以自己确认这一点。

6.  Use the `+` operator to combine the existing list of fruits with a new list containing only one fruit (`pears`):

    ```py
    dict_to_list = dict_to_list + ['pears']
    dict_to_list
    ```

    您的输出如下所示:

    ```py
    ['apples', 'oranges', 'bananas', 'pears']
    ```

    如果我们想对水果种类列表进行排序会怎么样？

    Python 提供了一个内置的`sorted()`函数可以用于此；它将返回输入的排序版本。在我们的例子中，这意味着水果类型的列表将按字母顺序排序。

7.  Sort the list of fruits in alphabetical order using the `sorted()` function, as shown in the following snippet:

    ```py
    sorted(dict_to_list)
    ```

    运行上述代码后，您应该会看到以下输出:

    ```py
    ['apples', 'bananas', 'oranges', 'pears']
    ```

Python 到此为止。我们将向您展示如何执行本书的代码，因此您的 Python 知识应该会不断提高。当您打开 Python 解释器时，您可能希望运行图 1.1 和图 1.2 所示的代码示例。当您使用完解释器后，您可以键入`quit()`退出。

注意

随着您了解的越来越多，并且不可避免地想要尝试新事物，请查阅官方 Python 文档:[https://docs.python.org/3/](https://docs.python.org/3/)。

# 不同类型的数据科学问题

作为一名数据科学家，你的大部分时间可能都花在争论数据上:弄清楚如何获取数据、获取数据、检查数据、确保数据的正确性和完整性，以及将数据与其他类型的数据结合起来。pandas 是一个在 Python 中广泛使用的数据分析工具，它可以方便您的数据探索过程，正如我们将在本章中看到的。然而，这本书的一个关键目标是让你开始成为机器学习数据科学家的旅程，为此你需要掌握**预测建模**的艺术和科学。这意味着使用数学模型或理想化的数学公式来学习数据之间的关系，希望在新数据出现时做出准确而有用的预测。

对于预测建模用例，数据通常以表格结构组织，具有**特征**和**响应变量**。例如，如果您想根据房屋的一些特征来预测其价格，如**面积**和**卧室数量**，这些属性将被视为特征，而房屋的**价格**将作为响应变量。响应变量有时称为**目标变量**或**因变量**，而特征变量也可称为**自变量**。

如果您有一个包含 1000 所房屋的数据集，其中包括这些特征的值和房屋的价格，您可以说您有 1000 个**样本**的**数据，这些标签是响应变量的已知值:不同房屋的价格。最常见的是，表格数据结构被组织成不同的行是不同的样本，而特征和响应占据不同的列，以及其他元数据如样本 id，如图*图 1.6* 所示:**

![Figure 1.6: Labeled data (the house prices are the known target variable)
](image/B16925_01_06.jpg)

图 1.6:标记数据(房价是已知的目标变量)

**回归问题**

一旦您训练了一个模型来使用您的标注数据学习要素和响应之间的关系，您就可以使用它来根据要素中包含的信息对您不知道价格的房屋进行预测。在这种情况下，预测建模的目标是能够做出接近房屋真实价值的预测。由于我们预测的是连续范围内的数值，这被称为**回归问题**。

**分类问题**

另一方面，如果我们试图对房子做一个定性的预测，回答一个**是**或**否**的问题，比如“这栋房子会在未来 5 年内出售吗？”或者“业主会不会拖欠房贷？”，我们将解决所谓的**分类问题**。在这里，我们希望正确回答是或否的问题。下图是说明模型训练如何工作的示意图，以及回归或分类模型的结果可能是什么:

![Figure 1.7: Schematic of model training and prediction for regression and classification
](image/B16925_01_07.jpg)

图 1.7:回归和分类的模型训练和预测示意图

分类和回归任务被称为**监督学习**，这是一类依赖于标记数据的问题。这些问题可以被认为是需要目标变量的已知值的“监督”。相比之下，还有**无监督学习**，它涉及更多开放式问题，试图在不一定有标签的数据集中找到某种结构。从更广泛的角度来看，任何类型的应用数学问题，包括各种领域，如**优化**、**统计推断**和**时间序列建模**，都可能被认为是数据科学家的适当职责。

# 加载 Jupyter 和 pandas 的案例研究数据

现在是时候先看看我们将在案例研究中使用的数据了。除了确保我们可以正确地将数据加载到 **Jupyter 笔记本**中，我们在这一部分不做任何事情。检查数据，理解你要用它解决的问题，将会在后面。

数据文件是一个名为`default_of_credit_card_clients__courseware_version_1_21_19.xls`的 Excel 电子表格。我们建议您首先在 Excel 或您选择的电子表格程序中打开电子表格。注意行数和列数。看看一些示例值。这将有助于您了解是否已将其正确加载到 Jupyter 笔记本中。

注意

数据集可以从以下链接获得:[https://packt.link/wensZ](https://packt.link/wensZ)。这是原始数据集的修改版本，该数据集来自 http://archive.ics.uci.edu/ml 的 UCI 机器学习知识库。加州欧文:加州大学信息与计算机科学学院。

什么是 Jupyter 笔记本？

Jupyter 笔记本是交互式编码环境，允许内嵌文本和图形。对于数据科学家来说，它们是交流和保存结果的绝佳工具，因为方法(代码)和信息(文本和图形)都是集成的。您可以将环境想象成一种可以编写和执行代码的网页。事实上，Jupyter 笔记本可以呈现为网页，就像 GitHub 上所做的那样。这里有一个笔记本的例子:[https://packt.link/pREet](https://packt.link/pREet)。仔细看看，了解一下你能做些什么。这里展示了该笔记本的摘录，展示了代码、图形和文字，在本文中称为**降价**:

![Figure 1.8: Example of a Jupyter notebook showing code, graphics, and Markdown text
](image/B16925_01_08.jpg)

图 1.8:Jupyter 笔记本示例，显示代码、图形和降价文本

关于 Jupyter 笔记本，首先要了解的是如何浏览和编辑。有两种模式可供您选择。如果选择一个单元格并按下*回车*，则处于**编辑模式**，可以编辑该单元格中的文本。如果您按下 *Esc* ，您将处于**命令模式**，您可以在笔记本上导航。

注意

如果你正在阅读这本书的印刷版本，你可以通过访问以下链接下载并浏览本章中一些图片的彩色版本:[https://packt.link/T5EIH](https://packt.link/T5EIH)。

当你处于命令模式时，有许多有用的热键可以使用。*向上*和*向下*箭头将帮助您选择不同的单元格并在笔记本中滚动。如果您在命令模式下按下选定单元格上的 *y* ，它会将其更改为**代码单元格**，其中的文本被解释为代码。按下 *m* 会将其变为 **Markdown 单元格**，您可以在其中写入格式化文本。 *Shift* + *Enter* 对单元格求值，根据具体情况呈现 Markdown 或执行代码。在下一个练习中，您将使用 Jupyter 笔记本进行一些练习。

在我们的第一个 Jupyter 笔记本中，我们的第一个任务是加载案例研究数据。为此，我们将使用一个名为**熊猫**的工具。可以毫不夸张地说，pandas 是 Python 中卓越的数据处理工具。

数据帧是熊猫的基础类。我们稍后会详细讨论什么是类，但是你可以把它看作是数据结构的模板，数据结构就像我们之前讨论的列表或字典。然而，DataFrame 在功能上比这两者都丰富得多。数据帧在许多方面类似于电子表格。有由行索引标记的行，也有通常被赋予类似列标题的标签的列，这些标签可以被认为是列索引。`Index`实际上是 pandas 中的一种数据类型，用于存储数据帧的索引，列有自己的数据类型，称为`Series`。

您可以使用数据框架做许多与 Excel 工作表相同的事情，例如创建数据透视表和筛选行。pandas 还包括类似 SQL 的功能。例如，您可以将不同的数据帧连接在一起。DataFrames 的另一个优势是，一旦您的数据包含在其中一个框架中，您就可以轻松使用 pandas 的大量功能进行数据分析。下图是熊猫数据帧的一个例子:

![Figure 1.9: Example of a pandas DataFrame with an integer row index at the left and a column index of strings
](image/B16925_01_09.jpg)

图 1.9:左边是一个整数行索引，列索引是字符串的熊猫数据帧的例子

图 1.9 中的例子实际上是案例研究的数据。作为 Jupyter 和 pandas 的第一步，我们现在将看到如何创建一个 Jupyter 笔记本并加载 pandas 的数据。您可以在 pandas 中使用几个方便的函数来浏览您的数据，包括`.head()`查看数据帧的前几行、`.info()`查看所有数据类型的列、`.columns`以字符串形式返回列名列表，以及我们将在下面的练习中了解的其他函数。

## 练习 1.02:将案例研究数据载入 Jupyter 笔记本

现在你已经了解了 Jupyter 笔记本，我们将在其中编写代码的环境，以及 pandas，数据争论包，让我们创建我们的第一个 Jupyter 笔记本。我们将在这个笔记本中使用 pandas 来加载案例研究数据，并对其进行简单的检查。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本可以在[https://packt.link/GHPSn](https://packt.link/GHPSn)找到。

1.  Open a Terminal (macOS or Linux) or a Command Prompt window (Windows) and type `jupyter notebook` (first activating your Anaconda environment if you're using one).

    您将在 web 浏览器中看到 Jupyter 界面。如果浏览器没有自动打开，请将 URL 从终端复制并粘贴到您的浏览器中。在此界面中，您可以从启动笔记本服务器时所在的目录开始浏览目录。

2.  Navigate to a convenient location where you will store the materials for this book, and create a new Python 3 notebook from the **New** menu, as shown here:![Figure 1.10: Jupyter home screen
    ](image/B16925_01_10.jpg)

    图 1.10: Jupyter 主屏幕

3.  Make your very first cell a Markdown cell by typing *m* while in command mode (press *Esc* to enter command mode), then type a number sign, `#`, at the beginning of the first line, followed by a space, for a heading. Add a title for your notebook here. On the next few lines, place a description.

    下面是一个示例的屏幕截图，包括其他类型的减价，如粗体、斜体，以及在减价单元格中编写代码样式文本的方式:

    ![Figure 1.11: Unrendered Markdown cell
    ](image/B16925_01_11.jpg)

    图 1.11:未呈现的降价单元格

    请注意，为您的笔记本添加一个标题和简短描述是一个很好的做法，以便向读者表明其目的。

4.  Press *Shift* + *Enter* to render the Markdown cell.

    这也应该创建一个新的单元，它将是一个代码单元。您可以通过按下 *m* 将其更改为 Markdown 单元格，并通过按下 *y* 将其更改回 code 单元格。你会知道这是一个代码单元，因为它旁边有`In [ ]:`。

5.  Type `import` `pandas` `as` `pd` in the new cell, as shown in the following screenshot:![Figure 1.12: Rendered Markdown cell and code cell
    ](image/B16925_01_12.jpg)

    图 1.12:呈现的降价单元格和代码单元格

    在您执行这个单元之后，`pandas`模块将被加载到您的计算环境中。用`as`导入模块来创建一个短别名，比如`pd`，这是很常见的。现在，我们将使用 pandas 来加载数据文件。是微软 Excel 格式的，我们可以用`pd.read_excel`。

    注意

    有关`pd.read_excel`所有可能选项的更多信息，请参考以下文档:[https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas . read _ excel . html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)。

6.  Import the dataset, which is in the Excel format, as a DataFrame using the `pd.read_excel()` method, as shown in the following snippet:

    ```py
    df = pd.read_excel('../../Data/default_of_credit_card_clients'\
                       '__courseware_version_1_21_19.xls')
    ```

    请注意，您需要将 Excel 阅读器指向文件所在的位置。如果它和你的笔记本在同一个目录下，你只需要输入文件名。`pd.read_excel`方法将把 Excel 文件加载到一个`DataFrame`中，我们称之为`df`。默认情况下，加载电子表格的第一个工作表，在这种情况下，这是唯一的工作表。我们现在可以利用熊猫的力量了。

    让我们在接下来的几个步骤中做一些快速检查。首先，行数和列数是否与我们在 Excel 中查看文件时所知道的相匹配？

7.  Use the `.shape` method to review the number of rows and columns, as shown in the following snippet:

    ```py
    df.shape
    ```

    运行该单元后，您将获得以下输出:

    ```py
    Out[3]: (30000, 25)
    ```

    这应该与您在电子表格中的观察结果相符。如果没有，你就需要查看`pd.read_excel`的各种选项，看看你是否需要调整一些东西。

通过这次练习，我们已经成功地将数据集加载到 Jupyter 笔记本中。您可能还希望尝试 DataFrame 上的`.info()`和`.head()`方法，这将告诉您所有列的信息，并分别显示`DataFrame`的前几行。现在，您可以在 pandas 中运行您的数据了。

最后要注意的是，虽然这一点可能已经很清楚了，但是请注意，如果您在一个代码单元中定义了一个变量，那么它在笔记本中的其他代码单元中也是可用的。这是因为只要笔记本在运行，笔记本内的代码单元就共享**范围**，如下面的屏幕截图所示:

![Figure 1.13: Variable in scope between cells
](image/B16925_01_13.jpg)

图 1.13:单元格之间的范围变量

每次您启动 Jupyter 笔记本时，虽然代码和 markdown 单元格是从您以前的工作中保存的，但环境会重新开始，您需要重新加载所有模块和数据才能再次使用它们。您也可以使用笔记本的**内核**菜单手动关闭或重启笔记本。关于 Jupyter 笔记本的更多细节可以在这里的文档中找到:【https://jupyter-notebook.readthedocs.io/en/stable/[。](https://jupyter-notebook.readthedocs.io/en/stable/)

注意

在这本书中，每一个新的练习和活动都将在一个新的 Jupyter 笔记本上完成。然而，一些练习笔记还包含额外的 Python 代码和练习前几节中介绍的输出。也有包含每章全部内容的参考笔记本。比如*第一章*、*数据探索清理*的笔记本，可以在这里找到:【https://packt.link/zwofX】的。

## 熟悉数据，进行数据清理

现在我们先来看看这个数据。在您作为数据科学家的工作中，有几种可能的情况会让您收到这样的数据集。其中包括以下内容:

1.  您创建了生成数据的 SQL 查询。
2.  一位同事根据您的输入为您编写了一个 SQL 查询。
3.  一个了解数据的同事给了你，但没有你的参与。
4.  你得到了一个鲜为人知的数据集。

在案例 1 和 2 中，您的输入涉及到数据的生成/提取。在这些场景中，您可能理解了业务问题，然后在数据工程师的帮助下找到了所需的数据，或者自己进行了研究并设计了生成数据的 SQL 查询。通常，尤其是当您在数据科学角色中获得更多经验时，第一步将是与业务合作伙伴会面，以了解和提炼业务问题的数学定义。然后，您将在定义数据集中的内容方面发挥关键作用。

即使你对数据的熟悉程度相对较高，做数据探索，看不同变量的**汇总统计**仍然是重要的第一步。这一步将帮助你选择好的特性，或者给你一些如何设计新特性的想法。然而，在第三种和第四种情况下，如果不涉及您的输入或者您对数据知之甚少，那么数据探索就更加重要了。

数据科学过程中另一个重要的初始步骤是检查**数据字典**。数据字典是解释数据所有者认为数据中应该包含什么的文档，例如列标签的定义。数据科学家的工作是仔细检查数据，以确保这些定义与数据中的实际内容相匹配。在案例 1 和 2 中，您可能需要自己创建数据字典，这应该被认为是必不可少的项目文档。在第三和第四种情况下，如果可能的话，你应该去查字典。

我们将在本书中使用的案例研究数据类似于这里的案例 3。

## 商业问题

我们的客户是一家信用卡公司。他们给我们带来了一个数据集，其中包括过去 6 个月的一些人口统计数据和最近的财务数据，样本包括 30，000 名账户持有人。该数据处于信用账户级别；换句话说，每个帐户对应一行(在数据集中，您应该始终明确行的定义)。在 6 个月的历史数据期后的下一个月，账户所有人是否违约，或者换句话说，是否未能支付最低还款额。

**目标**

你的目标是在给定人口统计数据和历史数据的情况下，开发一个账户下个月是否违约的预测模型。在本书的后面，我们将讨论该模型的实际应用。

数据已经准备好了，并且数据字典是可用的。随书提供的数据集`default_of_credit_card_clients__courseware_version_1_21_19.xls`，是 UCI 机器学习知识库中这个数据集的修改版本:[https://archive . ics . UCI . edu/ml/datasets/default+of+credit+card+clients](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)。看看那个网页，里面有数据字典。

## 数据探索步骤

既然我们已经理解了业务问题，并且对数据中应该有什么有了概念，我们可以将这些印象与我们在数据中实际看到的进行比较。你在数据探索中的工作不仅仅是直接浏览数据，使用数字和图形汇总，还要批判性地思考数据是否有意义，是否与你所了解的相符。这些是数据探索中有用的步骤:

1.  How many columns are there in the data?

    这些可能是要素、响应或元数据。

2.  有多少行(样本)？
3.  What kind of features are there? Which are **categorical** and which are **numerical**?

    分类特征具有离散类别的值，例如“是”、“否”或“可能”

    数字特征通常是连续的数字刻度，例如美元金额。

4.  What does the data look like in these features?

    例如，要了解这一点，您可以检查数值特征中的值范围，或者分类特征中不同类的出现频率。

5.  有没有数据缺失？

我们已经回答了上一节中的问题 1 和 2；有 30，000 行和 25 列。当我们在下面的练习中开始探索这些问题的其余部分时，熊猫将是我们的首选工具。在下一个练习中，我们首先验证基本数据的完整性。

注意

注意，相对于网站对数据字典的描述，`X6` - `X11`在我们的数据中被称为`PAY_1` - `PAY_6`。同样，`X12` - `X17`是`BILL_AMT1` - `BILL_AMT6`，`X18` - `X23`是`PAY_AMT1` - `PAY_AMT6`。

## 练习 1.03:验证基本数据的完整性

在本练习中，我们将对数据集是否包含我们期望的内容进行基本检查，并验证样本数量是否正确。

该数据应该具有对 30，000 个信用账户的观察。虽然有 30，000 行，但我们还应该检查是否有 30，000 个唯一的帐户 id。如果用于生成数据的 SQL 查询是在一个不熟悉的模式上运行的，那么被认为是唯一的值实际上可能并不唯一。

为了检查这一点，我们可以检查唯一帐户 id 的数量是否与行数相同。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本可以在这里找到:[https://packt.link/EapDM](https://packt.link/EapDM)。

1.  Import pandas, load the data, and examine the column names by running the following command in a cell, using *Shift* + *Enter*:

    ```py
    import pandas as pd
    df = pd.read_excel('../Data/default_of_credit_card'\
                       '_clients__courseware_version_1_21_19.xls')
    df.columns
    ```

    DataFrame 的`.columns`方法用于检查所有的列名。运行该单元后，您将获得以下输出:

    ![Figure 1.14: Columns of the dataset
    ](image/B16925_01_14.jpg)

    图 1.14:数据集的列

    可以看到，输出中列出了所有的列名。账户 ID 列被引用为`ID`。剩下的列是我们的特性，最后一列是响应变量。让我们快速回顾一下客户提供给我们的数据集信息:

    `LIMIT_BAL`:包括个人消费信贷和家庭(补充)信贷在内的信贷金额(新台币)。

    `SEX`:性别(1 =男性；2 =女性)。

    注意

    出于道德考虑，我们不会使用性别数据来决定信用价值。

    `EDUCATION`:学历(1 =研究生院；2 =大学；3 =高中；4 =其他)。

    `MARRIAGE`:婚姻状况(1 =已婚；2 =单身；3 =其他)。

    `AGE`:年龄(年)。

    `PAY_1`–`PAY_6`:过去付款的记录。从 4 月到 9 月记录的过去每月付款存储在这些列中。

    `PAY_1`代表 9 月份的还款情况；`PAY_2`是 8 月份的还款状态；以此类推直到`PAY_6`，代表 4 月份的还款情况。

    还款状况的衡量尺度如下:-1 =按时支付；1 =付款延迟 1 个月；2 =延迟付款 2 个月；以此类推最多 8 =延迟付款 8 个月；9 =付款延迟 9 个月及以上。

    `BILL_AMT1`–`BILL_AMT6`:账单金额(新台币)。

    `BILL_AMT1`代表 9 月份的账单对账单金额；`BILL_AMT2`代表 8 月份的账单对账单金额；以此类推，直到`BILL_AMT6`，它代表四月份的账单金额。

    `PAY_AMT1`–`PAY_AMT6`:前次付款金额(新台币)。`PAY_AMT1`代表 9 月份支付的金额；`PAY_AMT2`代表 8 月份支付的金额；以此类推，直到`PAY_AMT6`，它代表四月份支付的金额。

    现在让我们使用下一步中的`.head()`方法来观察前几行数据。默认情况下，这将返回前 5 行。

2.  Run the following command in the subsequent cell:

    ```py
    df.head()
    ```

    下面是您应该看到的部分输出:

    ![Figure 1.15: .head() of a DataFrame
    ](image/B16925_01_15.jpg)

    图 1.15:。数据帧的 head()

    ID 列似乎包含唯一的标识符。现在，为了验证它们在整个数据集中是否是唯一的，我们可以使用`.nunique()`方法对序列(又名列)`ID`计算唯一值的数量。我们首先使用方括号选择列。

3.  Select the column (`ID`) and count unique values using the following command:

    ```py
    df['ID'].nunique()
    ```

    以下是输出结果:

    ```py
    29687
    ```

    从前面的输出可以看出，唯一条目的数量是`29,687`。

4.  Run the following command to obtain the number of rows in the dataset:

    ```py
    df.shape 
    ```

    从下面的输出中可以看出，数据集中的总行数是`30,000`:

    ```py
    (30000, 25)
    ```

    我们在这里看到惟一 id 的数量少于行数。这意味着 ID 不是数据行的唯一标识符。所以我们知道有一些重复的 id。但是有多少呢？一个 ID 是否重复多次？有多少身份证是重复的？

    我们可以用 ID 系列上的`.value_counts()`方法开始回答这些问题。这类似于 SQL 中的 **group by/count** 过程。它将列出唯一的 id 以及它们出现的频率。我们将在下一步中执行该操作，并将计数值存储在`id_counts`变量中。

5.  Store the value counts in the variable defined as `id_counts` and then display the stored values using the `.head()` method, as shown:

    ```py
    id_counts = df['ID'].value_counts()
    id_counts.head()
    ```

    您将获得以下输出:

    ![Figure 1.16: Getting value counts of the account IDs
    ](image/B16925_01_16.jpg)

    图 1.16:获取帐户 id 的值计数

    注意，默认情况下，`.head()`返回前五行。您可以通过在括号中传递所需的数字`()`来指定要显示的项目数。

6.  Display the number of duplicated entries by running another value count:

    ```py
    id_counts.value_counts()
    ```

    您将获得以下输出:

    ![Figure 1.17: Getting value counts of the account IDs
    ](image/B16925_01_17.jpg)

图 1.17:获取帐户 id 的值计数

在这里，我们可以看到，正如所料，大多数 id 只出现一次。但是，313 IDs 出现了两次。所以，没有一个 ID 出现两次以上。有了这些信息，我们就可以开始仔细研究这个数据质量问题，并着手解决它。为此，我们将创建布尔掩码。

## 布尔掩码

为了帮助清理案例研究数据，我们引入了**逻辑掩码**的概念，也称为**布尔掩码**。逻辑掩码是一种根据某种条件过滤数组或序列的方法。例如，我们可以使用 Python 中的“等于”操作符`==`，来查找包含某个值的数组的所有位置。其他比较，如“大于”(`>`)、“小于”(`<`)、“大于等于”(`>=`)、“小于等于”(`<=`，也可以类似使用。这种比较的输出是一个数组或一系列`True/False`值，也称为**布尔**值。输出的每个元素对应于输入的一个元素，如果满足条件则为`True`，否则为`False`。为了说明这是如何工作的，我们将使用**合成数据**。合成数据是为探索或说明一个概念而创建的数据。首先，我们要导入 NumPy 包，它有许多生成随机数的功能，并给它起别名`np`。我们还将从 NumPy 中的 random 模块导入默认的随机数生成器:

```py
import numpy as np
from numpy.random import default_rng 
```

现在我们用所谓的**种子**作为随机数发生器。如果您设置了种子，您将从随机数生成器获得相同的运行结果。否则，这是没有保证的。如果您在工作中以某种方式使用随机数，并希望每次运行笔记本时都有一致的结果，这可能是一个有用的选项。我们任意将种子设置为`12345`:

```py
rg = default_rng(12345)
```

接下来，我们使用`rg`的`integers`方法，用适当的参数生成 100 个随机整数。我们生成 1 到 4 之间的整数。注意`high`参数默认指定一个开放端点，即不包括范围的上限:

```py
random_integers = rg.integers(low=1,high=5,size=100)
```

我们来看这个数组的前五个元素，用`random_integers[:5]`。输出应该如下所示:

```py
array ([3, 1, 4, 2, 1])
```

假设我们想知道所有等于 3 的元素的位置。为此，我们可以创建一个布尔掩码:

```py
is_equal_to_3 = random_integers == 3
```

通过检查前 5 个元素，我们知道第一个元素等于 3，但其他元素都不等于 3。所以在我们的布尔掩码中，我们期望`True`在第一个位置，而`False`在接下来的 4 个位置。是这样吗？

```py
is_equal_to_3[:5]
```

前面的代码应该给出以下输出:

```py
array([ True, False, False, False, False])
```

这是我们所期望的。这显示了布尔掩码的创建。但是我们还能用它们做什么呢？假设我们想知道有多少个元素等于 3。要了解这一点，您可以取一个布尔掩码的和，它将`True`解释为 1，将`False`解释为 0:

```py
sum(is_equal_to_3)
```

这将为我们提供以下输出:

```py
31
```

这是有意义的，因为对于 4 个可能值的随机的、同样可能的选择，我们期望每个值出现大约 25%的时间。除了查看数组中有多少值满足布尔条件，我们还可以使用布尔掩码来选择满足该条件的数组元素。布尔掩码可以直接用于索引数组，如下所示:

```py
random_integers[is_equal_to_3]
```

这将输出满足我们指定的布尔条件的`random_integers`的元素。在这种情况下，31 个元素等于 3:

![Figure 1.18: Using the Boolean mask to index an array
](image/B16925_01_18.jpg)

图 1.18:使用布尔掩码索引数组

现在你知道了布尔数组的基础，它在很多情况下都很有用。特别是，您可以使用 DataFrames 的`.loc`方法通过布尔掩码对行进行索引，通过标签对列进行索引，以获得满足潜在不同列中的条件的各列的值。让我们用这些技巧继续探索案例研究数据。

注意

在[https://packt.link/pT9gT](https://packt.link/pT9gT)可以找到 Jupyter 笔记本，其中包含前面章节中给出的代码和相应的输出。

## 练习 1.04:持续验证数据完整性

在本练习中，利用我们对布尔数组的了解，我们将检查一些我们发现的重复 id。在*练习 03* 、*验证基本数据完整性*中，我们了解到没有 ID 出现两次以上。我们可以使用这种学习来定位重复的 id 并检查它们。然后，我们采取措施从数据集中删除质量可疑的行。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本可以在这里找到:[https://packt.link/snAP0](https://packt.link/snAP0)。

1.  Continuing where we left off in *Exercise 1.03*, *Verifying Basic Data Integrity*, we need to get the locations of the `id_counts` Series, where the count is `2`, to locate the duplicates. First, we load the data and get the value counts of IDs to bring us to where we left off in *Exercise 03*, *Verifying Basic Data Integrity*, then we create a Boolean mask locating the duplicated IDs with a variable called `dupe_mask` and display the first five elements. Use the following commands:

    ```py
    import pandas as pd
    df = pd.read_excel('../../Data/default_of_credit_card_clients'\
                       '__courseware_version_1_21_19.xls')
    id_counts = df['ID'].value_counts()
    id_counts.head()
    dupe_mask = id_counts == 2
    dupe_mask[0:5]
    ```

    您将获得以下输出(注意，id 的排序在您的输出中可能会有所不同，因为`value_counts`按频率排序，而不是 id 的索引):

    ![Figure 1.19: A Boolean mask to locate duplicate IDs
    ](image/B16925_01_19.jpg)

    图 1.19:用于定位重复 id 的布尔掩码

    注意，在前面的输出中，我们只显示了前五个条目，使用`dupe_mask`来说明这个数组的内容。您可以编辑方括号(`[]`)中的整数索引，以更改显示的条目数。

    我们的下一步是使用这个逻辑掩码来选择重复的 id。id 本身包含在`id_count`系列的索引中。我们可以访问索引，以便使用我们的逻辑掩码进行选择。

2.  Access the index of `id_count` and display the first five rows as context using the following command:

    ```py
    id_counts.index[0:5]
    ```

    这样，您将获得以下输出:

    ![Figure 1.20: Duplicated IDs 
    ](image/B16925_01_20.jpg)

    图 1.20:重复的 id

3.  使用下面的命令:

    ```py
    dupe_ids = id_counts.index[dupe_mask]
    ```

    选择并存储复制的 id 到一个名为`dupe_ids`的新变量
4.  Convert `dupe_ids` to a list and then obtain the length of the list using the following commands:

    ```py
    dupe_ids = list(dupe_ids)
    len(dupe_ids)
    ```

    您应该获得以下输出:

    ```py
    313
    ```

    我们将`dupe_ids`变量更改为`list`，因为我们将在以后的步骤中需要它。列表的长度为`313`，如前面的输出所示，这与我们从值计数中了解到的重复 id 的数量相匹配。

5.  We verify the data in `dupe_ids` by displaying the first five entries using the following command:

    ```py
    dupe_ids[0:5]
    ```

    我们获得以下输出:

    ![Figure 1.21: Making a list of duplicate IDs
    ](image/B16925_01_21.jpg)

    图 1.21:制作重复 id 的列表

    我们可以从前面的输出中观察到，该列表包含重复 id 的必需条目。我们现在可以检查重复列表中的 id 数据了。特别是，我们想看看特性的值，看看这些重复条目之间有什么不同。为此，我们将使用数据帧`df`的`.isin`和`.loc`方法。

    使用我们的 dupes 列表中的前三个 id，`dupe_ids[0:3]`，我们将计划首先找到包含这些 id 的行。如果我们将这个 ID 列表传递给 ID Series 的`.isin`方法，这将创建另一个逻辑掩码，我们可以在更大的数据帧上使用它来显示具有这些 ID 的行。`.isin`方法嵌套在索引数据帧的`.loc`语句中，以便选择布尔掩码中包含`True`的所有行的位置。`.loc`索引语句的第二个参数是`:`，这意味着将选择所有列。通过执行以下步骤，我们实际上是在过滤数据帧，以便查看前三个重复 id 的所有列。

6.  Run the following command in your notebook to execute the plan we formulated in the previous step:

    ```py
    df.loc[df['ID'].isin(dupe_ids[0:3]),:]
    ```

    ![Figure 1.22: Examining the data for duplicate IDs
    ](image/B16925_01_22.jpg)

    图 1.22:检查重复 id 的数据

    我们在这里观察到，每个重复的 ID 似乎都有一行看似有效的数据，还有一行完全是零。花点时间想想你会用这些知识做什么。

    经过一番思考，应该很清楚应该删除全零的行。也许这些是由生成数据的 SQL 查询中的错误连接条件引起的？无论如何，一行全 0 肯定是无效数据，因为对于一个人来说，年龄为 0、信用额度为 0 等等是没有意义的。

    处理这个问题的一种方法是找到全为零的行，除了第一列，它有 id。在任何情况下，这些都是无效数据，如果我们去掉所有这些，我们也可以解决重复 id 的问题。基于“等于零”的条件，通过创建与整个数据帧大小相同的布尔矩阵，我们可以找到数据帧中等于零的条目。

7.  Create a Boolean matrix of the same size as the entire DataFrame using `==`, as shown:

    ```py
    df_zero_mask = df == 0
    ```

    在接下来的步骤中，我们将使用`df_zero_mask`，这是另一个包含布尔值的数据帧。目标是创建一个布尔序列`feature_zero_mask`，它标识从第二列开始的所有元素(特性和响应，但不是 id)都为 0 的每一行。为此，我们首先需要使用整数索引(`.iloc`)方法来索引`df_zero_mask`。在这个方法中，我们通过(`:`)检查所有行，通过(`1:`)检查从第二列开始的所有列(索引`1`)。最后，我们将沿着列轴(`axis=1`)应用`all()`方法，当且仅当该行中的每一列都是`True`时，该方法将返回`True`。这是一个需要考虑的问题，但是编码起来非常简单，这将在下面的步骤中观察到。目标是获得一个与数据帧长度相同的序列，告诉我们哪些行除了 ID 之外都是零。

8.  创建布尔序列`feature_zero_mask`，如下面的代码所示:

    ```py
    feature_zero_mask = df_zero_mask.iloc[:,1:].all(axis=1)
    ```

9.  Calculate the sum of the Boolean Series using the following command:

    ```py
    sum(feature_zero_mask)
    ```

    您应该获得以下输出:

    ```py
    315
    ```

    前面的输出告诉我们，除了第一列，315 行的每一列都有零。这大于重复 id 的数量(313)，所以如果我们删除所有的“零行”，我们可能会摆脱重复 ID 的问题。

10.  Clean the DataFrame by eliminating the rows with all zeros, except for the ID, using the following code:

    ```py
    df_clean_1 = df.loc[~feature_zero_mask,:].copy()
    ```

    在前面的步骤中执行清理操作时，我们返回一个名为`df_clean_1`的新数据帧。注意，这里我们在`.loc`索引操作之后使用了`.copy()`方法来创建这个输出的副本，而不是原始数据帧上的视图。您可以认为这是创建一个新的数据帧，而不是引用原来的数据帧。在`.loc`方法中，我们使用逻辑 not 操作符`~`，为所有特性和响应变量选择不为零的所有行，使用`:`选择所有列。这是我们希望保留的有效数据。这样做之后，我们现在想知道剩余行的数量是否等于唯一 id 的数量。

11.  Verify the number of rows and columns in `df_clean_1` by running the following code:

    ```py
    df_clean_1.shape
    ```

    您将获得以下输出:

    ```py
    (29685, 25)
    ```

12.  Obtain the number of unique IDs by running the following code:

    ```py
    df_clean_1['ID'].nunique()
    ```

    以下是输出结果:

    ```py
    29685
    ```

    从前面的输出中，我们可以看到我们已经成功地消除了重复，因为惟一 id 的数量等于行数。现在深呼吸，拍拍自己的背。这是对相当多的熊猫索引和表征数据技术的旋风般的介绍。现在我们已经过滤掉了重复的 id，我们可以开始查看实际的数据本身了:特性，最后是响应变量。

完成本练习后，按如下方式将您的进度保存到 CSV(逗号分隔值)文件中。请注意，我们在保存时不包括数据帧的索引，因为这是不必要的，并且会在稍后加载时创建额外的列:

```py
df_clean_1.to_csv('../../Data/df_clean_1.csv', index=False)
```

## 练习 1.05:探索和清理数据

到目前为止，我们已经发现了一个与元数据相关的数据质量问题:我们被告知数据集的每个样本都对应于一个惟一的帐户 ID，但是我们发现事实并非如此。我们能够使用逻辑索引和熊猫来纠正这个问题。这是一个基本的数据质量问题，只与基于元数据的样本有关。除此之外，我们对帐户 id 的元数据栏并不真正感兴趣:这些不会帮助我们开发一个信用违约的预测模型。

现在，我们准备开始检查特性和响应变量的值，这些数据将用于开发我们的预测模型。执行以下步骤来完成本练习:

注意

这个练习的 Jupyter 笔记本可以在这里找到:[https://packt.link/q0huQ](https://packt.link/q0huQ)。

1.  Load the results of the previous exercise and obtain the data type of the columns in the data by using the `.info()` method as shown:

    ```py
    import pandas as pd
    df_clean_1 = pd.read_csv('../../Data/df_clean_1.csv')
    df_clean_1.info()
    ```

    您应该会看到以下输出:

    ![Figure 1.23: Getting column metadata
    ](image/B16925_01_23.jpg)

    图 1.23:获取列元数据

    我们可以在*图 1.23* 中看到有 25 列。根据这个概要，每一行有 29，685 个非空的**值，这是 DataFrame 中的行数。这表明没有丢失数据，因为每个单元格都包含一些值。然而，如果有一个填充值来表示缺失的数据，这在这里并不明显。**

    我们还看到，大多数列的旁边都写着`int64`，表明它们是一种**整数**数据类型，也就是说，像这样的数字..., -2, -1, 0, 1, 2,...。例外的是`ID`和`PAY_1`。我们已经熟悉了`ID`；这包含字符串，也就是帐户 id。那么`PAY_1`呢？根据数据字典，我们希望它包含整数，就像所有其他特性一样。让我们仔细看看这个专栏。

2.  Use the `.head(n)` pandas method to view the top `n` rows of the `PAY_1` Series:

    ```py
    df_clean_1['PAY_1'].head(5)
    ```

    您应该获得以下输出:

    ![Figure 1.24: Examine a few columns' contents
    ](image/B16925_01_24.jpg)

    图 1.24:检查一些列的内容

    输出左侧的整数是 DataFrame 索引，它只是从 0 开始的连续整数。来自`PAY_1`栏的数据显示在右侧。这应该是最近一个月账单的支付状态，使用值-1、1、2、3 等等。但是，我们可以看到这里有 0 的值，这些值在数据字典中没有记录。根据数据字典，*“还款状态的衡量尺度为:-1 =按时还款；1 =延迟一个月付款；2 =付款延迟两个月；。。。；8 =付款延迟八个月；9 =付款延迟九个月及以上"*([https://archive . ics . UCI . edu/ml/datasets/default+of+credit+card+clients](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients))。让我们使用该列的值计数来更仔细地了解一下。

3.  Obtain the value counts for the `PAY_1` column by using the `.value_counts()` method:

    ```py
    df_clean_1['PAY_1'].value_counts()
    ```

    您应该会看到以下输出:

    ![Figure 1.25: Value counts of the PAY_1 column
    ](image/B16925_01_25.jpg)

    图 1.25:PAY _ 1 列的值计数

    前面的输出揭示了两个未记录的值的存在:0 和–2，以及 pandas 将该列作为`object`数据类型导入的原因，而不是我们所期望的整数数据的`int64`:在该列中有一个`'Not available'`字符串，表示缺失的数据。在本书的后面，当我们考虑如何处理缺失数据时，我们将回到这个问题。现在，我们将移除数据集中该要素缺少值的行。

4.  Use a logical mask with the `!=` operator (which means "does not equal" in Python) to find all the rows that don't have missing data for the `PAY_1` feature:

    ```py
    valid_pay_1_mask = df_clean_1['PAY_1'] != 'Not available'
    valid_pay_1_mask[0:5]
    ```

    通过运行上述代码，您将获得以下输出:

    ![Figure 1.26: Creating a Boolean mask
    ](image/B16925_01_26.jpg)

    图 1.26:创建布尔掩码

5.  Check how many rows have no missing data by calculating the sum of the mask:

    ```py
    sum(valid_pay_1_mask)
    ```

    您将获得以下输出:

    ```py
    26664
    ```

    我们看到 26，664 行在`PAY_1`列中没有值`'Not available'`。我们从值计数中看到，3，021 行确实有这个值。这有道理吗？从*图 1.23* 我们知道数据集中有 29，685 个条目(行)，29，685–3，021 = 26，664，所以这是正确的。

6.  通过删除缺少`PAY_1`值的行来清除数据，如图所示:

    ```py
    df_clean_2 = df_clean_1.loc[valid_pay_1_mask,:].copy()
    ```

7.  Obtain the shape of the cleaned data using the following command:

    ```py
    df_clean_2.shape
    ```

    您将获得以下输出:

    ```py
    (26664, 25)
    ```

    删除这些行后，我们检查生成的数据帧是否具有预期的形状。您也可以自己检查值计数是否表明所需的值已被删除，如下所示:`df_clean_2['PAY_1'].value_counts()`。

    最后，为了使该列的数据类型与其他列一致，我们将使用`.astype`方法将其从通用的`object`类型转换为所有其他特性的`int64`。然后我们选择几列，包括`PAY_1`，检查数据类型并确保它工作正常。

8.  运行以下命令将`PAY_1`的数据类型从`object`转换为`int64`，并通过使用列表选择多个列来显示`PAY_1`和`PAY_2`的列元数据:

    ```py
    df_clean_2['PAY_1'] = df_clean_2['PAY_1'].astype('int64')
    df_clean_2[['PAY_1', 'PAY_2']].info()
    ```

这是您将获得的输出:

![Figure 1.27: Check the data type of the cleaned column
](image/B16925_01_27.jpg)

图 1.27:检查已清理列的数据类型

恭喜您，您已经完成了第二次数据清理操作！但是，如果您还记得，在这个过程中，我们还注意到了`PAY_1`中未记录的值–2 和 0。现在，让我们假设我们与我们的业务伙伴取得了联系，并了解到以下信息:

*   -2 表示该帐户当月开始时余额为零，从未使用过任何信用。
*   -1 表示帐户余额已全额支付。
*   0 表示至少支付了最低还款额，但没有支付全部余额(也就是说，正余额结转到下个月)。

我们感谢我们的商业伙伴，因为这暂时回答了我们的问题。与业务伙伴保持良好的沟通渠道和工作关系非常重要，正如您在这里所看到的，这可能会决定项目的成败。

在你的笔记本上，像这样保存你的练习进度:

```py
df_clean_2.to_csv('../../Data/df_clean_2.csv', index=False)
```

# 数据质量保证与探索

到目前为止，我们仅通过询问基本问题或查看`.info()`摘要来解决两个数据质量问题。现在让我们来看看前几列数据。在我们得到历史账单支付之前，我们有了`LIMIT_BAL`账户的信用限额，以及`SEX`、`EDUCATION`、`MARRIAGE`和`AGE`人口统计特征。我们的商业伙伴向我们伸出了手，让我们知道性别不应该被用来预测信誉，因为按照他们的标准，这是不道德的。所以我们记住这一点，以备将来参考。现在我们将研究这些专栏的其余部分，进行必要的修正。

为了进一步探索数据，我们将使用**直方图**。直方图是可视化连续规模数据的好方法，如货币数量和年龄。直方图将相似的值分组到多个条块中，并将这些条块中的数据点数显示为条形图。

为了绘制直方图，我们将开始熟悉熊猫的图形能力。pandas 依赖另一个名为 **Matplotlib** 的库来创建图形，所以我们也将使用`matplotlib`来设置一些选项。使用这些工具，我们还将学习如何快速获得熊猫数据的统计摘要。

## 练习 1.06:探索信用限额和人口统计特征

在本练习中，我们将从信贷限额和年龄特征开始探索数据。我们将对它们进行可视化，并获得汇总统计数据，以检查这些特性中包含的数据是否合理。然后，我们将查看教育和婚姻的分类特征，看看那里的值是否有意义，并在必要时进行纠正。`LIMIT_BAL`和`AGE`是数字特征，意味着它们是在连续的尺度上测量的。因此，我们将使用直方图来可视化它们。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本在这里找到:[https://packt.link/PRdtP](https://packt.link/PRdtP)。

1.  In addition to pandas, import `matplotlib` and set up some plotting options with this code snippet. Note the use of comments in Python with `#`. Anything appearing after a `#` on a line will be ignored by the Python interpreter:

    ```py
    import pandas as pd
    import matplotlib.pyplot as plt #import plotting package
    #render plotting automatically
    %matplotlib inline
    import matplotlib as mpl #additional plotting functionality
    mpl.rcParams['figure.dpi'] = 400 #high resolution figures
    ```

    这将导入`matplotlib`并使用`.rcParams`来设置分辨率(`dpi` =每英寸点数)以获得清晰的图像；你可能不想担心这最后一部分，除非你正在准备演示的东西，因为它可能会使你的笔记本中的图像相当大。

2.  使用下面的代码加载上一个练习的进度:

    ```py
    df_clean_2 = pd.read_csv('../Data/df_clean_2.csv'),
    ```

3.  Run `df_clean_2[['LIMIT_BAL', 'AGE']].hist()` and you should see the following histograms:![Figure 1.28: Histograms of the credit limit and age data
    ](image/B16925_01_28.jpg)

    图 1.28:信贷限额和年龄数据的直方图

    这是这些特征的一个很好的视觉快照。通过这种方式，我们可以快速、粗略地查看所有数据。为了查看平均值和中间值(即第 50 个百分位)等统计数据，还有另一个有用的 pandas 函数。

4.  Generate a tabular report of summary statistics using the following command:

    ```py
    df_clean_2[['LIMIT_BAL', 'AGE']].describe()
    ```

    您应该会看到以下输出:

    ![Figure 1.29: Statistical summaries of credit limit and age data
    ](image/B16925_01_29.jpg)

    图 1.29:信贷限额和年龄数据的统计摘要

    基于直方图和由`.describe()`计算的方便的统计数据，包括非空值的计数、平均值和标准偏差、最小值、最大值和四分位数，我们可以做出一些判断。

    `LIMIT_BAL`、信用额度，似乎有道理。信用额度最低为 10，000 英镑。这个数据集来自台湾；确切的货币单位(新台币)可能不熟悉，但直觉上，信用额度应该大于零。我们鼓励您查找当地货币的兑换情况，并考虑这些信用限额。比如 1 美元约合 30 台币。

    `AGE`功能看起来也分布合理，21 岁以下的人都没有信用账户。

    对于分类特征，查看值计数是有用的，因为唯一值相对较少。

5.  Obtain the value counts for the `EDUCATION` feature using the following code:

    ```py
    df_clean_2['EDUCATION'].value_counts()
    ```

    您应该会看到以下输出:

    ![Figure 1.30: Value counts of the EDUCATION feature
    ](image/B16925_01_30.jpg)

    图 1.30:教育功能的值计数

    这里，我们看到未记录的教育水平 0、5 和 6，因为数据字典只描述了`Education (1 = graduate school; 2 = university; 3 = high school; 4 = others)`。我们的商业伙伴告诉我们，他们不知道其他人的情况。由于它们不是很普遍，我们将把它们归入`others`类别，这似乎是合适的。

6.  Run this code to combine the undocumented levels of the `EDUCATION` feature into the level for `others` and then examine the results:

    ```py
    df_clean_2['EDUCATION'].replace(to_replace=[0, 5, 6],\
                                    value=4, inplace=True)
    df_clean_2['EDUCATION'].value_counts()
    ```

    pandas `.replace`方法使得前面步骤中描述的替换变得非常快。运行代码后，您应该会看到以下输出:

    ![Figure 1.31: Cleaning the EDUCATION feature
    ](image/B16925_01_31.jpg)

    图 1.31:清理教育功能

    注意，这里我们在 ( `inplace=True`)的地方做了这个修改**。这意味着，该操作将对现有数据帧进行更改，而不是返回新的数据帧。**

7.  Obtain the value counts for the `MARRIAGE` feature using the following code:

    ```py
    df_clean_2['MARRIAGE'].value_counts()
    ```

    您应该获得以下输出:

    ![Figure 1.32: Value counts of the raw MARRIAGE feature
    ](image/B16925_01_32.jpg)

    图 1.32:原始婚姻特征的值计数

    这里的问题类似于`EDUCATION`特性遇到的问题；有一个值 0 没有记录在数据字典中:`1 = married; 2 = single; 3 = others`。所以我们把它和`others`放在一起。

8.  Change the values of 0 in the `MARRIAGE` feature to 3 and examine the result with this code:

    ```py
    df_clean_2['MARRIAGE'].replace(to_replace=0, value=3, \
                                   inplace=True)
    df_clean_2['MARRIAGE'].value_counts()
    ```

    输出应该如下所示:

    ![Figure 1.33: Value counts of the cleaned MARRIAGE feature
    ](image/B16925_01_33.jpg)

图 1.33:清理后的婚姻特征的值计数

我们现在已经完成了大量的数据探索和清理工作。稍后，我们将对数据框架中的金融历史特征进行更高级的可视化和探索。首先，我们将考虑`EDUCATION`特征的含义，它是我们数据集中的一个分类特征。

按如下方式保存本练习的进度:

```py
df_clean_2.to_csv('../../Data/df_clean_2_01.csv', index=False)
```

## 深潜:分类特征

机器学习算法只对数字起作用。例如，如果您的数据包含文本特征，则需要以某种方式转换为数字。我们从上面了解到，我们案例研究的数据实际上完全是数字。然而，有必要思考一下为什么会这样。具体来说，考虑一下`EDUCATION`特性。

这是所谓的**分类特征**的一个例子:你可以想象作为原始数据，这个列由文本标签`graduate school`、`university`、`high school`和`others`组成。这些被称为分类特征的**级别**；在这里，有四个层次。只有通过已经为我们选择的映射，这些数据才以数字 1、2、3 和 4 的形式存在于我们的数据集中。这种将类别分配给数字的特殊方式产生了所谓的**序数特征**，因为等级是按顺序映射到数字的。作为一名数据科学家，如果您自己没有选择这些映射，那么至少您需要知道这些映射。

这种映射的含义是什么？

对教育水平进行排序是有一定意义的，1 对应于我们数据集中的最高教育水平，2 对应于第二高的教育水平，3 对应于第二高的教育水平，4 大概包括最低的教育水平。但是，当您在机器学习模型中使用这种编码作为数字特征时，它将被视为与任何其他数字特征一样。对于某些型号，这种效果可能不理想。

如果一个模型试图寻找特征和反应之间的直线关系，会怎样？

这似乎是一个随意的问题，尽管在本书的后面你会学到区分线性和非线性模型的重要性。在本节中，我们将简要介绍一些模型寻找特征和响应变量之间的线性关系的概念。这是否会在教育功能的情况下很好地工作，取决于不同教育水平和我们试图预测的结果之间的实际关系。

在这里，我们检查了两个假设的情况下，合成数据与有序分类变量，每个有 10 个水平。这些级别衡量的是访问网站的客户自我报告的满意度。y 轴上绘制了报告每个级别的客户在网站上花费的平均分钟数。我们还绘制了每种情况下的最佳拟合线，以说明线性模型如何处理这些数据，如下图所示:

![Figure 1.34: Ordinal features may or may not work well in a linear model
](image/B16925_01_34.jpg)

图 1.34:序数特征在线性模型中可能有效，也可能无效

我们可以看到，如果一个算法假设特征和响应变量之间的线性(直线)关系，这可能会也可能不会工作得很好，这取决于真实的关系。请注意，在这个综合示例中，我们正在模拟一个回归问题:响应变量呈现连续的数字范围。虽然我们的案例研究涉及一个分类问题，但一些分类算法，如**逻辑回归**也假设特征的线性效应。稍后，当我们开始为案例研究建模数据时，我们将更详细地讨论这一点。

粗略地说，对于二元分类问题，这意味着响应变量只有两种结果，我们假设编码为 0 和 1，您可以根据每个级别内响应变量的平均值来查看分类特征的不同级别。这些平均值代表每个级别的阳性类别(即响应变量= 1 的样本)的“比率”。这可以让您了解序数编码是否适用于线性模型。假设您已经在 Jupyter 笔记本中导入了与前面部分相同的包，那么您可以使用 pandas 中的`groupby` / `agg` regate 过程和条形图快速查看这些包。

这将根据`EDUCATION`特性中的值对数据进行分组，然后在每个组中使用`default payment next month`响应变量的平均值将数据聚合在一起:

```py
df_clean_2 = pd.read_csv('../../Data/df_clean_2_01.csv')
df_clean_2.groupby('EDUCATION').agg({'default payment next '\
                                     'month':'mean'})\
                               .plot.bar(legend=False)
plt.ylabel('Default rate')
plt.xlabel('Education level: ordinal encoding')
```

运行代码后，您应该会获得以下输出:

![Figure 1.35: Default rate within education levels
](image/B16925_01_35.jpg)

图 1.35:教育水平内的违约率

类似于*图 1.34* 中的*示例 2* ，看起来直线拟合可能不是此处数据的最佳描述。如果一个特性有这样的非线性效应，使用更复杂的算法可能会更好，如**决策树**或**随机森林**。或者，如果需要更简单、更易解释的线性模型，如逻辑回归，我们可以避免序数编码，而使用不同的分类变量编码方式。一种流行的方法叫做**一键编码** ( **OHE** )。

OHE 是一种将分类特征(可能由原始数据中的文本标签组成)转换为可用于数学模型的数字特征的方法。

让我们通过一个练习来了解这一点。如果你想知道为什么逻辑回归更容易解释，而随机森林更复杂，我们将在后面的章节中详细了解这些概念。

## 练习 1.07:实现分类特征的 OHE

在本练习中，我们将对数据集中的`EDUCATION`要素进行“逆向工程”,以获得代表不同教育水平的文本标签，然后展示如何使用 pandas 创建 OHE。首先，请设置环境并加载之前练习的进度:

```py
import pandas as pd
import matplotlib as mpl #additional plotting functionality
mpl.rcParams['figure.dpi'] = 400 #high resolution figures
df_clean_2 = pd.read_csv('../../Data/df_clean_2_01.csv')
```

首先，让我们考虑一下我们的`EDUCATION`特性，在它被编码为序数之前。从数据字典中我们知道，1 =读研，2 =大学，3 =高中，4 =其他。我们希望重新创建一个包含这些字符串而不是数字的列。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本在这里找到:[https://packt.link/akAYJ](https://packt.link/akAYJ)。

1.  为名为`EDUCATION_CAT`的分类标签创建一个空列。使用下面的命令，每一行都将包含字符串`'none'` :

    ```py
    df_clean_2['EDUCATION_CAT'] = 'none'
    ```

2.  Examine the first few rows of the DataFrame for the `EDUCATION` and `EDUCATION_CAT` columns:

    ```py
    df_clean_2[['EDUCATION', 'EDUCATION_CAT']].head(10)
    ```

    输出应该如下所示:

    ![Figure 1.36: Selecting columns and viewing the first 10 rows
    ](image/B16925_01_36.jpg)

    图 1.36:选择列并查看前 10 行

    我们需要用适当的字符串填充这个新列。pandas 提供了将一个系列的所有值映射到新值的便利功能。这个函数实际上叫做`.map`，它依赖于一个字典来建立旧值和新值之间的对应关系。我们的目标是将`EDUCATION`中的数字映射到它们所代表的字符串上。例如，当`EDUCATION`列等于数字 1 时，我们将把`'graduate school'`字符串分配给`EDUCATION_CAT`列，对于其他教育水平也是如此。

3.  使用以下代码创建描述教育类别映射的字典:

    ```py
    cat_mapping = {1: "graduate school",\
                   2: "university",\
                   3: "high school",\
                   4: "others"}
    ```

4.  Apply the mapping to the original `EDUCATION` column using `.map` and assign the result to the new `EDUCATION_CAT` column:

    ```py
    df_clean_2['EDUCATION_CAT'] = df_clean_2['EDUCATION']\
                                  .map(cat_mapping)
    df_clean_2[['EDUCATION', 'EDUCATION_CAT']].head(10)
    ```

    运行这些行之后，您应该会看到以下输出:

    ![Figure 1.37: Examining the string values corresponding to the ordinal 
    encoding of EDUCATION
    ](image/B16925_01_37.jpg)

    图 1.37:检查与教育的序数编码相对应的字符串值

    太棒了。注意，我们可以跳过*步骤 1* ，在这里我们用`'none'`分配新列，并直接进入*步骤 3* 和 *4* 来创建新列。然而，有时创建一个用单个值初始化的新列是有用的，所以了解如何做是值得的。

    现在我们已经准备好一键编码了。我们可以通过向 pandas `get_dummies()`函数传递一系列的`DataFrame`来实现这一点。该函数之所以得名，是因为一键编码列也被称为**虚拟变量**。结果将是一个新的 DataFrame，其列数与分类变量的级别数一样多。

5.  Run this code to create a one-hot encoded DataFrame of the `EDUCATION_CAT` column. Examine the first 10 rows:

    ```py
    edu_ohe = pd.get_dummies(df_clean_2['EDUCATION_CAT'])
    edu_ohe.head(10)
    ```

    这将产生以下输出:

    ![Figure 1.38: DataFrame of one-hot encoding
    ](image/B16925_01_38.jpg)

    图 1.38:一键编码的数据帧

    现在，您可以看到为什么这被称为“一键编码”:在所有这些列中，任何特定的行都将在恰好 1 列中有 1，而在其余的列中有 0。对于给定的行，带有 1 的列应该与原始分类变量的级别相匹配。为了检查这一点，我们需要将新的数据帧与原始数据帧连接起来，并并排检查结果。我们将使用 pandas `concat`函数，向其传递我们希望连接的数据帧列表，并使用`axis=1`关键字来水平连接它们；即沿着列轴。这基本上意味着我们将“并排”组合这两个数据帧，我们知道我们可以做到这一点，因为我们刚刚从原始数据帧中创建了这个新的数据帧:我们知道它将具有相同的行数，并且与原始数据帧的顺序相同。

6.  Concatenate the one-hot encoded DataFrame to the original DataFrame as follows:

    ```py
    df_with_ohe = pd.concat([df_clean_2, edu_ohe], axis=1)
    df_with_ohe[['EDUCATION_CAT', 'graduate school',\
                 'high school', 'university', 'others']].head(10)
    ```

    您应该会看到以下输出:

    ![Figure 1.39: Checking the one-hot encoded columns
    ](image/B16925_01_39.jpg)

图 1.39:检查独热编码列

好吧，看起来这像预期的那样起作用了。OHE 是编码分类特征的另一种方式，它避免了序数编码中隐含的数字结构。但是，请注意这里发生了什么:我们取了一个单独的列`EDUCATION`，并将其展开成与特性中的级别一样多的列。在这种情况下，由于只有四个级别，这不是一件大事。但是，如果您的分类变量有非常多的级别，您可能需要考虑一个替代策略，例如将一些级别组合到一个类别中。

这是保存我们在这里创建的 DataFrame 的好时机，它封装了我们清理数据和添加 OHE 列的工作。

将最新的数据帧写入如下文件:`df_with_ohe.to_csv('../../Data/Chapter_1_cleaned_data.csv', index=False)`。

# 探索数据集中的财务历史特征

我们准备探索案例研究数据集中的其余要素。首先设置环境并加载上一个练习中的数据。这可以使用下面的代码片段来完成:

```py
import pandas as pd
import matplotlib.pyplot as plt #import plotting package
#render plotting automatically
%matplotlib inline
import matplotlib as mpl #additional plotting functionality
mpl.rcParams['figure.dpi'] = 400 #high resolution figures
import numpy as np
df = pd.read_csv('../../Data/Chapter_1_cleaned_data.csv')
```

注意

根据您保存 CSV 文件的位置，该文件的路径可能会有所不同。

剩下要检查的特性是财务历史特性。它们自然地分为三组:过去 6 个月的每月付款状态，以及同期的已开帐单和已付金额。首先，让我们看看付款状态。把这些列成一个清单很方便，这样我们可以一起研究。您可以使用以下代码来实现这一点:

```py
pay_feats = ['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', \
             'PAY_6']
```

我们可以对这六个系列使用`.describe`方法来检查汇总统计数据:

```py
df[pay_feats].describe()
```

这将产生以下输出:

![Figure 1.40: Summary statistics of payment status features
](image/B16925_01_40.jpg)

图 1.40:支付状态特征的汇总统计

这里，我们观察到所有这些特征的值的范围是相同的:-2，-1，0，...8.数据字典中描述为*付款延迟九个月及以上*的值 9 似乎从未被遵守。

我们已经阐明了所有这些级别的含义，其中一些不在原始数据字典中。现在我们再来看一下`PAY_1`的`value_counts()`，现在按照我们统计的数值排序，也就是这个系列的`index`:

```py
df[pay_feats[0]].value_counts().sort_index()
```

这将产生以下输出:

![Figure 1.41: Value counts of the payment status for the previous month
](image/B16925_01_41.jpg)

图 1.41:上个月付款状态的值计数

与正整数值相比，大多数值要么是-2，-1，要么是 0，这对应于上个月信誉良好的账户:未使用、全额支付或至少支付了最低还款额。

注意，由于这个变量的其他值的定义(1 =延迟支付 1 个月；2 =延迟支付 2 个月，等等)，这个特性是分类特性和数字特性的混合。为什么没有信用使用对应于值-2，而值 2 意味着延迟付款 2 个月，等等？我们应该承认，支付状态的数字编码-2、-1 和 0 构成了数据集的创建者就如何对某些分类特征进行编码而做出的决定，这些分类特征随后被归入一个真正的数字特征:支付延迟的月数(值为 1 或更大)。稍后，我们将考虑这种做事方式对该特性预测能力的潜在影响。

目前，我们将继续探索这些数据。这个数据集足够小，有 18 个这样的金融特征和一些其他特征，我们可以单独检查每个特征。如果数据集有数千个特征，我们可能会放弃这一点，转而探索**维度缩减**技术，这是一种将大量特征中的信息浓缩为少量派生特征的方法，或者，另一种方法是**特征选择**，这种方法可用于从众多候选字段中分离出重要特征。稍后我们将演示和解释一些特征选择技术。但是在这个数据集上，可视化每个特征是可行的。从上一章我们知道，直方图是一种很好的方式，可以快速直观地解释我们从数值计数表中获得的同类信息。您可以使用`df[pay_feats[0]].hist()`对最近一个月的付款状态功能进行尝试，以生成以下内容:

![Figure 1.42: Histogram of PAY_1 using default arguments
](image/B16925_01_42.jpg)

图 1.42:使用默认参数的 PAY_1 直方图

现在，我们将深入了解这张图片是如何制作的，并考虑它是否具有应有的信息量。关于熊猫的图形功能的一个关键点是**熊猫绘图实际上调用了 T4 的 matplotlib。请注意，pandas `.hist()`方法的最后一个可用参数是`**kwds`，文档显示这是`matplotlib`关键字参数。**

注意

更多信息请参考:[https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas。DataFrame.hist.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html)。

查看`matplotlib.pyplot.hist`的`matplotlib`文档显示了可以与熊猫`.hist()`方法一起使用的其他参数，例如要绘制的直方图的类型(有关更多详细信息，请参见[https://matplotlib . org/API/_ as _ gen/matplotlib . py plot . hist . html](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html))。一般来说，要获得更多关于绘图功能的细节，了解`matplotlib`是很重要的，在一些场景中，你会想要直接使用`matplotlib`，而不是熊猫，以便对绘图的外观有更多的控制。

你要知道熊猫用的是`matplotlib`，而 T0 又用的是 NumPy。用`matplotlib`绘制直方图时，组成直方图的数值计算实际上是由 NumPy `.histogram`函数执行的。这是代码重用的一个重要例子，或者说“没有重新发明轮子”如果一个标准功能，比如绘制直方图，已经在 Python 中有了很好的实现，那么就没有理由重新创建它。如果已经实现了为绘图创建直方图数据的数学运算，也应该利用这一点。这显示了 Python 生态系统的相互关联性。

我们现在将解决在计算和绘制直方图时出现的几个关键问题。

**箱子数量**

直方图通过将值分组到所谓的**箱**中来工作。仓的数量是构成我们看到的离散直方图的垂直条的数量。如果在一个连续的范围内有大量的唯一值，比如我们前面看到的年龄直方图，直方图绘制相对来说“开箱即用”，使用默认参数。但是，当唯一值的数量接近于条柱的数量时，结果可能会有点误导。箱的默认数量是 10，而在`PAY_1`特性中，有 11 个唯一值。在这种情况下，最好手动将直方图箱的数量设置为唯一值的数量。

在我们当前的例子中，由于在`PAY_1`的较高频段中有很少的值，所以图看起来不会有太大的不同。但一般来说，在绘制直方图时，记住这一点很重要。

**箱子边缘**

条柱边缘的位置决定了直方图中值的分组方式。您可以为关键字参数`bins`提供一个数字列表或数组，而不是向绘图函数指示容器的数量。该输入将被解释为 x 轴上的容器边缘位置。理解使用边缘位置将值分组到`matplotlib`的箱中的方式很重要。除了最后一个容器，所有容器都将低至左边缘且高至**的值组合在一起，但不包括高至右边缘的**值。换句话说，对于这些箱，左边缘是闭合的，但是右边缘是开放的。然而，最后一个面元包括两边；它有一个封闭的左右边缘。当您宁滨相对较少的唯一值时，这一点更有实际意义。

为了控制绘图外观，通常最好指定容器边缘位置。我们将创建一个由 12 个数字组成的数组，这将产生 11 个箱，每个箱以`PAY_1`的唯一值之一为中心:

```py
pay_1_bins = np.array(range(-2,10)) - 0.5
pay_1_bins
```

输出显示箱子边缘位置:

```py
array([-2.5, -1.5, -0.5, 0.5, 1.5, 2.5,\
       3.5,4.5, 5.5, 6.5, 7.5,8.5])
```

作为风格的最后一点，总是*给你的情节*贴上标签是很重要的，这样它们才是可解释的。我们还没有手动做到这一点，因为在某些情况下，熊猫会自动做到这一点，而在其他情况下，我们只是让地块不加标签。从现在开始，我们将遵循最佳实践，并标记所有地块。我们使用`matplotlib`中的`xlabel`和`ylabel`函数向该图添加轴标签。代码如下:

```py
df[pay_feats[0]].hist(bins=pay_1_bins)
plt.xlabel('PAY_1')
plt.ylabel('Number of accounts')
```

输出应该如下所示:

![Figure 1.43: A better histogram of PAY_1
](image/B16925_01_43.jpg)

图 1.43:更好的 PAY_1 直方图

*图 1.43* 显示了一个改进的直方图，因为条形以数据中的实际值为中心，每个唯一值有一个条形。虽然用默认参数调用绘图函数很有诱惑力，而且通常也足够了，但作为数据科学家，你的工作之一就是创建*准确且有代表性的数据可视化*。为了做到这一点，有时您需要深入研究绘制代码的细节，就像我们在这里所做的那样。

从这种数据可视化中我们学到了什么？

由于我们已经查看了值计数，这向我们证实了大多数帐户都处于良好状态(值-2、-1 和 0)。对于那些不是这样的人来说，更常见的是“晚了几个月”是一个较小的数字。这有道理；很可能，大多数人很快就会还清欠款。否则，他们的帐户可能会被关闭或出售给代收机构。检查您的要素分布并确保它看起来合理是一件与您的客户确认的好事情，因为这些数据的质量是您寻求做的预测建模的基础。

现在我们已经为直方图建立了一些好的绘图风格，让我们使用 pandas 一起绘制多个直方图，并可视化过去 6 个月中每个月的支付状态特征。我们可以传递我们的列名列表`pay_feats`来访问用`.hist()`方法绘制的多个列，指定我们已经确定的 bin 边缘，并指示我们想要一个 2×3 的绘图网格。首先，我们将字体大小设置得足够小，以适应这些**支线剧情**。以下是代码:

```py
mpl.rcParams['font.size'] = 4
df[pay_feats].hist(bins=pay_1_bins, layout=(2,3))
```

已经根据列名为我们自动创建了图标题。y 轴被理解为计数。结果可视化如下:

![Figure 1.44: Grid of histogram subplots
](image/B16925_01_44.jpg)

图 1.44:直方图子图网格

我们已经看到了第一个，这很有意义。其余的人呢？记住这些特性的正整数值的定义，以及每个特性的含义。比如`PAY_2`是 8 月份的还款状态，`PAY_3`是 7 月份的还款状态，其他的时间更往后。值 1 表示延迟支付 1 个月，而值 2 表示延迟支付 2 个月，依此类推。

你有没有注意到有些事情似乎不太对劲？考虑七月(`PAY_3`)和八月(`PAY_2`)之间的值。在 7 月，很少有账户有 1 个月的支付延迟；这个条在直方图中实际上是不可见的。然而，在 8 月份，突然有数千个账户出现 2 个月的支付延迟。这没有意义:给定月份中延迟 2 个月的账户数量应该小于或等于前一个月中延迟 1 个月的账户数量。

让我们仔细看看 8 月份延迟了 2 个月的账户，看看 7 月份的付款情况。我们可以使用布尔掩码和`.loc`通过下面的代码做到这一点，如下面的代码片段所示:

```py
df.loc[df['PAY_2']==2, ['PAY_2', 'PAY_3']].head()
```

其输出应该如下所示:

![Figure 1.45: Payment status in July (PAY_3) of accounts with a 2-month payment 
delay in August (PAY_2)
](image/B16925_01_45.jpg)

图 1.45:在 8 月(PAY_2)延迟付款 2 个月的账户在 7 月(PAY_3)的付款状态

从*图 1.45* 中可以明显看出，8 月份延迟 2 个月的账户对于 7 月份的付款状态具有无意义的值。进展到 2 个月延迟的唯一方法应该是从前一个月的 1 个月延迟开始，但是没有一个账户表明这一点。

当您在数据中看到类似这样的内容时，您需要检查用于创建数据集的查询中的逻辑，或者联系向您提供数据集的人。在仔细检查这些结果后，例如使用`.value_counts()`直接查看数字，我们联系我们的客户询问这个问题。

客户让我们知道，他们在提取最近一个月的数据时遇到了问题，导致帐户报告错误，延迟了 1 个月的付款。9 月份，他们已经基本解决了这些问题(虽然没有完全解决；正如我们所发现的，这就是为什么在`PAY_1`特性中会有缺失值。因此，在我们的数据集中，除了 9 月份之外，其他月份都少报了值 1(`PAY_1`特性)。理论上，客户可以创建一个查询来查看他们的数据库，并确定`PAY_2`、`PAY_3`等直到`PAY_6`的正确值。然而，由于实际原因，他们无法及时完成这份回顾性分析，以便我们接收它并将其包含在我们的项目中。

因此，只有最近一个月的付款状态数据是正确的。这意味着，在所有支付状态特征中，只有`PAY_1`代表未来数据，这些数据将用于我们开发的模型进行预测。这是一个关键点:*预测模型依赖于获得与用*构建的模型相同的数据来进行预测。这意味着我们可以在我们的模型中使用`PAY_1`作为一个功能，但不能使用`PAY_2`或前几个月的其他支付状态功能。

这一集显示了彻底检查数据质量的重要性。仔细梳理资料才发现这个问题。如果客户事先告诉我们，他们在过去几个月收集我们的数据集时遇到了报告问题，并且在此期间报告程序不**一致**，那就更好了。然而，最终建立一个可信的模型是我们的责任，所以我们需要通过这种详细的探索来确保我们相信数据是正确的。我们向客户解释，我们不能使用旧的特征，因为它们不代表模型将被**评分**的未来数据(即，对未来几个月做出预测)，并要求他们让我们知道他们所知道的任何进一步的数据问题。目前没有。

## 活动 1.01:探索数据集中剩余的金融特征

在本活动中，您将按照与我们检查`PAY_1`、`PAY_2`、`PAY_3`等类似的方式检查其余的财务特征。为了更好地可视化这些数据，我们将使用一个大家应该熟悉的数学函数:对数。您将使用 pandas 的`apply`方法，该方法用于将任何函数应用于整个列或数据帧。完成活动后，您应该有以下一组非零付款的对数变换直方图:

![Figure 1.46: Expected set of histograms
](image/B16925_01_46.jpg)

图 1.46:预期的直方图集合

执行以下步骤来完成活动:

在开始之前，按如下方式设置您的环境并加载到已清理的数据集中:

```py
import pandas as pd
import matplotlib.pyplot as plt #import plotting package
#render plotting automatically
%matplotlib inline
import matplotlib as mpl #additional plotting functionality
mpl.rcParams['figure.dpi'] = 400 #high resolution figures
mpl.rcParams['font.size'] = 4 #font size for figures
from scipy import stats
import numpy as np
df = pd.read_csv('../../Data/Chapter_1_cleaned_data.csv')
```

1.  为其余财务特性创建特性名称列表。
2.  使用`.describe()`检查账单金额特征的统计汇总。反思你所看到的。有意义吗？
3.  Visualize the bill amount features using a 2 by 3 grid of histogram plots.

    提示:您可以为该可视化使用 20 个箱。

4.  获取`.describe()`付款金额特征汇总。有意义吗？
5.  绘制一个类似于账单金额特征的账单支付特征直方图，但也对带有`xrot`关键字参数的 x 轴标签应用一些旋转，以便它们不重叠。在任何绘图函数中，您都可以包含`xrot=<angle>`关键字参数，以将 x 轴标签旋转给定的角度(以度为单位)。考虑结果。
6.  使用布尔掩码查看有多少支付金额数据正好等于 0。考虑到上一步中的直方图，这有意义吗？
7.  Ignoring the payments of 0 using the mask you created in the previous step, use pandas' `.apply()` and NumPy's `np.log10()` to plot histograms of logarithmic transformations of the non-zero payments. Consider the results.

    提示:您可以使用以下语法使用`.apply()`将任何函数(包括`log10`)应用于数据帧或列的所有元素:`.apply(<function_name>)`。

    注意

    包含 Python 代码的 Jupyter 笔记本和这个活动的相应输出可以在这里找到:[https://packt.link/FQQOB](https://packt.link/FQQOB)。通过[链接](B16925_Solution_ePub.xhtml#_idTextAnchor149)可以找到该活动的详细分步解决方案。

# 总结

在这一介绍性章节中，我们广泛使用 pandas 来加载和探索案例研究数据。我们学习了如何通过结合使用统计摘要和可视化来检查基本的一致性和正确性。我们回答了诸如“唯一的帐户 id 真的是唯一的吗？”，“是否有任何已被赋予填充值的缺失数据？”，以及“给定特性的定义，特性的值有意义吗？”

您可能会注意到，我们几乎花了本章的所有时间来识别和纠正数据集的问题。这通常是数据科学项目中最耗时的阶段。虽然这不一定是工作中最令人兴奋的部分，但它给了你构建令人兴奋的模型和见解所必需的原材料。这些将是本书其余大部分的主题。

掌握软件工具和数学概念可以让你在技术层面上执行数据科学项目。然而，管理您与客户的关系，客户依靠您的服务从他们的数据中获得洞察力，这对成功的项目同样重要。你必须尽可能利用你的业务伙伴对数据的理解。他们可能比你更熟悉它，除非你已经是该领域的主题专家。然而，即使在这种情况下，你的第一步也应该是对你正在使用的数据进行一次彻底的、批判性的审查。

在我们的数据探索中，我们发现了一个可能会破坏我们项目的问题:我们收到的数据内部不一致。大多数月份的付款状态功能都受到数据报告问题的困扰，包括无意义的值，并且不代表最近一个月的数据，也不代表模型未来可用的数据。我们只是通过仔细研究所有的特性才发现了这个问题。虽然这并不总是可能的，尤其是当有很多特性时，但是您应该总是花时间抽查尽可能多的特性。如果你不能检查每一个特性，当特性归入不同的类别时，检查每个类别的一些特性是有用的，比如财务或人口统计特性。

当和你的客户讨论这样的数据问题时，确保你是尊重和专业的。客户可能只是在向您提供数据时忘记了这个问题。或者，他们可能已经知道这件事，但出于某种原因认为它不会影响你的分析。无论如何，你让他们注意到这个问题，并向他们解释为什么使用有缺陷的数据来建立一个模型是一个问题，这是在为他们提供一个必要的服务。尽可能具体，展示你用来发现问题的图表类型。

在下一章中，我们将检查案例研究问题的响应变量，这完成了最初的数据探索。然后我们将开始获得一些机器学习模型的实践经验，并学习如何决定一个模型是否有用。当我们开始使用案例研究数据构建模型时，这些技能将非常重要。