

# 五、决策树和随机森林

概观

在这一章中，我们将把注意力转移到近年来席卷数据科学的另一种类型的机器学习模型:基于树的模型。在本章中，在单独学习了树之后，您将学习由许多树组成的模型(称为随机森林)如何改善与单个树相关的过度拟合。阅读本章后，你将能够为机器学习目的训练决策树，可视化训练好的决策树，以及训练随机森林并可视化结果。

# 简介

在上两章中，我们已经对逻辑回归的工作原理有了透彻的理解。在使用 Python 中的 scikit-learn 包创建逻辑回归模型方面，我们也获得了很多经验。

在本章中，我们将介绍一种功能强大的预测模型，它采用了与逻辑回归模型完全不同的方法:**决策树**。决策树和基于它们的模型是目前通用机器学习应用中最有效的模型。使用树形流程进行决策的概念很简单，因此，决策树模型很容易解释。然而，对决策树的一个常见批评是，它们过度适应训练数据。为了解决这个问题，研究人员开发了**集成方法**，如**随机森林**，它将许多决策树结合在一起工作，做出比任何一棵树都更好的预测。

我们将会看到，决策树和随机森林可以提高案例研究数据预测建模的质量，超过我们迄今为止使用逻辑回归所取得的成果。

# 决策树

**决策树**和基于它们的机器学习模型，特别是**随机森林**和**梯度提升树**，与广义线性模型(GLMs)是根本不同类型的模型，如逻辑回归。广义线性模型植根于历史悠久的经典统计理论。线性回归背后的数学最初是由勒让德和高斯在 19 世纪初开发的。因此，正态分布也被称为高斯分布。

相比之下，虽然使用树过程来做决策的想法相对简单，但决策树作为数学模型的流行是最近才出现的。我们目前在预测建模的上下文中用于公式化决策树的数学过程发表于 20 世纪 80 年代。这一最新发展的原因是，用于生长决策树的方法依赖于计算能力，即快速处理大量数据的能力。如今我们认为这样的能力是理所当然的，但是直到最近在数学史上它们才被广泛使用。

那么，什么是决策树呢？我们可以用一个实例来说明这个基本概念。想象一下，你正在考虑是否要在某一天去户外冒险。你做决定所依据的唯一信息包括天气，特别是，是否有阳光，天气有多暖和。如果天气晴朗，你对凉爽温度的耐受性会增加，如果温度至少为 10℃，你就会到户外去。

然而，如果是阴天，你需要稍微暖和一点的温度，只有当温度达到 15 摄氏度或更高时，你才会出门。您的决策过程可以用下面的树来表示:

![Figure 5.1: A decision tree for deciding whether to go outside given the weather
](image/B16392_05_01.jpg)

图 5.1:给定天气情况下决定是否外出的决策树

正如你所看到的，决策树有一个直观的结构，并模仿人类做出逻辑决策的方式。因此，它们是一种高度可解释的数学模型，这对于不同的受众来说是一种特别理想的属性。例如，数据科学项目的客户可能对模型如何工作的清晰理解特别感兴趣。决策树是满足这一需求的好方法，只要它们的性能足够好。

## 决策树的术语及其与机器学习的联系

查看*图 5.1* 中的树，我们可以开始熟悉决策树的一些术语。因为有两级决策，基于第一级的云条件和第二级的温度，我们说这个决策树的**深度**为 2。这里，在第二层的两个**节点**都是基于温度的决策，但是在一个层内决策的种类可以不同；例如，如果不是晴天，我们可以根据是否下雨来做决定。

在机器学习的背景下，用于在节点做出决策的量(换句话说，用于**分割**节点)是特征。*图 5.1* 中的示例中的特征是一个关于天气是否晴朗的二元分类特征，以及一个关于温度的连续特征。虽然我们只说明了每个特性在树的给定分支中使用一次，但是相同的特性可以在一个分支中使用多次。例如，我们可能会选择在气温至少为 10 摄氏度的晴天外出，但如果气温超过 40 摄氏度，我们就不会选择外出——那太热了！在这种情况下，*图 5.1* 的节点 4 将在“温度是否高于 40 摄氏度”的条件下被分割其中，如果答案为“是”，则结果为“留在室内”，如果答案为“否”，则结果为“外出”，这意味着温度介于 10°C 和 40°C 之间。因此，决策树能够捕捉特征的非线性影响，这与线性关系相反，线性关系可能假设天气越热，我们就越有可能外出，无论天气有多热。

考虑树的典型表示方式，例如在*图 5.1* 中。分支基于可以将节点分成两个以上节点的二元决策向下生长。这些二元决策可以被认为是“如果，那么”规则。换句话说，如果满足某个条件，就做这个，否则，就做别的。在我们的示例树中做出的决策类似于机器学习中响应变量的概念。如果我们为信用违约的案例研究问题制作一个决策树，决策将会是二元响应值的预测，即“该账户违约”或“该账户不违约”。回答二元是/否类型问题的树是**分类树**。然而，决策树是非常通用的，也可以用于多类分类和回归。

树底部的终端节点被称为**叶**，或叶节点。在我们的例子中，叶子是决定是出去还是呆在家里的最终决定。我们的树上有四片叶子，尽管你可以想象，如果树的深度只有一，我们只根据云的情况来做决定，那么就会有两片叶子；在*图 5.1* 中的节点 2 和 3 将是分别以“外出”和“留在”作为决策的叶节点。

在我们的例子中，最后一级之前的每一级的每个节点都被分割。这不是绝对必要的，因为你可以在任何一个阳光明媚的日子出门，不管气温如何。在这种情况下，节点 2 将不会被分割，因此树的这一分支将以“是”决定在第一层结束。然而，你在阴天的决定可能会涉及温度，这意味着这个分支可以延伸到更深的层次。在最后一层之前的每个节点都被分割的情况下，考虑叶子的数量随着层的数量增长有多快。

例如，如果我们将*图 5.1* 中的决策树向下扩展到一个额外的级别，可能带有风速特征，以考虑四种云条件和温度组合的风寒因素，会发生什么情况。现在是叶子的四个节点中的每一个，在*图 5.1* 中编号为 4 到 7 的节点，将根据每种情况下的风速再分成两个叶子节点。然后，将会有 *4 × 2 = 8* 个叶节点。一般来说，应该清楚的是，在具有 n 层的树中，在最后一层之前的每个节点都被分裂的情况下，将会有 *2n* 个叶节点。记住这一点很重要，因为**最大深度**是可以在 scikit-learn 中为决策树分类器设置的超参数之一。我们现在将在下面的练习中探索这一点。

## 练习 5.01:sci kit 中的决策树-学习

在本练习中，我们将使用案例研究数据来生成一个决策树，其中我们指定了最大深度。我们还将使用一些方便的功能来可视化决策树，以`graphviz`包的形式。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本可以在[https://packt.link/IUt7d](https://packt.link/IUt7d)找到。在开始练习之前，请确保您已经遵循了*前言*中关于设置环境和导入必要库的说明。

1.  加载我们一直在使用的几个包，以及另外一个包`graphviz`，这样我们就可以可视化决策树:

    ```
    import numpy as np #numerical computation
    import pandas as pd #data wrangling
    import matplotlib.pyplot as plt #plotting package
    #Next line helps with rendering plots
    %matplotlib inline
    import matplotlib as mpl #add'l plotting functionality
    mpl.rcParams['figure.dpi'] = 400 #high res figures
    import graphviz #to visualize decision trees
    ```

2.  Load the cleaned case study data:

    ```
    df = pd.read_csv('../Data/Chapter_1_cleaned_data.csv')
    ```

    注意

    根据您保存数据的位置，已清理数据的位置可能会有所不同。

3.  获取数据帧的列名列表:

    ```
    features_response = df.columns.tolist()
    ```

4.  列出要删除的不是特性或响应变量的列:

    ```
    items_to_remove = ['ID', 'SEX', 'PAY_2', 'PAY_3',\
                       'PAY_4', 'PAY_5', 'PAY_6',\
                       'EDUCATION_CAT', 'graduate school',\
                       'high school', 'none',\
                       'others', 'university']
    ```

5.  Use a list comprehension to remove these column names from our list of features and the response variable:

    ```
    features_response = [item for item in features_response if item not in items_to_remove]
    features_response
    ```

    这将输出特性列表和响应变量:

    ```
    ['LIMIT_BAL',
     'EDUCATION',
     'MARRIAGE',
     'AGE',
     'PAY_1',
     'BILL_AMT1',
     'BILL_AMT2',
     'BILL_AMT3',
     'BILL_AMT4',
     'BILL_AMT5',
     'BILL_AMT6',
     'PAY_AMT1',
     'PAY_AMT2',
     'PAY_AMT3',
     'PAY_AMT4',
     'PAY_AMT5',
     'PAY_AMT6',
     'default payment next month']
    ```

    现在特性列表已经准备好了。接下来，我们将从 scikit-learn 进行一些导入。我们想做一个训练/测试拆分，我们已经很熟悉了。我们还想导入决策树功能。

6.  Run this code to make imports from scikit-learn:

    ```
    from sklearn.model_selection import train_test_split
    from sklearn import tree
    ```

    scikit-learn 的`tree`库包含决策树相关的类。

7.  Split the data into training and testing sets using the same random seed that we have used throughout the book:

    ```
    X_train, X_test, y_train, y_test = \
    train_test_split(df[features_response[:-1]].values,
                     df['default payment next month'].values,
                     test_size=0.2, random_state=24)
    ```

    这里，我们使用列表中除最后一个元素之外的所有元素来获取特性的名称，但不使用响应变量:`features_response[:-1]`。我们用它从 DataFrame 中选择列，然后用`.values`方法检索它们的值。我们对响应变量也做了类似的事情，但是直接指定列名。在进行训练/测试分割时，我们使用了与之前工作中相同的随机种子，以及相同的分割大小。这样，我们可以直接将本章将要做的工作与以前的结果进行比较。此外，我们继续从模型开发过程中保留相同的“看不见的测试集”。

    现在我们准备实例化决策树类。

8.  Instantiate the decision tree class by setting the `max_depth` parameter to `2`:

    ```
    dt = tree.DecisionTreeClassifier(max_depth=2)
    ```

    我们使用了`DecisionTreeClassifier`类，因为我们有一个分类问题。由于我们指定了`max_depth=2`，当我们使用案例研究数据来增长决策树时，树将增长到最多`2`的深度。现在让我们来训练这个模型。

9.  Use this code to fit the decision tree model and grow the tree:

    ```
    dt.fit(X_train, y_train)
    ```

    这应该会显示以下输出:

    ```
    DecisionTreeClassifier(max_depth=2)
    ```

    既然我们已经拟合了这个决策树模型，我们可以使用`graphviz`包来显示树的图形表示。

10.  Export the trained model in a format that can be read by the `graphviz` package using this code:

    ```
    dot_data = tree.export_graphviz(dt,
                                    out_file=None,
                                    filled=True,
                                    rounded=True,
                                    feature_names=\
                                    features_response[:-1],
                                    proportion=True,
                                    class_names=[
                                    'Not defaulted', 'Defaulted'])
    ```

    这里，我们为`.export_graphviz`方法提供了许多选项。首先，我们需要说明我们想要绘制哪个训练好的模型，也就是`dt`。接下来，我们说我们不想要一个输出文件:`out_file=None`。相反，我们提供了`dot_data`变量来保存该方法的输出。其余选项设置如下:

    `filled=True`:每个节点都将被填充一种颜色。

    `rounded=True`:节点将以圆形边缘出现，而不是矩形。

    `feature_names=features_response[:-1]`:我们将使用列表中的特性名称，而不是通用名称，如`X[0]`。

    `proportion=True`:显示每个节点中训练样本所占的比例(这个我们后面会详细讨论)。

    `class_names=['Not defaulted', 'Defaulted']`:将为每个节点显示预测类的名称。

    这个方法的输出是什么？

    如果你检查`dot_data`的内容，你会发现它是一个很长的文本字符串。`graphviz`包可以解释这个文本字符串来创建一个可视化。

11.  Use the `.Source` method of the `graphviz` package to create an image from `dot_data` and display it:

    ```
    graph = graphviz.Source(dot_data) 
    graph
    ```

    输出应该如下所示:

    ![Figure 5.2: A decision tree plot from graphviz
    ](image/B16392_05_02.jpg)

    图 5.2:来自 graphviz 的决策树图

    *图 5.2* 中决策树的图形表示应该直接呈现在您的 Jupyter 笔记本中。

    注意

    或者，您可以通过提供一个指向`out_file`关键字参数的文件路径，将`.export_graphviz`的输出保存到磁盘。要将这个输出文件转换成一个图像文件，例如，一个可以在演示文稿中使用的`.png`文件，您可以在命令行运行这个代码，适当地替换文件名:`$ dot -Tpng <exported_file_name> -o <image_file_name_you_want>.png`。

    关于与`.export_graphviz`相关的选项的更多细节，您应该参考 scikit-learn 文档([https://scikit-learn . org/stable/modules/generated/sk learn . tree . export _ graphviz . html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html))。

    图 5.2 中的可视化包含了很多关于决策树是如何被训练的，以及如何被用来做预测的信息。我们稍后将更详细地讨论训练过程，但可以说训练决策树的工作方式是从树顶部初始节点中的所有训练样本开始，然后根据其中一个特征中的**阈值**将这些样本分成两组。切割点由第一个节点中的不等式`PAY_1 <= 1.5`表示。

    在此布尔条件下，`PAY_1`特征值小于或等于`1.5`切割点的所有样本将被表示为`True`。如图*图 5.2* 所示，这些样本按照旁边的箭头`True`被分类到树的左侧。

    正如您在图中看到的，每个被拆分的节点在文本的第一行包含拆分标准。下一行与`gini`有关，我们稍后会讨论。

    下面一行包含关于每个节点中样本比例的信息。在顶部节点中，我们从所有样本(`samples = 100.0%`)开始。在第一次分割之后，89.5%的样本被分类到左边的节点中，而剩余的 10.5%进入右边的节点中。该信息直接显示在可视化中，并反映了如何使用训练数据来创建树。让我们通过检查训练数据来确认这一点。

12.  To confirm the proportion of training samples where the `PAY_1` feature is less than or equal to `1.5`, first identify the index of this feature in the list of `features_response[:-1]` feature names:

    ```
    features_response[:-1].index('PAY_1')
    ```

    该代码应该输出以下内容:

    ```
    4
    ```

13.  Now, observe the shape of the training data:

    ```
    X_train.shape
    ```

    这将为您提供以下输出:

    ```
    (21331, 17)
    ```

    为了确认决策树第一次分割后的样本部分，我们需要知道样本的比例，其中`PAY_1`特征满足布尔条件，用于进行该分割。为此，我们可以使用训练数据中的`PAY_1`特征的索引，对应于特征名称列表中的索引，以及训练数据中的样本数，即我们从`.shape`观察到的行数。

14.  Use this code to confirm the proportion of samples after the first split of the decision tree:

    ```
    (X_train[:,4] <= 1.5).sum()/X_train.shape[0]
    ```

    输出应该如下所示:

    ```
    0.8946134733486475
    ```

    通过对与`PAY_1`特征相对应的训练数据列应用逻辑条件，然后对其求和，我们计算出满足该条件的样本数。然后，通过除以样本总数，我们将其转换为一个比例。我们可以看到，我们从训练数据中直接计算出的比例等于在*图 5.2* 中第一次拆分后左侧节点中显示的比例。

    在第一次分割之后，包含在第一层上的两个节点的每一个中的样本被再次分割。随着第一次拆分之后的进一步拆分，越来越小比例的训练数据将被分配给分支后续级别中的任何给定节点，如*图 5.2* 所示。

    现在我们要解释*图 5.2* 中节点的剩余文本行。以`value`开始的行给出了每个节点中包含的样本的响应变量的类分数。例如，在顶部节点中，我们看到`value = [0.777, 0.223]`。这些只是整个训练集的类分数，您可以在下面的步骤中确认。

15.  Calculate the class fraction in the training set with this code:

    ```
    y_train.mean()
    ```

    输出应该如下所示:

    ```
    0.223102526838873
    ```

    这等于顶层节点中跟在`value`后面的数字对的第二个成员；第一个数字就是 1 减去这个数，换句话说，就是负训练样本的分数。在每个后续节点中，显示该节点中包含的样本的类别分数。类分数也是节点着色的方式:负类比例高于正类的节点为橙色，较深的橙色表示比例较高，而正类比例较高的节点具有使用蓝色的类似方案。

    最后，以`class`开始的行表示决策树如何从一个给定的节点进行预测，如果该节点是一个叶节点的话。在给定特征值的情况下，分类决策树通过确定样本将被分类到哪个叶节点来进行预测，然后预测该叶节点中大多数训练样本的类别。这种策略意味着树结构和叶节点中的类别比例是进行预测所需的信息。

    例如，如果我们没有进行拆分，并且我们被迫在只知道总体训练数据的类分数的情况下进行预测，我们将简单地选择多数类。由于大多数人不默认，所以顶层节点的类是`Not defaulted`。但是，更深层次的节点中的类分数不同，导致预测不同。scikit-learn 如何决定树的结构？我们将在下一节讨论培训过程。

**最大深度的重要性**

回想一下，我们在本练习中指定的唯一超参数是`max_depth`，即在模型训练过程中决策树可以增长到的最大深度。原来这是最重要的超参数之一。如果不对深度设置限制，树将一直生长，直到由其他超参数指定的其他限制之一生效。这可能导致非常深的树，有非常多的节点。例如，考虑深度为 20 的树中有多少个叶节点。这将是 *220 个*叶节点，超过 100 万个！我们甚至有一百万个训练样本来整理所有这些节点吗？在这种情况下，我们没有。使用这种训练数据，显然不可能生成这样的树，在最后一级之前的每个节点都被分裂。但是，如果我们移除`max_depth`限制并重新运行本练习的模型训练，请观察效果:

![Figure 5.3: A portion of the decision tree grown with no maximum depth
](image/B16392_05_03.jpg)

图 5.3:没有最大深度的决策树的一部分

这里，我们展示了决策树的一部分，它是用默认选项生长的，包括`max_depth=None`，这意味着树的深度没有限制。整棵树的宽度大约是这里所示部分的两倍。有如此多的节点，以至于它们只显示为非常小的橙色或蓝色斑块；每个节点的确切解释并不重要，因为我们只是试图说明树可能有多大。应该清楚的是，如果没有超参数来管理树生长过程，可能会产生非常大和复杂的树。

## 训练决策树:节点杂质

此时，您应该已经了解决策树如何使用特征进行预测，以及叶子节点中训练样本的类别分数。现在，我们将学习如何训练决策树。训练过程包括选择要分割节点的特征，以及进行分割的阈值，例如`PAY_1 <= 1.5`用于先前练习的树中的第一次分割。从计算上来说，这意味着每个节点中的样本必须根据每个要素的值进行排序以考虑分割，并且考虑每对连续排序的特征值之间的分割。我们可能会考虑所有的特性，或者只考虑一部分特性，稍后我们会讲到。

**在培训过程中，如何决定拆分？**

假设预测的方法是获取叶节点的多数类，那么我们希望找到主要来自一个类或另一个类的叶节点是有意义的；选择多数类将是更准确的预测，节点越接近于仅包含一个类。在理想的情况下，训练数据可以被分割，使得每个叶节点包含完全正或完全负的样本。然后，我们将有很高的置信度，新样本一旦被分类到这些节点之一，将是阳性或阴性的。实际上，这种情况很少发生。然而，这说明了训练决策树的目标——即进行分裂，以便分裂后的下两个节点具有更高的**纯度**，或者换句话说，更接近于只包含正样本或负样本。

在实践中，决策树实际上是使用纯度的倒数，或**节点杂质**来训练的。这是节点距离具有属于一个类的 100%训练样本有多远的一些度量，并且类似于成本函数的概念，其表示给定的解决方案离理论上的完美解决方案有多远。节点杂质最直观的概念是**误分类率**。采用一种广泛使用的符号(例如，【https://scikit-learn.org/stable/modules/tree.html】)来表示每个节点中属于某一类的样本的比例，我们可以将 *p* mk 定义为第 *m* 个节点中属于第 *k* 类的样本的比例。在一个二元分类问题中，只有两类: *k* = 0 和 *k* = 1。对于给定的节点 *m* ，误分类率只是该节点中不太常见的类别的比例，因为当该节点中的多数类别被用作预测时，所有这些样本都将被误分类。

让我们把错误分类率形象化，作为开始思考决策树如何被训练的一种方式。在程序上，我们考虑可能的类分数， *p* m0，在负类的 0.01 和 0.99 之间， *k* = 0，在节点 *m* 中，使用 NumPy 的`linspace`函数:

```
pm0 = np.linspace(0.01,0.99,99)
pm1 = 1 - pm0
```

然后，这个节点的正类的分数是 1 减去 *p* m0:

![Figure 5.4: Equation for calculating the positive class fraction for node m0
](image/B16392_05_04.jpg)

图 5.4:计算节点 m0 的正类分数的方程

现在，这个节点的错误分类率将是在 *p* m0 和 *p* m1 之间的任何较小的类分数。我们可以通过使用`minimum`函数在 NumPy 中找到两个形状相同的数组之间较小的对应元素:

```
misclassification_rate = np.minimum(pm0, pm1)
```

相对于负类的可能类分数，误分类率看起来像什么？

我们可以使用以下代码来绘制它:

```
mpl.rcParams['figure.dpi'] = 400
plt.plot(pm0, misclassification_rate,
         label='Misclassification rate')
plt.xlabel('$p_{m0}$')
plt.legend()
```

您应该会得到这个图表:

![Figure 5.5: The misclassification rate for a node
](image/B16392_05_05.jpg)

图 5.5:节点的错误分类率

现在，很明显，负类的类分数 *p* m0 越接近 0 或 1，错误分类率就越低。在生成决策树时，如何使用这些信息？考虑可能遵循的流程。

在生成决策树时，每次拆分一个节点，都会创建两个新节点。由于来自这些新节点中的任一个的预测仅仅是多数类，所以一个重要的目标将是降低错误分类率。因此，我们将希望从所有可能的特征中找到一个特征，以及该特征的一个值，在该值处形成一个切割点，以便在对所有类进行平均时，两个新节点中的错误分类率尽可能低。这非常接近用于训练决策树的实际过程。

暂时继续最小化错误分类率的想法，决策树训练算法通过考虑所有特征来进行节点分裂，尽管如果将`max_features`超参数设置为小于特征总数的任何值，该算法可能只考虑随机选择的子集。我们稍后将讨论这样做的可能原因。在任一情况下，该算法考虑每个候选特征的每个可能的阈值，并选择导致最低杂质的阈值，计算为两个可能的新节点上的平均杂质，由每个节点中的样本数加权。节点拆分过程如图*图 5.6* 所示。重复该过程，直到达到树的停止标准，例如`max_depth`:

![Figure 5.6: How to select a feature and threshold in order to split a node
](image/B16392_05_06.jpg)

图 5.6:如何选择特征和阈值来分割节点

虽然错误分类率是杂质的直观测量，但在模型训练过程中，有更好的测量方法可用于发现分裂。scikit-learn 中可用于杂质计算的两个选项是**基尼杂质**和**交叉熵**，您可以使用`criterion`关键字参数指定这两个选项。在这里，我们将从数学上描述这些选项，并显示它们如何与错误分类率进行比较。

使用以下公式计算节点 *m* 的 Gini 杂质:

![Figure 5.7: Equation for calculating Gini impurity
](image/B16392_05_07.jpg)

图 5.7:计算基尼系数的公式

这里，求和是对所有类进行的。在二进制分类问题的情况下，只有两个类，我们可以如下以编程方式编写:

```
gini = (pm0*(1-pm0)) + (pm1*(1-pm1))
```

交叉熵的计算公式如下:

![Figure 5.8: Equation for calculating cross-entropy
](image/B16392_05_08.jpg)

图 5.8:计算交叉熵的等式

使用这段代码，我们可以计算交叉熵:

```
cross_ent = -1*((pm0*np.log(pm0)) + (pm1*np.log(pm1)))
```

为了将基尼不纯度和交叉熵添加到我们的错误分类率图中，并查看它们之间的比较情况，我们只需在绘制错误分类率图后添加以下代码行:

```
mpl.rcParams['figure.dpi'] = 400
plt.plot(pm0, misclassification_rate,\
         label='Misclassification rate')
plt.plot(pm0, gini, label='Gini impurity')
plt.plot(pm0, cross_ent, label='Cross entropy')
plt.xlabel('$p_{m0}$')
plt.legend()
```

最终的图应该如下所示:

![Figure 5.9: The misclassification rate, Gini impurity, and cross-entropy
](image/B16392_05_09.jpg)

图 5.9:错误分类率、基尼系数和交叉熵

注意

如果你正在阅读这本书的印刷版本，你可以通过访问下面的链接下载并浏览本章中一些图片的彩色版本:

[https://packt.link/mQ4Xn](https://packt.link/mQ4Xn)

像错误分类率一样，当类分数等于 0.5 时，基尼不纯度和交叉熵都是最高的，它们随着节点变得更纯而降低——换句话说，当它们只包含其中一个类的比例更高时。然而，基尼系数在某些类别分数区域比误分类率更陡，这使其能够更有效地找到最佳划分。交叉熵看起来更加陡峭。那么，哪个更适合你的工作呢？这是一种在所有数据集上都没有具体答案的问题。在超参数的交叉验证搜索中，您应该考虑这两种杂质指标，以确定合适的指标。注意，在 scikit-learn 中，Gini 杂质可以通过使用`'gini'`字符串的`criterion`参数来指定，而交叉熵只是被称为`'entropy'`。

## 用于第一次分割的特征:与单变量特征选择和相互作用的联系

基于*图 5.2* 中所示的小树，我们可以开始了解各种特征对决策树模型的重要性。注意`PAY_1`是为第一次分割选择的特征。这意味着就减少包含所有训练样本的节点上的节点杂质而言，这是最好的特征。回想一下我们在*第 3 章*、*逻辑回归和特征探索*中的单变量特征选择经验，其中`PAY_1`是 f 检验中的首选特征。因此，根据我们之前的分析，这个特性在决策树的第一次分裂中的出现是有意义的。

在树的第二层，在`PAY_1`上有另一个分割，以及在`BILL_AMT_1`上的分割。`BILL_AMT_1`在单变量特征选择中未被列为首选特征。然而，可能在`BILL_AMT_1`和`PAY_1`之间存在重要的相互作用，这不能通过单变量方法发现。具体来说，从决策树选择的拆分来看，那些同时具有 2 或更大的`PAY_1`值和大于 568 的`BILL_AMT_1`值的账户特别有违约风险。`PAY_1`和`BILL_AMT_1`的组合效应是一种相互作用，这也可能是我们能够通过在前一章的活动中包含相互作用项来提高逻辑回归性能的原因。

## 训练决策树:一种贪婪算法

不能保证由前面描述的过程训练的决策树将是寻找具有最低杂质的叶节点的最佳可能决策树。这是因为用于训练决策树的算法是所谓的贪婪算法。在这种情况下，这意味着在分割节点的每个机会，算法都在寻找该时间点的最佳可能分割，而不考虑后面分割的机会受到影响的事实。

例如，考虑以下假设场景:案例研究的训练数据的最佳初始分割涉及`PAY_1`，正如我们在*图 5.2* 中看到的。但是如果我们改为在`BILL_AMT_1`上拆分，然后在下一个级别的`PAY_1`上进行后续的拆分呢？尽管最初在`BILL_AMT_1`上的初始分割并不是最好的，但如果树以这种方式生长，最终结果可能会更好。如果存在这样的解，该算法无法找到，因为它只考虑每个节点的最佳可能分裂，而不考虑未来可能的分裂。

我们仍然使用贪婪的树生长算法的原因是，以一种能够找到真正最优的树的方式考虑所有可能的分裂需要相当长的时间。尽管决策树训练过程存在这一缺点，但您可以使用一些方法来减少贪婪算法可能产生的有害影响。可以将决策树类的关键字参数`splitter`设置为`random`,而不是在每个节点搜索最佳分割，以便选择一个随机特征进行分割。然而，默认为`best`，搜索所有特征以获得最佳分割。另一个选项，我们已经讨论过，是使用`max_features`关键字来限制每次分割时搜索的特征数量。最后，你也可以使用决策树的集合，比如随机森林，我们将很快描述它。注意，所有这些选项，除了可能避免贪婪算法的不良影响之外，也是解决决策树经常被批评的过度拟合的选项。

## 训练决策树:不同的停止标准和其他选择

我们已经回顾了使用`max_depth`参数作为树将增长多深的限制。然而，在 scikit-learn 中还有其他几个选项。这些主要与叶节点中存在多少样本有关，或者通过进一步分裂节点可以减少多少杂质。如前所述，您可能会受到数据集大小的限制，即树的深度。而且让树长得更深可能没有意义，尤其是如果分裂过程不再寻找具有高得多的纯度的节点。

我们总结了可以提供给 scikit 中的`DecisionTreeClassifier`类的所有关键字参数——在此了解:

![Figure 5.10: The complete list of options for the decision tree classifier in scikit-learn
](image/B16392_05_10.jpg)

图 5.10:sci kit-learn 中决策树分类器的完整选项列表

## 使用决策树:优势和预测概率

虽然决策树在概念上很简单，但它们有几个实际的优点。

**无需缩放特征**

考虑一下我们为什么需要为逻辑回归扩展特征的原因。一个原因是，对于一些基于梯度下降的求解算法，为了快速找到代价函数的最小值，特征必须在相同的尺度上。另一个是，当我们使用 L1 或 L2 正则化来惩罚系数时，所有的特征必须在相同的尺度上，以便它们被同等地惩罚。对于决策树，结点分割算法会单独考虑每个要素，因此，要素是否在相同的比例上并不重要。

**非线性关系和相互作用**

因为决策树中的每个连续分裂是在由先前分裂产生的训练样本的子集上执行的，所以决策树可以描述单个特征的复杂非线性关系，以及特征之间的相互作用。考虑我们之前在第一次分割使用的*特征:与单变量特征选择和交互*部分的讨论。此外，作为合成数据的假设示例，考虑以下数据集进行分类:

![Figure 5.11: An example classification dataset, with the classes shown in red and blue (if reading in black and white, please refer to the GitHub repository for a color version of this figure; the blue dots are on the inside circle)
](image/B16392_05_11.jpg)

图 5.11:一个示例分类数据集，用红色和蓝色显示了类(如果阅读的是黑白的，请参考 GitHub 存储库以获得该图的彩色版本；蓝点在内圆上)

我们从*第三章*、*逻辑回归的细节和特征探究*中得知，逻辑回归有一个线性的决策边界。那么，你认为逻辑回归将如何处理如图 5.11*所示的数据集？你会在哪里画一条线来区分蓝色和红色阶级？应该清楚的是，在没有工程附加特征的情况下，逻辑回归不太可能是该数据的良好分类器。现在考虑一组决策树的“如果，那么”规则，它们可以与图 5.11*的 *x* 和 *y* 轴上表示的特征一起使用。你认为决策树对这些数据有效吗？**

这里，我们在背景中用红色和蓝色绘制了这两个模型的类别成员的预测概率:

![Figure 5.12: Decision tree and logistic regression predictions
](image/B16392_05_12.jpg)

图 5.12:决策树和逻辑回归预测

在*图 5.12* 中，两个模型的预测概率都用了颜色，因此较暗的红色对应于红色类别的较高预测概率，而较暗的蓝色对应于蓝色类别。我们可以看到，决策树可以将红色点的圆圈中间的蓝色类隔离出来。这是因为，通过在节点分裂过程中使用 *x* 和 *y* 坐标的阈值，决策树可以数学地模拟蓝色和红色类别的位置同时取决于 *x* 和 *y* 坐标(相互作用)的事实，并且任一类别的可能性都不是 *x* 或 *y* 的线性递增或递减函数(非线性)。因此，决策树方法能够得到大多数正确的分类。

注意

生成*图 5.11* 和*图 5.12* 的代码可以在参考笔记本中找到:【https://packt.link/9W4WN】[。](https://packt.link/9W4WN)

然而，逻辑回归具有线性判定边界，其将是背景中最亮的蓝色和红色斑块之间的直线。逻辑回归决策边界正好穿过数据的中间，并且不提供有用的分类器。这显示了决策树“开箱即用”的强大功能，无需工程非线性或交互特性。

**预测概率**

我们知道逻辑回归产生概率作为原始输出。然而，决策树基于叶节点的多数类进行预测。那么，像图 5.12 所示的那样，预测的概率来自哪里呢？事实上，决策树确实提供了 scikit 中的`.predict_proba`方法——学习计算预测概率。概率基于叶节点中用于给定预测的多数类的比例。例如，如果叶节点中 75%的样本属于正类，则该节点的预测将是正类，并且预测概率将是 0.75。从决策树预测的概率在统计上不如从广义线性模型预测的概率严格，但它们仍然通常用于通过依赖于改变分类阈值的方法来测量模型的性能，如 ROC 曲线或精度-召回曲线。

注意

由于案例研究的性质，我们在这里将重点放在分类决策树上。然而，决策树也可以用于回归，使其成为一种通用的方法。回归的树生长过程类似于分类，除了回归树不是寻求减少节点杂质，而是寻求最小化其他度量，例如预测的**均方误差** ( **MSE** )或**平均绝对误差** ( **MAE** )，其中节点的预测可以分别是节点中样本的平均值或中值。

## 一种更方便的交叉验证方法

在*第 4 章*、*偏差方差权衡*中，我们通过编写自己的函数来实现交叉验证，使用`KFold`类来生成训练和测试指数，从而对交叉验证有了深入的了解。这有助于彻底理解该过程是如何工作的。然而，scikit-learn 提供了一个方便的类，可以为我们做更多的工作:`GridSearchCV`。`GridSearchCV`可以将我们希望找到最优超参数的模型作为输入，例如决策树或逻辑回归，以及我们希望对其执行交叉验证的超参数“网格”。例如，在逻辑回归中，我们可能想要获得正则化参数 **C** 的不同值的所有折叠的平均交叉验证分数。有了决策树，我们可能想要探索树的不同深度。

您还可以一次搜索多个参数，例如，如果我们想要尝试不同深度的树和不同数量的`max_features`来考虑每个节点分裂。

`GridSearchCV`对我们提供的所有可能的参数组合进行所谓的穷举网格搜索。这意味着，如果我们为两个超参数中的每一个提供五个不同的值，交叉验证过程将运行 5 x 5 = 25 次。如果您正在搜索许多超参数的许多值，交叉验证运行的数量会增长得非常快。在这些情况下，您可能希望使用`RandomizedSearchCV`，它从您提供的网格中所有可能性的总体中搜索超参数组合的随机子集。

`GridSearchCV`可以通过简化交叉验证流程来加快您的工作。您应该熟悉前一章中交叉验证的概念，所以我们直接列出所有可用于`GridSearchCV`的选项。

在下面的练习中，我们将对案例研究数据使用`GridSearchCV`进行实践，以便为决策树分类器搜索超参数。以下是`GridSearchCV`的选项:

![Figure 5.13: The options for GridSearchCV
](image/B16392_05_13.jpg)

图 5.13:GridSearchCV 的选项

在下面的练习中，我们将利用平均值的**标准误差来创建误差线。我们将对测试褶皱中的模型性能指标进行平均，误差线将帮助我们直观地了解褶皱中模型性能的差异。**

平均值的标准误差也称为样本平均值的抽样分布的标准偏差。这是一个很长的名字，但概念并不复杂。这背后的想法是，我们希望为其制作误差线的模型性能指标群体代表了一种对理论上更大的相似样本群体进行采样的可能方式，例如，如果有更多的数据可用，并且我们使用它来进行更多的测试折叠。如果我们可以从更大的总体中重复取样，这些取样事件中的每一个都会产生稍微不同的平均值(样本平均值)。从重复抽样事件中构建这些平均值的分布(样本平均值的抽样分布)将使我们能够知道该抽样分布的方差，这将有助于测量样本平均值的不确定性。事实证明，这种方差(姑且称之为![1](image/B16392_05_equation1.png)，其中![2](image/B16392_05_equation2.png)表示这是样本均值的方差)取决于我们样本中的观察次数(n):它与样本大小成反比，但也与更大的未观察人口![3](image/B16392_05_equation3.png)的方差成正比。如果你正在处理样本均值的标准差，只需取两边的平方根:![4](image/B16392_05_equation4.png)。虽然我们不知道![5](image/B16392_05_equation5.png)的真实值，因为我们没有观察理论总体，但我们可以用我们观察到的测试折叠总体的方差来估计它。

这是统计学中的一个关键概念，叫做**中心极限定理**。

## 练习 5.02:为决策树寻找最佳超参数

在本练习中，我们将使用`GridSearchCV`来调整决策树模型的超参数。您将了解一种使用 scikit-learn 搜索不同超参数的简便方法。执行以下步骤来完成练习:

注意

在开始本练习之前，您需要导入必要的包并加载清理后的数据帧。先决步骤可以参考下面的 Jupyter 笔记本:[https://packt.link/SKuoB](https://packt.link/SKuoB)。

1.  Import the `GridSearchCV` class with this code:

    ```
    from sklearn.model_selection import GridSearchCV
    ```

    下一步是定义我们希望使用交叉验证搜索的超参数。我们将使用`max_depth`参数找到树的最佳最大深度。更深的树有更多的节点分裂，这使用特征将训练集划分成越来越小的子空间。虽然我们事先不知道最佳最大深度，但在考虑用于网格搜索的参数范围时，考虑一些限制情况是有帮助的。

    我们知道一个是最小深度，由只有一个裂口的树组成。至于最大深度，您可以考虑在您的训练数据中有多少样本，或者在这种情况下，在交叉验证的每次拆分中，有多少样本将位于训练文件夹中。我们将像上一章一样进行四重交叉验证。那么，每个训练折叠中将有多少个样本，这与树的深度有什么关系？

2.  Find the number of samples in the training data using this code:

    ```
    X_train.shape
    ```

    输出应该如下所示:

    ```
    (21331, 17)
    ```

    对于 21，331 个训练样本和 4 重交叉验证，在每个训练折叠中将有四分之三的样本，或者大约 16，000 个样本。

    这对于我们希望我们的树长多深意味着什么？

    一个理论上的限制是我们在每片叶子上至少需要一个样本。从我们关于树的深度如何与叶子的数量相关的讨论中，我们知道在最后一层之前的每个节点分裂的树，具有 n 层，具有 *2n* 个叶子节点。因此，具有 L 个叶节点的树的深度大约为 *log2(L)* 。在极限情况下，如果我们将树长得足够深，使得每个叶节点对于给定的折叠都有一个训练样本，那么深度将是*log2(16000)≈14*。因此，在这种情况下，14 是我们可以生长的树的理论极限深度。

    实际上，您可能不希望树长得这么深，因为用于生成决策树的规则将非常特定于训练数据，并且模型可能会过拟合。然而，这给了你一个我们可能希望考虑的`max_depth`超参数值范围的概念。我们将探索从 1 到 12 的深度范围。

3.  Define a dictionary with the key being the hyperparameter name and the value being the list of values of this hyperparameter that we want to search in cross-validation:

    ```
    params = {'max_depth':[1, 2, 4, 6, 8, 10, 12]}
    ```

    在这种情况下，我们只搜索一个超参数。但是，您可以定义一个包含多个键值对的字典来同时搜索多个超参数。

4.  If you are running all the exercises for this chapter in a single notebook, you can reuse the decision tree object, `dt`, from earlier. If not, you need to create a decision tree object for the hyperparameter search:

    ```
    dt = tree.DecisionTreeClassifier()
    ```

    现在我们想实例化`GridSearchCV`类。

5.  Instantiate the `GridSearchCV` class using these options:

    ```
    cv = GridSearchCV(dt, param_grid=params, scoring='roc_auc',
                      n_jobs=None, refit=True, cv=4, verbose=1,
                      pre_dispatch=None, error_score=np.nan,
                      return_train_score=True)
    ```

    请注意，我们使用 ROC AUC 指标(`scoring='roc_auc'`)，我们进行 4 重交叉验证`(cv=4`，我们计算训练分数(`return_train_score=True`)来评估偏差方差权衡。

    一旦定义了交叉验证对象，我们就可以像处理模型对象一样简单地对它使用`.fit`方法。这基本上封装了我们在前一章中编写的交叉验证循环的所有功能。

6.  Perform 4-fold cross-validation to search for the optimal maximum depth using this code:

    ```
    cv.fit(X_train, y_train)
    ```

    输出应该如下所示:

    ![Figure 5.14: The cross-validation fitting output
    ](image/B16392_05_14.jpg)

    图 5.14:交叉验证拟合输出

    我们指定的所有选项都作为输出打印出来。此外，还有一些关于执行了多少次交叉验证拟合的输出信息。我们有 4 个折叠和 7 个超参数，这意味着进行了 4 x 7 = 28 次拟合。还会显示所用的时间。您可以使用`verbose`关键字参数控制从该过程中获得多少输出；更大的数字意味着更多的产出。

    现在是时候检查交叉验证过程的结果了。在适合的`GridSearchCV`对象上可用的方法中有`.cv_results_`。这是一个字典，包含作为键的结果名称和作为值的结果本身。例如，`mean_test_score`键保存七个超参数中每一个的平均测试分数。您可以通过在代码单元中运行`cv.cv_results_`来直接检查这个输出。然而，这并不容易读懂。具有这种结构的字典可以立即用于创建熊猫数据框架，这使得查看结果稍微容易一些。

7.  Run the following code to create and examine a pandas DataFrame of cross-validation results:

    ```
    cv_results_df = pd.DataFrame(cv.cv_results_)
    cv_results_df
    ```

    输出应该如下所示:

    ![Figure 5.15: First several columns of the cross-validation results DataFrame
    ](image/B16392_05_15.jpg)

    图 5.15:交叉验证结果数据框的前几列

    对于网格中超参数的每个组合，数据帧都有一行。因为我们在这里只搜索一个超参数，所以我们搜索的七个值各有一行。您可以看到每行的大量输出，例如四个折叠中的每一个用于训练(拟合)和测试(评分)的时间的平均值和标准偏差(秒)。还显示了搜索的超参数值。在*图 5.16* 中，我们可以看到第一个折叠(指数 0)的测试数据的 ROC AUC 得分。结果数据框架中的其余列是什么？

8.  View the names of the remaining columns in the results DataFrame using this code:

    ```
    cv_results_df.columns
    ```

    输出应该如下所示:

    ```
    Index(['mean_fit_time', 'std_fit_time',\
           'mean_score_time', 'std_score_time',\
           'param_max_depth', 'params',\
           'split0_test_score', 'split1_test_score',\
           'split2_test_score', 'split3_test_score',\
           'mean_test_score', 'std_test_score',\
           'rank_test_score', 'split0_train_score',\
           'split1_train_score', 'split2_train_score',\
           'split3_train_score', 'mean_train_score',\
           'std_train_score'],
          dtype='object')
    ```

    交叉验证结果数据框中的列包括每个折叠的测试分数、它们的平均值和标准偏差以及训练分数的相同信息。

    一般来说，超参数的“最佳”组合是具有最高平均测试分数的组合。这是对使用这些超参数拟合的模型在新数据上评分时表现如何的估计。让我们绘制一个图表，显示平均测试分数如何随`max_depth`超参数变化。我们还将在同一个图上显示平均训练分数，以了解当我们在模型拟合过程中允许更深更复杂的树生长时，偏差和方差是如何变化的。

    我们使用 Matplotlib `errorbar`函数，将 4 倍训练和测试分数的标准误差作为误差条。这给了你一个分数在不同折叠之间有多大变化的指示。

9.  Execute the following code to create an error bar plot of training and testing scores for each value of `max_depth` that was examined in cross-validation:

    ```
    ax = plt.axes()
    ax.errorbar(cv_results_df['param_max_depth'],
                cv_results_df['mean_train_score'],
                yerr=cv_results_df['std_train_score']/np.sqrt(4),
                label='Mean $\pm$ 1 SE training scores')
    ax.errorbar(cv_results_df['param_max_depth'],
                cv_results_df['mean_test_score'],
                yerr=cv_results_df['std_test_score']/np.sqrt(4),
                label='Mean $\pm$ 1 SE testing scores')
    ax.legend()
    plt.xlabel('max_depth')
    plt.ylabel('ROC AUC')
    ```

    该图应如下所示:

    ![Figure 5.16: An error bar plot of training and testing scores across the four folds
    ](image/B16392_05_16.jpg)

图 5.16:跨四个折叠的训练和测试分数的误差条形图

请注意，标准误差的计算方法是标准偏差除以折叠次数的平方根。训练和测试分数的标准误差在每个尝试的`max_depth`值处显示为垂直线；平均分上下的距离为 1 个标准差。无论何时绘制误差棒图，最好确保误差测量的单位与 *y* 轴的单位相同。在这种情况下，它们是相同的，因为标准误差与基础数据具有相同的单位，例如，与方差相反，方差具有平方单位。

误差线表示分数在折叠中的变化程度。如果褶皱之间存在大量的差异，这将表明褶皱之间的数据性质不同，从而影响我们的模型描述数据的能力。这可能是令人担忧的，因为这表明我们可能没有足够的数据来训练一个可以在新数据上可靠执行的模型。然而，在我们的例子中，褶皱之间没有太多的可变性，所以这不是问题。

不同`max_depth`值的训练和测试分数的总体趋势如何？我们可以看到，随着树越长越深，模型越来越适合训练数据。如前所述，如果我们将树长得足够深，使得每个叶节点只有一个训练样本，我们将创建一个特定于训练数据的模型。事实上，它将完美地符合训练数据。我们可以说这样的模型有极高的**方差**。

但是这种在训练集上的表现并不一定会转化到测试集上。在*图 5.16* 中，很明显增加`max_depth`只会在一定程度上增加测试分数，之后更深的树实际上测试性能更低。这是我们如何利用**偏差方差权衡**来创建更好的预测模型的另一个例子——类似于我们如何使用正则化逻辑回归。较浅的树有更多的**偏差**，因为它们也不适合训练数据。但这没什么，因为如果我们接受一些偏差，我们将在测试数据上有更好的表现，这是我们最终关心的指标。

在这种情况下，我们将选择`max_depth` = 6。您还可以通过尝试 2 到 12 之间的每个整数来执行更彻底的搜索，而不是像我们在这里所做的那样按 2s 进行搜索。一般来说，在计算时间允许的范围内，尽可能彻底地搜索参数空间是一个好主意。在这种情况下，会导致同样的结果。

**模型之间的比较**

在这一点上，我们已经在案例研究数据上计算了几个不同机器学习模型的 4 重交叉验证。我们进展如何？目前为止我们的最好成绩是多少？在最后一章中，我们通过逻辑回归得到了 0.718 的平均测试 ROC AUC，通过逻辑回归得到了 0.740 的平均测试 ROC AUC。这里用一个决策树，可以做到 0.745。因此，我们在模型性能方面取得了进展。现在，让我们探索另一种基于决策树的模型，看看我们是否能把性能推得更高。

# 随机森林:决策树的集合

正如我们在前面的练习中看到的，决策树容易过度拟合。这是对它们用法的主要批评之一，尽管事实上它们是高度可解释的。然而，通过限制树木生长的最大深度，我们能够在一定程度上限制这种过度适应。

基于决策树的概念，机器学习研究人员利用多棵树作为更复杂程序的基础，从而产生了一些最强大和最广泛使用的预测模型。在这一章中，我们将关注决策树的随机森林。随机森林是所谓集合模型的例子，因为它们是由其他更简单的模型组合而成的。通过结合许多模型的预测，有可能改进其中任何一个模型的缺陷。这有时被称为将许多弱学习者结合成一个强学习者。

一旦你理解了决策树，随机森林背后的概念就相当简单了。这是因为随机森林只是许多决策树的集合；这种集合中的所有模型都具有相同的数学形式。那么，一个随机森林会包含多少个决策树模型呢？这是超参数之一，`n_estimators`，需要在构建随机森林模型时指定。一般来说，树越多越好。随着树的数量增加，总体集合的方差将减小。这将导致随机森林模型对新数据具有更好的泛化能力，这将反映在测试分数的增加上。然而，将会有一个收益递减点，在该点之后，增加树的数量不会导致模型性能的实质性改善。

那么，随机森林如何减少影响决策树的高方差(过度拟合)问题呢？这个问题的答案在于森林中不同的树有什么不同。这些树有两个主要的不同之处，其中之一我们已经很熟悉了:

*   每次分割时考虑的特征数量
*   用于种植不同树木的训练样本

**每次分割考虑的特征数量**

我们已经熟悉了来自`DecisionTreeClassifier`类的这个选项:`max_features`。在我们之前对这个类的使用中，我们让`max_features`保持其默认值`None`，这意味着在每次分割时考虑所有的特性。通过使用所有特征来拟合训练数据，过拟合是可能的。通过限制每次分割时考虑的特征数量，随机森林中的一些决策树可能会找到更好的分割。这是因为，尽管他们仍在贪婪地寻找最佳分割，但他们是在有限的功能选择范围内进行的。这可能会使树中的某些拆分成为可能，如果在每次拆分时搜索所有特征，这些拆分可能不会被发现。

在 scikit-learn 的`RandomForestClassifier`类中有一个`max_features`选项，就像在`DecisionTreeClassifier`类中一样，选项是相似的。但是，对于随机森林，默认设置是`'auto'`，这意味着该算法将只在每次分割时搜索可能特征数量的平方根的随机选择，例如，从总共 9 个可能特征中随机选择√9 = 3 个特征。由于森林中的每棵树在生长过程中可能会选择不同的随机要素进行分割，因此森林中的树不会相同。

**用于种植不同树木的样品**

随机森林中的树彼此不同的另一个方面是，它们通常用不同的训练样本生长。为此，使用了一种称为 bootstrapping 的统计程序，这意味着从原始数据生成新的合成数据集。通过使用替换从原始数据集中随机选择样本来创建合成数据集。在这里，“替换”意味着如果我们选择了某个样本，我们将继续考虑选择它，也就是说，在我们对它进行采样后，它在原始数据集中被“替换”。合成数据集中的样本数量与原始数据集中的样本数量相同，但一些样本可能因为替换而重复，而另一些样本可能根本不存在。

使用随机采样来创建合成数据集，并分别在其上训练模型的过程被称为 bagging，这是 bootstrapped aggregation 的缩写。事实上，Bagging 可以用于任何机器学习模型，而不仅仅是决策树，scikit-learn 为分类(`BaggingClassifier`)和回归(`BaggingRegressor`)问题提供了这样做的功能。在随机森林的情况下，默认打开装袋，并且`bootstrap`选项设置为`True`。但是，如果您希望使用所有的训练数据来种植森林中的所有树木，您可以将此选项设置为`False`。

现在你应该对什么是随机森林有了很好的理解。如你所见，如果你已经熟悉决策树，理解随机森林并不涉及太多额外的知识。这个事实的一个反映是，scikit-learn 中的`RandomForestClassifier`类可用的超参数与那些用于`DecisionTreeClassifier`类的超参数基本相同。

除了我们之前讨论过的`n_estimators`和`bootstrap`之外，除了决策树之外，只有两个新选项:

*   `oob_score`、a `bool`:该选项控制是否计算每棵树的**出袋** ( **OOB** )得分。这可以被认为是一个测试分数，其中没有被装袋程序选择来生长给定树的样本被用于评估该树的模型性能。在这里，使用`True`来计算 OOB 分数或者`False`(默认)不计算。
*   `warm_start`，a `bool`:默认为`False`，如果设置为`True`，那么重用同一个随机森林模型对象将导致额外的树被添加到已经生成的森林中。
*   `max_samples`、an `int`或`float`:当使用引导程序时，控制使用多少样本来训练森林中的每棵树。默认情况下，使用与原始数据集相同的数字。

**其他种类的集合模型**

正如我们现在所知，随机森林是装袋组合的一个例子。另一种合奏是**增强**合奏。boosting 的一般思想是使用连续的同类型新模型，并根据以前模型的错误对它们进行训练。通过这种方式，连续的模型了解早期模型做得不好的地方，并纠正这些错误。Boosting 在决策树中得到了成功的应用，在 scikit-learn 和另一个流行的 Python 包 XGBoost 中也有提供。我们将在下一章讨论增压。

**堆叠**集成是一种更高级的集成，其中集成中的不同模型(估计器)不需要与打包和提升中的模型类型相同。例如，您可以使用随机森林和逻辑回归构建堆叠集合。使用另一个模型(T2 堆栈器 T3)将集合中不同成员的预测组合起来用于最终预测，该模型将 T4 堆栈器 T5 模型的预测作为特征。

## 随机森林:预测和可解释性

由于随机森林只是决策树的集合，所以必须以某种方式将所有这些树的预测组合起来，以创建随机森林的预测。

在模型训练之后，分类树将获取输入样本并产生预测的类，例如，在我们的案例研究问题中的信用账户是否会违约。将这些树的预测组合成森林的最终预测的一种直观方法是采取多数投票。也就是说，对于给定的样本，所有树的最常见预测成为森林的预测。这是在首次描述随机森林的出版物中采用的方法([https://sci kit-learn . org/stable/modules/ensemble . html # forest](https://scikit-learn.org/stable/modules/ensemble.html#forest))。然而，scikit-learn 使用了一种略有不同的方法:将每一类的预测概率相加，然后选择概率总和最高的一类。这从每个树中获取了比预测类更多的信息。

**随机森林的可解释性**

决策树的一个主要优点是，它可以直观地看到任何单个预测是如何做出的。您可以通过一系列用于进行预测的“if，then”规则来跟踪任何样本的决策路径，并确切地知道它是如何进行预测的。相比之下，假设你有一个由 1000 棵树组成的随机森林。这将意味着有 1000 套这样的规则，比一套规则更难与人类沟通！

也就是说，有各种方法可以用来理解随机森林如何进行预测。解释随机森林如何工作的一个简单方法是观察**特征重要性**，这在 scikit-learn 中可用。随机森林的要素重要性是在森林中种植树木时每个要素的有用程度的度量。这种有用性是通过结合使用每个特征分割的训练样本的分数和由此导致的节点杂质的减少来测量的。

由于要素重要性计算可用于根据要素在随机森林模型中的影响程度对要素进行排序，因此随机森林也可用于要素选择。

## 练习 5.03:拟合随机森林

在本练习中，我们将使用随机森林模型对案例研究中的训练数据进行交叉验证，从而扩展我们在决策树方面的工作。我们将观察增加森林中树木数量的影响，并检查可使用随机森林模型计算的要素重要性。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本可以在[https://packt.link/VSz2T](https://packt.link/VSz2T)找到。本笔记本包含导入必要的库和加载已清理的数据帧的先决步骤。请在开始本练习之前执行这些步骤。

1.  导入随机森林分类器模型类，如下所示:

    ```
    from sklearn.ensemble import RandomForestClassifier
    ```

2.  Instantiate the class using these options:

    ```
    rf = RandomForestClassifier(n_estimators=10,\
                                criterion='gini',\
                                max_depth=3,\
                                min_samples_split=2,\
                                min_samples_leaf=1,\
                                min_weight_fraction_leaf=0.0,\
                                max_features='auto',\
                                max_leaf_nodes=None,\
                                min_impurity_decrease=0.0,\
                                min_impurity_split=None,\
                                bootstrap=True,\
                                oob_score=False,\
                                n_jobs=None,
                                random_state=4,\
                                verbose=0,\
                                warm_start=False,\
                                class_weight=None)
    ```

    在本练习中，我们将主要使用默认选项。但是，请注意，我们将设置`max_depth =` 3。这里，我们只探讨使用不同数量的树的效果，为了缩短运行时间，我们将用相对较浅的树来说明。为了找到最佳的模型性能，我们通常会尝试更多的树和更深的树。

    我们还设置了`random_state`以获得一致的运行结果。

3.  Create a parameter grid for this exercise in order to search the numbers of trees, ranging from 10 to 100 by 10s:

    ```
    rf_params_ex = {'n_estimators':list(range(10,110,10))}
    ```

    我们使用 Python 的`range()`函数为我们想要的整数值创建一个迭代器，然后使用`list()`将它们转换成一个列表。

4.  使用上一步中的参数网格实例化随机森林模型的网格搜索交叉验证对象。否则，您可以使用用于决策树交叉验证的相同选项:

    ```
    cv_rf_ex = GridSearchCV(rf, param_grid=rf_params_ex,
                            scoring='roc_auc', n_jobs=None,
                            refit=True, cv=4, verbose=1,
                            pre_dispatch=None, error_score=np.nan,
                            return_train_score=True)
    ```

5.  Fit the cross-validation object as follows:

    ```
    cv_rf_ex.fit(X_train, y_train)
    ```

    拟合过程应输出以下内容:

    ![Figure 5.17: The output from the cross-validation of the random forest 
    across different numbers of trees
    ](image/B16392_05_17.jpg)

    图 5.17:跨不同数量的树的随机森林的交叉验证的输出

    您可能已经注意到，尽管我们只对 10 个超参数值进行了交叉验证，与我们在之前的练习中为决策树检查的 7 个值相比，这种交叉验证花费的时间明显更长。考虑一下在这种情况下我们种植了多少棵树。对于最后一个超参数`n_estimators = 100`，我们在所有的交叉验证分裂中总共生成了 400 棵树。

    我们刚刚尝试的不同数量的树的模型拟合花了多长时间？通过使用更多的树，我们在交叉验证测试性能方面获得了什么好处？这些都是用图来检验的好东西。首先，我们将把交叉验证结果放到 pandas 数据框架中，就像我们以前做的那样。

6.  Put the cross-validation results into a pandas DataFrame:

    ```
    cv_rf_ex_results_df = pd.DataFrame(cv_rf_ex.cv_results_)
    ```

    您可以在随附的 Jupyter 笔记本中检查整个数据帧。这里，我们直接创建感兴趣的量的图。我们将绘制一个线图，用符号表示每个超参数的平均拟合时间，包含在`mean_fit_time`列中，以及一个测试分数的误差棒图，我们已经为决策树完成了。这两幅图都将与轴 *x* 上的树木数量相对应。

7.  Create two subplots of the mean training time and mean testing scores with standard error:

    ```
    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))
    axs[0].plot(cv_rf_ex_results_df['param_n_estimators'],
                cv_rf_ex_results_df['mean_fit_time'],
                '-o')
    axs[0].set_xlabel('Number of trees')
    axs[0].set_ylabel('Mean fit time (seconds)')
    axs[1].errorbar(cv_rf_ex_results_df['param_n_estimators'],
                    cv_rf_ex_results_df['mean_test_score'],
                    yerr=cv_rf_ex_results_df['std_test_score']/np.sqrt(4))
    axs[1].set_xlabel('Number of trees')
    axs[1].set_ylabel('Mean testing ROC AUC $\pm$ 1 SE ')
    plt.tight_layout()
    ```

    这里，我们使用`plt.subplots`在一个图形中一次创建两个轴，一行两列。然后，我们通过索引从该操作返回的`axs`轴的数组来访问轴对象，以便创建绘图。

    输出应该类似于下图:

    ![Figure 5.18: The mean fitting time and testing scores for different numbers 
    of trees in the forest
    ](image/B16392_05_18.jpg)

    图 5.18:森林中不同数量的树的平均拟合时间和测试分数

    注意

    由于平台的不同或者您设置了不同的随机种子，您的结果可能会有所不同。

    关于这些可视化有几点需要注意。首先，我们可以看到，通过使用随机森林，我们在交叉验证测试折叠上的模型性能比我们以前的任何努力都有所提高。虽然我们还没有尝试调整随机森林超参数来实现我们可以实现的最佳模型性能，但这是一个有希望的结果，表明随机森林将是我们建模工作的一个有价值的补充。

    然而，除了这些更高的模型测试分数，请注意，与我们在决策树中看到的相比，折叠之间也有更多的可变性；这种可变性在褶皱间的模型测试分数中表现为较大的标准误差。虽然这表明使用该模型可能会有更大范围的模型性能，但鼓励您直接在 Jupyter 笔记本的 pandas 数据框中检查褶皱的模型测试分数。您应该看到，即使是单个文件夹的最低分数也仍然高于决策树的平均测试分数，这表明使用随机目录林会更好。

    那么，我们开始用这种可视化探索的其他问题呢？我们很想知道用不同数量的树来拟合随机森林模型需要多长时间，以及使用更多的树会提高模型的性能。图 5.18 左侧的子图显示，随着更多的树被添加到森林中，训练时间有一个相当线性的增加。这大概是可以预料的；我们只是通过添加更多的树来增加训练过程中要完成的计算量。

    但是，就提高模型性能而言，这种额外的计算时间值得吗？图 5.18 右侧的子图显示，在大约 20 棵树之后，添加更多的树是否能可靠地提高测试性能还不清楚。虽然具有 50 棵树的模型具有最高分数，但是添加更多的树实际上降低了测试分数的事实在某种程度上表明 50 棵树的 ROC AUC 的增加可能只是由于随机性，因为理论上添加更多的树应该提高模型性能。基于这种推理，如果我们被限制在`max_depth = 3`，我们可能会选择一个有 20 或 50 棵树的森林，然后继续。但是，我们将在本章末尾的活动中更全面地探索参数空间。

    最后，请注意，我们在这里没有显示训练 ROC AUC 指标。如果您绘制这些图或在结果数据框架中查找它们，您会看到训练分数高于测试分数，这表明发生了一定量的过度拟合。虽然这可能是事实，但这个随机森林模型的交叉验证测试分数确实比我们观察到的任何其他模型都要高。基于这个结果，我们可能会选择随机森林模型。

    为了更深入地了解我们可以使用拟合的交叉验证对象访问什么，让我们看看最佳超参数和特性重要性。

8.  Use this code to see the best hyperparameters from cross-validation:

    ```
    cv_rf_ex.best_params_
    ```

    输出应该是这样的:

    ```
    {'n_estimators': 50}
    ```

    这里，best 仅仅意味着产生最高平均模型测试分数的超参数。

9.  Run this code to create a DataFrame of the feature names and importances, and then show a horizontal bar plot sorted by importance:

    ```
    feat_imp_df = pd.DataFrame({
        'Importance':cv_rf_ex.best_estimator_.feature_importances_
        },
        index=features_response[:-1]) 
    feat_imp_df.sort_values('Importance', ascending=True).plot.barh()
    ```

    情节应该是这样的:

    ![Figure 5.19: Feature importance from a random forest
    ](image/B16392_05_19.jpg)

图 5.19:随机森林的特征重要性

在这段代码中，我们创建了一个具有特性重要性的字典，并使用它和特性名称作为索引来创建数据帧。特征重要性来自于拟合的交叉验证对象的`best_estimator_`方法，因此它指的是具有最高平均测试分数的模型(换句话说，具有 50 棵树的模型)。这是一种访问随机森林模型对象的方法，该对象使用交叉验证网格搜索找到的最佳超参数，根据所有训练数据进行训练。`feature_importances_`是一种可用于拟合随机森林模型的方法。

在访问所有这些属性之后，我们将它们绘制在一个水平条形图上，这是查看特性重要性的一种便捷方式。请注意，随机森林中最重要的前五个特征与第三章、*逻辑回归和特征探索*中通过方差分析 f 检验选择的前五个特征相同，尽管它们的顺序略有不同。这是不同方法之间的很好的证明。

## 棋盘图

在继续学习活动之前，我们将在 Matplotlib 中演示一种可视化技术。当您想要显示三维数据时，绘制带有彩色正方形或其他形状的二维网格会很有用。这里，颜色说明了第三个维度。例如，您可能希望可视化两个超参数网格上的模型测试分数，正如我们将在*活动 5.01* 、*随机森林交叉验证网格搜索*中所做的那样。

该过程的第一步是创建坐标为 *x* 和 *y* 的网格。NumPy `meshgrid`函数可以用来做这件事。该函数采用一维数组 *x* 和 *y* 坐标，并使用所有可能的坐标对创建网格。网格中的点将是棋盘图上每个方块的角。下面是代码如何寻找一个 4 x 4 的彩色补丁网格。因为我们指定了角，所以我们需要一个 5 x 5 的点网格。我们还显示了 x 坐标*和 y 坐标*的数组:

```
xx_example, yy_example = np.meshgrid(range(5), range(5))
print(xx_example)
print(yy_example)
```

输出如下所示:

```
[[0 1 2 3 4]
 [0 1 2 3 4]
 [0 1 2 3 4]
 [0 1 2 3 4]
 [0 1 2 3 4]]
[[0 0 0 0 0]
 [1 1 1 1 1]
 [2 2 2 2 2]
 [3 3 3 3 3]
 [4 4 4 4 4]]
```

要在此网格上绘制的数据网格应具有 4 x 4 的形状。我们制作一个 1 到 16 之间的一维整数数组，并将其重塑为一个二维的 4 x 4 网格:

```
z_example = np.arange(1,17).reshape(4,4)
z_example
```

这将输出以下内容:

```
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12],
       [13, 14, 15, 16]])
```

我们可以用下面的代码在`xx_example, yy_example`网格上绘制`z_example`数据。注意，我们用`pcolormesh`和`jet`色图绘制了这个图，给出了彩虹色标。我们添加一个`colorbar`，需要将`pcolormesh`返回的`pcolor_ex`对象作为参数传递给它，这样色阶的解释就很清楚了:

```
ax = plt.axes()
pcolor_ex = ax.pcolormesh(xx_example, yy_example, z_example,
                          cmap=plt.cm.jet)
plt.colorbar(pcolor_ex, label='Color scale')
ax.set_xlabel('X coordinate')
ax.set_ylabel('Y coordinate') 
```

![Figure 5.20: A pcolormesh plot of consecutive integers
](image/B16392_05_20.jpg)

图 5.20:连续整数的 pcolormesh 图

## 活动 5.01:使用随机森林进行交叉验证网格搜索

在本活动中，您将对案例研究数据上的随机森林模型的森林中的树木数量(`n_estimators`)和树木的最大深度(`max_depth`)进行网格搜索。然后，您将创建一个可视化工具，显示您搜索的超参数网格的平均测试分数。执行以下步骤来完成活动:

1.  为将要搜索的`max_depth`和`n_estimators`超参数创建一个表示网格的字典。包括深度为 3、6、9 和 12 以及 10、50、100 和 200 的树。保留其他超参数的默认值。
2.  使用本章前面的相同选项实例化一个`GridSearchCV`对象，但是使用步骤 1 中创建的超参数字典。设置`verbose=2`查看每次拟合的输出。您可以重用我们一直在使用的同一个随机森林模型对象`rf`，或者创建一个新的。
3.  将`GridSearchCV`对象放在训练数据上。
4.  将网格搜索的结果放入熊猫数据框中。
5.  Create a `pcolormesh` visualization of the mean testing score for each combination of hyperparameters. You should obtain a visualization similar to the following:![Figure 5.21: Results of cross-validation of a random forest 
    over a grid with two hyperparameters
    ](image/B16392_05_21.jpg)

    图 5.21:带有两个超参数的网格上随机森林的交叉验证结果

6.  Conclude which set of hyperparameters to use.

    注意

    包含此活动的 Python 代码的 Jupyter 笔记本可以在[https://packt.link/D0OBc](https://packt.link/D0OBc)找到。通过[链接](B16925_Solution_ePub.xhtml#_idTextAnchor157)可以找到该活动的详细分步解决方案。

# 总结

在这一章中，我们学习了如何使用决策树和由许多决策树组成的被称为随机森林的集合模型。通过交叉验证 ROC AUC 评分判断，使用这些简单构思的模型，我们能够做出比逻辑回归更好的预测。许多现实世界的问题往往都是如此。决策树对许多可能阻止逻辑回归模型获得良好性能的潜在问题具有鲁棒性，例如特征和响应变量之间的非线性关系，以及特征之间复杂交互的存在。

虽然单个决策树容易过度拟合，但随机森林集成方法已被证明可以减少这种高方差问题。随机森林是通过训练很多树建立起来的。通过增加森林中个体树的偏差，通过仅在可用训练集的一部分上训练它们(自举聚合或 bagging)，以及通过仅在每个节点分裂处考虑减少数量的特征，来实现树集合的减少的方差。

现在，我们已经尝试了几种不同的机器学习方法来对案例研究数据进行建模，我们发现有些方法比其他方法效果更好；例如，带有优化超参数的随机森林提供最高的平均交叉验证 ROC AUC 分数 0.776，正如我们在*活动 5* 、*随机森林交叉验证网格搜索*中看到的。

在下一章，我们将了解另一种类型的集成方法，称为梯度推进，它通常与决策树结合使用。对于二进制分类用例，梯度提升已经产生了所有机器学习模型中的一些最佳性能。我们还将学习一种强大的方法来解释和诠释树木梯度提升系综的预测，使用**匀称的附加解释** ( **SHAP** )值。