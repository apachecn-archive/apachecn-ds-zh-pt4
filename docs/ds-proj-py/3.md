

# 三、逻辑回归和特征探索的细节

概观

本章教你如何快速有效地评估特征，以便知道哪些特征对于机器学习模型可能是最重要的。一旦我们了解了这一点，我们将探索逻辑回归的内部工作方式，这样你就可以继续你掌握这一基本技术的旅程。阅读本章后，你将能够绘制许多特征和一个响应变量的相关图，并将逻辑回归解释为线性模型。

# 简介

在前一章中，我们使用 scikit-learn 开发了几个示例机器学习模型，以熟悉它的工作原理。然而，我们使用的特征`EDUCATION`和`LIMIT_BAL`并不是以系统的方式选择的。

在这一章中，我们将开始开发一些技术来评估特性在建模中的有用性。这将使你能够快速浏览所有候选特征，从而知道哪一个是最重要的。对于最有前途的功能，我们将看到如何创建可视化摘要，作为有用的交流工具。

接下来，我们将开始详细检查逻辑回归。我们将了解为什么逻辑回归被认为是线性模型，即使公式涉及一些非线性函数。我们将了解什么是决策边界，并看到作为其线性的一个关键结果，逻辑回归的决策边界可能会使准确分类响应变量变得困难。在这个过程中，通过使用列表理解和编写函数，我们将更加熟悉 Python。

# 检查特征和响应变量之间的关系

为了准确预测响应变量，良好的特征是必要的。我们需要以某种方式与响应变量明确关联的特征。到目前为止，我们已经检查了几个特性和响应变量之间的关系，或者通过计算特性和响应变量的`groupby` / `mean`，或者在模型中使用单个特性并检查性能。然而，我们还没有对所有特征如何与响应变量相关进行系统的探索。我们现在将会这样做，并开始利用我们在探索特性和确保数据质量良好时投入的所有辛勤工作。

快速查看所有特征如何与响应变量相关，以及这些特征如何相互关联的一种流行方法是使用**相关图**。我们将首先为案例研究数据创建一个关联图，然后讨论如何解释它，以及一些数学细节。

为了创建关联图，必要的输入包括我们计划探索的所有特征，以及响应变量。因为我们将使用数据帧中的大部分列名，所以在 Python 中获得适当列表的一个快速方法是从所有列名开始，去掉那些我们不需要的列名。首先，我们为这一章创建一个新的笔记本，加载包和从*第 1 章*、*数据探索和清理*中清理的数据，代码如下:

```py
import numpy as np #numerical computation
import pandas as pd #data wrangling
import matplotlib.pyplot as plt #plotting package
#Next line helps with rendering plots
%matplotlib inline
import matplotlib as mpl #add'l plotting functionality
import seaborn as sns #a fancy plotting package
mpl.rcParams['figure.dpi'] = 400 #high res figures
df = pd.read_csv('../../Data/Chapter_1_cleaned_data.csv')
```

注意

根据您在*第 1 章*、*数据探索和清理*中保存数据文件的位置，到您清理的数据文件的路径可能会有所不同。本节介绍的代码和输出也在参考笔记本中出现:【https://packt.link/pMvWa】T4。

请注意，这个笔记本与前一章的笔记本非常相似，除了我们还导入了 **Seaborn** 包，它有许多基于 **Matplotlib** 的方便的绘图功能。现在，让我们列出数据帧的所有列，并查看前五列和后五列:

![Figure 3.1: Get a list of column names
](image/B16925_03_01.jpg)

图 3.1:获取列名列表

回想一下，出于伦理考虑，我们不使用`gender`变量，我们知道`PAY_2`、`PAY_3`、…、`PAY_6`是不正确的，应该忽略。此外，我们不打算检查我们从`EDUCATION`变量创建的一次性编码，因为来自那些列的信息已经包含在原始特性中，至少以某种形式。我们将直接使用`EDUCATION`功能。最后，使用`ID`作为一个特性是没有意义的，因为这只是一个唯一的帐户标识符，与响应变量无关。让我们再列一个既不是特性也不是响应的列名列表。我们想从我们的分析中排除这些:

```py
items_to_remove = ['ID', 'SEX',\
                   'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\
                   'EDUCATION_CAT',\
                   'graduate school', 'high school', 'none',\
                   'others', 'university']
```

为了得到一个只包含我们将使用的特性和响应的列名列表，我们希望从包含在`features_response`中的当前列表中删除`items_to_remove`中的名称。在 Python 中有几种方法可以做到这一点。我们将利用这个机会学习在 Python 中构建列表的一种特殊方式，称为**列表理解**。当人们谈论某些结构是**Python**或者是 Python 语言的惯用结构时，列表理解经常是被提及的事情之一。

什么是列表理解？从概念上讲，它基本上与`for`循环相同。然而，列表理解使得列表的创建能够在一行中完成，列表可能在实际的`for`循环中跨几行。由于 Python 内部的优化，它们也比`for`循环稍快。虽然这可能不会节省我们很多时间，但这是一个熟悉他们的好机会。下面是一个理解列表的例子:

![Figure 3.2: Example of a list comprehension
](image/B16925_03_02.jpg)

图 3.2:列表理解的例子

这就是全部了。

我们也可以使用附加从句来使列表的理解更加灵活。例如，我们可以用它们给`features_response`变量重新赋值，用一个列表包含所有不在我们想要删除的字符串列表中的内容:

![Figure 3.3: Using a list comprehension to prune down the column names
](image/B16925_03_03.jpg)

图 3.3:使用列表理解来删减列名

在列表理解中使用`if`和`not in`是显而易见的。列表理解等结构中的易读性是 Python 流行的原因之一。

注意

Python 文档([https://docs.python.org/3/tutorial/datastructures.html](https://docs.python.org/3/tutorial/datastructures.html))将列表理解定义如下:

列表理解由括号组成，括号包含一个表达式，后跟一个 for 子句，然后是零个或多个 for 或 if 子句

因此，列表理解可以使你用更少的代码做事情，以一种通常非常可读和可理解的方式。

## 皮尔逊相关

现在我们准备创建我们的相关图。相关图的底层是一个相关矩阵，我们必须首先计算它。熊猫使这变得容易。我们只需要使用刚刚创建的列表选择我们的特性和响应值列，并在这些列上调用`.corr()`方法。当我们计算时，请注意我们在熊猫中可用的相关类型是**线性相关**，也称为**皮尔逊相关**。皮尔逊相关用于衡量两个变量之间线性关系的强度和方向(即正或负):

![Figure 3.4: First five rows and columns of the correlation matrix
](image/B16925_03_04.jpg)

图 3.4:关联矩阵的前五行和列

创建相关矩阵后，请注意行名和列名是相同的。然后，对于所有特征对之间的每个可能的比较，以及所有特征和响应，我们在前五行和列中还看不到，有一个数字。这个数字被称为这两列之间的**相关性**。所有的相关性都在-1 和 1 之间；一个列与其自身(相关矩阵的对角线)的相关性为 1，并且存在重复:每个比较出现两次，因为原始数据帧中的每个列名在相关矩阵中同时显示为行和列。在说更多关于相关性的内容之前，我们将使用 Seaborn 来绘制一个很好的图表。下面是绘图代码，后面是输出(如果你是黑白阅读，请查看 GitHub 上的笔记本以获得彩色图；这里很有必要——【https://packt.link/pMvWa[):](https://packt.link/pMvWa)

```py
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values,
            center=0)
```

您应该会看到以下输出:

![Figure 3.5: Heatmap of the correlation plot in Seaborn
](image/B16925_03_05.jpg)

图 3.5:Seaborn 相关图的热图

根据*图 3.5* 右侧的色标，Seaborn `heatmap`特征使相关矩阵明显可视化，这被称为**色条**。请注意，在调用`sns.heatmap`时，除了矩阵之外，我们还为 *x* 和 *y* 轴提供了**记号标签**，它们是特性和响应名称，并指示彩条的中心应该是 0，这样正相关和负相关就可以分别区分为红色和蓝色。

注意

如果你正在阅读这本书的印刷版本，你可以通过访问以下链接下载并浏览本章中一些图片的彩色版本:[https://packt.link/veMmT](https://packt.link/veMmT)。

这个情节告诉我们什么？在高层次上，如果两个特征，或者一个特征和响应，彼此**高度相关**，你可以说它们之间有很强的关联。与响应高度相关的特征将是用于预测的良好特征。这种高相关性可能是正的，也可能是负的；我们将很快解释其中的区别。

为了查看与响应变量的相关性，我们沿着最下面一行，或者相当于最后一列来看。这里我们看到`PAY_1`特征可能是与响应变量最相关的特征。我们还可以看到许多特性彼此高度相关，尤其是`BILL_AMT`特性。我们将在下一章讨论相互关联的特性的重要性；对于某些模型(如逻辑回归模型)来说，了解这一点非常重要，因为这些模型会对特征之间的相关性做出假设。目前，我们认为`PAY_1`可能是我们模型中最好的、最具预测性的特性之一。另一个看起来可能很重要的特征是`LIMIT_BAL`，负相关。取决于你的视觉有多敏锐，在*图 3.5* 的最后一行中，只有这两种颜色看起来是除了黑色以外的任何颜色(意味着 0 相关)。

## 线性相关的数学

数学上讲，什么是线性相关？如果你学过基本统计学，你可能已经熟悉线性相关。线性相关的工作原理与线性回归非常相似。对于两列 *X* 和 *Y* ，线性相关性 *ρ* (小写希腊字母“rho”)定义如下:

![Figure 3.6: Linear correlation equation
](image/B16925_03_06.jpg)

图 3.6:线性相关方程

这个等式描述了 *X* 的元素和它们的平均值x 之差的**期望值** ( *E* ，你可以认为是平均值)乘以 *Y* 的相应元素和它们的平均值y 之差。 *E* 的平均值是对 *X* ， *Y* 值的平均值。你可以想象，如果当 *X* 与其平均值相比相对较大时，*X、 *Y* 也趋向于类似的大，那么分子中的乘法项都趋向于正，导致正乘积和**正相关**在期望值 *E* 之后被取值。类似地，如果当 *X* 较小时 *Y* 趋于较小，则分子中的两项将为负，并再次导致正相关。相反，如果随着 *X* 的增加 *Y* 趋向于减少，那么它们就会有**负相关**。分母( *X* 和 *Y* 的**标准偏差**的乘积)用于将线性相关性标准化到`[-1, 1]`的刻度。因为皮尔逊相关是针对数据的均值和标准差进行调整的，所以数据的实际值没有 *X* 和 *Y* 之间的关系重要。*较强的线性相关性更接近于 1 或-1。如果 X 和 Y 之间没有线性关系，那么相关性将接近于 0。**

值得注意的是，虽然数据科学从业者经常在这种情况下使用皮尔逊相关，但它并不完全适合二元响应变量，正如我们在案例研究问题中所做的那样。从技术上讲，在其他限制中，皮尔逊相关性仅对**连续数据**有效，例如我们在*第 2 章*、*sci kit 简介-学习和模型评估*中用于线性回归练习的数据。然而，皮尔逊相关仍然可以实现快速给出特征的潜在有用性的目的。在熊猫等软件库中也可以方便地获得。

在一般的数据科学中，你会发现某些广泛使用的技术可能会应用于违反其正式统计假设的数据。了解分析方法背后的正式假设是很重要的。事实上，这些假设的知识可能会在数据科学工作的面试中得到检验。然而，在实践中，只要一种技术能够帮助我们理解问题并找到有效的解决方案，它仍然是一种有价值的工具。

也就是说，线性相关性不能有效衡量所有特征的预测能力。特别是，它只选取线性关系。暂时将我们的焦点转移到一个假设的回归问题上，看看下面的例子，并讨论你期望的线性相关性是什么。请注意， *x* 和 *y* 轴上的数据值没有标注；这是因为数据的位置(平均值)和标准偏差(标度)不会影响皮尔逊相关性，只会影响变量之间的关系，这可以通过将它们绘制在一起来辨别:

![Figure 3.7: Scatter plots of the relationship between example variables
](image/B16925_03_07.jpg)

图 3.7:示例变量之间关系的散点图

对于*示例 A* 和 *B* ，根据之前给出的公式，这些数据集的实际皮尔逊相关性分别为 0.96 和-0.97。从图上看，很明显，接近 1 或-1 的相关性提供了对这些变量之间关系的有用见解。对于*例 C* ，相关性为 0.06。更接近 0 的相关性看起来像是缺乏关联的有效指示:Y 的值似乎与 X 的值没有太大关系。然而，在*示例 D* 中，变量之间显然存在某种关系。但是线性相关性实际上低于前面的例子，为 0.02。这里， *X* 和 *Y* 倾向于在更小的尺度上“一起移动”，但是当计算线性相关性时，这是所有样本的平均值。

注意

可以在这里找到生成本节和上一节中呈现的图的代码:[https://packt.link/XrUJU](https://packt.link/XrUJU)。

最终，您可能选择任何汇总统计数据(如相关性)都只是一个汇总。它可以隐藏重要的细节。因此，直观地检查特征和响应之间的关系通常是个好主意。这可能会占用页面上的大量空间，因此我们不会在这里对案例研究中的所有功能进行演示。然而，熊猫和 Seaborn 都提供了创建所谓的**散点图矩阵**的功能。散点图矩阵类似于相关图，但它实际上将所有数据显示为所有特征和响应变量的散点图网格。这允许您以简洁的格式直接检查数据。由于这可能会包含大量数据和绘图，因此您可能需要对数据进行缩减采样，并查看数量减少的要素，以便有效运行函数。

## F 检验

虽然皮尔逊相关理论上对连续响应变量有效，但案例研究数据的二元响应变量可被视为分类数据，只有两个类别:0 和 1。在我们可以运行的不同类型的测试中，为了查看特征是否与分类响应相关联，有一种是 **ANOVA F-test** ，在 scikit-learn 中以`f_classif`的名称提供。 **ANOVA** 代表**方差分析**。ANOVA F-test 可与**回归 F-test** 进行对比，后者与 Pearson correlation 非常相似，在 scikit-learn 中也可作为`f_regression`使用。

在下面的练习中，我们将使用案例研究数据的候选特征进行方差分析 f 检验。您将看到输出由 F 统计数据和 **p 值**组成。我们如何解释这个输出？我们将重点关注 p 值，原因将在练习中变得清楚。p 值是一个适用于各种统计测量的有用概念。例如，虽然我们没有检查它们，但是为前面的相关矩阵计算的每个皮尔逊相关都有相应的 p 值。对应于线性回归系数、逻辑回归系数和其他测量值的 p 值也有类似的概念。

在 f 检验的上下文中，p 值回答了以下问题:“对于正类中的样本，该特征的平均值与负类中的样本的平均值相同的可能性有多大？”如果数据表明一个特征在正类和负类之间具有非常不同的平均值，则情况如下:

*   这些平均值不太可能相同(低 p 值)。
*   这在我们的模型中可能是一个很好的特性，因为它将帮助我们区分积极和消极的类别。

在下面的练习中，请记住这些要点。

## 练习 3.01: F 检验和单变量特征选择

在本练习中，我们将使用 f 检验来检查特征和响应变量之间的关系。我们将使用这种方法进行所谓的**单变量特征选择**:针对响应变量逐一测试特征的实践，以查看哪些特征具有预测能力。执行以下步骤来完成练习:

注意

这个练习的 Jupyter 笔记本可以在这里找到:[https://packt.link/ZDPYf](https://packt.link/ZDPYf)。此笔记本还包含加载已清理数据和导入必要库的先决步骤。这些步骤应该在本练习的步骤 1 之前执行。

1.  Our first step in doing the ANOVA F-test is to separate out the features and response as NumPy arrays, taking advantage of the list we created, as well as integer indexing in pandas:

    ```py
    X = df[features_response].iloc[:,:-1].values
    y = df[features_response].iloc[:,-1].values
    print(X.shape, y.shape)
    ```

    输出应显示特征和响应的形状:

    ```py
    (26664, 17) (26664, )
    ```

    有 17 个特征，并且特征和响应数组都具有与预期相同的样本数。

2.  Import the `f_classif` function and feed in the features and response:

    ```py
    from sklearn.feature_selection import f_classif
    [f_stat, f_p_value] = f_classif(X, y)
    ```

    `f_classif`有两个输出: **F 统计量**和 **p 值**，用于每个特征与响应变量的比较。让我们创建一个包含特性名称和这些输出的新数据帧，以方便我们的检查。指定新数据帧的一种方法是使用一个**字典**，带有 **key:value** 对列名和每列中包含的数据。我们显示了按 p 值排序(升序)的数据帧。

3.  Use this code to create a DataFrame of feature names, F-statistics, and p-values, and show it sorted on p-value:

    ```py
    f_test_df = pd.DataFrame({'Feature':features_response[:-1],
                              'F statistic':f_stat,
                              'p value':f_p_value})
    f_test_df.sort_values('p value')
    ```

    输出应该如下所示:

    ![Figure 3.8: Results of the ANOVA F-test
    ](image/B16925_03_08.jpg)

    图 3.8:方差分析 f 检验的结果

    请注意，p 值每降低一次，F 统计量就会增加一次，因此这些列中的信息在等级特征方面是相同的。

    我们从 F 统计和 p 值的数据框架中得出的结论与我们在相关图中观察到的类似:`PAY_1`和`LIMIT_BAL`似乎是最有用的特征。它们具有最小的 p 值，表明这些特征的平均值在正类和负类之间**显著不同**，这些特征将有助于预测样本属于哪个类。

    在 scikit-learn 中，f 检验等方法帮助我们执行**单变量特征选择**。如果您有大量的功能，其中许多可能完全没有用，并且希望快速获得哪些功能可能最有用的简短列表，这可能会很有帮助。例如，如果我们只想检索具有最高 F 统计值的 20%的要素，我们可以使用`SelectPercentile`类轻松实现。还要注意，有一个类似的类用于选择顶部的“ *k* ”特性(其中 *k* 是您指定的任意数字)，称为`SelectKBest`。这里我们演示如何选择前 20%。

4.  要根据 f 检验选择前 20%的特性，首先导入`SelectPercentile`类:

    ```py
    from sklearn.feature_selection import SelectPercentile
    ```

5.  实例化该类的一个对象，表明我们希望使用与我们在本练习中已经考虑过的相同的特性选择标准 ANOVA F-test，并且我们希望选择前 20%的特性:

    ```py
    selector = SelectPercentile(f_classif, percentile=20)
    ```

6.  Use the `.fit` method to fit the object on our features and response data, similar to how a model would be fit:

    ```py
    selector.fit(X, y)
    ```

    输出应该如下所示:

    ```py
    SelectPercentile(percentile=20)
    ```

    有几种方法可以直接访问所选的特性，您可以在 scikit-learn 文档中了解到这些方法(即`.transform`方法，或在与`.fit_transform`配合相同的步骤中)。然而，这些方法将返回 NumPy 数组，它不告诉您所选特性的名称，只告诉您值。为此，您可以使用特性选择器对象的`.get_support`方法，这将为您提供所选特性数组的列索引。

7.  Capture the indices of the selected features in an array named `best_feature_ix`:

    ```py
    best_feature_ix = selector.get_support()
    best_feature_ix
    ```

    输出应该如下所示，指示一个逻辑索引，该索引可以与一个特性名称数组以及值一起使用，假设它们与提供给`SelectPercentile`的特性数组的顺序相同:

    ```py
    array([ True, False, False, False, True, False, False, False, False,
               False, False, True, True, False, False, False, False])
    ```

8.  通过使用`:-1` :

    ```py
    features = features_response[:-1]
    ```

    进行索引，可以使用`features_response`列表中除最后一个元素(`name`响应变量)之外的所有元素来获得特征名称
9.  Use the index array we created in *Step 7* with a list comprehension and the `features` list, to find the selected feature names, as follows:

    ```py
    best_features = [features[counter]
                     for counter in range(len(features))
                     if best_feature_ix[counter]]
    best_features
    ```

    输出应该如下所示:

    ```py
    ['LIMIT_BAL', 'PAY_1', 'PAY_AMT1', 'PAY_AMT2']
    ```

    在这段代码中，list comprehension 使用`counter`循环增量遍历了`features`数组(`len(features)`)中的元素数，在`if`语句中使用了代表所选特性的`best_feature_ix`布尔数组来测试是否选择了每个特性，如果是，则捕获其名称。

    所选择的特征与 f 检验结果的数据帧的前四行一致，因此特征选择按预期工作。虽然没有必要两种方式都做，因为它们会导致相同的结果，但是检查你的工作是有好处的，尤其是当你正在学习新概念的时候。您应该知道，使用诸如`SelectPercentile`之类的便利方法，您无法获得 F 统计或 p 值的可见性。但是，在某些情况下，使用这些方法可能会更方便，因为 p 值可能不一定重要，除了它们在要素分级中的效用之外。

## f 检验的优点:相当于两类和注意事项的 t 检验

当我们使用 f 检验来查看两组之间均值的差异时，就像我们在这里对案例研究的二元分类问题所做的那样，我们正在执行的检验实际上简化为所谓的 **t 检验**。f 检验可扩展到三个或更多组，因此对于多类分类非常有用。t 检验只是比较两组样本的平均值，看看这些平均值的差异是否具有统计显著性**。**

 **虽然 f 检验服务于我们在这里的单变量特征选择的目的，有几个注意事项要记住。回到正式统计假设的概念，对于 f 检验，这些假设包括数据是**正态分布**。我们没有检查这个。此外，在将相同的响应变量`y`与来自矩阵的许多潜在特征`X`进行比较时，我们执行了统计学中所谓的**多重比较**。简而言之，这意味着通过一遍又一遍地将多个特征与同一个回答进行比较，我们找到我们认为是“好特征”的几率会增加。但是，这些特征可能无法推广到新数据。多重比较有统计**校正，相当于调整 p 值以说明这一点。**

即使我们没有遵循这些方法的所有统计规则，我们仍然可以从中获得有用的结果。当 p 值是最终感兴趣的量时，例如在进行统计推断时，多重比较校正更受关注。在这里，p 值只是对特性列表进行排序的一种手段。如果针对多重比较对 p 值进行了校正，则该排名的顺序不会改变。

除了知道哪些特性可能对建模有用之外，更深入地了解重要的特性也是很有好处的。因此，我们将在下一个练习中进行详细的图形探索。稍后，我们还将研究其他特征选择方法，这些方法不会做出与我们在此介绍的方法相同的假设，而是更直接地与我们将要构建的预测模型相集成。

## 假设和下一步措施

根据我们的单变量特征探索，与响应变量关联最强的特征是`PAY_1`。这有道理吗？`PAY_1`的解读是什么？`PAY_1`是账户最近一个月的支付状态。正如我们在最初的数据探索中了解到的，有一些值表明帐户处于良好状态:-2 表示没有帐户使用，-1 表示全额支付了余额，0 表示至少支付了最低金额。另一方面，正整数值表示付款延迟了那么多月。上个月延迟付款的账户可以被视为违约账户。这意味着，从本质上讲，这个特性捕获了响应变量的历史值。诸如此类的功能极其重要，因为*几乎任何机器学习问题的最佳预测者之一就是你试图预测的同一事物(即响应变量)*的历史数据。这应该是有道理的:以前违约过的人很可能再次违约的风险最高。

账户的信用额度`LIMIT_BAL`怎么样？想想信贷限额是如何分配的，很可能我们的客户在决定他们的信贷限额时已经评估了借款人的风险。风险较高的客户应该获得较低的限额，这样债权人的风险就较小。因此，我们可能会看到`LIMIT_BAL`值越低的账户违约概率越高。

我们从单变量特征选择练习中学到了什么？我们知道模型中最重要的特性可能是什么。从相关矩阵中，我们可以了解它们与响应变量的关系。然而，知道了我们所使用的测试的局限性，将这些特征可视化是一个好主意，以便更仔细地观察特征和响应变量之间的关系。我们也开始发展关于这些特征的**假设**:为什么我们认为它们很重要？现在，通过可视化特征和响应变量之间的关系，我们可以确定我们的想法是否与我们在数据中看到的一致。

这种假设和可视化通常是向客户展示您的结果的关键部分，客户可能对模型如何工作感兴趣，而不仅仅是它确实工作的事实。

## 练习 3.02:可视化特征和响应变量之间的关系

在本练习中，您将进一步了解本书前面使用的 Matplotlib 中的绘图函数。您将学习如何自定义图形，以更好地回答数据的特定问题。当你进行这些分析时，你会对`PAY_1`和`LIMIT_BAL`特征如何与响应变量相关联产生深刻的视觉效果，这可能为你对这些特征形成的假设提供支持。这将通过更加熟悉 Matplotlib **应用编程接口** ( **API** )来完成，换句话说，就是你用来与 Matplotlib 交互的语法。执行以下步骤来完成练习:

注意

在开始本练习的步骤 1 之前，请确保您已经导入了必要的库并加载了正确的数据帧。您可以参考下面的笔记本，了解这个练习的先决步骤和代码:[https://packt.link/DOrZ9](https://packt.link/DOrZ9)。

1.  Calculate a baseline for the response variable of the default rate across the whole dataset using pandas' `.mean()`:

    ```py
    overall_default_rate = df['default payment next month'].mean()
    overall_default_rate
    ```

    其输出应该如下所示:

    ```py
    0.2217971797179718
    ```

    对于不同的`PAY_1`特性值，有什么好的方法来可视化违约率？

    回想一下我们的观察，这个特征有点像混合的分类和数字特征。由于唯一值的数量相对较少，我们将选择以分类特征的典型方式绘制它。在*第 1 章*、*数据探索和清理*中，我们做了这个特性的`value_counts`作为数据探索的一部分，后来我们在查看`EDUCATION`特性时了解了`groupby` / `mean`。`groupby` / `mean`是再次可视化不同付款状态下的默认利率的好方法。

2.  Use this code to create a `groupby`/`mean` aggregation:

    ```py
    group_by_pay_mean_y = df.groupby('PAY_1').agg(
    ```

    { '下个月违约付款':np.mean})

    ```py
    group_by_pay_mean_y 
    ```

    输出应该如下所示:

    ![Figure 3.9: Mean of the response variable by groups of the PAY_1 feature
    ](image/B16925_03_09.jpg)

    图 3.9:按 PAY_1 功能分组的响应变量均值

    看着这些值，你可能已经能够辨别出趋势。让我们直接绘制它们。我们将一步一步地介绍一些新概念。你应该把从*步骤 3* 到 *6* 的所有代码放在一个代码单元中。

    在 Matplotlib 中，每个图都存在于一个轴上，并且在一个`figure`窗口内。通过为`axes`和`figure`创建对象，您可以直接访问和更改它们的属性，包括轴标签和轴上其他种类的注释。

3.  使用下面的代码在变量`axes`中创建一个`axes`对象:

    ```py
    axes = plt.axes()
    ```

4.  Plot the overall default rate as a red horizontal line.

    Matplotlib 使这变得容易；你只需要用`axhline`函数指出这条线的 *y* 截距。注意，现在我们不是从`plt`调用这个函数，而是作为`axes`对象上的一个方法来调用它:

    ```py
    axes.axhline(overall_default_rate, color='red')
    ```

    现在，在这条线上，我们要画出每组`PAY_1`值中的违约率。

5.  Use the `plot` method of the DataFrame of grouped data we created. Specify to include an `'x'` marker along the line plot, to not have a `legend` instance, which we'll create later, and that the **parent axes** of this plot should be the axes we are already working with (otherwise, pandas would erase what was already there and create new axes):

    ```py
    group_by_pay_mean_y.plot(marker='x', legend=False, ax=axes)
    ```

    这是我们想要绘制的所有数据。

6.  设置*y*-轴标签，并创建一个`legend`实例(控制图例外观有许多可能的选项，但一个简单的方法是提供一个字符串列表，按照图形元素添加到轴的顺序指示它们的标签):

    ```py
    axes.set_ylabel('Proportion of credit defaults')
    axes.legend(['Entire dataset', 'Groups of PAY_1'])
    ```

7.  Executing all the code from *Steps 3* through *6* in a single code cell should result in the following plot:![Figure 3.10: Credit default rates across the dataset
    ](image/B16925_03_10.jpg)

    图 3.10:整个数据集中的信用违约率

    我们对支付状态的可视化揭示了一个清晰的、可能是意料之中的故事:那些之前违约的人实际上更有可能再次违约。信誉良好的账户的违约率远低于整体违约率，我们之前知道整体违约率约为 22%。然而，根据这份报告，上个月拖欠的账户中有超过 30%将在下个月再次拖欠。这是一个很好的视觉效果，可以与我们的业务合作伙伴分享，因为它展示了可能是我们模型中最重要特征之一的效果。

    现在我们将注意力转向与目标变量关联第二强的特征:`LIMIT_BAL`。这是一个具有许多唯一值的数字特征。对于分类问题，可视化此类特征的一个好方法是在同一轴上绘制多个直方图，不同的类使用不同的颜色。作为一种分离类的方法，我们可以使用逻辑数组从数据帧中对它们进行索引。

8.  Use this code to create logical masks for positive and negative samples:

    ```py
    pos_mask = y == 1
    neg_mask = y == 0
    ```

    为了创建我们的双直方图，我们将创建另一个`axes`对象，然后对它调用两次`.hist`方法，用于正类直方图和负类直方图。我们提供了一些额外的关键字参数:第一个直方图将有黑边和白条，而第二个直方图将使用`alpha`来创建透明度，所以我们可以在它们重叠的地方看到两个直方图。一旦我们有了直方图，我们就旋转 *x* 轴记号标签，使它们更加清晰，并创建几个其他注释，这些注释应该是不言自明的。

9.  Use the following code to create the dual histogram plot with the aforementioned properties:

    ```py
    axes = plt.axes()
    axes.hist(df.loc[neg_mask, 'LIMIT_BAL'],\
              edgecolor='black', color='white')
    axes.hist(df.loc[pos_mask, 'LIMIT_BAL'],\
              alpha=0.5, edgecolor=None, color='black')
    axes.tick_params(axis='x', labelrotation=45)
    axes.set_xlabel('Credit limit (NT$)')
    axes.set_ylabel('Number of accounts')
    axes.legend(['Not defaulted', 'Defaulted'])
    axes.set_title('Credit limits by response variable')
    ```

    情节应该是这样的:

    ![Figure 3.11: Dual histograms of credit limits
    ](image/B16925_03_11.jpg)

    图 3.11:信贷限额的双重直方图

    虽然这个情节已经完成了我们希望呈现的所有格式，但它并不完全是可解释的。我们希望从研究中获得一些知识，了解信贷限额如何成为区分违约账户和非违约账户的好方法。然而，这里主要的视觉效果是透明直方图比灰色直方图大。这是因为违约的账户比不违约的少。我们已经通过检查类分数知道了这一点。

    显示这些直方图的形状如何不同，而不仅仅是它们的大小，会更有帮助。为了强调这一点，我们可以通过**归一化**使两个直方图的总绘制面积相同。Matplotlib 提供了一个关键字参数，使这变得容易，创建了一个可能被认为是经验版本的**概率质量函数**。这意味着每个直方图中包含的积分或面积在归一化后将等于 1，因为概率总和为 1。

    经过一些实验，我们决定制作一个有 16 个面元的直方图。因为最高信用额度是新台币 800，000 元，所以我们使用`range`，增量为新台币 50，000 元。以下是您可以使用的代码:

    ```py
    df['LIMIT_BAL'].max()
    ```

10.  Create and display the histogram bin edges with this code:

    ```py
    bin_edges = list(range(0,850000,50000))
    print(bin_edges)
    ```

    输出应该如下所示:

    ```py
    [0, 50000, 100000, 150000, 200000, 250000, 300000, 350000, 40000, 450000,
    500000, 550000, 600000, 650000, 700000, 750000, 800000]
    ```

    归一化直方图的绘制代码与之前类似，但有一些关键的变化:使用`bins`关键字来定义 bin 边缘位置，`density=True`来归一化直方图，以及对绘图注释的更改。最复杂的部分是我们需要调整 **y 轴刻度标签**，这样直方图仓的高度就有了比例的解释，比默认输出更直观。

    *Y*-轴刻度标签是显示在 *y* 轴刻度旁边的文本标签，通常只是这些位置的刻度值。但是，如果需要，您可以手动更改。

    注意

    根据 Matplotlib 文档，对于归一化直方图，柱高度是通过*“将计数除以观察次数乘以柱宽度”*([https://Matplotlib . org/API/_ as _ gen/Matplotlib . py plot . hist . html](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html))计算的。因此，我们需要将 *y* 轴刻度标签乘以新台币 50，000 元的箱宽，以便箱高表示每个箱中样本总数的比例。注意我们得到 y 轴刻度位置的两行，然后将标签设置为修改后的版本。由于浮点运算的微小误差，需要用`np.round`舍入到两位小数。

11.  Run this code to produce normalized histograms:

    ```py
    mpl.rcParams['figure.dpi'] = 400 
    axes = plt.axes()
    axes.hist(
        df.loc[neg_mask, 'LIMIT_BAL'],
        bins=bin_edges, density=True,
        edgecolor='black', color='white')
    axes.hist(
        df.loc[pos_mask, 'LIMIT_BAL'],
        bins=bin_edges, density=True, alpha=0.5,
        edgecolor=None, color='black')
    axes.tick_params(axis='x', labelrotation=45)
    axes.set_xlabel('Credit limit (NT$)')
    axes.set_ylabel('Proportion of accounts')
    y_ticks = axes.get_yticks()
    axes.set_yticklabels(np.round(y_ticks*50000,2))
    axes.legend(['Not defaulted', 'Defaulted'])
    axes.set_title('Normalized distributions of '\
                   'credit limits by response variable')
    ```

    情节应该是这样的:

    ![Figure 3.12: Normalized dual histograms
    ](image/B16925_03_12.jpg)

图 3.12:标准化双直方图

可以看到 Matplotlib 中的绘图是高度可定制的。为了查看你可以从 Matplotlib 轴上获得和设置的所有不同的东西，请看这里:[https://matplotlib.org/stable/api/axes_api.html](https://matplotlib.org/stable/api/axes_api.html)。

从这个情节中我们能学到什么？看起来违约的账户往往有较高比例的较低信用额度。信用额度低于新台币 15 万元的账户相对更容易违约，额度高于此的账户则相反。我们应该问自己，这有意义吗？我们的假设是客户会给高风险的账户更低的限额。这种直觉与我们在这里观察到的信用额度较低的违约者比例较高是一致的。

根据模型构建的进展情况，如果我们在本练习中检查的特征如我们所预期的那样对预测建模很重要，那么最好向我们的客户展示这些图表，作为我们工作演示的一部分。这将让客户了解模型如何工作，以及他们的数据。

从这一部分学到的一个关键是，有效的视觉展示需要大量的时间来制作。最好在项目工作流程中为此安排一些时间。令人信服的视觉效果是值得努力的，因为它们应该能够快速有效地向客户传达重要的发现。与向您创建的材料中添加大量文本相比，它们通常是更好的选择。定量概念的可视化交流是数据科学的核心技能。

# 单变量特征选择:它做什么和不做什么

在这一章中，我们学习了一个接一个地检查特征的技巧，看看它们是否有预测能力。这是很好的第一步，如果你已经有了对结果变量非常有预测性的特征，你可能不需要在建模前花太多时间考虑特征。然而，单变量特征选择也有缺点。特别是，它没有考虑特征之间的**交互**。比如专门针对既有一定教育水平又有一定信用额度范围的人，信用违约率很高怎么办？

此外，使用我们在这里使用的方法，只有特征的线性效果被捕获。如果一个特征在经历某种类型的**变换**时更具预测性，例如**多项式**或**对数**变换，或**宁滨** ( **离散化**)，单变量特征选择的线性技术可能无效。交互和转换是**特征工程**的例子，或者从现有特征中创建新特征。线性特征选择方法的缺点可以通过非线性建模技术来弥补，包括决策树和基于它们的方法，我们将在后面讨论。但是，寻找简单的关系仍然是有价值的，这种关系可以通过单变量特征选择的线性方法来找到，而且这种方法很快就可以做到。

## 使用 Python 中的函数语法理解逻辑回归和 Sigmoid 函数

在本节中，我们将全程打开逻辑回归的“黑箱”:我们将全面了解它是如何工作的。我们将从引入一个新的编程概念开始:**函数**。同时，我们将学习一个数学函数，sigmoid 函数，它在逻辑回归中起着关键作用。

从最基本的意义上说，计算机编程中的函数是一段代码，它接受输入并产生输出。整本书你都在使用函数:别人写的函数。任何时候你使用这样的语法:`output = do_something_to(input)`，你就使用了一个函数。例如，NumPy 有一个函数可用于计算输入的平均值:

```py
np.mean([1, 2, 3, 4, 5])
3.0
```

函数**抽象**正在执行的操作，这样，在我们的例子中，每次你需要计算平均值时，你就不需要看到所有的代码行。对于许多常见的数学函数，在诸如 NumPy 之类的包中已经有了预定义的版本。你不需要“重新发明轮子”流行包中的实现之所以流行是有原因的:人们已经花时间考虑如何以最有效的方式创建它们。所以，使用它们是明智的。然而，由于我们使用的所有包都是**开源的**，如果你有兴趣了解我们使用的库中的函数是如何实现的，你可以查看其中任何一个的代码。

现在，为了便于说明，让我们通过编写自己的算术平均值函数来学习 Python 函数语法。Python 中的函数语法类似于`for`或`if`块，因为函数体是缩进的，函数声明后面跟一个冒号。以下是计算平均值的函数代码:

```py
def my_mean(input_argument):
    output = sum(input_argument)/len(input_argument)
    return(output)
```

在使用此定义执行代码单元格后，笔记本中的其他代码单元格也可以使用该函数。举以下例子:

```py
my_mean([1, 2, 3, 4, 5])
3.0
```

定义函数的第一部分，如这里所示，是以`def`开始一行代码，后面是一个空格，再后面是您想要调用的函数的名称。在这之后是圆括号，在圆括号中指定了函数的**参数**的名称。参数是输入变量的名称，这些名称在函数体内部:定义为参数的变量名在函数内部可用，当**调用**(已用)时，但在函数外部不可用。可以有多个参数；它们将以逗号分隔。括号后面是一个冒号。

函数体是缩进的，可以包含任何对输入进行操作的代码。一旦这些操作完成，最后一行应该以`return`开始并包含输出变量，如果有多个变量，用逗号分隔。在这个非常简单的函数介绍中，我们省略了许多细节，但是这些是您需要开始的基本部分。

当你使用一个功能时，它的威力就显现出来了。请注意，在我们定义了函数之后，在一个单独的代码块中，我们可以**用我们给它的名字来调用**，并且它对我们**传递给**的任何输入进行操作。就好像我们已经将所有代码复制并粘贴到这个新位置。但这看起来比实际做起来要好得多。如果你打算多次使用相同的代码，函数可以大大减少代码的总长度。

作为一个简短的附加说明，您可以选择使用参数名称显式指定输入，当有许多输入时，这样会更清楚:

```py
my_mean(input_argument=[1, 2, 3])
2.0
```

现在我们已经熟悉了 Python 函数的基础，我们将考虑一个对逻辑回归很重要的数学函数，称为 **sigmoid** 。该功能也可以称为**后勤功能**。乙状结肠的定义如下:

![Figure 3.13: The sigmoid function
](image/B16925_03_13.jpg)

图 3.13:sigmoid 函数

我们将分解这个函数的不同部分。正如您所看到的，sigmoid 函数涉及到了无理数 e(T4)，它也被称为自然对数的底数，与我们之前在数据探索中使用的底数为 10 的对数形成对比。为了使用 Python 计算 *e* -X，我们实际上不需要手动执行取幂运算。NumPy 有一个方便的函数`exp`，它自动将`e`作为输入指数。如果你看文档，你会看到这个过程被称为取“指数”，听起来很模糊。但是在这种情况下，应该理解为指数的底数是 *e* 。一般来说，如果你想在 Python 中取一个指数，比如 23(“二的三次方”)，语法是两个星号:`2**3`，比如等于 8。

考虑如何将输入传递给`np.exp`函数。由于 NumPy 的实现是**矢量化**，这个函数可以接受单个数字以及数组或矩阵作为输入。为了说明各个论点，我们计算 1 的指数，它显示了 *e* 的近似值，以及 *e0* ，它当然等于 1，任何底数的零次方也是如此:

```py
np.exp(1)
2.718281828459045
np.exp(0)
1.0
```

为了说明`np.exp`的矢量化实现，我们使用 NumPy 的`linspace`函数创建一个数字数组。该函数将一个范围的起点和终点以及该范围内所需的值的数量作为输入，以创建一个由多个线性间隔的值组成的数组。这个函数的作用有点类似于 Python 的`range`，但是也可以产生十进制值:

![Figure 3.14: Using np.linspace to make an array
](image/B16925_03_14.jpg)

图 3.14:使用 np.linspace 创建数组

因为`np.exp`是矢量化的，所以它会以高效的方式一次计算整个数组的指数。下面是带有输出的代码，用于计算我们的`X_exp`数组的指数并检查前五个值:

![Figure 3.15: NumPy's exp function
](image/B16925_03_15.jpg)

图 3.15: NumPy 的 exp 函数

## 练习 3.03:绘制 Sigmoid 函数

在本练习中，我们将使用之前创建的`X_exp`和`Y_exp`来绘制区间`[-4, 4]`上的指数函数。您需要运行*图 3.14* 和 *3.15* 中的所有代码，以使这些变量可用于本练习。然后，我们将为 sigmoid 定义一个函数，创建一个曲线图，并考虑它与指数函数的关系。执行以下步骤来完成练习:

注意

在开始本练习的步骤 1 之前，请确保您已经导入了必要的库。导入库的代码以及练习中其余步骤的代码可以在这里找到:[https://packt.link/Uq012](https://packt.link/Uq012)。

1.  Use this code to plot the exponential function:

    ```py
    plt.plot(X_exp, Y_exp)
    plt.title('Plot of $e^X$')
    ```

    情节应该是这样的:

    ![Figure 3.14: Using np.linspace to make an array
    ](image/B16925_03_16.jpg)

    图 3.16:绘制指数函数

    注意，在给图加标题时，我们利用了一种叫做 **LaTeX** 的语法，它支持数学符号的格式化。我们不会在这里深入讨论 LaTeX 的细节，但可以说它非常灵活。请注意，将标题字符串的一部分用美元符号括起来会导致它使用 LaTeX 呈现，并且可以使用`^`创建上标。

    在*图 3.16* 中还要注意，许多间隔很近的点产生了一个平滑曲线的外观，但实际上，它是由线段连接的离散点的图形。

    关于指数函数，我们能观察到什么？

    它永远不会是负的:当 *X* 接近负无穷大时， *Y* 接近 0。

    随着 *X* 的增加， *Y* 起初缓慢增加，但很快“爆炸”这就是当人们说“指数增长”来表示快速增长时的含义。

    你如何从指数的角度来思考 sigmoid？

    第一，s 形涉及 *e* -X，与 *e* X 相对， *e* -X 的图形只是 *e* X 关于 *y* 轴的映射。这可以很容易地绘制出来，并在绘图标题中使用多字符上标的花括号进行注释。

2.  Run this code to see the plot of *e*-X:

    ```py
    Y_exp = np.exp(-X_exp)
    plt.plot(X_exp, Y_exp)
    plt.title('Plot of $e^{-X}$')
    ```

    输出应该如下所示:

    ![Figure 3.17: Plot of exp(-X)
    ](image/B16925_03_17.jpg)

    图 3.17:exp(-X)的曲线图

    现在，在 sigmoid 函数中， *e* -X 在分母中，加 1。分子是 1。那么，当 *X* 接近负无穷大时，乙状结肠会发生什么？我们知道 *e* -X“爆炸”，变得非常大。总的来说，分母变得很大，分数趋近于 0。当 *X* 向正无穷大增加时呢？我们可以看到 *e* -X 变得非常接近 0。因此，在这种情况下，sigmoid 函数大约为 *1/1 = 1* 。这应该给你一个直觉，sigmoid 函数保持在 0 和 1 之间。现在让我们用 Python 实现一个 sigmoid 函数，并用它来创建一个情节，看看现实如何与这种直觉相匹配。

3.  像这样定义一个 sigmoid 函数:

    ```py
    def sigmoid(X):
        Y = 1 / (1 + np.exp(-X))
        return Y
    ```

4.  Make a larger range of *x* values to plot over and plot the sigmoid. Use this code:

    ```py
    X_sig = np.linspace(-7,7,141)
    Y_sig = sigmoid(X_sig)
    plt.plot(X_sig,Y_sig)
    plt.yticks(np.linspace(0,1,11))
    plt.grid()
    plt.title('The sigmoid function')
    ```

    情节应该是这样的:

    ![Figure 3.18: A sigmoid function plot
    ](image/B16925_03_18.jpg)

图 3.18:一个 sigmoid 函数图

这个情节符合我们的预期。进一步，我们可以看到`sigmoid(0) = 0.5`。乙状结肠函数有什么特别之处？这个函数的输出严格限制在 0 和 1 之间。对于应该预测概率的函数来说，这是一个很好的属性，概率也要求在 0 和 1 之间。从技术上来说，概率可以正好等于 0 和 1，而 sigmoid 永远不会。但是乙状结肠可以足够近，这不是实际的限制。

回想一下，我们将逻辑回归描述为产生类别成员的**预测概率**，而不是直接预测类别成员。这使得逻辑回归的实现更加灵活，允许选择阈值概率。sigmoid 函数是这些预测概率的来源。简而言之，我们将看到在预测概率的计算中如何使用不同的特征。

## 功能范围

当你开始使用函数时，你应该意识到**作用域**的概念。注意，当我们编写`sigmoid`函数时，我们在函数内部创建了一个变量`Y`。在函数内部创建的变量不同于在函数外部创建的变量。当函数被调用时，它们在函数内部被有效地创建和销毁。这些变量在作用域上被称为**局部**:函数的局部变量。如果你已经在一个笔记本上按顺序运行了本章所写的所有代码，请注意，在使用`sigmoid`函数后，你将无法访问`Y`变量:

![Figure 3.19: The Y variable not in the scope of the notebook
](image/B16925_03_19.jpg)

图 3.19:不在笔记本范围内的 Y 变量

`Y`变量不在笔记本的**全局**范围内。然而，在函数外部创建的全局变量在函数的局部范围内是可用的，即使它们没有作为参数输入到函数中。这里我们演示了在函数外部创建一个全局变量，然后在函数内部访问它。该函数实际上根本不带任何参数，但是正如您所看到的，它可以使用全局变量的值来创建输出:

![Figure 3.20: Global variable available within the local scope of the function
](image/B16925_03_20.jpg)

图 3.20:函数局部范围内可用的全局变量

注意

**关于范围的更多细节**

变量的范围可能会让人感到困惑，但当您开始更高级地使用函数时，知道这一点是有好处的。虽然这些知识不是本书所必需的，但您可能希望在这里获得关于 Python 中变量作用域的更深入的观点:[https://nb viewer . jupyter . org/github/rasbt/Python _ reference/blob/master/tutorials/scope _ resolution _ legb _ rule . ipynb](https://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/scope_resolution_legb_rule.ipynb)。

**科学应用中的 Sigmoid 曲线**

除了是逻辑回归的基础之外，sigmoid 曲线还用于各种应用中。在生物学中，它们可以用来描述一个有机体的生长，它开始缓慢，然后有一个快速的阶段，然后随着最终大小的达到而逐渐变小。Sigmoids 也可以用来描述人口增长，它有一个类似的轨迹，快速增长，但随后随着环境承载能力的达到而放缓。

## 为什么 Logistic 回归被认为是线性模型？

我们之前提到过，逻辑回归被认为是一个**线性模型**，而我们正在探索特征和反应之间的关系是否类似于线性关系。回想一下，我们在第 1 章、*数据探索和清理*中绘制了`EDUCATION`特性的`groupby` / `mean`，以及本章中的`PAY_1`特性，以查看这些特性的值的违约率是否呈现线性趋势。虽然这是一个快速了解这些特征“线性与否”的好方法，但在这里我们将逻辑回归为什么是线性模型的概念形式化。

如果用于计算预测的特征变换是特征的**线性组合**，则认为模型是线性的。线性组合的可能性是每个特征可以乘以一个数值常数，这些项可以加在一起，并且可以添加一个附加常数。例如，在具有两个特征的简单模型中， *X* 1 和 *X* 2，线性组合将采用以下形式:

![Figure 3.21: Linear combination of X1 and X2
](image/B16925_03_21.jpg)

图 3.21:X1 和 X2 的线性组合

常数 *𝜃* i 可以是任意数字，正数、负数或零，其中 *i = 0、1 和 2* (尽管如果系数为 0，这将从线性组合中移除一个特征)。一个变量线性变换的常见例子是一条直线，方程式为 *y = mx + b* ，如第 2 章、*sci kit 简介-学习和模型评估*所述*。在这种情况下， *𝜃* o *= b* 和 *𝜃* 1 *= m* 。 *𝜃* o 被称为一个线性组合的**截距**，从代数上应该很熟悉。*

线性变换中什么样的事情是“不允许”的？除了刚才描述的数学表达式之外的任何其他数学表达式，例如:

*   将特征本身相乘；比如 *X* 12 或者 *X* 13。这些被称为多项式项。
*   将特征相乘；比如 *X* 1 *X* 2。这些被称为相互作用。
*   对特征应用非线性变换；例如，对数和平方根。
*   其他复杂的数学函数。
*   “If then”类型的语句。比如“如果*X*1*a*，那么 *y = b* ”

然而，虽然这些变换不是线性组合的基本公式的一部分，但是它们可以通过**工程特征**添加到线性模型中，例如，定义一个新特征 *X* 3 = *X* 12。

前面，我们了解到逻辑回归的预测，以概率的形式，是使用 sigmoid 函数进行的。再看这里，我们看到这个函数明显是非线性的:

![Figure 3.22: Non-linear sigmoid function
](image/B16925_03_22.jpg)

图 3.22:非线性 sigmoid 函数

那么，为什么逻辑回归被认为是线性模型呢？事实证明，这个问题的答案在于 sigmoid 方程的另一种形式，称为`logit`函数。我们可以通过求解 *X* 的 sigmoid 函数来导出`logit`函数；换句话说，寻找 sigmoid 函数的反函数。首先，我们设置 sigmoid 等于 *p* ，我们将其解释为观察到正类的概率，然后求解 *X* ，如下所示:

![Figure 3.23: Solving for X
](image/B16925_03_23.jpg)

图 3.23:求解 X

这里，我们使用了一些指数和对数定律来求解 *X* 。你也可以看到`logit`表达如下:

![Figure 3.24: The logit function
](image/B16925_03_24.jpg)

图 3.24:logit 函数

在这个表达式中，失败概率、 *q* 的**用成功概率**、 *p* 的**来表示；q = 1 - p ，因为概率总和为 1。尽管在我们的案例中，信用违约可能会被认为是现实世界结果意义上的失败，但积极的结果(二元问题中的响应变量= 1)在数学术语中通常被认为是“成功”。`logit`函数也被称为**对数比值**，因为它是**比值比**、 *p/q* 的自然对数。赔率可能在博彩业很常见，比如“a 队*和 b 队*的赔率是 2 比 1，b 队会被打败。”**

一般来说，我们在这些操作中称之为大写 *X* 可以代表所有特征的线性组合。例如，在我们的两个特征的简单例子中，这将是*x =**𝜃*o*+**𝜃*1*x*1*+**𝜃*2*x*2。逻辑回归被认为是线性模型，因为事实上，当响应变量被认为是对数概率时，包含在 *X* 中的特征只服从线性组合。与 sigmoid 方程相比，这是表述问题的另一种方式。

将这些片段放在一起，特征 *X* 1、 *X* 2、…、 *X* j 在逻辑回归的 sigmoid 方程版本中看起来是这样的:

![Figure 3.25: Sigmoid version of logistic regression
](image/B16925_03_25.jpg)

图 3.25:逻辑回归的 Sigmoid 版本

但在对数优势版本中，它们看起来像这样，这就是为什么逻辑回归被称为线性模型:

![Figure 3.26: Log odds version of logistic regression
](image/B16925_03_26.jpg)

图 3.26:对数优势版本的逻辑回归

由于这种看待逻辑回归的方式，理想情况下，逻辑回归模型的特征在响应变量的对数概率中是**线性的。我们将在下面的练习中了解这意味着什么。**

逻辑回归是被称为**广义线性模型** ( **GLMs** )的更广泛的一类统计模型的一部分。glm 与普通线性回归的基本概念有关，普通线性回归可能具有一个特征(即，最佳拟合的**线**， *y = mx + b* ，对于单个特征， *x* )或者在**多元线性回归中具有多个特征**。GLMs 和线性回归之间的数学联系是**链接函数**。逻辑回归的链接函数就是我们刚刚学过的 logit 函数。

## 练习 3.04:检验逻辑回归特征的适当性

在*练习 3.02* 、*中，可视化特征和响应变量*之间的关系，根据我们目前的探索，我们绘制了可能是模型最重要特征的`groupby` / `mean`:特征`PAY_1`。通过按照`PAY_1`的值对样本进行分组，然后查看响应变量的平均值，我们可以有效地查看每组中的违约概率 *p* 。

在本练习中，我们将评估`PAY_1`对于逻辑回归的适当性。我们将通过检查这些组中违约的对数概率来做到这一点，以查看响应变量在对数概率中是否是线性的，正如逻辑回归正式假设的那样。执行以下步骤来完成练习:

注意

在开始本练习的第 1 步之前，请确保您已经导入了必要的库。先决步骤可以参考以下笔记本:[https://packt.link/gtpF9](https://packt.link/gtpF9)。

1.  Confirm you still have access to the variables from *Exercise 3.02*, *Visualizing the Relationship between the Features and Response Variable*, in your notebook by reviewing the DataFrame of the average value of the response variable for different values of `PAY_1` with this code:

    ```py
    group_by_pay_mean_y
    ```

    输出应该如下所示:

    ![Figure 3.27: Rates of default within groups of PAY_1 values as probabilities of default
    ](image/B16925_03_27.jpg)

    图 3.27:作为违约概率的 PAY_1 值组内的违约率

2.  从这些组中提取响应变量的平均值，并将它们放入变量`p`，代表违约概率:

    ```py
    p = group_by_pay_mean_y['default payment next month'].values
    ```

3.  Create a probability, `q`, of not defaulting. Since there are only two possible outcomes in this binary problem, and probabilities of all outcomes always sum to 1, it is easy to calculate `q`. Also print the values of `p` and `q` to confirm:

    ```py
    q = 1-p
    print(p)
    print(q)
    ```

    输出应该如下所示:

    ![Figure 3.28: Calculating q from p
    ](image/B16925_03_28.jpg)

    图 3.28:从 p 计算 q

4.  Calculate the odds ratio from `p` and `q`, as well as the log odds, using the natural logarithm function from NumPy:

    ```py
    odds_ratio = p/q
    log_odds = np.log(odds_ratio)
    log_odds
    ```

    输出应该如下所示:

    ![Figure 3.29: Odds ratio and log odds
    ](image/B16925_03_29.jpg)

    图 3.29:优势比和对数优势

5.  In order to plot the log odds against the values of the feature, we can get the feature values from the index of the DataFrame containing `groupby`/`mean`. You can show the index like this:

    ```py
    group_by_pay_mean_y.index
    ```

    这将产生以下输出:

    ```py
    Int64Index([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8], dtype='int64', name='PAY_1')
    ```

6.  Create a similar plot to what we have already done, to show the log odds against the values of the feature. Here is the code:

    ```py
    plt.plot(group_by_pay_mean_y.index, log_odds, '-x')
    plt.ylabel('Log odds of default')
    plt.xlabel('Values of PAY_1')
    ```

    情节应该是这样的:

    ![Figure 3.30: Log odds of default for values of PAY_1
    ](image/B16925_03_30.jpg)

图 3.30:PAY _ 1 值的违约概率对数

我们可以在该图中看到，响应变量的对数赔率和`PAY_1`特征之间的关系与我们在*练习 3.02* 、*中绘制的违约率和该特征之间的关系并没有太大的不同，从而可视化特征和响应变量*之间的关系。出于这个原因，如果“违约率”是一个更简单的概念，你可以向业务伙伴传达，这可能是可取的。然而，就理解逻辑回归的工作原理而言，该图准确地显示了被假定为线性的东西。

对于这些数据，直线拟合是一个好的模型吗？

在这个图上画出的“最佳拟合线”似乎是从左到右的。与此同时，这些数据似乎不会导致一个真正的线性过程。看待这些数据的一种方式是，值-2、-1 和 0 看起来像是位于与其他值不同的对数概率范围内。`PAY_1 = 1`算是中等，其余的大多比较大。基于该变量的工程特征，或由-2、-1 和 0 表示的不同类别编码方式，可能对建模更有效。请记住这一点，我们将在本书后面用逻辑回归和其他方法对这些数据进行建模。

## 从逻辑回归系数到使用 Sigmoid 的预测

在下一个练习之前，让我们看看如何使用逻辑回归系数来计算预测概率，并最终对响应变量的类别进行预测。

回想一下，根据 sigmoid 方程，逻辑回归预测类成员的概率。对于具有截距的两个要素，公式如下:

![Figure 3.31: Sigmoid function to predict the probability of class membership for two features
](image/B16925_03_31.jpg)

图 3.31:预测两个特征的类成员概率的 Sigmoid 函数

当您使用训练数据调用 scikit-learn 中的逻辑回归模型对象的`.fit`方法时，𝜃 0、 *𝜃* 1 和 *𝜃* 2 参数(截距和系数)将根据这些标记的训练数据进行估算。实际上，scikit-learn 计算出如何为 *𝜃* 0、 *𝜃* 1 和 *𝜃* 2 选择值，以便它将尽可能正确地分类尽可能多的训练数据点。我们将在下一章深入了解这个过程是如何工作的。

当您调用`.predict`时，scikit-learn 根据拟合的参数值和 sigmoid 方程计算预测概率。如果 *p ≥ 0.5* ，给定样本将被分类为阳性，否则为阴性。

我们知道，sigmoid 方程的曲线看起来如下，我们可以通过代入*x =*𝜃0*+*t8】𝜃1*x*1*+**𝜃*2*x*2:

![Figure 3.32: Predictions and true classes plotted together
](image/B16925_03_32.jpg)

图 3.32:预测和真实类一起绘制

这里注意，如果*x =**𝜃*o*+**𝜃*1x*+**𝜃*2*x*2*≥0*在 *x* 轴上，那么预测概率将是 *p ≥ 0.5* 在 *y* 轴上否则，*p**<**0.5*该样本将被归类为阴性。根据 *X* 1 和 *X* 2 特征，使用系数和截距，我们可以使用该观察来计算正面预测的线性条件。求解不等式对于正预测，*x =**𝜃*o*+**𝜃*1*x*1*+**𝜃*2*x*2*≥0*，对于 *X* 2、 我们可以得到一个类似于线性方程的线性不等式 *y = mx + b* 形式:*x*2*≥-(**𝜃*1*/**𝜃*2*)x*1*-(**𝜃*o*/*

 *这将有助于在下面的练习中看到逻辑回归在 *X* 1 *-X* 2 **特征空间**中的线性决策边界。

我们现在已经从理论和数学的角度了解了为什么逻辑回归被认为是线性模型。我们还研究了单一特征，并考虑了线性假设是否合适。理解线性假设也是很重要的，根据我们期望逻辑回归有多灵活和强大。我们将在下面的练习中探讨这一点。

## 练习 3.05:逻辑回归的线性决策边界

在本练习中，我们说明了二元分类问题的**决策边界**的概念。我们使用合成数据来创建一个清晰的示例，说明与训练样本相比，逻辑回归的决策边界看起来如何。我们首先随机生成两个特征， *X* 1 和 *X* 2。由于有两个特征，我们可以说这个问题的数据是二维的。这很容易想象。我们在这里阐述的概念可以推广到两个以上特征的情况，例如您在工作中可能会看到的真实世界的数据集；然而，决策边界在高维空间中更难可视化。

执行以下步骤来完成练习:

注意

在开始本练习的第 1 步之前，请确保您已经导入了必要的库。先决步骤可以参考以下笔记本:[https://packt.link/35ge1](https://packt.link/35ge1)。

1.  Generate the features using the following code:

    ```py
    from numpy.random import default_rng
    rg = default_rng(4)
    X_1_pos = rg.uniform(low=1, high=7, size=(20,1))
    print(X_1_pos[0:3])
    X_1_neg = rg.uniform(low=3, high=10, size=(20,1))
    print(X_1_neg[0:3])
    X_2_pos = rg.uniform(low=1, high=7, size=(20,1))
    print(X_2_pos[0:3])
    X_2_neg = rg.uniform(low=3, high=10, size=(20,1))
    print(X_2_neg[0:3])
    ```

    你不需要太担心我们为什么选择我们所做的价值观；我们以后做的绘图应该会弄清楚。然而，请注意，我们已经同时指定了真实类，通过在这里定义哪些点(`X` 1 `, X` 2)将在正类和负类中。这样做的结果是，我们在阳性和阴性类别中各有 20 个样本，总共有 40 个样本，并且每个样本有两个特征。我们显示了正类和负类的每个特征的前三个值。

    输出应该如下所示:

    ![Figure 3.33: Generating synthetic data for a binary classification problem
    ](image/B16925_03_33.jpg)

    图 3.33:为二元分类问题生成合成数据

2.  Plot this data, coloring the positive samples as red squares and the negative samples as blue *x*'s. The plotting code is as follows:

    ```py
    plt.scatter(X_1_pos, X_2_pos, color='red', marker='s')
    plt.scatter(X_1_neg, X_2_neg, color='blue', marker='x')
    plt.xlabel(‚$X_1$')
    plt.ylabel(‚$X_2$')
    plt.legend(['Positive class', 'Negative class'])
    ```

    结果应该是这样的:

    ![Figure 3.34: Generating synthetic data for a binary classification problem
    ](image/B16925_03_34.jpg)

    图 3.34:为二元分类问题生成合成数据

    为了在 scikit-learn 中使用我们的合成功能，我们需要将它们组装成一个矩阵。我们使用 NumPy 的`block`函数来创建一个 40 乘 2 的矩阵。因为总共有 40 个样本，所以有 40 行，因为有 2 个特征，所以有 2 列。我们将进行安排，使阳性样本的特征出现在前 20 行，然后是阴性样本的特征。

3.  Create a 40 by 2 matrix and then show the shape and the first 3 rows:

    ```py
    X = np.block([[X_1_pos, X_2_pos], [X_1_neg, X_2_neg]])
    print(X.shape)
    print(X[0:3])
    ```

    输出应该如下所示:

    ```py
    (40, 2)
    [[6.65833663 5.15531227]
     [4.06796532 5.6237829  ]
     [6.85746223 2.14473103]]
    ```

    我们还需要一个响应变量来配合这些特性。我们知道我们是如何定义它们的，但是我们需要一组`y`值来让 scikit-learn 知道。

4.  Create a vertical stack (`vstack`) of 20 ones and then 20 zeros to match our arrangement of the features and reshape to the way that scikit-learn expects. Here is the code:

    ```py
    y = np.vstack((np.ones((20,1)), np.zeros((20,1)))).reshape(40,)
    print(y[0:5])
    print(y[-5:])
    ```

    您将获得以下输出:

    ```py
    [1\. 1\. 1\. 1\. 1.]
    [0\. 0\. 0\. 0\. 0.]
    ```

    此时，我们已经准备好使用 scikit-learn 将逻辑回归模型拟合到该数据。我们将使用所有数据作为训练数据，并检查线性模型与数据的拟合程度。接下来的几个步骤应该是你在前面章节中所熟悉的，关于如何实例化一个模型类并适应模型。

5.  首先，使用下面的代码导入模型类:

    ```py
    from sklearn.linear_model import LogisticRegression
    ```

6.  Now instantiate, indicating the `liblinear` solver, and show the model object using the following code:

    ```py
    example_lr = LogisticRegression(solver='liblinear')
    example_lr
    ```

    输出应该如下所示:

    ```py
    LogisticRegression(solver='liblinear')
    ```

    我们将在 scikit 中讨论一些可用于逻辑回归的不同解算器，在第四章*中学习*、*偏差方差权衡*，但现在我们将使用这个解算器。

7.  Now train the model on the synthetic data:

    ```py
    example_lr.fit(X, y)
    ```

    我们拟合的模型的预测看起来如何？

    我们首先需要通过对我们用于模型训练的相同样本使用训练模型的`.predict`方法来获得这些预测。然后，为了将这些预测添加到绘图中，我们将根据预测是 1 还是 0 来创建两个索引列表以用于数组。看看你是否能理解我们如何使用列表理解，包括一个`if`语句，来完成这个任务。

8.  Use this code to get predictions and separate them into indices of positive and negative class predictions. Show the indices of positive class predictions as a check:

    ```py
    y_pred = example_lr.predict(X)
    positive_indices = [counter for counter in range(len(y_pred))
                        if y_pred[counter]==1]
    negative_indices = [counter for counter in range(len(y_pred))
                        if y_pred[counter]==0]
    positive_indices
    ```

    输出应该如下所示:

    ```py
    [2, 3, 4, 5, 6, 7, 9, 11, 13, 15, 16, 17, 18, 19, 26, 34, 36]
    ```

    从正面预测的指数中，我们已经可以看出，并非训练数据中的每个样本都被正确分类:正面样本是前 20 个样本，但这里有超出该范围的指数。根据对数据的检查，您可能已经猜到线性决策边界无法完美地对这些数据进行分类。现在让我们把这些预测放在图上，以围绕每个数据点的正方形和圆形的形式，根据积极和消极的预测分别着色:红色代表积极，蓝色代表消极。

    您可以将内部符号(数据的真实标注)的颜色和形状与外部符号(预测)的颜色和形状进行比较，以查看哪些点分类正确或不正确。

9.  Here is the plotting code:

    ```py
    plt.scatter(X_1_pos, X_2_pos, color='red', marker='s')
    plt.scatter(X_1_neg, X_2_neg, color='blue', marker='x')
    plt.scatter(X[positive_indices,0], X[positive_indices,1],
                s=150, marker='s',
                edgecolors='red', facecolors='none')
    plt.scatter(X[negative_indices,0], X[negative_indices,1],
                s=150, marker='o',
                edgecolors='blue', facecolors='none')
    plt.xlabel('$X_1$')
    plt.ylabel('$X_2$')
    plt.legend(['Positive class', 'Negative class',\
                'Positive predictions', 'Negative predictions'])
    ```

    该图应如下所示:

    ![Figure 3.35: Predictions and true classes plotted together
    ](image/B16925_03_35.jpg)

    图 3.35:预测和真实类一起绘制

    从图中可以看出，分类器很难处理那些接近你想象的线性决策边界的数据点；其中一些可能会出现在错误的边界上。我们如何计算出决策边界的实际位置，并将其可视化？从上一节我们知道，在二维特征空间中，利用不等式*x*2*≥-(**𝜃*1*/**𝜃*2*x*1*-(**𝜃*0*/**𝜃*2*可以得到一个逻辑回归的决策边界。既然我们已经在这里拟合了模型，我们可以检索 *𝜃* 1 和 *𝜃* 2 系数，以及 *𝜃* 0 截距，以插入这个方程并创建曲线图。*

**   Use this code to get the coefficients from the fitted model and print them:

    ```py
    theta_1 = example_lr.coef_[0][0]
    theta_2 = example_lr.coef_[0][1]
    print(theta_1, theta_2)
    ```

    输出应该如下所示:

    ```py
    -0.16472042583006558 -0.25675185949979507
    ```

    *   Use this code to get the intercept:

    ```py
    theta_0 = example_lr.intercept_
    ```

    现在使用系数和截距来定义线性决策边界。这就抓住了不等式的分割线，*x*2*≥-(**𝜃*1*/**𝜃*2*x*1*-(**𝜃*0*/**𝜃*2*)【t21:*

    ```py
    X_1_decision_boundary = np.array([0, 10])
    X_2_decision_boundary = -(theta_1/theta_2)*X_1_decision_boundary\
                            - (theta_0/theta_2)
    ```

    总结一下最后几个步骤，在使用`.coef_`和`.intercept_`方法检索了 *𝜃* 1 和 *𝜃* 2 模型系数和 *𝜃* 0 截距之后，我们使用这些来创建一条由两点定义的线，根据我们为决策边界描述的方程。

    *   Plot the decision boundary using the following code, with some adjustments to assign the correct labels for the legend, and to move the legend to a location (`loc`) outside a plot that is getting crowded:

    ```py
    pos_true = plt.scatter(X_1_pos, X_2_pos,
                           color='red', marker='s',
                           label='Positive class')
    neg_true = plt.scatter(X_1_neg, X_2_neg,
                           color='blue', marker='x',
                           label='Negative class')
    pos_pred = plt.scatter(X[positive_indices,0],
                           X[positive_indices,1],
                           s=150, marker='s',
                           edgecolors='red', facecolors='none',
                           label='Positive predictions')
    neg_pred = plt.scatter(X[negative_indices,0],
                           X[negative_indices,1],
                           s=150, marker='o',
                           edgecolors='blue', facecolors='none',
                           label='Negative predictions')
    dec = plt.plot(X_1_decision_boundary, X_2_decision_boundary,
                   'k-', label='Decision boundary')
    plt.xlabel('$X_1$')
    plt.ylabel('$X_2$')
    plt.legend(loc=[0.25, 1.05])
    ```

    你将获得以下情节:

    ![Figure 3.36: True classes, predicted classes, and the decision boundary
     of a logistic regression
    ](image/B16925_03_36.jpg)*

 *图 3.36:逻辑回归的真实类、预测类和决策边界

**决策边界的位置与您认为的位置相比如何？**

**你能看出线性决策边界永远无法完美地对这些数据进行分类吗？**

作为一种解决方法，我们可以从这里的现有特征(如多项式或交互)创建**工程特征**，以允许逻辑回归中更复杂的非线性决策边界。或者，我们可以使用非线性模型，如随机森林，它也可以实现这一点，我们将在后面看到。

作为这里的最后一点，这个例子很容易在二维空间中可视化，因为只有两个特征。一般来说，决策边界可以用一个**超平面**来描述，它是一条直线到多维空间的推广。然而，线性决策边界的限制性本质仍然是超平面的一个因素。

## 活动 3.01:拟合逻辑回归模型并直接使用系数

在本活动中，我们将针对我们在单变量特征探索中发现的两个最重要的特征训练一个逻辑回归模型，并学习如何使用拟合模型中的系数手动实施逻辑回归。这将向您展示如何在 scikit-learn 可能不可用，但计算 sigmoid 函数所需的数学函数可用的计算环境中使用逻辑回归。成功完成活动后，您应该观察到使用 scikit-learn 预测计算的 ROC AUC 值与从手动预测获得的值应该相同:大约为 0.63。

执行以下步骤来完成活动:

1.  以`PAY_1`和`LIMIT_BAL`为特征创建一个训练/测试分割(80/20)。
2.  使用默认选项导入`LogisticRegression`，但将解算器设置为`'liblinear'`。
3.  对训练数据进行训练，并使用测试数据获得预测的类别以及类别概率。
4.  从训练好的模型中取出系数和截距，并手动计算预测概率。您需要向要素添加一列 1，乘以截距。
5.  使用阈值`0.5`，手动计算预测类别。将此与 scikit-learn 输出的类预测进行比较。
6.  Calculate the ROC AUC using both scikit-learn's predicted probabilities and your manually predicted probabilities, and compare them.

    注意

    包含这项活动代码的 Jupyter 笔记本可以在这里找到:[https://packt.link/4FHec](https://packt.link/4FHec)。这个笔记本只包含 Python 代码和相应的输出。完整的分步解决方案可通过[此链接](B16925_Solution_ePub.xhtml#_idTextAnchor153)找到。

# 总结

在本章中，我们学习了如何使用单变量特征选择方法(包括皮尔逊相关和方差分析 f 检验)一次探索一个特征。虽然以这种方式看待特性并不总是能说明全部情况，因为您可能会错过特性之间的重要交互，但这通常是一个有用的步骤。了解最具预测性的特征和响应变量之间的关系，并围绕它们创建有效的可视化，是向客户传达您的发现的一个很好的方式。我们使用定制的图，比如用 Matplotlib 创建的重叠直方图，来创建最重要特征的可视化。

然后，我们开始深入描述逻辑回归如何工作，探索 sigmoid 函数、对数概率和线性决策边界等主题。虽然逻辑回归是最简单的分类模型之一，并且通常不如其他方法强大，但它是使用最广泛的模型之一，并且是更复杂的模型(如用于分类的深度神经网络)的基础。因此，当您探索机器学习中更高级的主题时，对逻辑回归的详细理解可以很好地为您服务。在某些情况下，一个简单的逻辑回归可能就足够了。考虑所有其他的事情，满足需求的最简单的模型可能是最好的模型。

如果你掌握了本章和下一章的内容，你将为在工作中使用逻辑回归做好充分的准备。在下一章中，我们将基于我们在此处学到的基础知识，了解如何估计逻辑回归的系数，以及逻辑回归如何有效地用于大量要素以及如何用于要素选择。****