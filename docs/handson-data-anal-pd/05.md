

# 三、使用Pandas的处理数据

在前一章中，我们学习了主要的`pandas`数据结构，如何用我们收集的数据创建`DataFrame`对象，以及检查、汇总、过滤、选择和使用`DataFrame`对象的各种方法。既然我们已经很好地掌握了最初的数据收集和检查阶段，我们可以开始进入数据争论的世界了。

正如在 [*第一章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015) 、*数据分析简介*中提到的，为分析准备数据通常是那些处理数据的人工作时间上最大的部分，而且通常是最不愉快的。从好的方面来看，`pandas`已经准备好帮助完成这些任务，而且，通过掌握本书中介绍的技能，我们将能够更快地进入更有趣的部分。

应该注意的是，在我们的分析中，数据争论不是我们只做一次的事情；很有可能我们会进行一些数据争论，并转移到另一个分析任务，如数据可视化，却发现我们需要进行额外的数据争论。我们对数据越熟悉，就越能为我们的分析准备好数据。对于我们的数据应该是什么类型，我们需要什么格式的数据才能最好地传达我们想要显示的内容，以及我们应该收集哪些数据点来进行分析，形成一种直觉是至关重要的。这来自于经验，所以我们必须利用每一个机会在我们自己的数据上练习本章将要涉及的技巧。

由于这是一个非常大的话题，我们对数据争论的报道将分为本章和第四章[](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)*，*聚合Pandas数据帧*。在本章中，我们将对数据争论进行概述，然后探索用于气候数据的**国家环境信息中心** ( **NCEI** ) API，并浏览使用`requests`库从其中收集温度数据的过程。然后，我们将讨论处理为一些初始分析和可视化准备数据的数据争论任务(我们将在 [*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*使用 Pandas 和 Matplotlib* 可视化数据，以及 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*使用 Seaborn 绘图和定制技术*中了解)。我们将在 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*聚合Pandas数据帧*中解决与聚合和组合数据集相关的数据争论的一些更高级的方面。*

 *在本章中，我们将讨论以下主题:

*   了解数据争论
*   探索一个 API 来查找和收集温度数据
*   清理数据
*   重塑数据
*   处理重复、缺失或无效的数据

# 章节材料

本章的材料可以在 GitHub 上找到，网址是[https://GitHub . com/stef molin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch _ 03](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03)。我们将学习五个笔记本，每个笔记本根据使用时间进行编号，还有两个目录`data/`和`exercises/`，分别包含上述笔记本和章节结束练习所需的所有 CSV 文件。以下文件在`data/`目录下:

![Figure 3.1 – Breakdown of the datasets used in this chapter
](img/Figure_3.1_B16834.jpg)

图 3.1-本章中使用的数据集的分类

我们将在`1-wide_vs_long.ipynb`笔记本中开始讨论宽格式和长格式数据。然后，我们将从 https://www.ncdc.noaa.gov/cdo-web/webservices/v2 的[的`2-using_the_weather_api.ipynb`笔记本中收集 NCEI API 的每日温度数据。我们将使用的**全球历史气候网络-每日** ( **GHCND** )数据集的](https://www.ncdc.noaa.gov/cdo-web/webservices/v2)文档可以在[https://www1 . ncdc . NOAA . gov/pub/data/CDO/documentation/GH CND _ documentation . pdf](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf)中找到。

重要说明

号 NCEI 隶属于**号国家海洋和大气管理局** ( **号国家海洋和大气管理局**)。如 API 的 URL 所示，该资源是在 NCEI 被称为 NCDC 时创建的。如果这个资源的 URL 在将来发生变化，搜索 *NCEI 天气 API* 找到更新的那个。

在`3-cleaning_data.ipynb`笔记本中，我们将学习如何对温度数据和一些金融数据执行第一轮清理，这些数据是使用我们将在 [*第 7 章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析-比特币和股票市场*中构建的`stock_analysis`包收集的。之后，我们将通过各种方式重塑`4-reshaping_data.ipynb`笔记本中的数据。最后，在`5-handling_data_issues.ipynb`笔记本中，我们将学习一些使用`data/dirty_data.csv`中的脏数据来处理重复、缺失或无效数据的策略。该文本将指示何时在笔记本之间切换。

# 了解数据争论

像任何专业领域一样，数据分析充满了行话，新手通常很难理解行话——本章的主题也不例外。当我们执行**数据争论**时，我们将输入数据从其原始状态中取出，并将其放入一种我们可以对其执行有意义的分析的格式中。**数据处理**是指这一过程的另一种方式。没有设定的操作列表；唯一的目标是，数据后争论对我们来说比我们开始时更有用。在实践中，数据争论过程涉及三个常见任务:

*   数据清理
*   数据转换
*   数据丰富

应该注意的是，这些任务没有固有的顺序，在整个数据争论过程中，我们很可能会多次执行每一项任务。这个想法带来了一个有趣的难题:如果我们需要争论我们的数据来为我们的分析做准备，难道不可以以这样一种方式争论，我们告诉数据说什么，而不是我们学习它在说什么？

"如果你折磨数据足够长的时间，它会承认任何事情."

——罗纳德·科斯，诺贝尔经济学奖获得者

那些与数据打交道的人会发现，通过操纵数据来歪曲事实是非常容易的。但是，我们有责任尽最大努力避免欺骗，牢记我们的行为对数据完整性的影响，并向使用我们分析的人解释我们得出结论的过程，以便他们也可以做出自己的判断。

## 数据清理

一旦收集了我们的数据，将它放入`DataFrame`对象中，并使用了我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*中讨论过的处理Pandas数据帧*的技巧，为了熟悉数据，我们将需要执行一些数据清理。最初一轮的数据清理通常会给我们提供开始研究数据所需的最少数据。需要掌握的一些基本数据清理任务包括:

*   重新命名
*   排序和重新排序
*   数据类型转换
*   处理重复数据
*   解决缺失或无效数据的问题
*   过滤到所需的数据子集

数据清理是数据争论的最佳起点，因为将数据存储为正确的数据类型和易于引用的名称将为探索开辟许多途径，例如汇总统计、排序和过滤。由于我们已经在第二章 、*中介绍了过滤处理Pandas数据帧*，我们将在本章中重点讨论前面列表中的其他主题。

## 数据转换

通常，在一些初始数据清理之后，我们将进入数据转换阶段，但是完全有可能我们的数据集在当前的状态下是不可用的，我们必须在尝试进行任何数据清理之前对其进行重组。在**数据转换**中，我们专注于改变我们的数据结构，以便于我们的下游分析；这通常包括更改哪些数据沿着行，哪些数据沿着列。

我们将找到的大多数数据要么是**宽格式**要么是**长格式**；这些格式各有千秋，知道我们需要哪种格式进行分析很重要。通常，人们会以宽格式记录和呈现数据，但也有某些可视化要求数据为长格式:

![Figure 3.2 – (Left) wide format versus (right) long format
](img/Figure_3.2_B16834.jpg)

图 3.2—(左)宽格式与(右)长格式

宽格式是分析和数据库设计的首选，而长格式被认为是糟糕的设计，因为每一列都应该是自己的数据类型，并且具有单一的含义。然而，在向关系数据库的表中添加新字段(或删除旧字段)的情况下，数据库的维护人员可能会决定使用长格式，而不是每次都要修改所有的表。这允许他们为数据库用户提供固定的模式，同时能够根据需要更新数据库中包含的数据。构建 API 时，如果需要灵活性，可以选择长格式。也许 API 会提供一种通用的响应格式(例如，日期、字段名称和字段值),可以支持数据库中的各种表。这可能也与使响应更容易形成有关，这取决于数据如何存储在 API 使用的数据库中。因为我们会发现这两种格式的数据，所以了解如何使用这两种格式并从一种格式转换到另一种格式很重要。

现在，让我们导航到`1-wide_vs_long.ipynb`笔记本来看看一些例子。首先，我们将导入`pandas`和`matplotlib`(为了帮助说明每种格式在可视化方面的优缺点，我们将在 [*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*用 Pandas 和 Matplotlib* 可视化数据，以及 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*用 Seaborn 绘图和定制技术*中讨论)并读入包含宽而长格式数据的 CSV 文件:

```py
>>> import matplotlib.pyplot as plt
>>> import pandas as pd
>>> wide_df = \
...     pd.read_csv('data/wide_data.csv', parse_dates=['date'])
>>> long_df = pd.read_csv(
...     'data/long_data.csv', 
...     usecols=['date', 'datatype', 'value'], 
...     parse_dates=['date']
... )[['date', 'datatype', 'value']] # sort columns
```

### 宽数据格式

对于宽格式数据，我们用它们自己的列来表示变量的测量值，并且每一行表示那些变量的一个观察值。这使得我们可以很容易地跨观测值比较变量，获得汇总统计数据，执行操作，并呈现我们的数据；但是，有些可视化不支持这种数据格式，因为它们可能依赖长格式来拆分、调整和/或着色绘图内容。

让我们来看看`wide_df`中宽幅数据的前六项观察结果:

```py
>>> wide_df.head(6)
```

每一列包含以摄氏度为单位的特定温度数据类别的前六个观测值—最高温度(**【TMAX】**)、最低温度(**【TMIN】**)和观测时的温度(**)—以每日频率:**

**![Figure 3.3 – Wide format temperature data
](img/Figure_3.3_B16834.jpg)

图 3.3–宽格式温度数据

当处理宽格式数据时，我们可以通过使用`describe()`方法轻松地获取该数据的汇总统计信息。请注意，虽然旧版本的`pandas`将`datetimes`视为分类，但是`pandas`正朝着将它们视为数字的方向发展，因此我们通过`datetime_is_numeric=True`来隐藏警告:

```py
>>> wide_df.describe(include='all', datetime_is_numeric=True)
```

我们几乎不需要做任何努力，就可以获得日期、最高温度、最低温度和观测时温度的汇总统计数据:

![Figure 3.4 – Summary statistics for the wide format temperature data
](img/Figure_3.4_B16834.jpg)

图 3.4–宽格式温度数据的汇总统计

正如我们之前讨论过的，上表中的汇总数据易于获取且信息丰富。这种格式也可以很容易地用`pandas`来绘制，只要我们准确地告诉它我们想要绘制的内容:

```py
>>> wide_df.plot(
...     x='date', y=['TMAX', 'TMIN', 'TOBS'], figsize=(15, 5),
...     title='Temperature in NYC in October 2018' 
... ).set_ylabel('Temperature in Celsius')
>>> plt.show()
```

Pandas 将每日最高温度、最低温度和观察时的温度绘制为单线图中自己的线:

![Figure 3.5 – Plotting the wide format temperature data
](img/Figure_3.5_B16834.jpg)

图 3.5–绘制宽格式温度数据

重要说明

现在不要担心理解可视化代码；这里只是为了说明每种数据格式如何使某些任务变得更容易或更困难。我们将在第五章 、*中用`pandas`和`matplotlib`介绍可视化，用 Pandas 和 Matplotlib* 可视化数据。

### 长数据格式

长格式数据将为每个变量的观察值占一行；这意味着，如果我们每天测量三个变量，那么我们每天将有三行记录观察结果。长格式设置可以通过将变量列名转换为单个列来实现，其中数据是变量名，并将它们的值放在单独的列中。

我们可以查看`long_df`中长格式数据的前六行，以了解宽格式和长格式数据之间的区别:

```py
>>> long_df.head(6)
```

注意，现在每个日期有三个条目，并且**数据类型**列告诉我们**值**列中的数据是该行的什么:

![Figure 3.6 – Long format temperature data
](img/Figure_3.6_B16834.jpg)

图 3.6–长格式温度数据

如果我们试图获得汇总统计数据，就像我们对宽格式数据所做的那样，结果并不那么有用:

```py
>>> long_df.describe(include='all', datetime_is_numeric=True)
```

**值**列显示的是汇总统计数据，但这是汇总每天的最高温度、最低温度和观测时的温度。最大值将是每日最高温度的最大值，最小值将是每日最低温度的最小值。这意味着这些汇总数据没有太大帮助:

![Figure 3.7 – Summary statistics for the long format temperature data
](img/Figure_3.7_B16834.jpg)

图 3.7–长格式温度数据的汇总统计

这种格式不太容易理解，当然也不应该是我们呈现数据的方式；然而，它使得创建可视化变得容易，我们的绘图库可以根据变量名给线着色，根据某个变量的值确定点的大小，并为刻面执行分割。Pandas 希望其用于绘图的数据是宽格式的，因此，为了轻松地制作与我们使用宽格式数据相同的绘图，我们必须使用另一个名为`seaborn`的绘图库，我们将在 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*使用 Seaborn 和定制技术进行绘图*中介绍:

```py
>>> import seaborn as sns
>>> sns.set(rc={'figure.figsize': (15, 5)}, style='white')
>>> ax = sns.lineplot(
...     data=long_df, x='date', y='value', hue='datatype'
... )
>>> ax.set_ylabel('Temperature in Celsius')
>>> ax.set_title('Temperature in NYC in October 2018')
>>> plt.show()
```

Seaborn 可以基于`datatype`列进行子集划分，为我们提供每日最高温度、最低温度和观测时的温度的单独线条:

![Figure 3.8 – Plotting the long format temperature data
](img/Figure_3.8_B16834.jpg)

图 3.8–绘制长格式温度数据

Seaborn 让我们指定用于`hue`的列，根据温度类型给*图 3.8* 中的线条着色。然而，我们并不局限于此；使用长格式数据，我们可以轻松地将我们的图分成小平面:

```py
>>> sns.set(
...     rc={'figure.figsize': (20, 10)},
...     style='white', font_scale=2 
... )
>>> g = sns.FacetGrid(long_df, col='datatype', height=10)
>>> g = g.map(plt.plot, 'date', 'value')
>>> g.set_titles(size=25)
>>> g.set_xticklabels(rotation=45)
>>> plt.show()
```

Seaborn 可以使用长格式数据为`datatype`列中的每个不同值创建子图:

![Figure 3.9 – Plotting subsets of the long format temperature data
](img/Figure_3.9_B16834.jpg)

图 3.9–绘制长格式温度数据的子集

重要说明

虽然使用支线剧情有可能创造出类似于上一部的`pandas`和`matplotlib`的剧情，但是更复杂的层面组合会让使用`seaborn`变得更加容易。我们将在第六章[*`seaborn`、*用 Seaborn 和定制技术*绘图。*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)

在的*重塑数据*部分，我们将介绍如何通过熔化将数据从宽格式转换为长格式，以及通过透视将数据从长格式转换为宽格式。此外，我们将学习如何转置数据，即翻转列和行。

## 数据丰富

一旦我们以分析所需的格式整理了数据，我们可能会发现需要对数据进行一点充实。**数据丰富**通过某种方式增加数据来提高数据的质量。这个过程在建模和机器学习中变得非常重要，它构成了**特征工程**过程的一部分(我们将在 [*第 10 章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217) 、*做出更好的预测——优化模型*中触及)。

当我们想要丰富我们的数据时，我们可以**将**新数据与原始数据合并(通过追加新的行或列),或者使用原始数据创建新数据。以下是使用原始数据增强我们的数据的方法:

*   **添加新列**:使用函数对现有列中的数据进行来创建新值。
*   **宁滨**:将连续数据或具有许多不同值的离散数据变成桶，这使得列离散，同时让我们控制列中可能值的数量。
*   **汇总**:汇总数据并汇总。
*   **重采样**:以特定的间隔聚合时间序列数据。

既然我们了解了什么是数据争论，那么让我们收集一些数据来处理。请注意，我们将在本章中介绍数据清理和转换，而数据丰富将在第四章[](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)*、*聚合Pandas数据帧*中介绍。*

 *# 探索一个 API 来查找和收集温度数据

在 [*第二章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*处理Pandas数据帧*中，我们处理了数据收集以及  to 如何执行初始检查和数据过滤；这个通常给我们一些在我们进一步分析之前需要解决的事情的想法。因为本章是建立在这些技能之上的，所以我们也将在这里练习其中的一些。首先，我们将从探索 NCEI 提供的天气 API 开始。然后，在下一节中，我们将了解使用以前从该 API 获得的温度数据的数据争论。

重要说明

要使用 NCEI API，您需要填写您的电子邮件地址:[https://www.ncdc.noaa.gov/cdo-web/token](https://www.ncdc.noaa.gov/cdo-web/token)来申请一个令牌。

在本节中，我们将在`2-using_the_weather_api.ipynb`笔记本中向NCEI API 请求温度数据。正如我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) ，*使用Pandas数据帧*中所了解的，我们可以使用`requests`库与 API 进行交互。在下面的代码块中，我们导入了`requests`库并创建了一个方便的函数，用于向特定的端点发出请求，同时发送我们的令牌。要使用这个函数，我们需要提供一个标记，如粗体所示:

```py
>>> import requests
>>> def make_request(endpoint, payload=None):
...     """
...     Make a request to a specific endpoint on the 
...     weather API passing headers and optional payload.
...     Parameters:
...         - endpoint: The endpoint of the API you want to 
...                     make a GET request to.
...         - payload: A dictionary of data to pass along 
...                    with the request.
...     
...     Returns:
...         A response object.
...     """
...     return requests.get(
...         'https://www.ncdc.noaa.gov/cdo-web/'
...         f'api/v2/{endpoint}',
...         headers={'token': 'PASTE_YOUR_TOKEN_HERE'},
...         params=payload
...     )
```

小费

这个函数是利用 **f 字符串**，这是在 Python 3.6 中引入的；与使用`format()`方法相比，它们提高了代码的可读性，减少了冗长。

要使用`make_request()`函数，我们需要学习如何形成我们的请求。NCEI 有一个有用的入门页面([https://www . ncdc . NOAA . gov/CDO-web/web services/v2 # getting started](https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted))，向我们展示如何形成请求；我们可以通过页面上的标签来找出我们想要的查询过滤器。`requests`库负责将我们的搜索参数字典(作为`payload`传入)转换成追加到最终 URL 的**查询字符串**(例如，如果我们为`start`传递`2018-08-28`，为`end`传递`2019-04-15`，我们将得到`?start=2018-08-28&end=2019-04-15`，就像网站上的例子一样。这个 API 提供了许多不同的端点，用于探索所提供的内容和构建我们对实际数据集的最终请求。我们将首先使用`datasets`端点计算出我们想要查询的数据集的 ID(`datasetid`)。让我们检查一下哪些数据集的数据在 2018 年 10 月 1 日到今天的日期范围内:

```py
>>> response = \
...     make_request('datasets', {'startdate': '2018-10-01'})
```

记住，我们检查了`status_code`属性以确保请求成功。或者，如果一切按预期进行，我们可以使用`ok`属性获得一个布尔指示器:

```py
>>> response.status_code
200
>>> response.ok
True
```

小费

API 限制我们每秒 5 个请求，每天 10，000 个请求。如果我们超过这些限制，状态代码将指示客户端错误(意味着错误似乎是由我们造成的)。客户端错误的状态代码为 400s 例如，404，如果请求的资源找不到，或者 400，如果服务器不能理解我们的请求(或者拒绝处理它)。有时，服务器在处理我们的请求时会出现问题，在这种情况下，我们会看到 500 的状态代码。你可以在[https://restfulapi.net/http-status-codes/](https://restfulapi.net/http-status-codes/)找到常见状态代码及其含义的列表。

一旦我们得到了响应，我们就可以使用`json()`方法来获得有效载荷。然后，我们可以使用字典方法来确定我们想要查看的部分:

```py
>>> payload = response.json()
>>> payload.keys()
dict_keys(['metadata', 'results'])
```

JSON 有效负载的`metadata`部分告诉我们关于结果的信息，而`results`部分包含实际的结果。让我们看看我们得到了多少数据，这样我们就知道我们是否可以打印结果，或者我们是否应该尝试限制输出:

```py
>>> payload['metadata']
{'resultset': {'offset': 1, 'count': 11, 'limit': 25}}
```

我们得到了 11 行，所以让我们看看 JSON 有效负载的`results`部分有哪些字段。`results`键包含一个字典列表。如果我们选择第一个，我们可以查看键，看看数据包含哪些字段。然后，我们可以将输出减少到我们关心的字段:

```py
>>> payload['results'][0].keys()
dict_keys(['uid', 'mindate', 'maxdate', 'name', 
           'datacoverage', 'id'])
```

对于我们的目的，我们希望查看数据集的 id 和名称，所以让我们使用列表理解来只查看那些:

```py
>>> [(data['id'], data['name']) for data in payload['results']]
[('GHCND', 'Daily Summaries'),
 ('GSOM', 'Global Summary of the Month'),
 ('GSOY', 'Global Summary of the Year'),
 ('NEXRAD2', 'Weather Radar (Level II)'),
 ('NEXRAD3', 'Weather Radar (Level III)'),
 ('NORMAL_ANN', 'Normals Annual/Seasonal'),
 ('NORMAL_DLY', 'Normals Daily'),
 ('NORMAL_HLY', 'Normals Hourly'),
 ('NORMAL_MLY', 'Normals Monthly'),
 ('PRECIP_15', 'Precipitation 15 Minute'),
 ('PRECIP_HLY', 'Precipitation Hourly')]
```

结果中的第一个条目就是我们要找的。现在我们有了`datasetid` ( `GHCND`)的值，我们继续为`datacategoryid`确定一个值，我们需要它来请求温度数据。我们使用`datacategories`端点来实现。这里，我们可以打印 JSON 有效负载，因为它并不大(只有 9 个条目):

```py
>>> response = make_request(
...     'datacategories', payload={'datasetid': 'GHCND'}
... )
>>> response.status_code
200
>>> response.json()['results']
[{'name': 'Evaporation', 'id': 'EVAP'},
 {'name': 'Land', 'id': 'LAND'},
 {'name': 'Precipitation', 'id': 'PRCP'},
 {'name': 'Sky cover & clouds', 'id': 'SKY'},
 {'name': 'Sunshine', 'id': 'SUN'},
 {'name': 'Air Temperature', 'id': 'TEMP'},
 {'name': 'Water', 'id': 'WATER'},
 {'name': 'Wind', 'id': 'WIND'},
 {'name': 'Weather Type', 'id': 'WXTYPE'}]
```

基于前面的结果，我们知道我们想要`datacategoryid`的值`TEMP`。接下来，我们通过使用`datatypes`端点来识别我们想要的数据类型。我们将再次使用 list comprehension 来仅打印姓名和 id；这仍然是一个相当大的列表，所以输出被简化为:

```py
>>> response = make_request(
...     'datatypes', 
...     payload={'datacategoryid': 'TEMP', 'limit': 100}
... )
>>> response.status_code
200
>>> [(datatype['id'], datatype['name'])
...  for datatype in response.json()['results']]
[('CDSD', 'Cooling Degree Days Season to Date'),
 ...,
 ('TAVG', 'Average Temperature.'),
 ('TMAX', 'Maximum temperature'),
 ('TMIN', 'Minimum temperature'),
 ('TOBS', 'Temperature at the time of observation')]
```

我们正在寻找`TAVG`、`TMAX`和`TMIN`数据类型。现在我们已经拥有了请求所有位置的温度数据所需的一切，我们需要将范围缩小到一个特定的位置。为了确定`locationcategoryid`的值，我们必须使用`locationcategories`端点:

```py
>>> response = make_request(
...     'locationcategories', payload={'datasetid': 'GHCND'}
... )
>>> response.status_code
200
```

请注意，我们可以使用来自 Python 标准库的`pprint`(【https://docs.python.org/3/library/pprint.html】的)以更易读的格式打印我们的 JSON 有效负载:

```py
>>> import pprint
>>> pprint.pprint(response.json())
{'metadata': {
     'resultset': {'count': 12, 'limit': 25, 'offset': 1}},
 'results': [{'id': 'CITY', 'name': 'City'},
             {'id': 'CLIM_DIV', 'name': 'Climate Division'},
             {'id': 'CLIM_REG', 'name': 'Climate Region'},
             {'id': 'CNTRY', 'name': 'Country'},
             {'id': 'CNTY', 'name': 'County'},
             ...,
             {'id': 'ST', 'name': 'State'},
             {'id': 'US_TERR', 'name': 'US Territory'},
             {'id': 'ZIP', 'name': 'Zip Code'}]}
```

我们想看看纽约市，所以对于`locationcategoryid`过滤器来说，`CITY`是合适的值。我们正在使用的笔记本有一个在 API 上使用**二分搜索法**按名称搜索字段的功能；二分搜索法是通过有序列表搜索的一种更有效的方式。因为我们知道字段可以按字母顺序排序，并且 API 给我们关于请求的元数据，所以我们知道 API 对于给定的字段有多少项，并且可以判断我们是否已经通过了我们正在寻找的项。

对于每个请求，我们抓取中间的条目，并将在字母表中的位置与我们的目标进行比较；如果结果比我们的目标提前，我们会查看比我们刚刚得到的数据多的那一半数据；否则，我们看较小的一半。每一次，我们都将数据切成两半，所以当我们抓住中间的条目进行测试时，我们正在向我们寻求的值靠近(参见*图 3.10* ):

```py
>>> def get_item(name, what, endpoint, start=1, end=None):
...     """
...     Grab the JSON payload using binary search.
... 
...     Parameters:
...         - name: The item to look for.
...         - what: Dictionary specifying what item `name` is.
...         - endpoint: Where to look for the item.
...         - start: The position to start at. We don't need
...           to touch this, but the function will manipulate
...           this with recursion.
...         - end: The last position of the items. Used to 
...           find the midpoint, but like `start` this is not 
...           something we need to worry about.
... 
...     Returns: Dictionary of the information for the item 
...              if found, otherwise an empty dictionary.
...     """
...     # find the midpoint to cut the data in half each time 
...     mid = (start + (end or 1)) // 2
...     
...     # lowercase the name so this is not case-sensitive
...     name = name.lower()
...     # define the payload we will send with each request
...     payload = {
...         'datasetid': 'GHCND', 'sortfield': 'name',
...         'offset': mid, # we'll change the offset each time
...         'limit': 1 # we only want one value back
...     }
...     
...     # make request adding additional filters from `what`
...     response = make_request(endpoint, {**payload, **what})
...     
...     if response.ok:
...         payload = response.json()
...     
...         # if ok, grab the end index from the response 
...         # metadata the first time through
...         end = end or \
...             payload['metadata']['resultset']['count']
...         
...         # grab the lowercase version of the current name
...         current_name = \
...             payload['results'][0]['name'].lower()  
...
...         # if what we are searching for is in the current 
...         # name, we have found our item
...         if name in current_name:
...             # return the found item
...             return payload['results'][0] 
...         else:
...             if start >= end: 
...                 # if start index is greater than or equal
...                 # to end index, we couldn't find it
...                 return {}
...             elif name < current_name:
...                 # name comes before the current name in the 
...                 # alphabet => search further to the left
...                 return get_item(name, what, endpoint, 
...                                 start, mid - 1)
...             elif name > current_name:
...                 # name comes after the current name in the 
...                 # alphabet => search further to the right
...                 return get_item(name, what, endpoint,
...                                 mid + 1, end) 
...     else:
...         # response wasn't ok, use code to determine why
...         print('Response not OK, '
...               f'status: {response.status_code}')
```

这是算法的一个**递归**实现，意思是我们从内部调用函数本身；当我们这样做时，我们必须非常小心地定义一个基础条件，这样它最终会停止，而不会进入一个无限循环。这可以迭代地实现。关于二分搜索法和**递归**的附加阅读，参见本章末尾的*进一步阅读*部分。

重要说明

在二分搜索法的传统实现中，找到我们正在搜索的列表的长度是很简单的事情。使用 API，我们必须发出一个请求来获取计数；因此，我们必须要求第一个条目(偏移量为 1)来确定自己的方向。这意味着，如果我们在开始之前就知道列表中有多少个位置，那么与我们所需要的相比，我们在这里做了一个额外的请求。

现在，让我们使用二分搜索法实现来查找纽约市的 ID，这将是我们在后续查询中用于`locationid`的值:

```py
>>> nyc = get_item(
...     'New York', {'locationcategoryid': 'CITY'}, 'locations'
... )
>>> nyc
{'mindate': '1869-01-01',
 'maxdate': '2021-01-14',
 'name': 'New York, NY US',
 'datacoverage': 1,
 'id': 'CITY:US360019'}
```

通过在这里使用二分搜索法，我们在 **8** 请求中找到**纽约**，尽管接近 1983 个条目的中间！相比之下，使用线性搜索，我们在找到它之前会查看 1254 个条目。在下图中，我们可以看到二分搜索法如何系统地消除位置列表中的部分，这在数字线上用黑色表示(白色表示所需值仍有可能在该部分中):

![Figure 3.10 – Binary search to find New York City
](img/Figure_3.10_B16834.jpg)

图 3.10-二分搜索法查找纽约市

小费

一些 API(比如 NCEI API)限制了我们在特定时间段内可以发出的请求数量，所以明智地处理我们的请求是很重要的。当搜索一个很长的有序列表时，想想二分搜索法。

可选地，我们可以深入到正在收集数据的站的 ID。这个是最细粒度的级别。再次使用二分搜索法，我们可以获取中央公园站的站 ID:

```py
>>> central_park = get_item(
...     'NY City Central Park',
...     {'locationid': nyc['id']}, 'stations'
... )
>>> central_park
{'elevation': 42.7,
 'mindate': '1869-01-01',
 'maxdate': '2020-01-13',
 'latitude': 40.77898,
 'name': 'NY CITY CENTRAL PARK, NY US',
 'datacoverage': 1,
 'id': 'GHCND:USW00094728',
 'elevationUnit': 'METERS',
 'longitude': -73.96925}
```

现在，让我们请求从中央公园记录的纽约市 2018 年 10 月的摄氏温度数据。为此，我们将使用`data`端点，并提供我们在探索 API 的过程中获得的所有参数:

```py
>>> response = make_request(
...     'data', 
...     {'datasetid': 'GHCND',
...      'stationid': central_park['id'],
...      'locationid': nyc['id'],
...      'startdate': '2018-10-01',
...      'enddate': '2018-10-31',
...      'datatypeid': ['TAVG', 'TMAX', 'TMIN'],
...      'units': 'metric',
...      'limit': 1000}
... )
>>> response.status_code
200
```

最后，我们将创建一个`DataFrame`对象；由于JSON 有效载荷的`results`部分是一个字典列表，我们可以将其直接传递给`pd.DataFrame()`:

```py
>>> import pandas as pd
>>> df = pd.DataFrame(response.json()['results'])
>>> df.head()
```

我们得到长格式的数据。**数据类型**列是被测量的温度变量，**值**列包含测量的温度:

![Figure 3.11 – Data retrieved from the NCEI API
](img/Figure_3.11_B16834.jpg)

图 3.11-从 NCEI API 检索的数据

小费

我们可以使用前面的代码将我们在本节中处理的任何 JSON 响应转换成一个`DataFrame`对象，如果我们发现这样更容易处理的话。然而，应该强调的是，就 API 而言，JSON 有效负载几乎无处不在(作为 Python 用户，我们应该熟悉类似字典的对象)，所以熟悉它们不会有什么坏处。

我们要求`TAVG`、`TMAX`和`TMIN`，但是注意我们没有得到`TAVG`。这是因为中央公园站没有记录平均温度，尽管在 API 中被列为提供平均温度——真实世界的数据是不干净的:

```py
>>> df.datatype.unique()
array(['TMAX', 'TMIN'], dtype=object)
>>> if get_item(
...     'NY City Central Park', 
...     {'locationid': nyc['id'], 'datatypeid': 'TAVG'}, 
...     'stations'
... ):
...     print('Found!')
Found!
```

B 计划的时间到了:让我们用拉瓜迪亚机场代替中央公园作为本章剩余部分的车站。或者，我们可以获取覆盖纽约市的所有站点的数据；然而，由于这将为我们提供每天多个条目的一些温度测量，我们不会做所以在这里-我们将需要的技能将涵盖在 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*汇总Pandas数据帧*中，以处理这些数据。

从 LaGuardia 机场站收集天气数据的过程与从中央公园站收集数据的过程相同，但为了简洁起见，我们将在下一个笔记本中讨论清理数据时读入 LaGuardia 的数据。请注意，当前笔记本底部的单元格包含用于收集这些数据的代码。

# 清洗数据

让我们转到`3-cleaning_data.ipynb`笔记本来讨论数据清理。像往常一样，我们将开始导入`pandas`并读入我们的数据。对于本节，我们将使用`nyc_temperatures.csv`文件，其中包含 2018 年 10 月纽约市拉瓜迪亚机场站的最高日温度(`TMAX`)、最低日温度(`TMIN`)和平均日温度(`TAVG`):

```py
>>> import pandas as pd
>>> df = pd.read_csv('data/nyc_temperatures.csv')
>>> df.head()
```

我们从 API 中检索长格式数据；对于我们的分析，我们需要宽格式数据，但是我们将在本章后面的*透视数据框架*部分中解决这个问题:

![Figure 3.12 – NYC temperature data
](img/Figure_3.12_B16834.jpg)

图 3.12–纽约市温度数据

现在，我们将集中于对数据做一些小的调整，使我们更容易使用:重命名列，将每一列转换成最合适的数据类型，排序和重新索引。通常，这将是过滤数据的时候，但是我们是在从 API 请求数据时这样做的；关于使用`pandas`过滤的回顾，请参考第二章 、*使用Pandas数据帧*。

## 重命名列

因为我们使用的 API 端点可以返回任何单位和类别的数据，所以它必须调用那个列`value`。我们只提取了摄氏温度的数据，所以我们所有的观察都有相同的单位。这意味着我们可以重命名`value`列，以便清楚我们正在处理的数据:

```py
>>> df.columns
Index(['date', 'datatype', 'station', 'attributes', 'value'],
      dtype='object')
```

`DataFrame`类有一个`rename()`方法，该方法使用一个字典将旧的列名映射到新的列名。除了重命名`value`列之外，让我们将`attributes`列重命名为`flags`，因为 API 文档提到该列包含有关数据收集信息的标志:

```py
>>> df.rename(
...     columns={'value': 'temp_C', 'attributes': 'flags'},
...     inplace=True
... )
```

大多数时候，`pandas`会返回一个新的`DataFrame`对象；然而，由于我们传入了`inplace=True`，我们的原始数据帧被更新了。始终小心就地操作，因为它们可能很难或不可能撤消。我们的专栏现在有了新名称:

```py
>>> df.columns
Index(['date', 'datatype', 'station', 'flags', 'temp_C'], 
      dtype='object')
```

小费

也可以使用`rename()`方法重命名`Series`和`Index`对象。只需传入新名称。例如，如果我们有一个名为`temperature`的`Series`对象，我们想将其重命名为`temp_C`，我们可以运行`temperature.rename('temp_C')`。该变量仍将被称为`temperature`，但序列本身的数据名称现在将为`temp_C`。

我们也可以用`rename()`对列名进行转换。例如，我们可以将所有列名都大写:

```py
>>> df.rename(str.upper, axis='columns').columns
Index(['DATE', 'DATATYPE', 'STATION', 'FLAGS', 'TEMP_C'], 
      dtype='object')
```

这个方法甚至可以让我们重命名索引的值，尽管我们现在还没有用到它，因为我们的索引只是数字。然而，作为参考，我们可以简单地将前面代码中的`axis='columns'`改为`axis='rows'`。

## 类型转换

现在列名表明了它们包含的数据，我们可以检查它们包含什么类型的数据。当我们之前用`head()`方法检查数据帧时，在看了前几行之后，我们应该已经形成了数据类型应该是什么的直觉。对于类型转换，我们的目标是使当前的数据类型与我们认为它们应该是什么相一致；我们将改变数据的表示方式。

注意，有时候，我们可能有我们认为应该是某种类型的数据，比如日期，但它是作为字符串存储的；这可能是一个非常合理的原因—数据可能丢失。在丢失编码为文本的数据的情况下(例如，`?`或`N/A` ), `pandas`将在读入该数据时将其存储为字符串。当我们在数据帧上使用`dtypes`属性时，它将被标记为`object`。如果我们试图转换(或**转换**)这些列，我们要么会得到一个错误，要么我们的结果将不是我们所期望的。例如，如果我们有十进制数的字符串，但是试图将该列转换成整数，我们将得到一个错误，因为 Python 知道它们不是整数；但是，如果我们试图将十进制数转换成整数，我们将丢失小数点后的任何信息。

也就是说，让我们检查一下温度数据中的数据类型。请注意，`date`列实际上并没有存储为日期时间:

```py
>>> df.dtypes
date         object
datatype     object
station      object
flags        object 
temp_C      float64
dtype: object
```

我们可以使用`pd.to_datetime()`函数将其转换成日期时间:

```py
>>> df.loc[:,'date'] = pd.to_datetime(df.date)
>>> df.dtypes 
date        datetime64[ns] 
datatype            object
station             object
flags               object 
temp_C             float64
dtype: object
```

这样好多了。现在，当我们总结`date`列时，我们可以获得有用的信息:

```py
>>> df.date.describe(datetime_is_numeric=True)
count                     93
mean     2018-10-16 00:00:00
min      2018-10-01 00:00:00
25%      2018-10-08 00:00:00
50%      2018-10-16 00:00:00
75%      2018-10-24 00:00:00
max      2018-10-31 00:00:00
Name: date, dtype: object
```

处理日期可能很棘手，因为它们有许多不同的格式和时区；幸运的是，`pandas`有更多的方法可以用来处理日期时间对象的转换。例如，当使用一个`DatetimeIndex`对象时，如果我们需要跟踪时区，我们可以使用`tz_localize()`方法将我们的日期时间与时区相关联:

```py
>>> pd.date_range(start='2018-10-25', periods=2, freq='D')\
...     .tz_localize('EST')
DatetimeIndex(['2018-10-25 00:00:00-05:00', 
               '2018-10-26 00:00:00-05:00'], 
              dtype='datetime64[ns, EST]', freq=None)
```

这也适用于索引类型为`DatetimeIndex`的`Series`和`DataFrame`对象。我们可以再次读入 CSV 文件，这一次，指定`date`列将是我们的索引，我们应该将 CSV 文件中的任何日期解析为 datetimes:

```py
>>> eastern = pd.read_csv(
...     'data/nyc_temperatures.csv',
...     index_col='date', parse_dates=True
... ).tz_localize('EST')
>>> eastern.head()
```

在这个例子中，我们不得不再次读入文件，因为我们还没有学会如何改变数据的索引(在本章后面的*重新排序、重新索引和排序数据*一节中讨论)。请注意，我们已经将东部标准时间偏移(-05:00，UTC)添加到索引中的日期时间:

![Figure 3.13 – Time zone-aware dates in the index
](img/Figure_3.13_B16834.jpg)

图 3.13–索引中的时区感知日期

我们可以使用`tz_convert()`方法将时区更改为不同的时区。让我们将数据转换为 UTC:

```py
>>> eastern.tz_convert('UTC').head()
```

现在，偏移量是 UTC (+00:00)，但是请注意，日期的时间部分现在是上午 5 点；这种转换考虑了-05:00 的偏移:

![Figure 3.14 – Converting data into another time zone
](img/Figure_3.14_B16834.jpg)

图 3.14–将数据转换成另一个时区

我们还可以用`to_period()`方法截断日期时间，如果我们不关心完整的日期，这个方法会很方便。例如，如果我们想按月聚合我们的数据，我们可以将我们的索引截断到月份和年份，然后执行聚合。由于我们将在第 4 章[](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)*、*聚合Pandas数据帧*中讨论聚合，我们将在此处进行截断。注意，我们首先移除时区信息，以避免来自`pandas`的警告，即`PeriodArray`类没有时区信息，因此它将会丢失。这是因为`PeriodIndex`对象的底层数据存储为`PeriodArray`对象:*

```py
>>> eastern.tz_localize(None).to_period('M').index
PeriodIndex(['2018-10', '2018-10', ..., '2018-10', '2018-10'],
            dtype='period[M]', name='date', freq='M')
```

我们可以使用`to_timestamp()`方法将我们的`PeriodIndex`对象转换成一个`DatetimeIndex`对象；但是，现在日期时间都从每月的第一天开始:

```py
>>> eastern.tz_localize(None)\
...     .to_period('M').to_timestamp().index
DatetimeIndex(['2018-10-01', '2018-10-01', '2018-10-01', ...,
               '2018-10-01', '2018-10-01', '2018-10-01'],
              dtype='datetime64[ns]', name='date', freq=None)
```

或者，我们可以使用`assign()`方法来处理任何类型转换，方法是将列名作为命名参数传递，并将它们的新值作为方法调用的参数值。实际上，这将更加有益，因为我们可以在一次调用中执行许多任务，并使用我们在该调用中创建的列来计算附加的列。例如，让我们将`date`列转换为 datetime，并添加一个新列来表示华氏温度(`temp_F`)。`assign()`方法返回一个新的`DataFrame`对象，所以如果我们想保留它，我们必须记住将它赋给一个变量。在这里，我们将创建一个新的。注意，我们最初的日期转换修改了列，因此，为了说明我们可以使用`assign()`，我们需要再次读取我们的数据:

```py
>>> df = pd.read_csv('data/nyc_temperatures.csv').rename(
...     columns={'value': 'temp_C', 'attributes': 'flags'}
... )
>>> new_df = df.assign(
...     date=pd.to_datetime(df.date),
...     temp_F=(df.temp_C * 9/5) + 32
... )
>>> new_df.dtypes
date        datetime64[ns] 
datatype            object
station             object
flags               object 
temp_C             float64
temp_F             float64
dtype: object
>>> new_df.head()
```

我们现在在`date`列和一个新列`temp_F`中有了日期时间:

![Figure 3.15 – Simultaneous type conversion and column creation
](img/Figure_3.15_B16834.jpg)

图 3.15–同时进行类型转换和列创建

此外，我们可以使用`astype()`方法一次转换一列。举个例子，假设我们只关心每个整数的温度，但是我们不希望四舍五入。在这种情况下，我们只是想去掉小数点后的信息。为了实现这一点，我们可以将浮点数转换为整数。这一次，我们将使用 **lambda 函数**(在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) ，*使用Pandas数据帧*中首次介绍)，它使得访问`temp_F`列来创建`temp_F_whole`列成为可能，尽管在我们调用`assign()`之前`df`没有这个列。将 lambda 函数与`assign()`一起使用非常常见(也非常有用):

```py
>>> df = df.assign(
...     date=lambda x: pd.to_datetime(x.date),
...     temp_C_whole=lambda x: x.temp_C.astype('int'),
...     temp_F=lambda x: (x.temp_C * 9/5) + 32,
...     temp_F_whole=lambda x: x.temp_F.astype('int')
... )
>>> df.head()
```

注意，如果使用 lambda 函数，我们可以引用刚刚创建的列。还有一点很重要，我们不必知道是将列转换成浮点数还是整数:我们可以使用`pd.to_numeric()`，如果它看到小数，就会将数据转换成浮点数。如果所有的数字都是整数，它们将被转换成整数(显然，如果数据根本不是数字，我们仍然会得到错误):

![Figure 3.16 – Creating columns with lambda functions
](img/Figure_3.16_B16834.jpg)

图 3.16–使用 lambda 函数创建列

最后，我们有两个列，其中的数据当前存储为字符串，可以用更好的方式表示这个数据集。`station`和`datatype`列分别只有一个和三个不同的值，这意味着我们没有有效地使用内存，因为我们将它们存储为字符串。我们可能会在进一步的分析中遇到问题。Pandas有能力将列定义为**分类**；在`pandas`和其他软件包中的某些统计操作将能够处理这些数据，提供有意义的统计数据，并正确地使用它们。分类变量可以取几个值中的一个；例如，血型是一个分类变量——人只能有 A、B、AB 或 o 中的一种。

回到温度数据，我们对于`station`列只有一个值，对于`datatype`列只有三个不同的值(`TAVG`、`TMAX`、`TMIN`)。我们可以使用`astype()`方法将它们分类，并查看汇总统计数据:

```py
>>> df_with_categories = df.assign(
...     station=df.station.astype('category'),
...     datatype=df.datatype.astype('category')
... )
>>> df_with_categories.dtypes 
date            datetime64[ns]
datatype              category
station               category
flags                   object
temp_C                 float64
temp_C_whole             int64
temp_F                 float64
temp_F_whole             int64
dtype: object
>>> df_with_categories.describe(include='category')
```

类别的汇总统计数据就像字符串的统计数据一样。我们可以看到非空条目的数量( **count** )、唯一值的数量( **unique** )、模式( **top** )以及模式的出现次数( **freq** ):

![Figure 3.17 – Summary statistics for the categorical columns
](img/Figure_3.17_B16834.jpg)

图 3.17–分类列的汇总统计数据

我们刚刚做的分类没有任何顺序，但是`pandas`支持这一点:

```py
>>> pd.Categorical(
...     ['med', 'med', 'low', 'high'], 
...     categories=['low', 'med', 'high'], 
...     ordered=True
... )
['med', 'med', 'low', 'high'] 
Categories (3, object): ['low' < 'med' < 'high']
```

当我们的数据框架中的列以适当的类型存储时，它为探索开辟了额外的途径，例如计算统计数据、聚合数据和对值进行排序。例如，根据我们的数据源，数字数据可能被表示为一个字符串，在这种情况下，尝试对值进行排序将按词汇对内容进行重新排序，这意味着结果可能是 1，10，11，2，而不是 1，2，10，11(数字排序)。类似地，如果我们用 YYYY-MM-DD 之外的格式将日期表示为字符串，对这些信息进行排序可能会导致非时间顺序；然而，通过用`pd.to_datetime()`转换日期字符串，我们可以按时间顺序对以任何格式提供的日期进行排序。类型转换可以根据数值和日期的值，而不是它们最初的字符串表示形式，对它们进行重新排序。

## 对数据进行重新排序、重新索引和排序

我们经常会发现需要根据一列或多列的值对数据进行排序。假设我们希望找到 2018 年 10 月纽约市气温最高的天；我们可以按照`temp_C`(或`temp_F`)列降序排列我们的值，并使用`head()`选择我们想要查看的天数。为此，我们可以使用`sort_values()`方法。让我们来看看前 10 天:

```py
>>> df[df.datatype == 'TMAX']\
...     .sort_values(by='temp_C', ascending=False).head(10)
```

这向我们显示，根据拉瓜迪亚站的数据，10 月 7 日和 10 月 10 日，气温达到了 2018 年 10 月份的最高值。我们在 10 月 2 日和 4 日、10 月 1 日和 9 日以及 10 月 5 日和 8 日之间也有联系，但是请注意这些日期并不总是排序的——10 日在 7 日之后，但是 4 日在 2 日之前:

![Figure 3.18 – Sorting the data to find the warmest days
](img/Figure_3.18_B16834.jpg)

图 3.18–对数据进行排序，找出最热的日子

`sort_values()`方法可以和列名列表一起使用来打破平局。在中提供列的顺序将决定的排序顺序，随后的每一列用于打破平局。例如，在打破僵局时，让我们确保日期按升序排序:

```py
>>> df[df.datatype == 'TMAX'].sort_values(
...     by=['temp_C', 'date'], ascending=[False, True] 
... ).head(10)
```

因为我们是按升序排序的，所以在出现平局的情况下，一年中较早的日期将在较晚的日期之上。请注意，尽管 10 月 2 日和 10 月 4 日的温度读数相同，但这两天的温度却高于 10 月 4 日:

![Figure 3.19 – Sorting the data with multiple columns to break ties
](img/Figure_3.19_B16834.jpg)

图 3.19-用多列排序数据以打破平局

小费

在`pandas`中，索引被绑定到行上——当我们删除行、过滤或做任何只返回部分行的事情时，我们的索引可能看起来乱序(正如我们在前面的例子中看到的)。目前，索引只是表示我们数据中的行号，所以我们可能有兴趣改变这些值，以便在索引`0`处有第一个条目。为了让`pandas`自动这样做，我们可以将`ignore_index=True`传递给`sort_values()`。

Pandas 还提供了查看排序值子集的额外方法；我们可以根据特定的标准使用`nlargest()`来获取具有最大值的`n`行，使用`nsmallest()`来获取`n`最小的行，而不需要事先对数据进行排序。两者都接受列名列表或单个列的字符串。这次让我们抓住平均温度最高的 10 天:

```py
>>> df[df.datatype == 'TAVG'].nlargest(n=10, columns='temp_C')
```

10 月(平均而言)是最热的一天:

![Figure 3.20 – Sorting to find the 10 warmest days on average
](img/Figure_3.20_B16834.jpg)

图 3.20–排序以找到平均最热的 10 天

我们不局限于对价值进行排序;如果我们愿意，我们甚至可以按字母顺序对列进行排序，并按索引值对行进行排序。对于这些任务，我们可以使用`sort_index()`方法。默认情况下，`sort_index()`将把行作为目标，这样我们就可以在执行一个打乱索引的操作后对索引进行排序。例如，`sample()`方法会给我们随机选择的行，这将导致一个混乱的索引，所以我们可以使用`sort_index()`对它们进行排序:

```py
>>> df.sample(5, random_state=0).index
Int64Index([2, 30, 55, 16, 13], dtype='int64')
>>> df.sample(5, random_state=0).sort_index().index
Int64Index([2, 13, 16, 30, 55], dtype='int64')
```

小费

如果我们需要`sample()`的结果是可重现的，我们可以传入一个**种子**，设置为我们选择的数字(使用`random_state`参数)。种子初始化伪随机数发生器，因此，如果使用相同的种子，结果将是相同的。

当我们想要定位列时，我们必须传入`axis=1`；行将是默认的(`axis=0`)。注意，这个参数出现在许多`pandas`方法和函数中(包括`sample()`，所以理解它的意思很重要。让我们利用这些知识按字母顺序对数据框中的列进行排序:

```py
>>> df.sort_index(axis=1).head()
```

当使用`loc[]`时，让我们的列按字母顺序排列会给带来便利，因为我们可以指定一系列具有相似名称的列；例如，我们现在可以使用`df.loc[:,'station':'temp_F_whole']`轻松获取我们所有的温度列，以及站点信息:

![Figure 3.21 – Sorting the columns by name
](img/Figure_3.21_B16834.jpg)

图 3.21–按名称对列进行排序

重要说明

`sort_index()`和`sort_values()`都返回新的`DataFrame`对象。我们必须传入`inplace=True`来更新我们正在处理的数据帧。

当我们测试两个数据帧是否相等时,`sort_index()`方法也可以帮助我们得到准确的答案。Pandas 将检查在比较行时，除了具有相同的数据之外，两者还具有相同的索引值。如果我们按照摄氏温度对数据帧进行排序，并检查它是否与原始数据帧相等，`pandas`会告诉我们不相等。我们必须对索引进行排序，以确保它们是相同的:

```py
>>> df.equals(df.sort_values(by='temp_C'))
False
>>> df.equals(df.sort_values(by='temp_C').sort_index())
True
```

有时，我们不太关心数字索引，但是我们希望使用一个(或多个)其他列作为索引。在这种情况下，我们可以使用`set_index()`方法。让我们将`date`列设置为我们的索引:

```py
>>> df.set_index('date', inplace=True)
>>> df.head()
```

请注意，`date`列移到了索引所在的最左边，我们不再有数字索引:

![Figure 3.22 – Setting the date column as the index
](img/Figure_3.22_B16834.jpg)

图 3.22–将日期列设置为索引

小费

我们还可以提供一个列列表来作为索引。这将创建一个`MultiIndex`对象，其中列表中的第一个元素是最外层，最后一个是最内层。我们将在*旋转数据框架*部分进一步讨论这个问题。

将索引设置为 datetime 让我们可以利用 datetime 切片和索引，我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*使用 Pandas 数据帧*中简要讨论了这一点。只要我们提供一个`pandas`理解的日期格式，我们就可以抓取数据。要选择 2018 的全部，我们可以用`df.loc['2018']`；2018 年第四季度，可以用`df.loc['2018-Q4']`；而 10 月份，我们可以用`df.loc['2018-10']`。这些也可以结合起来，以建立范围。注意使用范围时`loc[]`是可选的:

```py
>>> df['2018-10-11':'2018-10-12']
```

这为我们提供了从 2018 年 10 月 11 日到 2018 年 10 月 12 日(包括两个端点)的数据:

![Figure 3.23 – Selecting date ranges
](img/Figure_3.23_B16834.jpg)

图 3.23–选择日期范围

我们可以使用`reset_index()`方法来恢复`date`列:

```py
>>> df['2018-10-11':'2018-10-12'].reset_index()
```

我们的索引现在从`0`开始，日期现在在名为`date`的列中。如果我们有不想在索引中丢失的数据，例如日期，但是需要执行一个操作，就好像它不在索引中一样，这就特别有用:

![Figure 3.24 – Resetting the index
](img/Figure_3.24_B16834.jpg)

图 3.24–重置索引

在某些情况下，我们可能有一个想要继续使用的索引，但是我们需要将它与某些值对齐。为此，我们有了`reindex()`方法。我们向它提供一个索引来对齐我们的数据，它相应地调整索引。注意，这个新索引不一定是数据的一部分——我们只是有一个索引，并希望将当前数据与它匹配。

例如，我们将转向`sp500.csv`文件中的标准普尔 500 股票数据。它包含从 2017 年到 2018 年底 S & P 500 每日的**开盘**、**高位**、**低位**、**收盘**(也称为 **OHLC** )价格，以及成交量和调整后的收盘价(我们不会使用)。让我们读入它，将`date`列设置为索引并解析日期:

```py
>>> sp = pd.read_csv(
...     'data/sp500.csv', index_col='date', parse_dates=True
... ).drop(columns=['adj_close']) # not using this column
```

让我们看看我们的数据是什么样子，并为每一行标记一周中的哪一天，以便理解索引包含了什么。我们可以很容易地从类型为`DatetimeIndex`的索引中分离出日期部分。当分离日期部分时，`pandas`将给出我们所寻找的数字表示；如果我们正在寻找字符串版本，在编写我们自己的转换函数之前，我们应该看看是否已经有一个方法。在这种情况下，它就是`day_name()`:

```py
>>> sp.head(10)\
...     .assign(day_of_week=lambda x: x.index.day_name())
```

小费

我们也可以用一个系列来做这件事，但是首先，我们需要访问`dt`属性。例如，如果我们在`sp`数据框架中有一个`date`列，我们可以用`sp.date.dt.month`来抓取月份。你可以在[https://pandas . pydata . org/pandas-docs/stable/reference/series . html # datetime like-properties](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties)找到可以访问的完整列表。

由于股市在周末(和节假日)休市，我们只有工作日的数据:

![Figure 3.25 – S&P 500 OHLC data
](img/Figure_3.25_B16834.jpg)

图 3.25-标准普尔 500 OHLC 数据

如果我们正在分析一个投资组合中一组资产的表现，该投资组合包括标准普尔 500 和周末交易的一些东西，如比特币，我们将需要 S & P 500 在一年中每天的值。否则，当我们查看我们投资组合的每日价值时，我们会发现,股市每天都在大幅下跌。为了说明这一点，让我们从`bitcoin.csv`文件中读入比特币数据，并将 S & P 500 和比特币数据组合成一个投资组合。比特币数据也包含 OHLC 数据和交易量，但它附带了一个我们不需要的名为`market_cap`的列，所以我们必须先删除它:

```py
>>> bitcoin = pd.read_csv(
...     'data/bitcoin.csv', index_col='date', parse_dates=True
... ).drop(columns=['market_cap'])
```

为了分析投资组合，我们需要按天汇总数据；这是第四章 、*的主题[、](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)*汇总Pandas数据帧，所以，现在，不要太担心这种汇总是如何执行的——只需知道我们正在汇总每天的数据。例如，每天的收盘价将是 S & P 500 的收盘价和比特币的收盘价之和:

```py
# every day's closing price = S&P 500 close + Bitcoin close
# (same for other metrics)
>>> portfolio = pd.concat([sp, bitcoin], sort=False)\
...     .groupby(level='date').sum()
>>> portfolio.head(10).assign(
...     day_of_week=lambda x: x.index.day_name()
... )
```

现在，如果我们检查我们的投资组合，我们会看到一周的每一天都有个值；到目前为止，一切顺利:

![ Figure 3.26 – Portfolio of the S&P 500 and bitcoin
](img/Figure_3.26_B16834.jpg)

图 3.26–标准普尔 500 和比特币的投资组合

然而，这种方法有一个问题，通过可视化更容易发现。绘制将在 [*第五章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*用 Pandas 和 Matplotlib* 可视化数据、 [*第六章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*用 Seaborn 和定制技术绘制*中深入讨论，所以现在不用担心细节:

```py
>>> import matplotlib.pyplot as plt # module for plotting
>>> from matplotlib.ticker import StrMethodFormatter 
# plot the closing price from Q4 2017 through Q2 2018
>>> ax = portfolio['2017-Q4':'2018-Q2'].plot(
...     y='close', figsize=(15, 5), legend=False,
...     title='Bitcoin + S&P 500 value without accounting '
...           'for different indices'
... )
# formatting
>>> ax.set_ylabel('price')
>>> ax.yaxis\
...     .set_major_formatter(StrMethodFormatter('${x:,.0f}'))
>>> for spine in ['top', 'right']:
...     ax.spines[spine].set_visible(False)
# show the plot
>>> plt.show()
```

注意这里有一个循环模式吗？每天市场关闭时，它都在下降，因为聚合只有这些天的比特币数据:

![Figure 3.27 – Portfolio closing price without accounting for stock market closures
](img/Figure_3.27_B16834.jpg)

图 3.27–不考虑股市收盘的投资组合收盘价

显然，这个是有问题的；每当市场关闭时，资产的价值不会降到零。如果我们希望`pandas`为我们填充缺失的数据，我们将需要使用`reindex()`方法用比特币的索引重新索引 S & P 500 数据，将以下策略之一传递给`method`参数:

*   `'ffill'`:这个方法将值前移。在前面的示例中，这用市场在这些天之前最后一次开放的数据填充了市场关闭的天数。
*   `'bfill'`:这个方法反向传播值，这将导致把未来的结果带到过去的日期，这意味着这不是正确的选择。
*   `'nearest'`:这个方法根据最接近丢失行的行进行填充，在这种情况下，这将导致星期日获得接下来的星期一的数据，星期六获得前一个星期五的数据。

向前填充似乎是最好的选择，但由于我们不确定，我们将先看看这在几行数据上是如何工作的:

```py
>>> sp.reindex(bitcoin.index, method='ffill').head(10)\
...     .assign(day_of_week=lambda x: x.index.day_name())
```

注意到这有什么问题吗？好吧，交易量(`volume`)栏让我们看起来好像我们使用远期交易的日子实际上是市场开放的日子:

![Figure 3.28 – Forward-filling dates with missing values
](img/Figure_3.28_B16834.jpg)

图 3.28–向前填充缺失值的日期

小费

`compare()`方法将向我们显示相同标签的数据帧(相同的索引和列)中不同的值；我们可以使用它来隔离我们的数据中的变化，当在这里向前填充时。笔记本里有个例子。

理想情况下，我们只想在股市关闭时维持股票的价值——交易量应该为零。为了以不同的方式处理每一列的`NaN`值，我们将求助于`assign()`方法。为了用`0`填充`volume`列中的任何`NaN`值，我们将使用`fillna()`方法，我们将在本章后面的*处理重复、缺失或无效数据*部分中看到更多。`fillna()`方法还允许我们传入一个方法而不是一个值，因此我们可以向前填充`close`列，这是我们之前尝试中唯一有意义的列。最后，我们可以对剩余的列使用`np.where()`函数，这允许我们构建一个矢量化的`if...else`。它采取以下形式:

```py
np.where(boolean condition, value if True, value if False)
```

**对数组中的所有元素一次执行矢量化运算**；由于每个元素具有相同的数据类型，这些计算可以相当快地运行。作为一个通用的经验法则，对于`pandas`，我们应该避免编写循环来支持向量化操作，以获得更好的性能。NumPy 函数被设计成在数组上工作，所以它们是高性能代码的完美候选。这将便于我们将`open`、`high`或`low`列中的任何`NaN`值设置为同一天`close`列中的值。由于这些是在对`close`列进行处理之后发生的，所以我们将为`close`使用向前填充的值，以便在必要时用于其他列:

```py
>>> import numpy as np
>>> sp_reindexed = sp.reindex(bitcoin.index).assign(
...     # volume is 0 when the market is closed
...     volume=lambda x: x.volume.fillna(0),
...     # carry this forward
...     close=lambda x: x.close.fillna(method='ffill'),
...     # take the closing price if these aren't available
...     open=lambda x: \
...         np.where(x.open.isnull(), x.close, x.open),
...     high=lambda x: \
...         np.where(x.high.isnull(), x.close, x.high),
...     low=lambda x: np.where(x.low.isnull(), x.close, x.low)
... )
>>> sp_reindexed.head(10).assign(
...     day_of_week=lambda x: x.index.day_name()
... )
```

在 1 月 7 日星期六和 1 月 8 日星期天，我们现在的交易量为零。OHLC 的价格都等于 6 日星期五的收盘价:

![Figure 3.29 – Reindexing the S&P 500 data with specific strategies per column
](img/Figure_3.29_B16834.jpg)

图 3.29-用每列的特定策略重新索引标准普尔 500 数据

小费

这里，我们使用`np.where()`到来引入我们将在本书中看到的一个函数，并且使理解发生的事情变得更容易，但是注意`np.where(x.open.isnull(), x.close, x.open)`可以用`combine_first()`方法来代替，它(对于这个用例)相当于`x.open.combine_first(x.close)`。我们将在本章后面的*处理重复、缺失或无效数据*部分使用`combine_first()`方法。

现在，让我们使用重新索引的标准普尔 500 数据重新创建文件夹，并使用可视化将其与之前的尝试进行比较(同样，不用担心绘图代码，这些代码将在 [*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*使用 Pandas 和 Matplotlib* 可视化数据，以及 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*使用 Seaborn 和定制技术*中介绍):

```py
# every day's closing price = S&P 500 close adjusted for
# market closure + Bitcoin close (same for other metrics)
>>> fixed_portfolio = sp_reindexed + bitcoin
# plot the reindexed portfolio's close (Q4 2017 - Q2 2018)
>>> ax = fixed_portfolio['2017-Q4':'2018-Q2'].plot(
...     y='close', figsize=(15, 5), linewidth=2, 
...     label='reindexed portfolio of S&P 500 + Bitcoin', 
...     title='Reindexed portfolio vs.' 
...           'portfolio with mismatched indices'
... )
# add line for original portfolio for comparison
>>> portfolio['2017-Q4':'2018-Q2'].plot(
...     y='close', ax=ax, linestyle='--',
...     label='portfolio of S&P 500 + Bitcoin w/o reindexing' 
... ) 
# formatting
>>> ax.set_ylabel('price')
>>> ax.yaxis\
...     .set_major_formatter(StrMethodFormatter('${x:,.0f}'))
>>> for spine in ['top', 'right']:
...     ax.spines[spine].set_visible(False)
# show the plot
>>> plt.show() 
```

橙色的虚线是我们研究投资组合(没有重新索引)的原始尝试，而蓝色的实线是我们刚刚构建的投资组合，其中每列都有重新索引和不同的填充策略。在 [*第 7 章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币和股票市场*的练习中，请记住这个策略:

![Figure 3.30 – Visualizing the effect of reindexing
](img/Figure_3.30_B16834.jpg)

图 3.30–可视化重新索引的效果

小费

我们还可以使用`reindex()`方法对行进行重新排序。例如，如果我们的数据存储在`x`中，那么`x.reindex([32, 20, 11])`将返回一个新的包含三行的`DataFrame`对象:32、20 和 11(按这个顺序)。这可以通过`axis=1`沿着列进行(对于行，默认为`axis=0`)。

现在，让我们把注意力转向重塑数据。回想一下，我们必须首先通过`datatype`列过滤温度数据，然后进行排序以找到最暖和的日子；重塑数据将使这变得更容易，也使我们有可能聚合和总结数据。

# 重塑数据

数据并不总是以最方便我们分析的格式提供给我们。因此，我们需要能够根据我们想要执行的分析，将数据重组为宽格式和长格式。对于许多分析，我们将需要宽格式的数据，以便我们可以轻松地查看汇总统计数据，并以该格式共享我们的结果。

然而，这并不总是像从长格式到宽格式或反之亦然那样黑白分明。考虑来自*练习*部分的以下数据:

![Figure 3.31 – Data with some long and some wide format columns
](img/Figure_3.31_B16834.jpg)

图 3.31–具有一些长格式列和一些宽格式列的数据

有些数据可能是宽格式的(**打开**、**高**、**低**、**关闭**、**音量**)，但其他数据是长格式的(**收报机**)。在这个数据上使用`describe()`的汇总统计是没有帮助的，除非我们首先在 **ticker** 上过滤。这种格式便于比较股票；然而，正如我们在学习宽和长格式时简要讨论的那样，我们无法使用`pandas`轻松绘制每只股票的收盘价——我们需要`seaborn`。或者，我们可以为可视化重组数据。

既然我们已经理解了重组数据的动机，让我们转到下一个笔记本，`4-reshaping_data.ipynb`。我们将从导入`pandas`和读取`long_data.csv`文件开始，在华氏列(`temp_F`中添加温度，并执行我们刚刚了解的一些数据清理:

```py
>>> import pandas as pd
>>> long_df = pd.read_csv(
...     'data/long_data.csv',
...     usecols=['date', 'datatype', 'value']
... ).rename(columns={'value': 'temp_C'}).assign(
...     date=lambda x: pd.to_datetime(x.date),
...     temp_F=lambda x: (x.temp_C * 9/5) + 32
... )
```

我们的 long 格式数据看起来是这样的:

![Figure 3.32 – Long format temperature data
](img/Figure_3.32_B16834.jpg)

图 3.32–长格式温度数据

在这一节中，我们将讨论转置、旋转和融合我们的数据。请注意，在重塑数据后，我们将经常重新访问数据清理任务，因为事情可能已经发生了变化，或者我们可能需要更改以前不容易访问的内容。例如，如果所有的值都被转换成长格式的字符串，我们会希望执行一些类型转换，但是在宽格式中，一些列显然是数字。

## 转置数据帧

虽然我们几乎只会处理宽格式或长格式，但`pandas`提供了一些方法让按照我们认为合适的方式重组我们的数据，包括采用**转置**(翻转行和列)，当我们打印数据帧的一部分时，这可能会有助于更好地利用我们的显示区域:

```py
>>> long_df.set_index('date').head(6).T
```

请注意，索引现在位于列中，列名也位于索引中:

![Figure 3.33 – Transposed temperature data
](img/Figure_3.33_B16834.jpg)

图 3.33–转置温度数据

这可能不会立即显现出它有多有用，但是我们会在本书中多次看到这一点；例如，在 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币和股票市场*中使内容更容易显示，在 [*第九章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188) 、*Python 中的机器学习入门*中为机器学习构建特定的可视化。

## 旋转数据框架

我们将数据从长格式转换成宽格式。`pivot()`方法执行我们的`DataFrame`对象的这种重构。为了进行透视，我们需要告诉`pandas`哪一列当前保存了值(使用`values`参数)以及包含将成为宽格式列名的内容的列(`columns`参数)。可选地，我们可以提供一个新的索引(`index`参数)。让我们转到宽格式，其中每个温度测量值都有一列，以摄氏度为单位，并使用日期作为索引:

```py
>>> pivoted_df = long_df.pivot(
...     index='date', columns='datatype', values='temp_C'
... )
>>> pivoted_df.head()
```

在我们的起始数据帧中，有一个`datatype`列，它只包含作为字符串的`TMAX`、`TMIN`或`TOBS`。这些是列名，因为我们传入了`columns='datatype'`。通过传入`index='date'`，列`date`成为我们的索引，而不需要运行`set_index()`。最后，`date`和`datatype`的每个组合的值都是从我们进入`values='temp_C'`以来相应的摄氏温度:

![Figure 3.34 – Pivoting the long format temperature data into wide format
](img/Figure_3.34_B16834.jpg)

图 3.34–将长格式温度数据转换为宽格式

正如我们在本章开始时所讨论的，使用宽格式的数据，我们可以通过`describe()`方法轻松获得有意义的汇总统计数据:

```py
>>> pivoted_df.describe()
```

我们可以看到所有三个温度测量值有 31 个观测值，并且这个月的温度范围很大(最高日最高温度为 26.7°C，最低日最低温度为-1.1°C):

![Figure 3.35 – Summary statistics on the pivoted temperature data
](img/Figure_3.35_B16834.jpg)

图 3.35-枢轴温度数据的汇总统计

然而，我们失去了华氏温度。如果我们想保留它，我们可以向`values`提供多个列:

```py
>>> pivoted_df = long_df.pivot(
...     index='date', columns='datatype',
...     values=['temp_C', 'temp_F']
... )
>>> pivoted_df.head()
```

然而，我们现在在列名之上多了一层。这叫做**层次索引**:

![Figure 3.36 – Pivoting with multiple value columns
](img/Figure_3.36_B16834.jpg)

图 3.36–多值列透视

有了这个层次索引，如果我们想在华氏温度中选择`TMIN`，我们将首先需要选择`temp_F`，然后选择`TMIN`:

```py
>>> pivoted_df['temp_F']['TMIN'].head()
date
2018-10-01    48.02
2018-10-02    57.02
2018-10-03    60.08
2018-10-04    53.06
2018-10-05    53.06
Name: TMIN, dtype: float64
```

重要说明

如果我们需要在透视时执行聚合(由于索引中的重复值)，我们可以使用`pivot_table()`方法，我们将在 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*聚合Pandas数据帧*中讨论该方法。

在这一章中，我们一直使用单一的索引；然而，我们可以用`set_index()`从任意数量的列创建一个索引。这给了我们一个类型为`MultiIndex`的索引，其中最外层对应于提供给`set_index()`的列表中的第一个元素:

```py
>>> multi_index_df = long_df.set_index(['date', 'datatype'])
>>> multi_index_df.head().index
MultiIndex([('2018-10-01', 'TMAX'),
            ('2018-10-01', 'TMIN'),
            ('2018-10-01', 'TOBS'),
            ('2018-10-02', 'TMAX'),
            ('2018-10-02', 'TMIN')],
           names=['date', 'datatype'])
>>> multi_index_df.head()
```

请注意，现在在索引中有两个级别— `date`是最外层，`datatype`是最内层:

![Figure 3.37 – Working with a multi-level index
](img/Figure_3.37_B16834.jpg)

图 3.37–使用多级索引

`pivot()`方法期望数据只有一列设置为索引；如果我们有一个多级索引，我们应该使用`unstack()`方法。我们可以在`multi_index_df`上使用`unstack()`,得到与之前相似的结果。这里的顺序很重要，因为默认情况下，`unstack()`会将索引的最内层移到列中；在这种情况下，这意味着我们将在索引中保留`date`级别，并将`datatype`级别移动到列名中。要拆分不同的级别，只需将级别的索引传递给拆分，其中 0 是最左边的，而-1 是最右边的，或者是级别的名称(如果有)。这里，我们将使用默认值:

```py
>>> unstacked_df = multi_index_df.unstack()
>>> unstacked_df.head()
```

使用`multi_index_df`，我们将`datatype`作为索引的最内层，因此，在使用`unstack()`之后，它沿着列。请注意，我们再次在列中使用了分层索引。在 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) ，*聚合Pandas数据帧*中，我们将讨论一种方法来将它压缩回一个单独的列级别:

![Figure 3.38 – Unstacking a multi-level index to pivot data
](img/Figure_3.38_B16834.jpg)

图 3.38–将多层索引拆分为透视数据

`unstack()`方法有一个额外的好处，它允许我们指定如何填充在重塑数据时出现的缺失值。为此，我们可以使用`fill_value`参数。想象一下，我们只获得了 2018 年 10 月 1 日`TAVG`的数据。我们可以将它附加到`long_df`，并将我们的索引设置为`date`和`datatype`列，就像我们之前做的那样:

```py
>>> extra_data = long_df.append([{
...     'datatype': 'TAVG', 
...     'date': '2018-10-01', 
...     'temp_C': 10, 
...     'temp_F': 50
... }]).set_index(['date', 'datatype']).sort_index()
>>> extra_data['2018-10-01':'2018-10-02']
```

我们现在有 2018 年 10 月 1 日的四次温度测量，但剩下的几天只有三次:

![Figure 3.39 – Introducing an additional temperature measurement into the data
](img/Figure_3.39_B16834.jpg)

图 3.39–在数据中引入额外的温度测量

使用`unstack()`，正如我们之前所做的，将为大多数`TAVG`数据产生`NaN`值:

```py
>>> extra_data.unstack().head()
```

拆分后，看一下`TAVG`栏:

![Figure 3.40 – Unstacking can lead to null values
](img/Figure_3.40_B16834.jpg)

图 3.40–拆垛会导致空值

为了解决这个问题，我们可以传入一个合适的`fill_value`。然而，我们被限制为在中为此传递一个值，而不是一个策略(正如我们在讨论重新索引时所看到的)，所以尽管对于这个案例没有好的值，我们可以使用`-40`来说明这是如何工作的:

```py
>>> extra_data.unstack(fill_value=-40).head()
```

`NaN`值现在已经被替换为`-40.0`。然而，注意现在`temp_C`和`temp_F`都有相同的温度读数。其实这就是我们选`-40`做`fill_value`的原因；它是华氏温度和摄氏温度相等时的温度，所以我们不会把人们与它们都是相同的数字相混淆；比如`0`(因为 0°C = 32°F，0°F =-17.78°C)。由于这一温度也比纽约市测得的温度低得多，并且对于我们所有的数据都低于`TMIN`，与我们使用`0`相比，这更有可能被视为数据输入错误或数据丢失的信号。请注意，在实践中，如果我们与其他人共享丢失的数据，最好是明确说明，并保留`NaN`值:

![Figure 3.41 – Unstacking with a default value for missing combinations
](img/Figure_3.41_B16834.jpg)

图 3.41–缺省组合的缺省值拆分

总而言之，当我们有一个多级索引并且想要将一个或多个级别移到列中时，`unstack()`应该是我们的选择；然而，如果我们简单地使用一个单一的索引，`pivot()`方法的语法可能更容易被正确地指定，因为哪个数据将在哪里结束更明显。

## 熔化数据帧

从宽格式到长格式，我们需要**融化**数据。熔化会撤消枢轴。对于这个示例，我们将从`wide_data.csv`文件中读入数据:

```py
>>> wide_df = pd.read_csv('data/wide_data.csv')
>>> wide_df.head()
```

我们的 wide 数据包含一个日期列和一个温度测量列:

![Figure 3.42 – Wide format temperature data
](img/Figure_3.42_B16834.jpg)

图 3.42–宽格式温度数据

我们可以使用`melt()`方法进行灵活的整形——允许我们将它转换成长格式，类似于我们从 API 中得到的。熔化要求我们规定以下内容:

*   哪一列唯一地标识了宽格式数据中带有`id_vars`参数的一行
*   哪一列包含带有`value_vars`参数的变量

可选地，我们还可以指定如何命名包含长格式数据中变量名的列(`var_name`)以及包含变量值的列的名称(`value_name`)。默认情况下，它们分别是`variable`和`value`。

现在，让我们使用`melt()`方法将宽格式数据转换成长格式:

```py
>>> melted_df = wide_df.melt(
...     id_vars='date', value_vars=['TMAX', 'TMIN', 'TOBS'], 
...     value_name='temp_C', var_name='measurement'
... )
>>> melted_df.head()
```

`date`列是我们的行的标识符，所以我们将其提供为`id_vars`。我们将`TMAX`、`TMIN`和`TOBS`列中的值与温度(`value_vars`)合并成一列，并使用它们的列名作为测量列(`var_name='measurement'`)的值。最后，我们将值列命名为(`value_name='temp_C'`)。我们现在只有三列；日期、以摄氏度为单位的温度读数(`temp_C`)以及一列指示该行的`temp_C`单元格中的温度测量值(`measurement`):

![Figure 3.43 – Melting the wide format temperature data
](img/Figure_3.43_B16834.jpg)

图 3.43–融合宽格式温度数据

正如我们有一种用`unstack()`方法透视数据的替代方法一样，我们也有另一种用`stack()`方法融合数据的方法。这个方法将把列旋转到索引的最内层(产生一个类型为`MultiIndex`的索引)，所以我们需要在调用它之前仔细检查我们的索引。如果我们愿意，它还允许我们删除不产生任何数据的行/列组合。我们可以做以下事情来获得类似于`melt()`方法的输出:

```py
>>> wide_df.set_index('date', inplace=True)
>>> stacked_series = wide_df.stack() # put datatypes in index
>>> stacked_series.head()
date          
2018-10-01  TMAX    21.1
            TMIN     8.9
            TOBS    13.9
2018-10-02  TMAX    23.9
            TMIN    13.9
dtype: float64
```

注意，结果作为一个`Series`对象返回，所以我们需要再次创建`DataFrame`对象。我们可以使用`to_frame()`方法，并在列成为数据帧后为其传递一个名称:

```py
>>> stacked_df = stacked_series.to_frame('values')
>>> stacked_df.head()
```

现在，我们有了一个多级索引的数据框架，包含`date`和`datatype`，只有`values`一列。但是，请注意，只有索引的`date`部分有名称:

![Figure 3.44 – Stacking to melt the temperature data into long format
](img/Figure_3.44_B16834.jpg)

图 3.44–叠加将温度数据转换成长格式

最初，我们使用`set_index()`来设置`date`列的索引，因为我们不想融化它；这就形成了多级索引的第一级。然后，`stack()`方法将`TMAX`、`TMIN`和`TOBS`列移到索引的第二层。然而，这个级别从未被命名，所以它显示为`None`，但我们知道这个级别应该被称为`datatype`:

```py
>>> stacked_df.head().index
MultiIndex([('2018-10-01', 'TMAX'),
            ('2018-10-01', 'TMIN'),
            ('2018-10-01', 'TOBS'),
            ('2018-10-02', 'TMAX'),
            ('2018-10-02', 'TMIN')],
           names=['date', None])
```

我们可以使用`set_names()`方法来解决这个问题:

```py
>>> stacked_df.index\
...     .set_names(['date', 'datatype'], inplace=True)
>>> stacked_df.index.names
FrozenList(['date', 'datatype'])
```

既然我们已经了解了数据清理和整形的基础知识，我们将通过一个例子来说明在处理包含各种问题的数据时，如何将这些技术结合起来。

# 处理重复、缺失或无效的数据

到目前为止，我们已经讨论了我们可以用零分支的数据表示方式来改变的事情。然而，我们没有讨论数据清理的一个非常重要的部分:如何处理看起来重复、无效或丢失的数据。这是与其余的数据清理讨论分开的,因为它是一个例子，我们将做一些初始的数据清理，然后重塑我们的数据，最后寻求处理这些潜在的问题；这也是一个相当沉重的话题。

我们将在`5-handling_data_issues.ipynb`笔记本中工作，并使用`dirty_data.csv`文件。先从导入`pandas`开始，读入数据:

```py
>>> import pandas as pd
>>> df = pd.read_csv('data/dirty_data.csv')
```

`dirty_data.csv`文件包含来自 weather API 的宽格式数据，该数据已经被修改，引入了许多我们在野外会遇到的常见数据问题。它包含以下字段:

*   `PRCP`:降水量，单位为毫米
*   `SNOW`:降雪量，单位为毫米
*   `SNWD`:积雪深度，单位为毫米
*   `TMAX`:日最高温度，单位为摄氏度
*   `TMIN`:每日最低温度，单位为摄氏度
*   `TOBS`:观察时的温度，单位为摄氏度
*   `WESF`:雪的水当量，单位为毫米

本节分为两部分。在第一部分，我们将讨论一些发现数据集中问题的策略，在的第二部分，我们将学习如何缓解数据集中出现的一些问题。

## 查找有问题的数据

在 [*第二章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*处理Pandas数据帧*中，我们了解了当我们获得数据时检查我们的数据的重要性；检查数据的许多方法将帮助我们找到这些问题，这不是巧合。检查对数据调用`head()`和`tail()`的结果总是好的第一步:

```py
>>> df.head()
```

实际上，`head()`和`tail()`不像我们将在本节中讨论的其他内容那样健壮，但是我们仍然可以从这里开始获得一些有用的信息。我们的数据是宽格式的，快速浏览一下，我们可以看到我们有一些潜在的问题。有时，`station`字段被记录为`?`，而其他时候，它有一个站 ID。积雪深度(`SNWD`)的值为负无穷大(`-inf`)，还有一些非常热的温度(`TMAX`)。最后，我们可以在几列中观察到许多`NaN`值，包括`inclement_weather`列，它似乎也包含布尔值:

![Figure 3.45 – Dirty data
](img/Figure_3.45_B16834.jpg)

图 3.45–脏数据

使用`describe()`，我们可以查看是否有任何缺失的数据，并查看 5 个数字的摘要以发现潜在的问题:

```py
>>> df.describe()
```

`SNWD`栏显得没用，`TMAX`栏显得不靠谱。从长远来看，太阳光球层的温度约为 5505 摄氏度，因此我们肯定不会期望在纽约市(或地球上的任何地方，就此而言)观察到这样的气温。这可能意味着`TMAX`列在不可用时被设置为一个无意义的大数字。它如此之大的事实实际上是使用我们从`describe()`获得的汇总统计数据来帮助识别它。如果未知数是用另一个值编码的，比如说 40°C，我们不能确定它不是实际数据:

![Figure 3.46 – Summary statistics for the dirty data
](img/Figure_3.46_B16834.jpg)

图 3.46–脏数据的汇总统计

我们可以使用`info()`方法来查看是否有任何缺失值，并检查我们的列是否具有预期的数据类型。在这样做的时候，我们立即看到了两个问题:我们有 765 行，但是对于其中的五列，我们的非空条目要少得多。这个输出还向我们展示了`inclement_weather`列的数据类型不是 Boolean，尽管我们可能从名称上认为是这样。请注意，当我们使用`head()`时，我们在`station`列中看到的`?`值没有出现在这里——从许多不同的角度检查我们的数据很重要:

```py
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 765 entries, 0 to 764
Data columns (total 10 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   date               765 non-null    object 
 1   station            765 non-null    object 
 2   PRCP               765 non-null    float64
 3   SNOW               577 non-null    float64
 4   SNWD               577 non-null    float64
 5   TMAX               765 non-null    float64
 6   TMIN               765 non-null    float64
 7   TOBS               398 non-null    float64
 8   WESF               11 non-null     float64
 9   inclement_weather  408 non-null    object 
dtypes: float64(7), object(3)
memory usage: 59.9+ KB
```

现在，让我们追踪那些空值。`Series`和`DataFrame`对象都提供了这样做的两种方法:`isnull()`和`isna()`。注意，如果我们在`DataFrame`对象上使用该方法，结果将告诉我们哪些行全部为空值，这在本例中不是我们想要的。这里，我们想要检查在`SNOW`、`SNWD`、`TOBS`、`WESF`或`inclement_weather`列中有空值的行。这意味着我们需要用`|`(按位 or)操作符组合对每一列的检查:

```py
>>> contain_nulls = df[
...     df.SNOW.isna() | df.SNWD.isna() | df.TOBS.isna() 
...     | df.WESF.isna() | df.inclement_weather.isna()
... ]
>>> contain_nulls.shape[0]
765
>>> contain_nulls.head(10)
```

如果我们查看我们的`contain_nulls`数据帧的`shape`属性，我们会看到每一行都包含一些空数据。查看前 10 行，我们可以在每行中看到一些`NaN`值:

![Figure 3.47 – Rows in the dirty data with nulls
](img/Figure_3.47_B16834.jpg)

图 3.47–包含空值的脏数据中的行

小费

默认情况下，我们在本章前面讨论的`sort_values()`方法会将任何`NaN`值放在最后。我们可以通过传入`na_position='first'`来改变这种行为(将它们放在第一位)，当排序列有空值时，这在查找数据模式时也很有帮助。

注意，我们不能检查列的值是否等于`NaN`，因为`NaN`不等于任何值:

```py
>>> import numpy as np
>>> df[df.inclement_weather == 'NaN'].shape[0] # doesn't work
0
>>> df[df.inclement_weather == np.nan].shape[0] # doesn't work
0
```

我们必须使用上述选项(`isna()` / `isnull()`):

```py
>>> df[df.inclement_weather.isna()].shape[0] # works
357
```

注意`inf`和`-inf`其实是`np.inf`和`-np.inf`。因此，我们可以通过执行以下操作找到具有`inf`或`-inf`值的行数:

```py
>>> df[df.SNWD.isin([-np.inf, np.inf])].shape[0]
577
```

不过，这只告诉我们关于单个列的信息，所以我们可以编写一个函数，使用字典理解来返回数据帧中每列的无限值的数量:

```py
>>> def get_inf_count(df):
...     """Find the number of inf/-inf values per column"""
...     return {
...         col: df[
...             df[col].isin([np.inf, -np.inf])
...         ].shape[0] for col in df.columns
...     }
```

使用我们的函数，我们发现`SNWD`列是唯一具有无限值的列，但是该列中的大多数值都是无限的:

```py
>>> get_inf_count(df)
{'date': 0, 'station': 0, 'PRCP': 0, 'SNOW': 0, 'SNWD': 577,
 'TMAX': 0, 'TMIN': 0, 'TOBS': 0, 'WESF': 0,
 'inclement_weather': 0}
```

在我们决定如何处理积雪深度的无限值之前，我们应该查看降雪的汇总统计数据(`SNOW`)，这构成了确定积雪深度(`SNWD`)的一大部分。为此，我们可以制作一个包含两个系列的数据帧，其中一个包含当积雪深度为`np.inf`时降雪列的汇总统计数据，另一个包含当积雪深度为`-np.inf`时降雪列的汇总统计数据。此外，我们将使用`T`属性来转置数据，以便于查看:

```py
>>> pd.DataFrame({
...     'np.inf Snow Depth':
...         df[df.SNWD == np.inf].SNOW.describe(),
...     '-np.inf Snow Depth': 
...         df[df.SNWD == -np.inf].SNOW.describe()
... }).T
```

没有降雪时，积雪深度被记录为负无穷大；然而，我们不能确定这不仅仅是一个巧合。如果我们只是要处理这个固定的日期范围，我们可以将它视为深度为`0`或`NaN`，因为它没有下雪。不幸的是，我们真的不能对正无穷大做任何假设。他们肯定不是这样，但我们无法决定他们应该是什么，所以最好不要管他们，或者不要看这个专栏:

![Figure 3.48 – Summary statistics for snowfall when snow depth is infinite
](img/Figure_3.48_B16834.jpg)

图 3.48–当积雪深度为无穷大时，降雪的汇总统计数据

我们正在处理一年的数据，但不知何故，我们有 765 行，所以我们应该检查为什么。我们需要检查的列只有`date`和`station`列。我们可以使用`describe()`方法来查看它们的汇总统计数据:

```py
>>> df.describe(include='object')
```

在 765 行数据中，`date`列只有 324 个唯一值(意味着有些日期缺失)，有些日期出现了多达 8 次( **freq** )。对于`station`列只有两个唯一的值，最常见的是 **GHCND:USC00280907** 。由于我们在前面使用`head()`(*图 3.45* )时看到了一些带有`?`值的站 id，我们知道那是另一个值；然而，如果没有的话，我们可以使用`unique()`来查看所有的唯一值。我们还知道`?`出现 367 次(765 - 398)，不需要使用`value_counts()`:

![Figure 3.49 – Summary statistics for the non-numeric columns in the dirty data
](img/Figure_3.49_B16834.jpg)

图 3.49–脏数据中非数字列的汇总统计信息

在实践中，我们可能不知道为什么电台有时会被记录为`?`——这可能是故意让显示他们没有电台，记录软件中的错误，或意外遗漏而被编码为`?`。正如我们将在下一节中讨论的那样，我们如何处理这个问题将是一个判断问题。

看到我们有 765 行数据和两个不同的车站 ID 值时，我们可能会假设每天有两个条目—每个车站一个。然而，这只占 730 行，我们现在也知道我们遗漏了一些日期。让我们看看是否能找到任何重复的数据来解释这一点。我们可以使用`duplicated()`方法的结果作为布尔掩码来查找重复的行:

```py
>>> df[df.duplicated()].shape[0]
284
```

根据我们试图实现的目标，我们可能会以不同的方式处理重复。可以用`keep`参数修改返回的行。默认情况下，它是`'first'`，并且，对于不止一次出现的每一行，我们将只获得额外的行(除了第一行)。然而，如果我们传入`keep=False`，我们将得到所有出现不止一次的行，而不仅仅是它们出现的每一次:

```py
>>> df[df.duplicated(keep=False)].shape[0] 
482
```

还有一个`subset`参数(第一个位置参数)，它允许我们只关注某些列的副本。使用这个，我们可以看到当`date`和`station`列被复制时，其余的数据也被复制，因为我们得到了和以前一样的结果。然而，我们不知道这实际上是否是一个问题:

```py
>>> df[df.duplicated(['date', 'station'])].shape[0]
284
```

现在，让我们检查几个重复的行:

```py
>>> df[df.duplicated()].head()
```

只看前五行就可以看出，有些行至少重复了三次。请记住`duplicated()`的默认行为是不显示第一次出现的值，这意味着数据中的行 **1** 和 **2** 有另一个匹配值(行 **5** 和 **6** 也是如此):

![Figure 3.50 – Examining the duplicate data
](img/Figure_3.50_B16834.jpg)

图 3.50–检查重复数据

现在我们知道了如何在数据中发现问题，让我们学习一些解决问题的方法。请注意，这里没有灵丹妙药，它通常会归结为了解我们正在处理的数据并做出判断。

## 缓解问题

我们的数据并不令人满意，尽管我们可以努力改进，但最佳的行动计划并不总是显而易见的。面对这类数据问题，我们能做的最简单的事情可能就是删除重复的行。然而，我们评估这一决定可能对我们的分析产生的影响是至关重要的。即使我们正在处理的数据似乎是从具有附加列的更大的数据集中收集的，从而使我们的所有数据都不同，我们也不能确定删除这些列是复制剩余数据的原因，我们需要查阅数据的来源和任何可用的文档。

因为我们知道这两个站都是为纽约市服务的，所以我们可能决定删除`station`列——它们可能只是收集了不同的数据。如果我们随后决定使用`date`列删除重复行并保留非`?`站的数据，在出现重复的情况下，我们将丢失`WESF`列的所有数据，因为`?`站是唯一一个报告`WESF`测量值的站:

```py
>>> df[df.WESF.notna()].station.unique()
array(['?'], dtype=object)
```

在这种情况下，一个令人满意的解决方案可能是采取以下行动:

1.  对`date`列进行类型转换:

    ```py
    >>> df.date = pd.to_datetime(df.date)
    ```

2.  将`WESF`列保存为系列:

    ```py
    >>> station_qm_wesf = df[df.station == '?']\
    ...     .drop_duplicates('date').set_index('date').WESF
    ```

3.  按`station`列降序排列数据帧，将无 ID ( `?`)的站放在最后:

    ```py
    >>> df.sort_values(
    ...     'station', ascending=False, inplace=True
    ... )
    ```

4.  删除基于日期重复的行，保留第一次出现的行，即`station`列有 ID 的行(如果该站有测量值)。注意`drop_duplicates()`可以就地完成，但是如果我们要做的事情很复杂，最好不要从就地操作开始:

    ```py
    >>> df_deduped = df.drop_duplicates('date')
    ```

5.  删除`station`列，并将索引设置为`date`列(以便与`WESF`数据相匹配):

    ```py
    >>> df_deduped = df_deduped.drop(columns='station')\
    ...     .set_index('date').sort_index()
    ```

6.  使用`combine_first()`方法更新`WESF`列，以**合并**(就像 SQL 中来自 SQL 后台的那些)第一个非空条目的值；这意味着，如果我们有来自两个站的数据，我们将首先获取由具有 ID 的站提供的值，如果(且仅当)该站为空，我们将从没有 ID 的站获取值(`?`)。因为`df_deduped`和`station_qm_wesf`都使用日期作为索引，所以这些值与适当的日期相匹配:

    ```py
    >>> df_deduped = df_deduped.assign(WESF= 
    ...     lambda x: x.WESF.combine_first(station_qm_wesf)
    ... )
    ```

这听起来可能有点复杂，但这主要是因为我们还没有学习聚合。在 [*第四章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*汇总Pandas数据帧*中，我们将看看另一种方式来处理这个问题。让我们看看使用上述实现的结果:

```py
>>> df_deduped.shape
(324, 8)
>>> df_deduped.head()
```

我们现在剩下 324 行——每一行代表数据中的一个唯一日期。我们能够通过将`WESF`列与来自另一个站的数据放在一起来保存它:

![Figure 3.51 – Using data wrangling to keep the information in the WESF column
](img/Figure_3.51_B16834.jpg)

图 3.51–使用数据争论将信息保存在 WESF 列中

小费

我们也可以用`keep`参数指定保留最后一个条目而不是第一个，或者删除所有重复条目，就像我们用`duplicated()`检查重复条目一样。请记住这一点，因为`duplicated()`方法在给出重复数据删除任务的模拟运行结果时非常有用。

现在，让我们来处理空数据。我们可以选择放弃它，用某个任意值替换它，或者用周围的数据估算它。这些选择都有其后果。如果我们丢弃数据，我们将只利用部分数据进行分析；如果我们最终删除了一半的行，这将会产生很大的影响。当改变数据的值时，我们可能会影响我们的分析结果。

为了删除所有包含空数据的行(不一定要对该行的所有列都这样，所以要小心)，我们使用了`dropna()`方法；在我们的例子中，只剩下 4 行:

```py
>>> df_deduped.dropna().shape
(4, 8)
```

我们可以使用`how`参数将默认行为更改为仅在所有列都为空时删除一行，但这不会删除任何内容:

```py
>>> df_deduped.dropna(how='all').shape # default is 'any'
(324, 8)
```

幸运的是，我们还可以使用列的子集来决定删除什么。假设我们想看雪的数据。我们很可能想要确保我们的数据有`SNOW`、`SNWD`和`inclement_weather`的值。这可以通过`subset`参数实现:

```py
>>> df_deduped.dropna(
...     how='all', subset=['inclement_weather', 'SNOW', 'SNWD']
... ).shape
(293, 8)
```

注意，这个操作也可以沿着列执行，我们可以提供一个空值数量的阈值，必须观察这个阈值才能用`thresh`参数删除数据。例如，如果我们说至少 75%的行必须为空才能删除该列，我们将删除`WESF`列:

```py
>>> df_deduped.dropna(
...     axis='columns', 
...     thresh=df_deduped.shape[0] * .75 # 75% of rows
... ).columns
Index(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TOBS',
       'inclement_weather'],
      dtype='object')
```

由于我们有很多空值，我们可能会对保留这些值更感兴趣，也许会找到更好的方法来表示它们。如果我们替换空数据，我们必须谨慎决定用什么来代替；用其他值填充所有我们没有的值可能会在以后产生奇怪的结果，所以我们必须首先考虑如何使用这些数据。

为了用其他数据填充空值，我们使用了`fillna()`方法，该方法为我们提供了指定值或如何执行填充的策略的选项。我们将首先讨论用单个值填充。`WESF`列主要包含空值，但是由于它是以毫升为单位的测量值，当没有降雪的水当量时，它取`NaN`的值，所以我们可以用零填充空值。请注意，这可以就地完成(同样，作为一般的经验法则，我们应该谨慎使用就地操作):

```py
>>> df_deduped.loc[:,'WESF'].fillna(0, inplace=True)
>>> df_deduped.head()
```

`WESF`列不再包含`NaN`值:

![Figure 3.52 – Filling in null values in the WESF column
](img/Figure_3.52_B16834.jpg)

图 3.52–在 WESF 列中填写空值

在这一点上，我们已经在不扭曲数据的情况下做了我们能做的一切。我们知道我们丢失了日期，但是如果我们重新索引，我们不知道如何填充得到的`NaN`值。根据天气数据，我们不能因为某一天下了雪，就认为第二天也会下雪，或者气温会一样。出于这个原因，请注意下面的例子仅仅是为了说明的目的——仅仅因为我们可以做一些事情并不意味着我们应该做。正确的解决方案很可能取决于领域和我们要解决的问题。

也就是说，让我们尝试解决温度数据的一些遗留问题。我们知道当`TMAX`是太阳的温度时，一定是因为没有测到值，所以我们用`NaN`来代替。我们也将为`TMIN`这样做，它目前使用零下 40 摄氏度作为占位符，尽管纽约有记录以来的最低温度是零下 15 华氏度(-26.1 摄氏度)，1934 年 2 月 9 日([https://www . weather . gov/media/okx/Climate/central park/extremes . pdf](https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf)):

```py
>>> df_deduped = df_deduped.assign(
...     TMAX=lambda x: x.TMAX.replace(5505, np.nan), 
...     TMIN=lambda x: x.TMIN.replace(-40, np.nan) 
... )
```

我们还将假设温度不会每天都发生剧烈变化。注意，这实际上是一个很大的假设，但是它将允许我们理解当我们通过`method`参数提供策略时`fillna()`方法是如何工作的:`'ffill'`向前填充或者`'bfill'`向后填充。注意我们没有`'nearest'`选项，就像我们在重建索引时做的那样，这本来是最好的选项；因此，为了说明这是如何工作的，让我们使用向前填充:

```py
>>> df_deduped.assign(
...     TMAX=lambda x: x.TMAX.fillna(method='ffill'),
...     TMIN=lambda x: x.TMIN.fillna(method='ffill')
... ).head()
```

看一下 1 月 1 日和 4 日的`TMAX`和`TMIN`栏目。两者都是 1 号的`NaN`,因为我们没有之前的数据，但是 4 号现在和 3 号有相同的值:

![Figure 3.53 – Forward-filling null values
](img/Figure_3.53_B16834.jpg)

图 3.53-向前填充空值

如果我们想处理`SNWD`列中的空值和无穷大值，我们可以使用`np.nan_to_num()`函数；它将`NaN`变成 0，将`inf` / `-inf`变成非常大的正/负有限数，使得机器学习模型(在 [*第 9 章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188) *中讨论，Python 中的机器学习入门*)可以从这些数据中学习:

```py
>>> df_deduped.assign(
...     SNWD=lambda x: np.nan_to_num(x.SNWD)
... ).head()
```

但是这对我们的用例来说没有多大意义。对于`-np.inf`的实例，我们可以选择将`SNWD`设置为 0，因为我们看到那些天没有降雪。然而，我们不知道如何处理`np.inf`，而且可以说，巨大的正数让我们更加难以理解:

![Figure 3.54 – Replacing infinite values
](img/Figure_3.54_B16834.jpg)

图 3.54–替换无穷大值

根据我们正在处理的数据，我们可能会选择使用`clip()`方法来替代`np.nan_to_num()`函数。`clip()`方法使得在将值限制在特定的最小和/或最大阈值成为可能。既然雪的深度不能是负数，我们就用`clip()`来强制一个零下限。为了说明上限是如何工作的，我们将使用降雪量(`SNOW`)作为估计值:

```py
>>> df_deduped.assign(
...     SNWD=lambda x: x.SNWD.clip(0, x.SNOW)
... ).head()
```

1 月 1 日至 3 日的`SNWD`值现在是`0`而不是`-inf`，而 1 月 4 日和 5 日的`SNWD`值从`inf`变为当天的`SNOW`值:

![Figure 3.55 – Capping values at thresholds
](img/Figure_3.55_B16834.jpg)

图 3.55–阈值的上限值

我们最后的策略是归罪。当我们使用汇总统计或其他观察数据，用从数据中得出的新值替换缺失值时，这被称为**插补**。例如，我们可以用平均值来代替温度值。不幸的是，如果我们只缺少 10 月末的值，并且我们用该月其余时间的平均值替换它们，这很可能偏向极端值，在这种情况下，这是 10 月初的较高温度。像本节中讨论的其他事情一样，我们必须谨慎行事，考虑我们行为的任何潜在后果或副作用。

我们可以将插补与`fillna()`方法结合起来。作为一个例子，让我们用它们的中间值填充`TMAX`和`TMIN`的`NaN`值，用`TMIN`和`TMAX`的平均值填充`TOBS`(在输入它们之后):

```py
>>> df_deduped.assign(
...     TMAX=lambda x: x.TMAX.fillna(x.TMAX.median()),
...     TMIN=lambda x: x.TMIN.fillna(x.TMIN.median()),
...     # average of TMAX and TMIN
...     TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)
... ).head()
```

请注意 1 月 1 日和 4 日的数据变化，最高和最低气温中位数分别为 14.4℃和 5.6℃。这意味着当我们估算`TOBS`并且数据中没有`TMAX`和`TMIN`时，我们得到 10 C:

![Figure 3.56 – Imputing missing values with summary statistics
](img/Figure_3.56_B16834.jpg)

图 3.56–使用汇总统计输入缺失值

如果我们想在所有列上运行相同的计算，我们应该使用`apply()`方法而不是`assign()`，因为它节省了我们为每一列编写相同计算的冗余。例如，让我们用滚动 7 天中值来填充所有缺失值，将计算所需的周期数设置为零，以确保我们不会引入额外的空值。我们将在第四章 、*聚合Pandas数据帧*中介绍滚动计算和`apply()`，因此这只是一个预览:

```py
>>> df_deduped.apply(lambda x:
...     # Rolling 7-day median (covered in chapter 4).
...     # we set min_periods (# of periods required for
...     # calculation) to 0 so we always get a result
...     x.fillna(x.rolling(7, min_periods=0).median())
... ).head(10)
```

很难说我们的估算值在哪里——气温每天都会有很大的波动。我们知道 1 月 4 日的数据在我们之前的尝试中丢失了；采用这种策略，我们那天的估算温度比周围的温度更低。实际上，那天稍微暖和一些(大约零下 3 摄氏度):

![Figure 3.57 – Imputing missing values with the rolling median
](img/Figure_3.57_B16834.jpg)

图 3.57–用滚动中值输入缺失值

重要说明

重要的是在估算时要谨慎。如果我们为数据选择了错误的策略，我们会把事情弄得一团糟。

另一种输入缺失数据的方法是让`pandas`用`interpolate()`方法计算值应该是多少。默认情况下，它将执行线性插值，使假设所有的行是均匀间隔的。我们的数据是日常数据，虽然有些日子是缺失的，所以只是先重新索引的问题。让我们将这个方法与`apply()`方法结合起来，一次性内插我们所有的列:

```py
>>> df_deduped.reindex(
...     pd.date_range('2018-01-01', '2018-12-31', freq='D')
... ).apply(lambda x: x.interpolate()).head(10)
```

看看 1 月 9 日，这是我们以前没有的——`TMAX`、`TMIN`和`TOBS`的值是前一天(1 月 8 日)和后一天(1 月 10 日)的平均值:

![Figure 3.58 – Interpolating missing values
](img/Figure_3.58_B16834.jpg)

图 3.58–插值缺失值

不同的插值策略可以通过`method`参数指定；请务必查阅`interpolate()`方法文档，查看可用选项。

# 总结

祝贺你通过了这一章！数据争论可能不是分析工作流程中最令人兴奋的部分，但我们将在这方面花费大量时间，因此最好精通`pandas`所能提供的内容。

在这一章中，我们了解了更多关于什么是数据争论(除了一个数据科学术语)的知识，并获得了一些清理和重塑数据的第一手经验。利用`requests`库，我们再次练习使用 API 来提取感兴趣的数据；然后，我们使用`pandas`开始我们对数据争论的介绍，我们将在下一章继续。最后，我们学习了如何以各种方式处理重复、缺失和无效的数据点，并讨论了这些决策的后果。

基于这些概念，在下一章中，我们将学习如何聚合数据框架和处理时间序列数据。在继续之前，请务必完成本章末尾的练习。

# 练习

使用我们在本书中学到的知识和`exercises/`目录中的数据完成以下练习:

1.  We want to look at data for the **Facebook, Apple, Amazon, Netflix, and Google** (**FAANG**) stocks, but we were given each as a separate CSV file (obtained using the `stock_analysis` package we will build in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial Analysis – Bitcoin and the Stock Market*). Combine them into a single file and store the dataframe of the FAANG data as `faang` for the rest of the exercises:

    a)读入`aapl.csv`、`amzn.csv`、`fb.csv`、`goog.csv`、`nflx.csv`文件。

    b)给每个数据帧添加一列，称为`ticker`，表明它的股票代号(例如，苹果的是 Apple 这是你查找股票的方法。在这种情况下，文件名恰好是股票代码。

    c)将它们一起添加到单个数据帧中。

    d)将结果保存在名为`faang.csv`的 CSV 文件中。

2.  使用`faang`，使用类型转换将`date`列的值转换为日期时间，将`volume`列的值转换为整数。然后，按`date`和`ticker`排序。
3.  找出`faang`中`volume`值最低的七行。
4.  目前，数据介于长格式和宽格式之间。使用`melt()`使其完全成为长格式。提示:`date`和`ticker`是我们的 ID 变量(它们唯一地标识每一行)。我们需要融化剩下的部分，这样我们就没有单独的列用于`open`、`high`、`low`、`close`和`volume`。
5.  假设我们在 2018 年 7 月 26 日发现数据记录方式出现了小故障。我们应该如何处理这个问题？注意，这个练习不需要编码。
6.  The **European Centre for Disease Prevention and Control** (**ECDC**) provides an open dataset on COVID-19 cases called *daily number of new reported cases of COVID-19 by country worldwide* ([https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide)). This dataset is updated daily, but we will use a snapshot that contains data from January 1, 2020 through September 18, 2020\. Clean and pivot the data so that it is in wide format:

    a)读入`covid19_cases.csv`文件。

    b)使用`dateRep`列中的数据和`pd.to_datetime()`函数创建一个`date`列。

    c)将`date`列设置为索引，并对索引进行排序。

    d)分别用`USA`和`UK`替换所有出现的`United_States_of_America`和`United_Kingdom`。提示:`replace()`方法可以作为一个整体在数据帧上运行。

    e)使用`countriesAndTerritories`列，过滤到阿根廷、巴西、中国、哥伦比亚、印度、意大利、墨西哥、秘鲁、俄罗斯、西班牙、土耳其、英国和美国的已清理新冠肺炎案例数据。

    f)透视数据，使索引包含日期，列包含国家名称，值为案例计数(`cases`列)。务必用`0`填写`NaN`值。

7.  为了有效地确定每个国家的病例总数，我们需要我们将在第 4 章 、*汇总Pandas数据帧*中学习的汇总技巧，因此`covid19_cases.csv`文件中的 ECDC 数据已经为我们汇总并保存在`covid19_total_cases.csv`文件中。它包含每个国家的病例总数。使用此数据查找新冠肺炎病例总数最多的 20 个国家。提示:在读入 CSV 文件时，传入`index_col='cases'`，并注意在隔离国家之前转置数据会有所帮助。

# 延伸阅读

请查阅以下资源，了解本章所涵盖主题的更多信息:

*   *关系数据库设计快速入门教程*:[https://www . NTU . edu . SG/home/ehchua/programming/SQL/Relational _ Database _ Design . html](https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html)
*   *二分搜索法*:[https://www . khanacademy . org/computing/计算机科学/算法/二进制搜索/a/二进制搜索](https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search)
*   *递归如何工作——用流程图和视频解释*:[https://www . freecodecamp . org/news/How-Recursion-Works-explained-with-flow-and-a-video-de 61 f 40 CB 7 f 9/](https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/)
*   *Python f 弦*:【https://realpython.com/python-f-strings/ 
*   *整理数据(文章作者哈德利·韦翰)*:【https://www.jstatsoft.org/article/view/v059i10 
*   *优秀 Web API 设计的 5 条黄金法则*:[https://www . top tal . com/API-developers/5-Golden-Rules-for-Design-a-Great-we b-API](https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api)*****