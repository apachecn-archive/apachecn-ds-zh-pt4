<title>C13322_04_Epub_Final_SW</title> <link href="css/epub.css" rel="stylesheet" type="text/css"> 

# *第四章*

# 降维与无监督学习

## 学习目标

本章结束时，您将能够:

*   比较层次聚类分析(HCA)和 k 均值聚类
*   进行 HCA 并解释输出
*   为 k-means 聚类调整多个聚类
*   为降维选择最佳数量的主成分
*   使用线性判别函数分析(LDA)执行监督维度压缩

这一章将涵盖降维和无监督学习下的各种概念。

## 简介

在无监督学习中，**描述模型**用于探索性分析，以揭示未标记数据中的模式。无监督学习任务的例子包括用于**聚类**和用于**降维**的算法。在聚类中，观察值被分配到组内同质性高和组间异质性高的组中。简单地说，观察值被放入与其他观察值非常相似的样本簇中。聚类算法的用例非常多。例如，寻求通过针对特定客户进行营销广告和促销来提升销售额的分析师根据客户的购物行为来区分客户。

#### 注意

此外，在学术神经科学和运动行为研究中已经实现了层次聚类(https://www . research gate . net/profile/Ming-Yang _ Cheng/project/The-Effect-of neuro feedback-Training-on-Mental-Representation-and-Golf-puting-Performance/attachment/57c 8419808 aeef 0362 AC 36 a 5/AS:401522300080128 @ 1472741784217/download/Schack+)context=ProjectUpdatesLog )和 k-means 聚类已经用于欺诈检测(https://www . semantic scholar . org/paper/Fraud-Detection-in-Credit-Card-by-Clustering-Tech/3e 98 a9 AC 78 b5 b 89944720 C2 b 428 ebf 3 e 46d 9950 f)。

但是，在构建描述性或预测性模型时，确定要在模型中包含哪些功能以改进模型，以及要排除哪些功能(因为它们会削弱模型)可能是一项挑战。太多的要素可能会带来麻烦，因为模型中的变量数量越多，多重共线性和随后模型过度拟合的可能性就越大。此外，许多特征增加了模型的复杂性，并增加了模型调整和拟合的时间。

对于较大的数据集，这变得很麻烦。幸运的是，无监督学习的另一个用例是通过创建原始要素的组合来减少数据集中的要素数量。减少数据中的要素数量有助于消除多重共线性，并收敛于要素的组合，以最佳方式生成对看不见的测试数据表现良好的模型。

#### 注意

多重共线性是指至少两个变量相关的情况。这是线性回归模型中的一个问题，因为它不允许隔离每个独立变量和结果度量之间的关系。因此，系数和 p 值变得不稳定和不精确。

在本章中，我们将介绍两种广泛使用的无监督聚类算法:*层次聚类分析(HCA)* 和 *k 均值聚类*。此外，我们将使用*主成分分析(PCA)* 探索降维，并观察降维如何提高模型性能。最后，我们将实现线性判别函数分析 *(LDA)* 用于监督降维。

## 系统聚类分析(HCA)

当用户没有要构建的先验数量的聚类时，最好实施分层聚类分析。因此，使用 HCA 作为其他聚类技术的前身是一种常见的方法，其中推荐预定数量的聚类。HCA 的工作方式是将相似的观察值合并成聚类，并继续合并最接近的聚类，直到所有观察值合并成一个聚类。

HCA 将相似性确定为观察值之间的欧几里得距离，并在两点所在的距离处创建链接。

使用由 *n* 表示的特征数量，欧几里德距离使用以下公式计算:

![Figure 4.1: The Euclidean distance](image/C13322_04_01.jpg)

###### 图 4.1:欧几里德距离

计算完观察值和聚类之间的距离后，所有观察值之间的关系将使用树状图显示。树状图是树状结构，以水平线显示链接之间的距离。

托马斯·沙克博士()https://www . research gate . net/profile/Ming-Yang _ Cheng/project/The-Effect-of 神经反馈-心理表征训练-高尔夫推杆-表演/attachment/57c 8419808 aeef 0362 AC 36 a 5/AS:401522300080128 @ 1472741784217/下载/沙克+-+2012+-+测量 context=ProjectUpdatesLog )将这种结构与人脑联系起来，其中每个观察都是一个节点，观察之间的链接是神经元。

这创建了一个层次结构，其中密切相关的项目被一起“分块”成簇。此处显示了一个树状图示例:

![Figure 4.2: An example dendrogram
 
](image/C13322_04_02.jpg)

###### 图 4.2:一个示例树状图

y 轴表示欧几里得距离，而 x 轴表示每个观察的行索引。水平线表示观察之间的联系；越靠近 x 轴的链接表示距离越短，关系越密切。在本例中，似乎有三个集群。第一个分类包括绿色的观察值，第二个分类包括红色的观察值，第三个分类包括蓝绿色的观察值。

### 练习 34:构建 HCA 模型

为了演示 HCA，我们将使用来自加州大学欧文分校的 glass 数据集的改编版本(https://github . com/TrainingByPackt/Data-Science-with-Python/tree/master/chapter 04)。该数据包含 218 个观察值和 9 个特征，对应于玻璃中发现的各种氧化物的重量百分比:

*   RI:折射率
*   Na:钠的重量百分比
*   Mg:镁的重量百分比
*   铝:铝的重量百分比
*   Si:硅的重量百分比
*   k:钾的重量百分比
*   钙:钙的重量百分比
*   Ba:钡的重量百分比
*   铁:铁的重量百分比

在本练习中，我们将使用每种氧化物的折射率(RI)和重量百分比来划分玻璃类型。

1.  首先，我们将导入 pandas 并使用下面的代码读取`glass.csv`文件:

    ```
    import pandas as pd
    df = pd.read_csv('glass.csv')
    ```

2.  通过使用以下代码将`df.info()`打印到控制台来查找一些基本的数据帧信息:

    ```
    print(df.info()):
    ```

    ![Figure 4.3: DataFrame information
    ](image/C13322_04_03.jpg)

    ###### 图 4.3:数据帧信息

3.  为了消除数据中任何可能的顺序效应，我们将在构建任何模型之前打乱行，并将其保存为新的数据框对象，如下所示:

    ```
    from sklearn.utils import shuffle
    df_shuffled = shuffle(df, random_state=42)
    ```

4.  通过使用

    ```
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler() 
    scaled_features = scaler.fit_transform(df_shuffled)
    ```

    拟合和转换混洗数据，将每个观察值转换为 z 值
5.  使用`scaled_features`上的链接功能执行层次聚类。下面的代码将向您展示如何操作:

    ```
    from scipy.cluster.hierarchy import linkage 
    model = linkage(scaled_features, method='complete')
    ```

恭喜你！您已经成功构建了一个 HCA 模型。

### 练习 35:绘制 HCA 模型并分配预测

既然 HCA 模型已经建立，我们将通过使用树状图可视化聚类并使用可视化生成预测来继续分析。

1.  Display the dendrogram by plotting the linkage model as follows:

    ```
    import matplotlib.pyplot as plt 
    from scipy.cluster.hierarchy import dendrogram
    plt.figure(figsize=(10,5))
    plt.title('Dendrogram for Glass Data')
    dendrogram(model, leaf_rotation=90, leaf_font_size=6)
    plt.show()
    ```

    ![Figure 4.4: Dendogram for glass data
    ](image/C13322_04_04.jpg)

    ###### 图 4.4:玻璃数据的树枝状图

    #### 注意

    数据集中每个观察值或行的索引位于 x 轴上。欧几里得距离在 y 轴上。水平线是观察之间的联系。默认情况下，scipy 将对它找到的不同集群进行颜色编码。

    现在我们已经有了预测的观察值的聚类，我们可以使用`fcluster`函数来生成一个标签数组，这些标签对应于`df_shuffled`中的行。

2.  使用以下代码生成一个观察所属的聚类的预测标签:

    ```
    from scipy.cluster.hierarchy import fcluster
    labels = fcluster(model, t=9, criterion='distance')
    ```

3.  将标签数组作为一列添加到混洗数据中，并使用以下代码预览前五行:

    ```
    df_shuffled['Predicted_Cluster'] = labels
    print(df_shuffled.head(5))
    ```

4.  检查下图中的输出:

![Figure 4.5: The first five rows of df_shuffled after predictions have been matched to observations.
](image/C13322_04_05.jpg)

###### 图 4.5:预测与观察值匹配后 df_shuffled 的前五行。

我们已经成功地了解了监督学习和非监督学习之间的区别，如何建立 HCA 模型，如何可视化和解释 HCA 树状图，以及如何将预测的聚类标签分配给适当的观察。

在这里，我们利用 HCA 将我们的数据分为三组，并将观察值与其预测的聚类相匹配。HCA 模型的一些优点包括:

*   它们很容易建造
*   不需要预先指定簇的数量
*   可视化很容易解释

然而，HCA 的一些缺点包括:

*   终止标准方面的模糊性(即何时最终确定集群数量)
*   一旦做出聚类决定，该算法就不能调整
*   在包含许多要素的大型数据集上构建 HCA 模型的计算成本非常高

接下来给大家介绍另一种聚类算法，k-means 聚类。该算法通过具有在最初生成聚类时进行调整的能力，解决了 HCA 的一些缺点。它比 HCA 更节省计算量。

## K-均值聚类

像 HCA 一样，K-means 也使用距离将观察值分配到数据中未标记的聚类中。然而，k-means 并不像 HCA 那样将观察值相互关联，而是将观察值分配给 *k* (用户定义的数字)聚类。

为了确定每个观测值所属的聚类，随机生成 k 个聚类中心，并将观测值分配到其欧几里德距离最接近聚类中心的聚类中。像人工神经网络中的起始权重一样，聚类中心是随机初始化的。在随机生成聚类中心之后，有两个阶段:

*   分配阶段
*   Updating phase

    #### 注意

    记住随机生成的聚类中心是很重要的，我们将在本章的后面访问它。一些人认为这种随机生成的聚类中心是该算法的一个弱点，因为在对相同数据拟合相同模型时，结果会有所不同，并且不能保证将观察值分配给适当的聚类。我们可以通过利用循环的力量将其转化为优势。

在分配阶段，观察值被分配给具有最小欧氏距离的聚类，如下图所示:

![Figure 4.6: A scatterplot of observations and the cluster centers as denoted by the star, triangle, and diamond.](image/C13322_04_06.jpg)

###### 图 4.6:观察值和聚类中心的散点图，用星形、三角形和菱形表示。

接下来，在更新阶段，聚类中心被移动到该聚类中点的平均位置。这些聚类平均值称为质心，如下图所示:

![Figure 4.7: Shifting of the cluster centers to the cluster centroid.
](image/C13322_04_07.jpg)

###### 图 4.7:聚类中心向聚类质心的移动。

然而，一旦计算了质心，一些观察值由于比先前的聚类中心更靠近新的质心而被重新分配到不同的聚类。因此，模型必须再次更新其质心。如下图所示:

![Figure 4.8: Updating of the centroids after observation reassignment.
](image/C13322_04_08.jpg)

###### 图 4.8:重新分配观测值后质心的更新。

这种更新质心的过程一直持续到没有进一步的观测重新分配为止。最终质心如下图所示:

![Figure 4.9: The final centroid position and cluster assignments.
](image/C13322_04_09.jpg)

###### 图 4.9:最终的质心位置和集群分配。

使用来自*练习 34* 、*构建 HCA 模型*的相同 glass 数据集，我们将拟合具有用户定义的聚类数的 k-means 模型。接下来，由于组质心选择的随机性，我们将通过建立具有给定数量的聚类的 k 均值模型的集合并将每个观察值分配给预测的聚类的模式来增加我们预测的置信度。之后，我们将通过监控集群数量的平均*惯性*或集群内平方和来调整最佳集群数量，并通过添加更多集群来找到惯性收益递减的点。

### 练习 36:拟合 k 均值模型并分配预测

由于我们的数据已经准备好(参见*练习 34，构建 HCA 模型*)，并且我们理解了 k-means 算法背后的概念，我们将了解拟合 k-Means 模型、生成预测以及将这些预测分配给适当的观察值是多么容易。

导入、混洗和标准化玻璃数据集后:

1.  用任意数量的集群实例化一个 KMeans 模型，在本例中是两个集群，如下所示:

    ```
    from sklearn.cluster import KMeans
    model = KMeans(n_clusters=2)
    ```

2.  使用下面一行代码将模型与`scaled_features`匹配:

    ```
    model.fit(scaled_features)
    ```

3.  使用以下代码将我们模型中的分类标签保存到数组标签中:

    ```
    labels = model.labels_
    ```

4.  Generate a frequency table of the labels:

    ```
    import pandas as pd
    pd.value_counts(labels)
    ```

    为了获得更好的想法，请参考下面的截图:

    ![Figure 4.10: Frequency table of two clusters
    ](image/C13322_04_10.jpg)

    ###### 图 4.10:两个集群的频率表

    使用两个组，61 个观察值被放入第一组，157 个观察值被分组到第二组。

5.  将标签数组作为'`Predicted Cluster`'列添加到`df_shuffled`数据框中，并使用以下代码预览前五行:

    ```
    df_shuffled['Predicted_Cluster'] = labels
    print(df_shuffled.head(5))
    ```

6.  检查下图中的输出:

![Figure 4.11: First five rows of df_shuffled
](image/C13322_04_11.jpg)

###### 图 4.11:df _ shuffled 的前五行

### 活动 12: 集合 k-means 聚类和计算预测

当算法使用随机性作为其寻找最优解的方法的一部分时(即在人工神经网络和 k-means 聚类中)，对相同的数据运行相同的模型可能会导致不同的结论，从而限制我们对预测的信心。建议多次运行这些模型，并使用所有模型的汇总度量(即平均值、中值和众数)来生成预测。在本练习中，我们将构建 100 个 k 均值聚类模型的集合。

在玻璃数据集被导入、混洗和标准化后(参见*练习 34* 、*构建 HCA 模型*):

1.  实例化一个空数据框，为每个模型添加标签，并将其保存为新的数据框对象`labels_df`。
2.  使用 for 循环，遍历 100 个模型，在每次迭代中将预测的标签附加到`labels_df`作为新列。计算`labels_df`中每行的模式，并将其保存为`labels_df`中的新列。输出应该如下所示:

![Figure 4.12: First five rows of labels_df
](image/C13322_04_12.jpg)

###### 图 4.12:前五行标签 _df

#### 注意

这项活动的解决方案可在第 356 页找到。

我们通过迭代许多模型，在每次迭代中保存预测，并将最终预测指定为这些预测的模式，极大地增加了预测的可信度。然而，这些预测是由使用预定数量的聚类的模型生成的。除非我们事先知道聚类的数目，否则我们将希望发现最佳的聚类数目来分割我们的观察结果。

### 练习 37:通过 n 个簇计算平均惯性

k-means 算法通过最小化类内平方和或惯性将观察值分组到类中。因此，为了提高我们对 k-means 模型的聚类调整数量的信心，我们将把我们在*活动 12 中创建的循环，集合 k-means 聚类和计算预测*(有一些小的调整)放在另一个循环内，该循环将迭代一个范围`n_clusters`。这创建了一个嵌套循环，它遍历`n_clusters`的 10 个可能值，并在每次迭代中构建 100 个模型。在 100 次内部迭代中的每一次，都将计算模型惯性。对于 10 次外部迭代中的每一次，将计算 100 个模型的平均惯性，从而得到每个`n_clusters`值的平均惯性值。

在玻璃数据集被导入、混洗和标准化后(参见*练习 34，构建 HCA 模型*):

1.  在循环之外导入我们需要的包，如下所示:

    ```
    from sklearn.cluster import KMeans
    import numpy as np
    ```

2.  从内向外工作更容易构建和理解嵌套循环。首先，实例化一个空列表`inertia_list`，我们将在内部循环的每次迭代后为其追加惯性值，如下所示:

    ```
    inertia_list = []
    ```

3.  在 for 循环中，我们将使用下面的代码遍历 100 个模型:

    ```
    for i in range(100):
    ```

4.  Inside the loop, build a `KMeans` model with `n_clusters=x`, as follows:

    ```
    model = KMeans(n_clusters=x)
    ```

    #### 注意

    x 的值是由外部的 for 循环决定的，我们还没有涉及到它，但是我们很快就会详细介绍它。

5.  将模型拟合到`scaled_features`，如下所示:

    ```
    model.fit(scaled_features)
    ```

6.  获取惯性值并保存到物体惯性中，如下:

    ```
    inertia = model.inertia_
    ```

7.  使用以下代码将惯性附加到`inertia_list`:

    ```
    inertia_list.append(inertia)
    ```

8.  移动到外部循环，实例化另一个空列表来存储平均惯性值，如下所示:

    ```
    mean_inertia_list = []
    ```

9.  使用下面的代码遍历`n_clusters`的值 1 到 10:

    ```
    for x in range(1, 11):
    ```

10.  在 inside for 循环已经运行了 100 次迭代，并且 100 个模型中的每一个的惯性值已经被附加到`inertia_list`之后，计算这个列表的平均值并保存为对象，`mean_inertia`如下:

    ```
    mean_inertia = np.mean(inertia_list)
    ```

11.  将`mean_inertia`追加到`mean_inertia_list`如下图:

    ```
    mean_inertia_list.append(mean_inertia)
    ```

12.  在总共 1000 次迭代中完成 10 次 100 次迭代后，`mean_inertia_list`包含 10 个值，它们是`n_clusters`的每个值的平均惯性值。
13.  如下面的代码所示打印`mean_inertia_list`。数值如下图所示:

    ```
    print(mean_inertia_list)  
    ```

![Figure 4.13: mean_inertia_list
](image/C13322_04_13.jpg)

###### 图 4.13:均值惯性列表

### 练习 38:绘制 n 个簇的平均惯性

上接练习 38:

现在我们已经为`n_clusters`的每个值生成了 100 个模型的平均惯性，我们将通过`n_clusters`绘制平均惯性。然后，我们将讨论如何直观地评估使用`n_clusters`的最佳价值。

1.  首先，导入 matplotlib 如下:

    ```
    import matplotlib.pyplot as plt
    ```

2.  创建一个数字列表，并将其保存为对象 x，这样我们就可以在 x 轴上绘制它，如下所示:

    ```
    x = list(range(1, len(mean_inertia_list)+1))
    ```

3.  保存`mean_inertia_list`，作为对象 y，如下图:

    ```
    y = mean_inertia_list
    ```

4.  根据聚类数绘制平均惯性，如下:

    ```
    plt.plot(x, y)
    ```

5.  使用

    ```
     plt.title('Mean Inertia by n_clusters') 
    ```

    将图标题设置为“`Mean Inertia by n_clusters`
6.  使用`plt.xlabel('n_clusters')`标记 x 轴“`n_clusters`”，使用以下代码标记 y 轴“`Mean Inertia`”:

    ```
    plt.ylabel ('Mean Inertia')
    ```

7.  使用以下代码将 x 轴上的刻度标签设置为 x 中的值:

    ```
    plt.xticks(x)
    ```

8.  Display the plot used in `plt.show()`. To better understand, refer to the following code:

    ```
    plt.plot(x, y)
    plt.title('Mean Inertia by n_clusters')
    plt.xlabel('n_clusters')
    plt.xticks(x)
    plt.ylabel('Mean Inertia')
    plt.show()
    ```

    对于结果输出，请参考下面的屏幕截图:

![Figure 4.14: Mean inertia by n_clusters
](image/C13322_04_14.jpg)

###### 图 4.14:n 个集群的平均惯性

为了确定`n_clusters`的最佳数量，我们将使用“肘法”也就是说，在图中的这个点上，更多集群增加的复杂性会产生递减的回报。从图 4.14 中，我们可以看到，从`n_clusters` 1 到 3，平均惯性迅速减小。在`n_clusters`等于 3 之后，平均惯性的下降似乎变得不那么快，并且惯性的下降可能不值得增加额外的集群的复杂性。因此，在这种情况下，`n_clusters`的合适数量是 3。

然而，如果数据有太多的维度，k-means 算法可能会受到膨胀的欧几里德距离和随后的错误结果的维数灾难的影响。因此，在拟合 k 均值模型之前，鼓励使用降维策略。

减少维度的数量有助于消除多重共线性，并减少拟合模型的时间。**主成分分析** ( **PCA** )是通过发现数据中的一组底层线性变量来降低维数的常用方法。

## 主成分分析

在高层次上，PCA 是一种从称为**成分**的原始特征创建不相关线性组合的技术。在主成分中，第一个成分解释了数据中最大比例的方差，而随后的成分解释了逐渐减少的方差。

为了演示 PCA，我们将:

*   用所有主要成分拟合 PCA 模型
*   通过设置解释方差的阈值来调整主成分的数量，以保留在数据中
*   将这些成分拟合到 k-均值聚类分析中，并比较 PCA 变换前后的 k-均值性能

### 练习 39:拟合主成分分析模型

在本练习中，您将学习使用我们在*练习 34 中准备的数据拟合一个通用 PCA 模型，构建一个 HCA 模型*并简要解释 PCA。

1.  如下所示实例化一个 PCA 模型:

    ```
    from sklearn.decomposition import PCA
    model = PCA()
    ```

2.  将 PCA 模型拟合到`scaled_features`，如下面的代码所示:

    ```
    model.fit(scaled_features)
    ```

3.  获取每个组件的数据中解释的方差的比例，将数组保存为对象`explained_var_ratio`，并将值打印到控制台，如下所示:

    ```
    explained_var_ratio = model.explained_variance_ratio_
    print(explained_var_ratio)
    ```

4.  对于结果输出，请参考下面的屏幕截图:

![Figure 4.15: Explained variance in the data for each principal component
](image/C13322_04_15.jpg)

###### 图 4.15:解释每个主成分的数据差异

每个主成分解释了数据中方差的一部分。在这个练习中，第一个主成分解释了数据中 0.35 的方差，第二个主成分解释了。25，第三个. 13%，以此类推。总之，这九个组成部分解释了 100%的数据差异。降维的目标是减少数据中的维数，目的是限制过度拟合和拟合后续模型的时间。因此，我们不会保留所有九个组件。然而，如果我们保留的成分太少，数据中可解释方差的百分比将会很低，随后的模型将会不适合。因此，数据科学家面临的一个挑战是确定最小化过度拟合和欠拟合的`n_components`的数量。

### 练习 40:使用解释方差的阈值选择 n 个分量

在*练习 39* 、*拟合 PCA 模型*中，您学习了用所有可用的主成分拟合 PCA 模型。然而，保留所有的主成分并不会减少数据的维数。在本练习中，我们将通过保留解释数据中方差阈值的组件来减少数据的维数。

1.  Determine the number of principal components in which a minimum of 95% of the variance in the data is explained by calculating the cumulative sum of explained variance by the principal component. Let's look at the following code, to see how it's done:

    ```
    import numpy as np
    cum_sum_explained_var = np.cumsum(model.explained_variance_ratio_)
    print(cum_sum_explained_var)
    ```

    对于结果输出，请参考下面的屏幕截图:

    ![Figure 4.16: The cumulative sum of the explained variance for each principal component
    ](image/C13322_04_16.jpg)

    ###### 图 4.16:每个主成分的解释方差的累积和

2.  将数据中保留的差异百分比阈值设置为 95%，如下:

    ```
    threshold = .95
    ```

3.  使用这个阈值，我们将遍历累积解释方差的列表，并查看它们在哪里解释了数据中不少于 95%的方差。由于我们将遍历`cum_sum_explained_var`的索引，我们将使用下面的代码实例化我们的循环:

    ```
    for i in range(len(cum_sum_explained_var)):
    ```

4.  检查`cum_sum_explained_var`项是否大于等于 0.95，如下图:

    ```
    if cum_sum_explained_var[i] >= threshold:
    ```

5.  If that logic is met, then we will add 1 to that index (because we cannot have 0 principal components), save the value as an object, and break the loop. To do this, we will use `best_n_components = i+1` inside of the if statement and break in the next line. Look at the following code to get an idea:

    ```
    best_n_components = i+1
    break
    ```

    if 语句中的最后两行指示循环在不符合逻辑的情况下不做任何事情:

    ```
    else:
    pass
    ```

6.  Print a message detailing the best number of components using the following code:

    ```
    print('The best n_components is {}'.format(best_n_components))
    ```

    查看前一行代码的输出:

![Figure 4.17: The output message displaying number of components
](image/C13322_04_17.jpg)

###### 图 4.17:显示组件数量的输出消息

`best_n_components`的值是 6。我们可以用`n_components = 6`改装另一个 PCA 模型，将数据转换成主成分，并在新的 k-means 模型中使用这些成分来降低惯性值。此外，我们可以将使用 PCA 转换数据构建的模型的惯性值与使用非 PCA 转换数据构建的模型的`n_clusters`值进行比较。

### 活动 13:评估 PCA 转换后的聚类平均惯性

现在，我们已经知道了保留数据中至少 95%方差的分量数，如何将我们的特征转换为主分量，以及如何通过嵌套循环调整 k-means 聚类的最佳聚类数，我们将在本练习中将它们放在一起。

从*练习 40* 继续:

1.  实例化一个 PCA 模型，其参数`n_components`的值等于`best_n_components`(也就是说，记住，`best_n_components = 6`)。
2.  将模型拟合到`scaled_features`，并将其转换为前六个主要成分
3.  使用嵌套循环，计算`n_clusters`在值 1 到 10 的 100 个模型上的平均惯性(参见*练习 40* ，*使用解释方差阈值选择 n 个组件*)。

![Figure 4.18: mean_inertia_list_PCA
](image/C13322_04_18.jpg)

###### 图 4.18: mea n_inertia_list_PCA

现在，很像在*练习 38* ，*中通过 n_clusters* 绘制平均惯性，我们对`n_clusters` (1 到 10)的每个值都有一个平均惯性值。然而，`mean_inertia_list_PCA`包含 PCA 变换后`n_clusters`的每个值的平均惯性值。但是，我们如何知道 k-means 模型在 PCA 变换后是否表现得更好呢？在下一个练习中，我们将直观地比较在每个`n_clusters`值下 PCA 变换前后的平均惯性值。

#### 注意

这项活动的解决方案可在第 357 页找到。

### 练习 41:n 个簇的惯性的视觉比较

为了直观地比较 PCA 变换前后`n_clusters`的平均惯性，我们将稍微修改在*练习 38 中创建的图，绘制 n_clusters 的平均惯性，*为:

*   在曲线图中添加第二条线，通过`n_clusters`显示 PCA 变换后的平均惯性
*   创建区分线条的图例
*   Changing the title

    #### 注意

    为了使这种可视化正常工作，来自*练习 38* 的`mean_inertia_list`，*通过 n_clusters* 绘制平均惯性，必须仍然在环境中。

从*活动 13* 继续:

1.  使用以下代码导入 Matplotlib】
2.  创建一个数字列表，并将其保存为对象 x，这样我们就可以在 x 轴上绘制它，如下所示:

    ```
    x = list(range(1,len(mean_inertia_list_PCA)+1))
    ```

3.  使用以下代码将`mean_inertia_list_PCA`保存为对象 y:

    ```
    y = mean_inertia_list_PCA
    ```

4.  使用以下命令将`mean_inertia_list`保存为对象 y2:

    ```
    y2 = mean_inertia_list
    ```

5.  Plot mean inertia after a PCA transformation by number of clusters using the following code:

    ```
    plt.plot(x, y, label='PCA')
    ```

    在 PCA 变换之前，使用下面的等式，通过聚类数添加我们的第二条平均惯性线:

    ```
    plt.plot(x, y2, label='No PCA)
    ```

6.  将图标题设置为“T0”，如下所示:

    ```
    plt.title('Mean Inertia by n_clusters for Original Features and PCA Transformed Features')
    ```

7.  使用以下代码标记 x 轴'`n_clusters`':

    ```
    plt.xlabel('n_clusters')
    ```

8.  使用

    ```
    plt.ylabel('Mean Inertia')
    ```

    标记 y 轴“`Mean Inertia`
9.  使用`plt.xticks(x)`将 x 轴上的刻度标签设置为 x 中的值。
10.  使用

    ```
    plt.legend()
    plt.show()
    ```

    显示图例，并按如下方式显示图形

![Figure 4.19: Mean inertia by n_clusters for original features (orange) and PCA transformed features (blue)](image/C13322_04_19.jpg)

###### 图 4.19:原始特征(橙色)和 PCA 变换特征(蓝色)的 n 簇平均惯性

从该图中，我们可以看到，在使用 PCA 变换特征的模型中，每个数量的聚类的惯性都较低。这表明，在 PCA 变换之后，相对于变换之前，每个聚类中的组质心和观测值之间的距离更小。因此，对原始特征使用 PCA 变换，我们能够减少特征的数量，同时通过减少类内平方和(即惯性)来改进我们的模型。

HCA 和 k-means 聚类是两种广泛使用的用于分割的无监督学习技术。主成分分析可用于帮助减少数据的维数，并以无监督的方式改进模型。另一方面，线性判别函数分析(LDA)是一种通过数据压缩减少维数的监督方法。

## 使用线性鉴别分析(LDA)的监督数据压缩

如前所述，PCA 将特征转换成一组变量，以最大化特征之间的差异。在 PCA 中，拟合模型时不考虑输出标签。同时，LDA 使用因变量来帮助将数据压缩成最能区分结果变量类别的特征。在本节中，我们将介绍如何使用 LDA 作为监督数据压缩技术。

为了演示使用 LDA 作为监督维度压缩技术，我们将:

*   用所有可能的数据拟合 LDA 模型`n_components`
*   将我们的功能转换为`n_components`
*   调整`n_components`的数量

### 练习 42:拟合 LDA 模型

为了作为监督学习者使用 LDA 算法的默认参数来拟合模型，我们将使用稍微不同的 glass 数据集，`glass_w_outcome.csv`。(https://github . com/TrainingByPackt/Data-Science-with-Python/tree/master/chapter 04)这个数据集包含与 glass 相同的 9 个特征，还包含一个与 glass 类型相对应的结果变量 Type。对于浮法加工的建筑窗户、非浮法加工的建筑窗户和前照灯，类型分别标记为 1、2 和 3。

1.  导入`glass_w_outcome.csv`文件，并使用以下代码将其保存为对象 df:

    ```
    import pandas as pd
    df = pd.read_csv(‘glass_w_outcome.csv’)
    ```

2.  打乱数据以消除任何排序效果，并将其保存为数据帧`df_shuffled`，如下所示:

    ```
    from sklearn.utils import shuffle
    df_shuffled = shuffle(df, random_state=42)
    ```

3.  将'`Type`'保存为`DV`(即因变量)，如下:

    ```
    DV = ‘Type’
    ```

4.  分别使用`X = df_shuffled.drop(DV, axis=1)`和`y = df_shuffled[DV]`将混洗的数据分成特征(即 X)和结果(即 y)。
5.  将 X 和 y 分解为测试和训练，如下所示:

    ```
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
    ```

6.  标度`X_train`和`X_test`很少使用下面的代码:

    ```
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler() 
    X_train_scaled = scaler.fit_transform(X_train) 
    X_test_scaled = scaler.fit_transform(X_test)
    ```

7.  Instantiate the LDA model and save it as model. The following will show you how.

    ```
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    -model = LinearDiscriminantAnalysis()
    ```

    #### 注意

    通过实例化没有参数`for n_components`的 LDA 模型，我们将返回所有可能的组件。

8.  使用以下公式将模型拟合到训练数据:

    ```
    model.fit(X_train_scaled, y_train)
    ```

9.  See the result output below:![Figure 4.20: Output from fitting linear discriminant function analysis
    ](image/C13322_04_20.jpg)

    ###### Figure 4.20: Output of fitting linear discriminant function analysis

10.  Much like in PCA, we can return the percentage of variance explained by each component.

    ```
    model.explained_variance_ratio_
    ```

    输出如下图所示。

![Figure 4.21: Explained variance by component.
](image/C13322_04_21.jpg)

###### 图 4.21:按组件解释的差异。

#### 注意

第一个成分解释了数据中 95.86%的方差，第二个成分解释了数据中 4.14%的方差，总计 100%。

我们已经成功地拟合了 LDA 模型，将我们的数据从九个特征压缩到两个特征。将特征减少到两个减少了调整和适应机器学习模型的时间。然而，在分类器模型中使用这些特征之前，我们必须将训练和测试特征转换成它们的两个组成部分。在下一个练习中，我们将展示如何做到这一点。

### 练习 43:在分类模型中使用 LDA 转换组件

使用监督数据压缩，我们将把我们的训练和测试特征(即分别为`X_train_scaled`和`X_test_scaled`)转换成它们的组件，并在它们上面拟合一个`RandomForestClassifier`模型。

从*练习 42* 继续:

1.  将`X_train_scaled`压缩成如下组件:

    ```
    X_train_LDA = model.transform(X_train_scaled)
    ```

2.  使用

    ```
    X_test_LDA = model.transform(X_test_scaled)
    ```

    将`X_test`压缩成它的组件
3.  Instantiate a `RandomForestClassifier` model as follows:

    ```
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier()
    ```

    #### 注意

    我们将使用`RandomForestClassifier`模型的默认超参数，因为调整超参数超出了本章的范围。

4.  Fit the model to the compressed training data using the following code:

    ```
    model.fit(X_train_LDA, y_train)
    ```

    请参见下面的结果输出:

    ![Figure 4.22: Output after fitting random forest classifier model
    ](image/C13322_04_22.jpg)

    ###### 图 4.22:拟合随机森林分类器模型后的输出

5.  在`X_test_LDA`上生成预测并将它们保存为数组，预测使用下面的代码:

    ```
    predictions = model.predict(X_test_LDA)
    ```

6.  Evaluate model performance by comparing predictions to `y_test` using a confusion matrix. To generate and print a confusion matrix see the code below:

    ```
    from sklearn.metrics import confusion_matrix 
    import pandas as pd
    import numpy as np
    cm = pd.DataFrame(confusion_matrix(y_test, predictions))
    cm[‘Total’] = np.sum(cm, axis=1)
    cm = cm.append(np.sum(cm, axis=0), ignore_index=True)
    cm.columns = [‘Predicted 1’, ‘Predicted 2’, ‘Predicted 3’, ‘Total’]
    cm = cm.set_index([[‘Actual 1’, ‘Actual 2’, ‘Actual 3’, ‘Total’]])
    print(cm)
    ```

    输出如下图所示:

    ![Figure 4.23: 3x3 confusion matrix for evaluating RandomForestClassifier model performance using the LDA compressed data](image/C13322_04_23.jpg)

###### 图 4.23: 3x 3 使用 LDA 压缩数据评估 RandomForestClassifier 模型性能的混淆矩阵

## 总结

本章向您介绍了两种广泛使用的无监督聚类算法，HCA 和 k-means 聚类。在学习 k-means 聚类的过程中，我们利用循环的力量来创建模型集，以调整聚类的数量，并在我们的预测中获得更多的信心。在 PCA 部分，我们确定了用于降维的主成分的数量，并将这些成分拟合到 k-means 模型。此外，我们比较了 PCA 变换前后 k-means 模型性能的差异。我们介绍了一种算法，LDA，它以监督的方式降低维数。最后，我们调整了 LDA 中组件的数量，方法是遍历组件的所有可能值，并以编程方式返回随机森林分类器模型中产生最佳准确度分数的值。现在你应该对降维和无监督学习技术感到满意了。

在这一章中，我们简要地介绍了创建情节；然而，在下一章，我们将学习结构化数据以及如何使用 XGboost 和 Keras 库