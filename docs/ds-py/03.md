

# 三、通过 Scikit-Learn 入门机器学习

## 学习目标

本章结束时，您将能够:

*   为不同类型的监督学习模型准备数据。
*   使用网格搜索调整模型超参数。
*   从优化模型中提取特征重要性。
*   评估分类和回归模型的性能。

在这一章中，我们将讨论处理数据和为分析准备数据的重要概念。

## 简介

scikit-learn 是一个免费的开源库，为 Python 而建，包含各种有监督和无监督的机器学习算法。此外，scikit-learn 提供了数据预处理、超参数调整和模型评估等功能，我们将在接下来的章节中介绍这些功能。它简化了模型构建过程，并且易于安装在各种平台上。scikit-learn 始于 2007 年，是 David Corneapeau 的谷歌代码之夏项目，经过一系列的开发和发布，scikit-learn 已经发展成为学者和专业人士用于机器学习的主要工具之一。

在本章中，我们将学习构建各种广泛使用的建模算法，即线性和逻辑回归、支持向量机(SVM)、决策树和随机森林。首先，我们将涵盖线性和逻辑回归。

## 线性和逻辑回归简介

在回归中，使用一个或多个自变量来预测单个因变量或结果变量。回归的用例包括但不限于预测:

*   在给定各种团队统计数据的情况下，团队的胜率
*   考虑到家族史和一些生理和心理特征，心脏病的风险
*   给定几种气候测量，降雪的可能性

线性和逻辑回归是预测这种结果的流行选择，因为它们的可解释性简单而透明，并且能够推断出训练数据中没有的值。线性回归的最终目标是通过观察值绘制一条直线，使直线和观察值之间的绝对距离最小(即最佳拟合线)。因此，在线性回归中，假设特征和连续因变量之间的关系遵循直线。直线以斜率-截距形式定义(即 *y = a + bx* ，其中 *a* 为截距(即 *x* 为 0 时 *y* 的值)， *b* 为斜率， *x* 为自变量。本章将介绍两种类型的线性回归:简单线性回归和多元线性回归。

### 简单线性回归

简单线性回归模型使用*y =**α**+**β**x*定义一个特征和连续结果变量之间的关系。这个等式就像斜率-截距形式，其中 *y* 表示因变量的预测值， *α* 表示截距， *β* (beta)表示斜率， *x* 是自变量的值。给定 *x* ，回归模型计算出 *α* 和 *β* 的值，使预测的 *y* 值(即 *ŷ* )和实际的 *y* 值之间的绝对差值最小。

例如，如果我们使用以米为单位的身高(m)作为唯一的预测变量来预测以千克(kg)为单位的个人体重，并且简单线性回归模型计算出 1.5 作为 *α* 的值，50 作为 *β* 的系数，则该模型可以解释为身高每增加 1 米，体重增加 50 kg。因此，我们可以使用 y = 1.5 + (50 x 1.8)来预测身高 1.8 米的人的体重为 91.5 千克。在下面的练习中，我们将使用 scikit-learn 演示简单的线性回归。

### 练习 21:为线性回归模型准备数据

为了准备简单线性回归模型的数据，我们将使用 Szeged 2006-2016 年数据集中的随机天气子集，该数据集包括匈牙利 Szeged 从 2006 年 4 月 1 日到 2016 年 9 月 9 日的每小时天气测量值。调整后的数据以一个`.csv`文件(https://github . com/TrainingByPackt/Data-Science-with-Python/blob/master/chapter 02/weather . CSV)的形式提供，由 8 个变量的 10，000 个观测值组成:

*   `Temperature_c` : 摄氏温度
*   `Humidity`:湿度的比例
*   `Wind_Speed_kmh`:风速，单位为千米/小时
*   `Wind_Bearing_Degrees`:从正北顺时针以度为单位的风向
*   `Visibility_km`:以公里为单位的能见度
*   `Pressure_millibars`:以毫巴为单位测量的大气压力
*   `Rain`:雨= 1，雪= 0
*   温暖、正常或寒冷

1.  使用以下代码导入`weather.csv`数据集:

    ```py
    import pandas as pd
    df = pd.read_csv('weather.csv')
    ```

2.  使用`df.info()` :![Figure 3.1: Information describing df
    ](image/C13322_03_01.jpg)

    ###### 浏览数据图 3.1:描述 df 的信息

3.  The `Description` column is the lone categorical variable in `df`. Check the number of levels in `Description` as follows:

    ```py
    levels = len(pd.value_counts(df['Description']))
    print('There are {} levels in the Description column'.format(levels))
    ```

    关卡数量如以下截图所示:

    ![Figure 3.2: Number of levels in the 'Description' column](image/C13322_03_02.jpg)

    ###### 图 3.2:“描述”列中的级别数

    #### 注意

    多类、分类变量必须通过一个称为“虚拟编码”的过程转换成虚拟变量对多类分类变量进行虚拟编码会创建 n-1 个新的二进制特征，这些特征对应于分类变量中的级别。例如，具有三个级别的多类分类变量将创建两个二元要素。在多类、分类特征被虚拟编码后，原始特征必须被丢弃。

4.  To dummy code all multi-class, categorical variables, refer to the following code:

    ```py
    import pandas as pd
    df_dummies = pd.get_dummies(df, drop_first=True)
    ```

    #### 注意

    最初的数据框架`df`由八列组成，其中一列(即描述)是一个多类的分类变量，有三个级别。

5.  在第 4 步中，我们将这个特征转换为 n-1(即 2)，分离哑变量，并丢弃原始特征`Description`。因此，`df_dummies`现在应该比 df 多包含一列(即 9 列)。使用下面的代码检查一下:

    ```py
    print('There are {} columns in df_dummies'	.format(df_dummies.shape[1]))
    ```

    ![Figure 3.3: Number of columns after dummy coding
    ](image/C13322_03_03.jpg)

    ###### 图 3.3:虚拟编码后的列数

6.  为了消除数据中任何可能的顺序效应，在将数据分成特征(`X`)和结果(`y`)之前，最好先打乱数据的行。要打乱`df_dummies`中的行，请参考这里的代码:

    ```py
    from sklearn.utils import shuffle
    df_shuffled = shuffle(df_dummies, random_state=42)
    ```

7.  Now that the data has been shuffled, we will split the rows in our data into features (`X`) and the dependent variable (`y`).

    #### 注意

    线性回归用于预测连续的结果。因此，在本练习中，我们将假设连续变量`Temperature_c`(以摄氏度表示的温度)是因变量，并且我们正在准备数据以拟合线性回归模型。

8.  将`df_shuffled`拆分为`X`和`y`如下:

    ```py
    DV = 'Temperature_c'
    X = df_shuffled.drop(DV, axis=1)
    y = df_shuffled[DV]
    ```

9.  Split `X` and `y` into testing and training data using the code here:

    ```py
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
    ```

    既然数据已经被虚拟编码、混洗、分割成`X`和`y`，并进一步分割成测试和训练数据集，它就准备好用于线性或逻辑回归模型。

    这里的截图显示了`X_train`的前五行:

![Figure 3.4: The first five rows of X_train
](image/C13322_03_04.jpg)

###### 图 3.4:X _ train 的前五行

### 练习 22:拟合一个简单的线性回归模型并确定截距和系数

在本练习中，我们将继续使用我们在练习 21 中准备的数据来拟合一个简单的线性回归模型，以从湿度预测摄氏温度。

继续练习 21，执行以下步骤:

1.  要实例化一个线性回归模型，参考这里的代码:

    ```py
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    ```

2.  使用以下代码将模型拟合到训练数据的`Humidity`列:

    ```py
    model.fit(X_train[['Humidity']], y_train)
    ```

    ![Figure 3.5: The output from fitting the simple linear regression model
    ](image/C13322_03_05.jpg)

    ###### 图 3.5:拟合简单线性回归模型的输出

3.  使用以下代码提取截距值:

    ```py
    intercept = model.intercept_
    ```

4.  如下提取`coefficient`的值:

    ```py
    coefficient = model.coef_
    ```

5.  现在，我们可以使用这里的代码:

    ```py
    print('Temperature = {0:0.2f} + ({1:0.2f} x Humidity)'.format(intercept, coefficient[0]))
    ```

    ![Figure 3.6: A formula to predict temperature in Celsius from humidity using simple linear regression
    ](image/C13322_03_06.jpg)打印一条消息，其中包含预测摄氏温度的公式

###### 图 3.6:使用简单线性回归从湿度预测摄氏温度的公式

干得好！根据这个简单的线性回归模型，湿度值为 0.78 的一天的预测温度为 10.56 摄氏度。既然我们已经熟悉了提取简单线性回归模型的截距和系数，那么是时候生成预测并随后评估模型在看不见的测试数据上的表现了。

#### 教学技巧

练习计算不同湿度下的温度。

### 练习 23:生成预测并评估简单线性回归模型的性能

监督学习的真正目的是使用现有的标记数据来生成预测。因此，本练习将演示如何对测试特征生成预测，并通过将预测值与实际值进行比较来生成模型性能指标。

从*练习 22* 继续，执行以下步骤:

1.  Generate predictions on the test data using the following:

    ```py
    predictions = model.predict(X_test[['Humidity']])
    ```

    #### 注意

    评估模型性能的常用方法是使用散点图检查预测值和实际值之间的相关性。散点图显示实际值和预测值之间的关系。完美的回归模型将在预测值和实际值之间显示一条直的对角线。预测值和实际值之间的关系可以使用 Pearson r 相关系数来量化。在下面的步骤中，我们将创建预测值和实际值的散点图。

2.  It is helpful if the correlation coefficient is displayed in the plot's title. The following code will show us how to do this:

    ```py
    import matplotlib.pyplot as plt
    from scipy.stats import pearsonr
    plt.scatter(y_test, predictions)
    plt.xlabel('Y Test (True Values)')
    plt.ylabel('Predicted Values')
    plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))
    plt.show()
    ```

    以下是结果输出:

    ![Figure 3.7: Predicted versus actual values from a simple linear regression model
    ](image/C13322_03_07.jpg)

    ###### 图 3.7:简单线性回归模型的预测值和实际值

    #### 注意

    Pearson r 值为 0.62，预测值和实际值之间存在适度的正线性相关。完美的模型应该是图上的所有点都在一条直线上，r 值为 1.0。

3.  A model that fits the data very well will have normally distributed residuals. To create a density plot of the residuals, refer to the following code:

    ```py
    import seaborn as sns
    from scipy.stats import shapiro
    sns.distplot((y_test - predictions), bins = 50)
    plt.xlabel('Residuals')
    plt.ylabel('Density')
    plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))
    plt.show()
    ```

    请参考此处的结果输出:

    ![Figure 3.8: A histogram of residuals from a simple linear regression model
    ](image/C13322_03_08.jpg)

    ###### 图 3.8:简单线性回归模型的残差直方图

    #### 注意

    直方图显示残差是负偏的，标题中的 Shapiro W p 值告诉我们该分布不是正态分布。这进一步证明了我们的模型还有改进的空间。

4.  Lastly, we will compute metrics for mean absolute error, mean squared error, root mean squared error, and R-squared, and put them into a DataFrame using the code here:

    ```py
    from sklearn import metrics
    import numpy as np
    metrics_df = pd.DataFrame({'Metric': ['MAE', 
                                          'MSE', 
                                          'RMSE', 
                                          'R-Squared'],
                              'Value': [metrics.mean_absolute_error(y_test, predictions),
                                        metrics.mean_squared_error(y_test, predictions),
                                        np.sqrt(metrics.mean_squared_error(y_test, predictions)),
                                        metrics.explained_variance_score(y_test, predictions)]}).round(3)
    print(metrics_df)
    ```

    请参考结果输出:

![Figure 3.9: Model evaluation metrics from a simple linear regression model
](image/C13322_03_09.jpg)

###### 图 3.9:来自简单线性回归模型的模型评估指标

**平均绝对误差** ( **MAE** )是预测值与实际值之间的平均绝对差值。**均方误差** ( **MSE** )是预测值和实际值的平方差的平均值。**均方根误差** ( **RMSE** )是 MSE 的平方根。r 平方告诉我们模型可以解释的因变量中方差的比例。因此，在这个简单的线性回归模型中，湿度只能解释温度变化的 38.9%。此外，我们的预测误差在 6.052 摄氏度以内。

在这里，我们已经成功地使用 scikit-learn 来拟合和评估一个简单的线性回归模型。这是成为机器学习大师的激动人心的旅程的第一步。接下来，我们将继续扩展我们的回归知识，并通过探索多元线性回归来改进这个模型。

## 多元线性回归

多元线性回归模型使用*y =**α**+*+*β**1**x**i1**+**β**2**x**I2**+**β*同样， *α* 表示截距， *β* 表示模型中每个特征( *x* )的斜率。因此，如果我们使用以*米*为单位的身高、以毫克每分升(*毫克/分升)*为单位的总胆固醇以及每天心血管锻炼的分钟数来预测一个人的体重，那么多元线性回归模型会计算出 1.5 作为 *α* 的值，50 作为*β*1 的系数，0.1 作为*β*2】2 的系数 以及-0.4 作为 *β* *3* 的系数，该模型可以解释为高度每增加 1 *m* ，重量增加 50 kg，控制模型中的所有其他特征。 此外，总胆固醇每增加 1 mg/dL，体重增加 0.1 kg，控制模型中的所有其他特征。最后，在控制模型中所有其他特征的情况下，每天心血管锻炼每一分钟，体重减少 0.4 千克。这样，我们就可以用*y = 1.5+(0.1x 50)+(200 x 0.5)+(30x-0.4)*来预测一个身高 1.8 m，总胆固醇为 200 mg/dL，每天完成 30 分钟心血管运动的个体的体重为 99.5 kg。在下面的练习中，我们将演示如何使用 scikit-learn 进行多元线性回归。

### 练习 24:拟合多元线性回归模型并确定截距和系数

在本练习中，我们将继续使用我们在*练习 21* 、*为线性回归模型准备数据*中准备的数据，来拟合多元线性回归模型，以根据数据中的所有特征预测摄氏温度。

继续练习 23，执行以下步骤:

1.  要实例化一个线性回归模型，参考这里的代码:

    ```py
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    ```

2.  使用以下代码将模型拟合到训练数据:

    ```py
    model.fit(X_train, y_train) 
    ```

    ![Figure 3.10: The output from fitting the multiple linear regression model
    ](image/C13322_03_10.jpg)

    ###### 图 3.10:拟合多元线性回归模型的输出

3.  使用以下代码提取截距值:

    ```py
    intercept = model.intercept_
    ```

4.  如下提取系数的值:

    ```py
    coefficients = model.coef_
    ```

5.  Now, we can print a message with the formula for predicting temperature in Celsius using the code here:

    ```py
    print('Temperature = {0:0.2f} + ({1:0.2f} x Humidity) + ({2:0.2f} x Wind Speed) + ({3:0.2f} x Wind Bearing Degrees) + ({4:0.2f} x Visibility) + ({5:0.2f} x Pressure) + ({6:0.2f} x Rain) + ({7:0.2f} x Normal Weather) + ({8:0.2f} x Warm Weather)'.format(intercept,
    coefficients[0],
    coefficients[1],
    coefficients[2],
    coefficients[3],
    coefficients[4],
    coefficients[5],
    coefficients[6],
    coefficients[7]))
    ```

    我们的输出应该如下所示:

    ![Figure 3.11: A formula to predict temperature in Celsius from humidity using multiple linear regression
    ](image/C13322_03_11.jpg)

###### 图 3.11:使用多元线性回归从湿度预测摄氏温度的公式

干得好！根据该多元回归模型，湿度为 0.78、风速为 5.0 km/h、风向从正北顺时针方向为 81 度、能见度为 3 km、压力为 1000 毫巴、没有雨并且被描述为正常的一天，预测的温度为 5.72 摄氏度。现在我们已经熟悉了如何提取多元线性回归模型的截距和系数，我们可以生成预测并评估模型在测试数据上的表现。

### 活动 5:生成预测并评估多元线性回归模型的性能

在*练习 23* 、*生成预测并评估简单线性回归模型的性能*中，我们学习了如何使用各种方法生成预测并评估简单线性回归模型的性能。为了减少代码冗余，我们将使用*练习 23* 的*步骤 4* 中的指标来评估多元线性回归模型的性能，并且我们将确定多元线性回归模型相对于简单线性回归模型的性能是更好还是更差。

继续练习 24，执行以下步骤:

1.  使用所有特征生成对测试数据的预测。
2.  使用散点图绘制预测值与实际值。
3.  画出残差的分布。
4.  计算平均绝对误差、均方误差、均方根误差和 R 平方的指标，并将它们放入数据帧中。
5.  Determine if the multiple linear regression model performed better or worse in relation to the simple linear regression model.

    #### 注意

    这项活动的解决方案可在第 343 页找到。

您应该会发现，相对于简单线性回归模型，多元线性回归模型在每个指标上都表现得更好。最值得注意的是，在简单的线性回归模型中，该模型仅描述了 38.9%的温度变化。然而，在多元线性回归模型中，86.6%的温度变化是由特征的组合解释的。此外，我们的简单线性回归模型预测的平均温度在 6.052 度以内，而我们的多元线性回归模型预测的平均温度在 2.861 度以内。

截距和贝塔系数的透明性质使得线性回归模型非常容易解释。在商业中，通常要求数据科学家解释某个特征对结果的影响。因此，线性回归提供了允许更早对业务查询做出合理响应的度量。

然而，很多时候，一个问题需要数据科学家预测一个不是连续的，而是分类的结果度量。比如保险，给定客户的某些特征，这个客户不续保的概率是多少？在这种情况下，数据中的特征和结果变量之间没有线性关系，因此线性回归将会失败。对分类因变量进行回归分析的一个可行选项是逻辑回归。

## 逻辑回归

逻辑回归使用分类变量和连续变量来预测分类结果。当选择的因变量有两个分类结果时，该分析称为二元逻辑回归。然而，如果结果变量包含两个以上的水平，该分析被称为多项逻辑回归。就本章而言，我们将把学习重点放在前者上。

当预测二元结果时，我们在特征和结果变量之间没有线性关系；线性回归的假设。因此，为了以线性方式表达非线性关系，我们必须使用对数变换来变换数据。因此，逻辑回归允许我们预测在给定模型特征的情况下发生二元结果的概率。

对于具有 1 个预测值的逻辑回归，逻辑回归方程如下所示:

![Figure 3.12: Logistic regression formula with 1 predictor
](image/C13322_03_12.jpg)

###### 图 3.12:带有 1 个预测值的逻辑回归公式

在上图中， *P(Y)* 是结果发生的概率， *e* 是自然对数的底数， *α* 是截距， *β* 是β系数， *x* 是预测值。使用此处的公式，该等式可扩展到多个预测值:

![Figure 3.13: Logistic regression formula with more than one predictor
](image/C13322_03_13.jpg)

###### 图 3.13:有多个预测因子的逻辑回归公式

因此，使用逻辑回归来模拟事件发生的概率与拟合线性回归模型是一样的，只是连续结果变量已被二元结果变量的成功概率的对数(一种表达概率的替代方式)所取代。在线性回归中，我们假设预测变量和结果变量之间存在线性关系。另一方面，逻辑回归假设预测变量和自然对数 *p/(1-p)* 之间存在线性关系，其中 *p* 是事件发生的概率。

在以下练习中，我们将使用`weather.csv`数据集来演示如何使用数据中的所有要素构建逻辑回归模型来预测下雨的概率。

### 练习 25:拟合逻辑回归模型并确定截距和系数

为了使用我们数据中的所有特征对降雨(相对于降雪)的概率进行建模，我们将使用`weather.csv`文件并存储二分变量`Rain`作为结果度量。

1.  使用以下代码导入数据:

    ```py
    import pandas as pd
    df = pd.read_csv('weather.csv')
    ```

2.  虚拟编码`Description`变量如下:

    ```py
    import pandas as pd
    df_dummies = pd.get_dummies(df, drop_first=True)
    ```

3.  使用这里的代码混洗`df_dummies`:

    ```py
    from sklearn.utils import shuffle
    df_shuffled = shuffle(df_dummies, random_state=42)
    ```

4.  将特征和结果分别拆分为`X`和`y`，如下:

    ```py
    DV = 'Rain' 
    X = df_shuffled.drop(DV, axis=1) 
    y = df_shuffled[DV] 
    ```

5.  使用下面的代码将特性和结果分成训练和测试数据:

    ```py
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
    ```

6.  使用以下代码实例化逻辑回归模型:

    ```py
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression()
    ```

7.  使用`model.fit(X_train, y_train`)将逻辑回归模型拟合到训练数据。我们应该得到以下输出:![Figure 3.14: The output from fitting a logistic regression model
    ](image/C13322_03_14.jpg)

    ###### 图 3.14:拟合逻辑回归模型的输出

8.  使用以下命令获取截距:

    ```py
    intercept = model.intercept_
    ```

9.  使用以下公式提取系数:

    ```py
    coefficients = model.coef_
    ```

10.  将系数放入如下列表:

    ```py
    coef_list = list(coefficients[0,:])
    ```

11.  Match features to their coefficients, place them in a DataFrame, and print the DataFrame to the console as follows:

    ```py
    coef_df = pd.DataFrame({'Feature': list(X_train.columns),
                            'Coefficient': coef_list})
    print(coef_df)
    ```

    请参考此处的结果输出:

![Figure 3.15: Features and their coefficients from the logistic regression model
](image/C13322_03_15.jpg)

###### 图 3.15:逻辑回归模型中的特征及其系数

温度系数可以解释为温度每增加 1 度，降雨的对数几率增加 5.69，控制模型中的所有其他特征。为了生成预测，我们可以将对数赔率转换为赔率，将赔率转换为概率。然而，scikit-learn 具有生成预测概率以及预测类别的功能。

### 练习 26:生成预测并评估逻辑回归模型的性能

在*练习 25* 中，我们学习了如何拟合逻辑回归模型并提取生成预测所需的元素。然而，scikit-learn 通过为我们提供预测结果概率的函数以及结果的类别，使我们的生活变得更加容易。在本练习中，我们将学习生成预测概率和分类，以及使用混淆矩阵和分类报告评估模型性能。

继续练习 25，执行以下步骤:

1.  使用以下代码生成预测概率:

    ```py
    predicted_prob = model.predict_proba(X_test)[:,1]
    ```

2.  使用以下内容生成预测类:

    ```py
    predicted_class = model.predict(X_test)
    ```

3.  Evaluate a performance using a confusion matrix as follows:

    ```py
    from sklearn.metrics import confusion_matrix
    import numpy as np
    cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))
    cm['Total'] = np.sum(cm, axis=1)
    cm = cm.append(np.sum(cm, axis=0), ignore_index=True)
    cm.columns = ['Predicted No', 'Predicted Yes', 'Total']
    cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])
    print(cm)
    ```

    请参考此处的结果输出:

    ![Figure 3.16: The confusion matrix from our logistic regression model
    ](image/C13322_03_16.jpg)

    ###### 图 3.16:来自我们的逻辑回归模型的混淆矩阵

    #### 注意

    从混淆矩阵中，我们可以看到，在 383 个没有被分类为多雨的观测值中，有 377 个被正确分类，在 2917 个被分类为多雨的观测值中，有 2907 个被正确分类。为了使用精度、召回率和 f1 分数等指标进一步检查我们模型的性能，我们将生成一个分类报告。

4.  Generate a classification report using the following code:

    ```py
    from sklearn.metrics import classification_report
    print(classification_report(y_test, predicted_class))
    ```

    参考结果输出:

![Figure 3.17: The classification report generated from our logistic regression model
](image/C13322_03_17.jpg)

###### 图 3.17:从我们的逻辑回归模型生成的分类报告

从我们的混淆矩阵和分类报告中可以看出，我们的模型表现非常好，可能很难改进。然而，包括逻辑回归在内的机器学习模型由许多超参数组成，这些超参数可以被调整以进一步提高模型性能。在下一个练习中，我们将学习寻找超参数的最佳组合，以最大化模型性能。

### 练习 27:调整多重逻辑回归模型的超参数

在*练习 25* 的*步骤 7* 中，我们拟合了一个逻辑回归模型，该模型的后续输出如图 3.14 所示。`LogisticRegression()`函数中的每个参数都被设置为默认的超参数。为了调整模型，我们将使用 scikit-learn 的网格搜索功能，该功能为可能的超参数值的每个组合拟合一个模型，并确定每个超参数的值，从而产生最佳模型。在本练习中，我们将学习如何使用网格搜索来优化模型。

从*练习 26* 继续:

1.  数据已经为我们准备好了(见练习 26)；因此，我们可以直接实例化一个可能的超参数值的网格，如下所示:

    ```py
    import numpy as np
    grid = {'penalty': ['l1', 'l2'],
            'C': np.linspace(1, 10, 10),
            'solver': ['liblinear']}
    ```

2.  实例化一个网格搜索模型，找到具有最大`f1`得分(即精度和召回率的调和平均值)的模型如下:

    ```py
    from sklearn.model_selection import GridSearchCV
    from sklearn.linear_model import LogisticRegression
    model = GridSearchCV(LogisticRegression(solver='liblinear'), grid, scoring='f1', cv=5)
    ```

3.  Fit the model on the training using `model.fit(X_train, y_train)` (keep in mind, this may take a while) and find the resultant output here:![Figure 3.18: The output from our logistic regression grid search model
    ](image/C13322_03_18.jpg)

    ###### 图 3.18:我们的逻辑回归网格搜索模型的输出

4.  We can return the optimal combination of hyperparameters as a dictionary as follows:

    ```py
    best_parameters = model.best_params_
    print(best_parameters)
    ```

    请参考此处的结果输出:

![Figure 3.19: The tuned hyperparameters from our logistic regression grid search model
](image/C13322_03_19.jpg)

###### 图 3.19:来自我们的逻辑回归网格搜索模型的调整过的超参数

我们已经找到了使`f1`分数最大化的超参数组合。请记住，简单地使用*练习 25* 中的默认超参数会产生一个在测试数据上表现非常好的模型。因此，在下面的活动中，我们将评估带有调整后的超参数的模型在测试数据上的表现。

### 活动 6:生成预测并评估调整后的逻辑回归模型的性能

一旦超参数的最佳组合收敛，我们需要评估模型性能，就像我们在*练习 25* 中所做的那样。

上接练习 27:

1.  生成降雨的预测概率。
2.  生成预测的降雨等级。
3.  使用混淆矩阵评估性能，并将其存储为数据帧。
4.  Print a classification report.

    #### 注意

    这项活动的解决方案可在第 346 页找到。

通过调整逻辑回归模型的超参数，我们能够改进已经表现很好的逻辑回归模型。在下面的练习和活动中，我们将继续扩展对不同类型模型的调优。

## 使用支持向量机的最大间隔分类

SVM 是一种用于监督学习的算法，可以解决分类和回归问题。然而，SVM 最常用于分类问题，因此，为了本章的目的，我们将把 SVM 作为一个二元分类器。SVM 的目标是确定超平面的最佳位置，该超平面在多维空间上绘制的数据点之间创建类边界。为了帮助澄清这个概念，请参考图 3.20。

![Figure 3.20: Hyperplane (blue) separating the circles from the squares in three dimensions
](image/C13322_03_20.jpg)

###### 图 3.20:超平面(蓝色)在三维空间中将圆和正方形分开

在图 3.20 中，正方形和圆形是同一数据框架中的观察值，代表不同的类别。在该图中，超平面由位于圆和正方形之间的半透明蓝色边界来描述，该边界将观察结果分成两个不同的类别。在这个例子中，观察值被称为是线性可分的。

超平面的位置是通过找到在两个类之间产生最大间隔(即边距)的位置来确定的。因此，这被称为**最大边缘超平面** (MMH)，并提高了点保持在超平面边界正确一侧的可能性。可以使用每个类中最接近 MMH 的点来表示 MMH。这些点被称为支持向量，每个类别至少有 1 个。图 3.21 直观地描述了与二维 MMH 相关的支持向量:

![Figure 3.21: Support vectors in relation to the MMH
](image/C13322_03_21.jpg)

###### 图 3.21:与 MMH 相关的支持向量

实际上，大多数数据不是线性可分的。在这种情况下，SVM 利用了一个松弛变量，它创建了一个软余量(与最大余量相对)，允许一些观察值落在不正确的线上。见下图:

![Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter Χi) fall on the incorrect side of the soft margin line
](image/C13322_03_22.jpg)

###### 图 3.22: 2 观察值(用灰色阴影和希腊字母χI 表示)落在软边缘线的错误一侧

将成本值应用于错误分类的数据点，并且该算法不是寻找最大限度，而是最小化总成本。随着成本参数的增加，更难的 SVM 优化将达到 100%分离，并且可能过度拟合训练数据。相反，较低的成本参数强调较宽的余量，可能会使训练数据不足。因此，为了创建在测试数据上表现良好的 SVM 模型，确定平衡过拟合和欠拟合的成本参数是很重要的。

此外，不可线性分离的数据可以使用内核技巧转换到更高维度的空间。在这种映射到更高维度的空间之后，非线性关系可以表现为线性关系。通过转换原始数据，SVM 可以发现原始特征中不明显的关联。scikit-learn 默认使用高斯 RBF 内核，但也配备了常见的内核，如线性、多项式和 sigmoid。为了最大化 SVM 分类器模型的性能，必须确定核和成本函数的最佳组合。幸运的是，使用网格搜索超参数调整可以很容易地实现这一点，如练习 27 中所介绍的。在下面的练习和活动中，我们将学习这一壮举是如何完成的。

### 练习 28:为支持向量分类器(SVC)模型准备数据

在拟合 SVM 分类器模型以预测二元结果变量之前；在这种情况下，无论下雨还是下雪，我们都必须准备好数据。由于 SVM 是一个黑箱，这意味着输入和输出之间的过程是不明确的，我们不需要担心可解释性。因此，在拟合模型之前，我们会将数据中的要素转换为 z 分数。以下步骤将展示如何做到这一点:

1.  使用以下代码导入`weather.csv`:

    ```py
    import pandas as pd
    df = pd.read_csv('weather.csv')
    ```

2.  虚拟编码分类特征`Description`，如下:

    ```py
    import pandas as pd
    df_dummies = pd.get_dummies(df, drop_first=True)
    ```

3.  洗牌`df_dummies`用这里的代码去掉任何排序效果:

    ```py
    from sklearn.utils import shuffle
    df_shuffled = shuffle(df_dummies, random_state=42)
    ```

4.  使用以下代码将`df_shuffled`拆分为`X`和【T2:

    ```py
    DV = 'Rain'
    X = df_shuffled.drop(DV, axis=1) 
    y = df_shuffled[DV]
    ```

5.  使用下面的代码将`X`和`y`分割成测试和训练数据:

    ```py
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
    ```

6.  为了防止任何数据泄漏，通过将缩放器模型拟合到`X_train`来缩放`X_train`和`X_test`，并将它们分别转换为 z 分数，如下所示:

    ```py
    from sklearn.preprocessing import StandardScaler
    model = StandardScaler() 
    X_train_scaled = model.fit_transform(X_train)
    X_test_scaled = model.transform(X_test)
    ```

既然我们的数据已经被适当地划分为特征和结果变量，被划分为测试和训练数据，并且被分别缩放，我们可以使用网格搜索来调整我们的 SVC 模型的超参数。

### 练习 29:使用网格搜索调整 SVC 模型

之前，我们讨论了为 SVM 分类器模型确定最佳成本函数和核的重要性。在练习 27 中，我们学习了如何使用 scikit-learn 的网格搜索功能找到超参数的最佳组合。在本练习中，我们将演示如何使用网格搜索来找到成本函数和内核的最佳组合。

从*练习 28* 继续:

1.  使用以下代码实例化要搜索的网格:

    ```py
    import numpy as np
    grid = {'C': np.linspace(1, 10, 10),
            'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}
    ```

2.  实例化`GridSearchCV`模型，将`gamma`超参数设置为`auto`以避免警告，并将概率设置为`True`，这样我们可以提取降雨概率如下:

    ```py
    from sklearn.model_selection import GridSearchCV
    from sklearn.svm import SVC
    model = GridSearchCV(SVC(gamma='auto'), grid, scoring='f1', cv=5)
    ```

3.  Fit the grid search model using `model.fit(X_train_scaled, y_train)`:![Figure 3.23: The output from fitting the SVC grid search model
    ](image/C13322_03_23.jpg)

    ###### Figure 3.23: Output of fitting SVC grid search model

4.  Print the best parameters using the following code:

    ```py
    best_parameters = model.best_params_
    print(best_parameters)
    ```

    请参见下面的结果输出:

![Figure 3.24: Tuned hyperparameters for our SVC grid search model
](image/C13322_03_24.jpg)

###### 图 3.24:我们的 SVC 网格搜索模型的调优超参数

一旦确定了超参数的最佳组合，就应该生成预测，并随后评估我们的模型在未知测试数据上的表现。

### 活动 7:生成预测并评估 SVC 网格搜索模型的性能

在之前的练习/活动中，我们学习了生成预测和评估分类器模型性能。在本活动中，我们将再次通过生成预测、创建混淆矩阵和打印分类报告来评估模型的性能。

上接练习 29:

1.  提取预测的类。
2.  创建并打印混淆矩阵。
3.  Generate and print a classification report.

    #### 注意

    这项活动的解决方案可在第 348 页找到。

在这里，我们演示了如何使用网格搜索来调整 SVC 模型的超参数。在调整 SVC 模型后，它在预测雨/雪方面的表现不如调整后的逻辑回归模型。此外，SVC 模型是一个**黑盒**,因为它们不提供对结果度量的特性贡献的洞察。在即将到来的*决策树*部分，我们将介绍一种称为决策树的不同算法，它使用一种“*分而治之*方法来生成预测，并提供一个特性重要性属性来确定每个特性对结果的重要性。

## 决策树

想象一下我们正在考虑换工作。我们正在权衡未来工作机会的利弊，在我们目前的职位上工作几年后，我们开始意识到对我们来说重要的事情。然而，并不是职业生涯的所有方面都同等重要。事实上，在工作几年后，我们决定一个职位最重要的方面是我们对将要做的项目的兴趣，其次是薪酬，然后是与工作有关的压力，最后是通勤时间，最后是福利。我们刚刚创建了认知决策树的框架。我们可以进一步说，我们想要一份对分配的项目非常感兴趣的工作，年薪至少 55k 美元，工作压力小，通勤时间不超过 30 分钟，有良好的牙科保险。创建心理决策树是我们所有人天生都会利用的决策过程，也是决策树成为当今最广泛使用的机器学习算法之一的原因之一。

在机器学习中，决策树使用*基尼*杂质或*熵*信息增益作为衡量分裂质量的标准。首先，决策树算法确定使指示分割质量的值最大化的特征。这被称为根节点，因为它是数据中最重要的特征。在前面提到的工作邀请中，对潜在项目非常感兴趣将被视为根节点。考虑到根节点，工作机会分为有非常感兴趣的项目和没有非常感兴趣的项目。

接下来，这两个类别中的每一个都被分成下一个最重要的特征，给定前面的特征，等等，直到潜在的工作被识别为感兴趣或不感兴趣。

这种方法被称为递归分区，或“*分而治之*”，因为它继续拆分和子集化数据的过程，直到算法确定数据中的子集足够同质，或者:

*   对应节点上几乎所有的观测值都具有相同的类别(即纯度)。
*   数据中没有要分割的其他要素。
*   该树已达到先验决定的大小限制。

例如，如果纯度是由熵决定的，我们必须理解熵是一组值内随机性的度量。决策树通过选择最小化熵(随机性)的分裂来运行，从而最大化信息增益。信息增益被计算为该分裂和所有其他后续分裂之间的熵差。然后，通过取每个分区中熵的总和，用分区中观察值的比例加权，计算总熵。幸运的是，scikit-learn 为我们提供了一个完成所有这些工作的函数。在下面的练习和活动中，我们将使用熟悉的`weather.csv`数据集实现决策树分类器模型来预测是下雨还是下雪。

### 活动 8:为决策树分类器准备数据

在本活动中，我们将为决策树分类器模型准备数据。执行以下步骤来完成活动:

1.  导入`weather.csv`并将其存储为数据帧
2.  多级分类特征的虚拟代码`Summary`
3.  打乱数据以消除任何可能的顺序效应
4.  将数据分为特征和结果
5.  将功能和结果进一步划分为测试和训练数据
6.  Scale `X_train` and `X_test` using the following code:

    ```py
    from sklearn.preprocessing import StandardScaler
    model = StandardScaler()
    X_train_scaled = model.fit_transform(X_train)
    X_test_scaled = model.transform(X_test)
    ```

    #### 注意

    这项活动的解决方案可在第 349 页找到

在下面的练习中，我们将学习调整和拟合决策树分类器模型..

### 练习 30:使用网格搜索调整决策树分类器

在当前的练习中，我们将实例化一个超参数空间，并使用网格搜索调整决策树分类器的超参数。

从*活动 8* 继续，执行以下步骤:

1.  指定超参数空间如下:

    ```py
    import numpy as np
    grid = {'criterion': ['gini', 'entropy'],
            'min_weight_fraction_leaf': np.linspace(0.0, 0.5, 10),
            'min_impurity_decrease': np.linspace(0.0, 1.0, 10),
            'class_weight': [None, 'balanced'],
    'presort': [True, False]}Instantiate the GridSearchCV model
    ```

2.  使用下面的代码实例化一个网格搜索模型:

    ```py
    from sklearn.model_selection import GridSearchCV
    from sklearn.tree import DecisionTreeClassifier
    model = GridSearchCV(DecisionTreeClassifier(), grid, scoring='f1', cv=5)
    ```

3.  Fit to the training set using the following:

    ```py
    model.fit(X_train_scaled, y_train)
    ```

    查看此处显示的结果输出:

    ![Figure 3.25: The output from fitting our decision tree classifier grid search model
    ](image/C13322_03_25.jpg)

    ###### 图 3.25:拟合决策树分类器网格搜索模型的输出

4.  Print the tuned parameters:

    ```py
    best_parameters = model.best_params_
    print(best_parameters)
    ```

    请参见下面的结果输出:

![](image/C13322_03_26.jpg)

###### 图 3.26:为我们的决策树分类器网格搜索模型调整的超参数

从图 3.26 中我们可以看到，它用**基尼**杂质作为衡量一个拆分质量的标准。对超参数的进一步解释超出了本章的范围，但是可以在决策树分类器 scikit-learn 文档中找到。

请记住，在实践中，决策者经常会问各种特征是如何影响预测的。在线性和逻辑回归中，截距和系数使模型预测非常透明。

#### 注意

决策树也非常容易解释，因为我们可以看到决策是在哪里做出的，但这需要安装和正确配置 Graphviz，以及未缩放的功能。

在下面的练习中，我们将探索 scitkit-learn 基于树的模型算法中的一个属性'`feature_importances_`'，它返回一个数组，其中包含每个特性的相对特性重要性值。值得注意的是，这个属性在网格搜索模型中是不可用的。因此，在下一个练习中，我们将学习以编程方式从`best_parameters`字典中提取值，并重新调整决策树模型，从而允许我们访问决策树分类器函数提供的属性。

### 练习 31:从决策树分类器网格搜索模型中以编程方式提取优化的超参数

在前面的练习中，我们将调优的超参数作为键值对保存在`best_parameters`字典中。这允许我们以编程方式访问这些值，并将它们分配给决策树分类器模型的适当超参数。通过调整决策树模型，我们将能够访问 scikit-learn 决策树分类器函数提供的属性。

从*练习 30* 继续，执行以下步骤:

1.  Prove that we can access the value for '`Tree_criterion`' using:

    ```py
    print(best_parameters['criterion'])
    ```

    在此处查看结果输出:

    ![Figure 3.27: The value assigned to the ‘Tree_criterion’ key in the best_parameters dictionary
    ](image/C13322_03_27.jpg)

    ###### 图 3.27:分配给 best_parameters 字典中“Tree_criterion”键的值

2.  实例化决策树分类器模型，并将值分配给相应的超参数，如下:

    ```py
    from sklearn.tree import DecisionTreeClassifier
    model = DecisionTreeClassifier(class_weight=best_parameters['class_weight'],
                                   criterion=best_parameters['criterion'],
                          min_impurity_decrease=best_parameters['min_impurity_decrease'],
                   min_weight_fraction_leaf=best_parameters['min_weight_fraction_leaf'],
                                   presort=best_parameters['presort'])
    ```

3.  使用以下:

    ```py
    model.fit(X_train_scaled, y_train)
    ```

    ![Figure 3.28: The output from fitting the decision tree classifier model with tuned hyperparameters
    ](image/C13322_03_28.jpg)

    ###### 将网格搜索模型拟合到缩放的训练数据图 3.28:用调整后的超参数拟合决策树分类器模型的输出

4.  Extract `feature_importances` attribute using:

    ```py
    print(model.feature_importances_)
    The resultant output is shown below:
    ```

    ![Figure 3.29: An array of feature importance from our tuned decision tree classifier model
    ](image/C13322_03_29.jpg)

    ###### 图 3.29:来自我们优化的决策树分类器模型的一系列特征重要性

    从图 3.29 的数组中，我们可以看到，就特征重要性而言，第一个特征完全支配了其他变量。

5.  Visualize this using the following code:

    ```py
    import pandas as pd
    import matplotlib.pyplot as plt
    df_imp = pd.DataFrame({'Importance': list(model.feature_importances_)}, index=X.columns)
    df_imp_sorted = df_imp.sort_values(by=('Importance'), ascending=True)
    df_imp_sorted.plot.barh(figsize=(5,5))
    plt.title('Relative Feature Importance')
    plt.xlabel('Relative Importance')
    plt.ylabel('Variable')
    plt.legend(loc=4)
    plt.show()
    ```

    在此处查看结果输出:

![Figure 3.30: Feature importance from a tuned decision tree classifier model
](image/C13322_03_30.jpg)

###### 图 3.30:来自优化的决策树分类器模型的特征重要性

看起来摄氏温度是这个分类问题的唯一驱动力。由于结果测量是`rain ('Rain'=1)`或`snow ('Rain'=0)`，决策树通过*分而治之*做出分裂决策，因此该算法使用温度来确定测量时是否有降雨或降雪是合理的。在接下来的活动中，我们将评估模型的表现。

### 活动 9:生成预测并评估决策树分类器模型的性能

我们已经在之前的练习和活动中生成了预测并评估了模型性能。在本次活动中，我们将采用相同的方法来评估优化后的决策树分类器模型的性能。

继续练习 31，执行以下步骤:

1.  生成降雨的预测概率。
2.  生成预测的降雨等级。
3.  生成并打印混淆矩阵。
4.  Print a classification report.

    #### 注意

    这项活动的解决方案可在第 350 页找到。

你应该发现只有一个错误分类的观察。因此，通过在我们的`weather.csv`数据集上调整决策树分类器模型，我们能够非常准确地预测降雨(或降雪)。我们可以看到唯一的驱动特征是以摄氏度为单位的温度。由于决策树使用递归划分进行预测的方式，这是有意义的。

有时候经过评估，单个模型是弱学习者，表现不好。然而，通过结合弱学习者，我们创造了一个更强的学习者。将众多弱学习者组合起来以创建更强学习者的方法被称为集成。随机森林模型结合了大量的决策树模型来创建一个更强的集合模型。随机森林可用于分类或回归问题。

## 随机森林

如前所述，随机森林是决策树的集合，可用于解决分类或回归问题。随机森林使用一小部分数据来拟合每棵树，因此它们可以处理非常大的数据集，并且相对于其他算法，它们不太容易出现“*维数灾难*”。维数灾难是指数据中的大量特征降低了模型的性能。然后通过组合每棵树的预测来确定随机森林的预测。像 SVM 一样，随机森林是一个**黑盒**，其输入和输出无法解释。

在接下来的练习和活动中，我们将使用网格搜索来调整和拟合一个随机森林回归变量，以预测摄氏温度。然后，我们将评估模型的性能。

### 练习 32:为随机森林回归变量准备数据

首先，我们将为随机森林回归变量准备数据，将'`Temperature_c`'作为因变量，就像我们在练习 21 中所做的那样:

1.  使用以下代码导入'`weather.csv`'并保存为`df`:

    ```py
    import pandas as pd
    df = pd.read_csv('weather.csv')
    ```

2.  多类分类变量的虚拟代码，描述如下:

    ```py
    import pandas as pd
    df_dummies = pd.get_dummies(df, drop_first=True)
    ```

3.  通过使用下面的代码

    ```py
    from sklearn.utils import shuffle
    df_shuffled = shuffle(df_dummies, random_state=42)
    ```

    打乱`df_dummies`来消除任何可能的排序效果
4.  使用以下代码将`df_shuffled`拆分为`X`和【T2:

    ```py
    DV = 'Temperature_c'
    X = df_shuffled.drop(DV, axis=1)
    y = df_shuffled[DV]
    ```

5.  将`X`和`y`分解成如下测试和训练数据:

    ```py
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
    ```

6.  使用此处的代码

    ```py
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    ```

    缩放`X_train`和`X_test`

既然我们已经导入、重组、将数据分成特征(`X`)和因变量(`y`)，将`X`和`y`分成测试和训练数据，并缩放`X_train`和`X_test`，我们将使用网格搜索调整随机森林回归模型。

### 活动 10:调整随机森林回归量

这些数据已准备好纳入随机森林回归器。现在，我们必须设置超参数空间，并使用网格搜索找到超参数的最佳组合。

继续练习 32，执行以下步骤:

1.  指定超参数空间。
2.  实例化`GridSearchCV`模型，优化解释的差异。
3.  使网格搜索模型适合训练集。
4.  Print the tuned parameters.

    #### 注意

    这项活动的解决方案可在第 351 页找到。

在对我们的随机森林回归超参数执行网格搜索之后，我们需要用调整后的超参数来拟合随机森林回归模型。我们将以编程方式提取`best_parameters`字典中的值，并将它们分配给随机森林回归函数中相应的超参数，这样我们就可以从随机森林回归函数中访问属性。

### 练习 33:从随机森林回归器网格搜索模型中以编程方式提取优化的超参数并确定特征重要性

通过从`best_parameters`字典中的键值对中提取值，我们消除了人工错误的可能性，并使我们的代码更加自动化。在本练习中，我们将重复*练习 31* 中的步骤，但将针对随机森林回归模型修改我们的代码。

从*活动 10* 继续，执行以下步骤:

1.  用分配给相应超参数

    ```py
    from sklearn.ensemble import RandomForestRegressor
    model = RandomForestRegressor(criterion=best_parameters['criterion'],
                                  max_features=best_parameters['max_features'],
                                  min_impurity_decrease=best_parameters['min_impurity_decrease'],
                                  bootstrap=best_parameters['bootstrap'],
                                  warm_start=best_parameters['warm_start'])
    ```

    的`best_parameters`字典中每个键的值实例化一个随机森林回归模型
2.  Fit the model on the training data using the following:

    ```py
    model.fit(X_train_scaled, y_train)
    ```

    在此处查找结果输出:

    ![Figure 3.31: The output from fitting the random forest regressor model with tuned hyperparameters
    ](image/C13322_03_31.jpg)

    ###### 图 3.31:用调整的超参数拟合随机森林回归模型的输出

3.  Plot feature importance in descending order using the following code:

    ```py
    import pandas as pd
    import matplotlib.pyplot as plt
    df_imp = pd.DataFrame({'Importance': list(model.feature_importances_)}, index=X.columns)
    df_imp_sorted = df_imp.sort_values(by=('Importance'), ascending=True)
    df_imp_sorted.plot.barh(figsize=(5,5))
    plt.title('Relative Feature Importance')
    plt.xlabel('Relative Importance')
    plt.ylabel('Variable')
    plt.legend(loc=4)
    plt.show()
    ```

    在此处查看结果输出:

![Figure 3.32: Feature importance from a random forest regressor model with tuned hyperparameters
](image/C13322_03_32.jpg)

###### 图 3.32:带有优化超参数的随机森林回归模型的特征重要性

从图 3.32 中，我们可以看到'`Description_Warm`'虚拟变量和'`Humidity`'是以摄氏度为单位的温度的主要驱动因素。同时，`Visibility_km`和`Wind_Bearing_degrees`对温度的影响较小。现在让我们来看看我们的模型在测试数据上的表现。

### 活动 11:生成预测并评估优化的随机森林回归模型的性能

在*练习 23* 和*活动 5* 中，我们学习了生成预测和评估预测连续结果的回归模型的性能。在本活动中，我们将采用相同的方法来评估随机森林回归模型预测摄氏温度的性能。

从*练习 33* 继续，执行以下步骤:

1.  根据测试数据生成预测。
2.  绘制预测值和实际值的相关性。
3.  画出残差的分布。
4.  Compute metrics, then place them in a DataFrame and print it.

    #### 注意

    这项活动的解决方案可在第 352 页找到。

与多元线性回归相比，随机森林回归模型 l 似乎表现不佳，其证据是较大的 MAE、MSE 和 RMSE 值，以及较少解释的方差。此外，预测值和实际值之间的相关性较弱，残差进一步偏离正态分布。然而，通过利用随机森林回归的集成方法，我们构建了一个模型，解释了 75.8%的温度变化，并预测温度为摄氏 3.781 度。

## 总结

在这一章中，我们介绍了 Python 的开源机器学习库 scikit-learn。您学习了预处理数据，以及如何调整和适应一些不同的回归和分类算法。最后，您学习了如何快速有效地评估分类和回归模型的性能。这是对 scikit-learn 库的非常全面的介绍，这里采用的策略可以应用于构建 scikit-learn 提供的许多其他算法。

在下一章中，你将学习降维以及无监督学习。