

# 一、数据科学和数据预处理简介

## 学习目标

本章结束时，您将能够:

*   使用各种 Python 机器学习库
*   处理缺失数据和异常值
*   执行数据集成，将不同来源的数据整合在一起
*   执行数据转换，将数据转换成机器可读的形式
*   缩放数据以避免不同量值的问题
*   将数据分成训练和测试数据集
*   描述不同类型的机器学习
*   描述机器学习模型的不同性能测量

本章介绍了数据科学，涵盖了构建机器学习模型的各种过程，尤其侧重于预处理。

## 简介

我们生活在一个不断被数据包围的世界。因此，能够理解和处理数据是绝对必要的。

数据科学是一个研究数据描述、分析和预测的领域。考虑一个我们日常生活中的例子:每天，我们在手机上使用多种社交媒体应用。这些应用程序收集和处理数据，以便为每个用户创造更加个性化的体验——例如，向我们展示我们可能感兴趣的新闻文章，或者根据我们的位置定制搜索结果。数据科学的这个分支被称为**机器学习**。

机器学习是在没有人类干预的情况下，计算机用来完成任务的程序和统计表示的系统学习。换句话说，就是在没有明确指令的情况下，仅依靠模式和推理，教会计算机自己执行任务的过程。机器学习算法的一些常见用途是在电子邮件过滤、计算机视觉和计算语言学中。

这本书将关注使用 Python 的机器学习和数据科学的其他方面。Python 是数据科学的一种流行语言，因为它功能多样且相对容易使用。它还有几个现成的图书馆，设备齐全，可以处理数据。

## Python 库

在本书中，我们将使用各种 Python 库，包括 pandas、Matplotlib、Seaborn 和 scikit-learn。

熊猫

pandas 是一个开源包，它具有许多加载和处理数据的功能，以便为机器学习任务做准备。它还拥有可以用来分析和操作数据的工具。使用 pandas 可以读取多种格式的数据。在本书中，我们将主要使用 CSV 数据。要读取 CSV 数据，您可以通过将`filename.csv`作为参数传递来使用`read_csv()`函数。这里显示了一个例子:

```py
>>> import pandas as pd
>>> pd.read_csv("data.csv")
```

在前面的代码中，`pd`是熊猫的别名。给出别名不是强制性的。为了可视化熊猫数据帧，可以使用`head()`函数列出前五行。这将在下面的一个练习中演示。

#### 注意

请访问以下链接了解更多关于熊猫的信息:https://pandas.pydata.org/pandas-docs/stable/。

**NumPy**

NumPy 是 Python 必须提供的主要包之一。它主要用于与科学计算相关的实践以及数学运算。它包含了使我们能够使用数组和数组对象的工具。

**Matplotlib**

Matplotlib 是一个数据可视化包。在 NumPy 的帮助下，这对于在 2D 空间中绘制数据点非常有用。

**Seaborn**

Seaborn 也是一个基于 matplotlib 的数据可视化库。就图形而言，使用 Seaborn 创建的可视化效果比使用 matplotlib 创建的可视化效果更有吸引力。

**scikit-learn**

scikit-learn 是一个用于机器学习的 Python 包。它被设计成能够与 Python 中的其他数字和科学库进行互操作，以实现算法的实现。

这些现成的库已经引起了开发人员的兴趣和关注，尤其是在数据科学领域。既然我们已经介绍了 Python 中的各种库，在下一节中，我们将探索构建机器学习模型的路线图。

## 构建机器学习模型的路线图

构建机器学习模型的路线图非常简单，由五个主要步骤组成，如下所述:

*   **Data Pre-processing**

    这是建立机器学习模型的第一步。数据预处理是指在将数据输入模型之前对其进行转换。它涉及的技术是用来把无用的原始数据转换成干净可靠的数据。

    由于数据收集通常不是以受控方式执行的，因此原始数据通常包含异常值(例如，年龄= 120)、无意义的数据组合(例如，型号:自行车、类型:4 轮车)、缺失值、比例问题等等。正因为如此，原始数据不能被输入到机器学习模型中，因为它可能会损害结果的质量。因此，这是数据科学过程中最重要的一步。

*   **Model Learning**

    在对数据进行预处理并将其分成训练/测试集(稍后会有更多介绍)之后，我们继续建模。模型只不过是一组定义明确的方法，称为算法，使用预处理数据来学习模式，这些模式随后可用于进行预测。有不同类型的学习算法，包括监督、半监督、非监督和强化学习。这些将在后面讨论。

*   **Model Evaluation**

    在这一阶段，借助特定的性能指标对模型进行评估。有了这些指标，我们可以继续调整模型的超参数，以便改进它。这个过程叫做**超参数优化**。我们将重复这一步骤，直到我们对性能感到满意。

*   **Prediction**

    一旦我们对评估步骤的结果感到满意，我们将继续进行预测。当经过训练的模型暴露于新的数据集时，它会做出预测。在商业环境中，这些预测可以与决策者分享，以做出有效的商业选择。

*   **Model Deployment**

    机器学习的整个过程不仅仅停留在模型建立和预测上。它还涉及到利用模型用新数据构建应用程序。根据业务需求，部署可能是一份报告，也可能是要执行的一些重复的数据科学步骤。部署后，模型需要定期进行适当的管理和维护，以保持其正常运行。

本章将主要关注预处理。我们将讨论数据预处理中涉及的不同任务，比如数据表示、数据清理等等。

## 数据表示

机器学习的主要目标是建立理解数据和发现潜在模式的模型。为了做到这一点，以计算机可以理解的方式输入数据是非常重要的。要将数据输入到模型中，必须将其表示为所需维度的表或矩阵。将数据转换成正确的表格形式是预处理正常开始之前的第一步。

**用表格表示的数据**

数据应该排列在由行和列组成的二维空间中。这种类型的数据结构使得理解数据和查明问题变得容易。以下是一些存储为 CSV ( **逗号分隔值**)文件的原始数据的示例:

![Figure 1.1: Raw data in CSV format](image/C13322_01_01.jpg)

###### 图 1.1:CSV 格式的原始数据

表中相同数据的表示如下:

![Figure 1.2: CSV data in table format](image/C13322_01_02.jpg)

###### 图 1.2:表格格式的 CSV 数据

如果您比较 CSV 和表格格式的数据，您会发现两者都缺少值。我们将在本章的后面讨论如何处理这些。为了加载一个 CSV 文件并把它作为一个表格来处理，我们使用 pandas 库。这里的数据被加载到名为 DataFrames 的表中。

#### 注意

想了解更多关于熊猫的信息，请访问以下链接:http://pandas . pydata . org/pandas-docs/version/0.15/tutorials . html。

### 自变量和目标变量

我们使用的数据帧包含变量或特征，可以分为两类。这些是自变量(也称为**预测变量**)和因变量(也称为**目标变量**)。自变量用于预测目标变量。顾名思义，自变量应该是相互独立的。如果不是，这需要在预处理(清洗)阶段解决。

**独立变量**

除了**目标变量**，这些都是数据帧中的特征。它们的大小为(m，n)，其中 m 是观察的数量，n 是特征的数量。这些变量必须呈正态分布，并且不应包含:

*   缺少值或值为空
*   高度分类的数据特征或高基数(这些术语将在后面详细介绍)
*   极端值
*   不同尺度的数据
*   人为误差
*   多重共线性(相关的独立变量)
*   非常大的独立特征集(独立变量太多，难以管理)
*   稀疏数据
*   特殊字符

**特征矩阵和目标向量**

一个单独的数据称为标量。一组标量称为向量，一组向量称为矩阵。矩阵用行和列来表示。特征矩阵数据由独立的列组成，目标向量依赖于特征矩阵列。为了更好地理解这一点，我们来看看下表:

![Figure 1.3: Table containing car details](image/C13322_01_03.jpg)

###### 图 1.3:包含汽车详细信息的表格

正如您在表格中看到的，有各种列:汽车型号、汽车容量、汽车品牌和汽车价格。除汽车价格之外的所有列都是独立变量，代表特征矩阵。汽车价格是依赖于其他列(汽车型号、汽车容量和汽车品牌)的因变量。它是目标向量，因为它依赖于特征矩阵数据。在下一部分中，我们将通过一个基于特性和目标矩阵的练习来获得彻底的理解。

#### 注意

所有练习和活动将主要在 Jupyter 笔记本上进行。建议为不同的作业准备一个单独的笔记本，除非有人建议不要这样做。此外，为了加载样本数据集，将使用 pandas 库，因为它将数据显示为表格。加载数据的其他方式将在后续章节中解释。

### 练习 1:加载样本数据集并创建特征矩阵和目标矩阵

在本练习中，我们将把`House_price_prediction`数据集加载到 pandas 数据框架中，并创建特征和目标矩阵。`House_price_prediction`数据集取自 UCI 机器学习知识库。该数据是从美国的各个郊区收集的，包括 5000 个条目和与房屋相关的 6 个特征。按照以下步骤完成本练习:

#### 注意

`House_price_prediction`数据集可以在这个位置找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/USA _ housing . CSV。

1.  打开一个 Jupyter 笔记本，添加以下代码来导入熊猫:

    ```py
    import pandas as pd 
    ```

2.  Now we need to load the dataset into a pandas DataFrame. As the dataset is a CSV file, we'll be using the `read_csv()` function to read the data. Add the following code to do this:

    ```py
    dataset = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv"
    df = pd.read_csv(dataset, header = 0)
    ```

    正如您在前面的代码中看到的，数据存储在一个名为`df`的变量中。

3.  To print all the column names of the DataFrame, we'll use the `df.columns` command. Write the following code in the notebook:

    ```py
    df.columns
    ```

    上述代码生成以下输出:

    ![Figure 1.4: List of columns present in the dataframe](image/C13322_01_04.jpg)

    ###### 图 1.4:数据帧中的列列表

4.  The dataset contains n number of data points. We can find the total number of rows using the following command:

    ```py
    df.index
    ```

    上述代码生成以下输出:

    ![Figure 1.5: Total Index in the dataframe](image/C13322_01_05.jpg)

    ###### 图 1.5:数据框中的总索引

    正如您在上图中看到的，我们的数据集包含 5000 行，从索引 0 到 5000。

    #### 注意

    您可以使用 pandas 中的`set_index()`函数将数据帧中的列转换为行索引。这有点像使用该列中的值作为行标签。

    `Dataframe.set_index('column name', inplace = True')'`

5.  Let's set the `Address` column as an index and reset it back to the original DataFrame. The pandas library provides the `set_index()` method to convert a column into an index of rows in a DataFrame. Add the following code to implement this:

    ```py
    df.set_index('Address', inplace=True)
    df
    ```

    上述代码生成以下输出:

    ![Figure 1.6: DataFrame with an indexed Address column](image/C13322_01_06.jpg)

    ###### 图 1.6:带有索引地址列的数据帧

    `set_index()`功能中的`inplace`参数默认设置为`False`。如果值更改为`True`，那么无论我们执行什么操作，数据帧的内容都会直接更改，而不会创建副本。

6.  In order to reset the index of the given object, we use the `reset_index()` function. Write the following code to implement this:

    ```py
    df.reset_index(inplace=True)
    df
    ```

    上述代码生成以下输出:

    ![Figure 1.7: DataFrame with the index reset](image/C13322_01_07.jpg)

    ###### 图 1.7:索引重置后的数据帧

    #### 注意

    索引就像给行和列起的名字。行和列都有索引。您可以按行/列编号或行/列名称进行索引。

7.  我们可以使用行号和列号检索前四行和前三列。这可以使用 pandas 中的`iloc`索引器来完成，该索引器使用索引位置来检索数据。为此，添加以下代码:

    ```py
    df.iloc[0:4 , 0:3]
    ```

    ![Figure 1.8: Dataset of four rows and three columns](image/C13322_01_08.jpg)

    ###### 图 1.8:四行三列的数据集

8.  为了使用标签检索数据，我们使用了`loc`索引器。添加以下代码以检索收入和年龄列的前五行:

    ```py
    df.loc[0:4 , ["Avg. Area Income", "Avg. Area House Age"]]
    ```

    ![Figure 1.9: Dataset of five rows and two columns](image/C13322_01_09.jpg)

    ###### 图 1.9:五行两列的数据集

9.  Now create a variable called `X` to store the independent features. In our dataset, we will consider all features except Price as independent variables, and we will use the `drop()` function to include them. Once this is done, we print out the top five instances of the `X` variable. Add the following code to do this:

    ```py
    X = df.drop('Price', axis=1)
    X.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.10: Dataset showing the first five rows of the feature matrix](image/C13322_01_10.jpg)

    ###### 图 1.10:显示特征矩阵前五行的数据集

    #### 注意

    头部的默认实例数是五个，所以如果您不指定这个数字，它将默认输出五个观察值。前面截图中的 axis 参数表示是要从行(axis = 0)还是列(axis = 1)中删除标签。

10.  Print the shape of your newly created feature matrix using the `X.shape` command. Add the following code to do this:

    ```py
    X.shape
    ```

    上述代码生成以下输出:

    ![](image/C13322_01_11.jpg)

    ###### 图 1.11:特征矩阵的形状

    在上图中，第一个值表示数据集中的观测值数量( **5000** )，第二个值表示要素数量( **6** )。

11.  Similarly, we will create a variable called `y` that will store the target values. We will use indexing to grab the target column. Indexing allows you to access a section of a larger element. In this case, we want to grab the column named Price from the `df` DataFrame. Then, we want to print out the top 10 values of the variable. Add the following code to implement this:

    ```py
    y = df['Price']
    y.head(10)
    ```

    上述代码生成以下输出:

    ![Figure 1.12: Dataset showing the first 10 rows of the target matrix](image/C13322_01_12.jpg)

    ###### 图 1.12:显示目标矩阵前 10 行的数据集

12.  Print the shape of your new variable using the `y.shape` command. The shape should be one-dimensional, with a length equal to the number of observations (**5000**) only. Add the following code to implement this:

    ```py
    y.shape
    ```

    上述代码生成以下输出:

![Figure 1.13: Shape of the target matrix](image/C13322_01_13.jpg)

###### 图 1.13:目标矩阵的形状

您已经成功创建了数据集的特征矩阵和目标矩阵。您已经完成了建立预测模型过程的第一步。该模型将从特征矩阵(在`X`中的列)中学习模式，以及它们如何映射到目标向量中的值(`y`)。然后，可以根据这些新房子的特征，使用这些模式从新数据中预测房价。

在下一节中，我们将探讨预处理中涉及的更多步骤。

## 数据清理

数据清理包括填充缺失值和处理不一致等过程。它检测损坏的数据，并替换或修改它。

**缺失值**

如果您想掌握成功管理和理解数据的技能，理解缺失值的概念是很重要的。我们来看看下图:

![Figure 1.14: Bank customer credit data](image/C13322_01_14.jpg)

###### 图 1.14:银行客户信用数据

如你所见，数据属于一家银行；每行是一个单独的客户，每列包含他们的详细信息，如年龄和信用额度。有些单元格要么有 **NA** 要么就是空的。这是丢失的数据。关于客户的每一条信息对银行都至关重要。如果缺少任何信息，银行将很难预测向客户提供贷款的风险。

**处理缺失数据**

对缺失数据的智能处理将导致构建一个能够处理复杂任务的健壮模型。有许多方法可以处理丢失的数据。现在让我们来看看其中的一些方法。

**删除数据**

检查缺失值是数据预处理的第一步，也是最重要的一步。模型不能接受缺少值的数据。这是一个非常简单且常用的处理缺失值的方法:如果缺失值对应于行中的位置，我们删除一行，或者如果一列中有超过 70%-75%的缺失数据，我们删除该列。同样，阈值不是固定的，取决于您希望固定多少。

这种方法的好处是快速且容易做到，而且在许多情况下，没有数据总比坏数据好。缺点是您最终可能会丢失重要信息，因为您删除的是基于几个缺失值的整个要素。

### 练习 2:删除丢失的数据

在本练习中，我们将把`Banking_Marketing.csv`数据集加载到 pandas 数据帧中，并处理缺失的数据。该数据集与一家葡萄牙银行机构的直接营销活动相关。营销活动包括给客户打电话，试图让他们订购某种特定的产品。该数据集包含所联系的每个客户的详细信息，以及他们是否订阅了产品。按照以下步骤完成本练习:

#### 注意

`Banking_Marketing.csv`数据集可以在这个位置找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Banking _ marketing . CSV。

1.  打开一个 Jupyter 笔记本。插入一个新的单元格，并添加以下代码来导入 pandas 并获取`Banking_Marketing.csv`数据集:

    ```py
    import pandas as pd
    dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'
    #reading the data into the dataframe into the object data
    df = pd.read_csv(dataset, header=0)
    ```

2.  Once you have fetched the dataset, print the datatype of each column. To do so, use the `dtypes` attribute from the pandas DataFrame:

    ```py
    df.dtypes
    ```

    上述代码生成以下输出:

    ![Figure 1.15: Data types of each feature](image/C13322_01_15.jpg)

    ###### 图 1.15:每个特征的数据类型

3.  Now we need to find the missing values for each column. In order to do that, we use the `isna()` function provided by pandas:

    ```py
    df.isna().sum()
    ```

    上述代码生成以下输出:

    ![Figure 1.16: Missing values of each column in the dataset](image/C13322_01_16.jpg)

    ###### 图 1.16:数据集中每一列的缺失值

    在上图中，我们可以看到有三列数据缺失，分别是`age`、`contact`和`duration`。在**年龄**栏中有两个 NAs，在**联系人**中有六个 NAs，在**持续时间**中有七个 NAs。

4.  一旦你找出了所有丢失的细节，我们就从数据帧中删除所有丢失的行。为此，我们使用`dropna()`函数:

    ```py
    #removing Null values
    data = data.dropna()
    ```

5.  To check whether the missing vales are still present, use the `isna()` function:

    ```py
    df.isna().sum()
    ```

    上述代码生成以下输出:

![Figure 1.17: Each column of the dataset with zero missing values](image/C13322_01_17.jpg)

###### 图 1.17:数据集的每一列都没有缺失值

您已经成功地从数据帧中删除了所有丢失的数据。在下一节中，我们将看看处理缺失数据的第二种方法，它使用插补。

**均值/中值/众数插补**

在数字数据的情况下，我们可以计算其平均值或中值，并用结果来替换缺失值。在分类(非数字)数据的情况下，我们可以计算它的模式来替换丢失的值。这就是所谓的插补。

使用插补而不仅仅是删除数据的好处是，它可以防止数据丢失。缺点是你不知道在给定的情况下使用均值、中值或众数会有多准确。

让我们来看一个练习，在这个练习中，我们将使用插补方法来解决缺失数据问题。

### 练习 3:输入缺失数据

在本练习中，我们将把`Banking_Marketing.csv`数据集加载到 pandas 数据帧中，并处理缺失的数据。我们将使用插补方法。按照以下步骤完成本练习:

#### 注意

`Banking_Marketing.csv`数据集可以在这个位置找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Banking _ marketing . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。将数据集加载到熊猫数据框架中。为此，添加以下代码:

    ```py
    import pandas as pd
    dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'
    df = pd.read_csv(dataset, header=0)
    ```

2.  Impute the numerical data of the `age` column with its mean. To do so, first find the mean of the `age` column using the `mean()` function of pandas, and then print it:

    ```py
    mean_age = df.age.mean()
    print(mean_age)
    ```

    上述代码生成以下输出:

    ![Figure 1.18: Mean of the age column](image/C13322_01_18.jpg)

    ###### 图 1.18:年龄列的平均值

3.  完成后，使用`fillna()`函数估算缺失数据及其平均值。这可以用下面的代码来完成:

    ```py
    df.age.fillna(mean_age, inplace=True)
    ```

4.  现在我们用它的中间值估算持续时间列的数值数据。为此，首先使用熊猫的`median()`函数找到持续时间列的中间值。为此，添加以下代码:

    ```py
    median_duration = df.duration.median()
    print(median_duration)
    ```

    ![Figure 1.19: Median of the duration](image/C13322_01_19.jpg)

    ###### 图 1.19:持续时间的中位数

5.  使用`fillna()`函数估算持续时间的缺失数据及其中值。

    ```py
    df. duration.fillna(median_duration,inplace=True)
    ```

6.  用其模式输入联系人列的分类数据。为此，首先使用熊猫的`mode()`功能找到联系人列的模式。为此，添加以下代码:

    ```py
    mode_contact = df.contact.mode()[0]
    print(mode_contact)
    ```

    ![](image/C13322_01_20.jpg)

    ###### 图 1.20:联系人的模式

7.  Impute the missing data of the contact column with its mode using the `fillna()` function. Add the following code to do this:

    ```py
    df.contact.fillna(mode_contact,inplace=True)
    ```

    与平均值和中值不同，一列中可能有多个模式。所以，我们只取指数为 0 的第一种模式。

你已经成功地用不同的方法估算出缺失的数据，并使数据完整而清晰。

数据清理的另一部分是处理异常值，这将在下一节讨论。

**离群值**

异常值是相对于其他数据的分布非常大或非常小的值。我们只能在数字数据中发现异常值。箱线图是查找数据集中异常值的一种好方法，如下图所示:

![Figure 1.21: Sample of outliers in a box plot](image/C13322_01.21.jpg)

###### 图 1.21:箱线图中异常值的示例

#### 注意

离群值并不总是坏数据！在业务理解和客户互动的帮助下，你可以辨别是去除还是保留离群值。

让我们用一个简单的例子来学习如何发现异常值。考虑来自一个地方不同时间的温度样本数据集:

```py
71, 70, 90, 70, 70, 60, 70, 72, 72, 320, 71, 69
```

我们现在可以执行以下操作:

1.  首先，我们将对数据进行排序:

    ```py
    60,69, 70, 70, 70, 70, 71, 71, 72, 72, 90, 320
    ```

2.  Next, we'll calculate the median (Q2). The median is the middle data after sorting.

    这里的中项是对列表排序后的 70 和 71。

    中位数为 *(70 + 71) / 2 = 70.5*

3.  Then we'll calculate the lower quartile (Q1). Q1 is the middle value (median) of the first half of the dataset.

    前半部分数据= `60, 69, 70, 70, 70, 70`

    底部 6 的点 3 和点 4 都等于 70。

    平均值为 *(70 + 70) / 2 = 70*

    Q1 = 70

4.  Then we calculate the upper quartile (Q3).

    Q3 是数据集后半部分的中间值(中位数)。

    后半部分数据= `71, 71, 72, 72, 90, 320`

    上 6 的点 3 和点 4 分别是 72 和 72。

    平均值为 *(72 + 72) / 2 = 72*

    Q3 = 72

5.  Then we find the interquartile range (IQR).

    IQR =第三季度–Q1 = 72–70

    IQR = 2

6.  Next, we find the upper and lower fences.

    下围栏= Q1–1.5(IQR)= 70–1.5(2)= 67

    上围栏= Q3 + 1.5 (IQR) = 71.5 + 1.5(2) = 74.5

    我们围栏的边界= 67 和 74.5

任何低于下围栏但高于上围栏的数据点都是异常值。因此，我们示例中的异常值是 60、90 和 320。

### 练习 4:查找并删除数据中的异常值

在本练习中，我们将把`german_credit_data.csv`数据集加载到 pandas 数据帧中，并删除离群值。该数据集包含 1000 个条目，具有 Hofmann 教授准备的 20 个类别/符号属性。在这个数据集中，每个条目代表一个从银行贷款的人。根据属性集，每个人被划分为好的或坏的信用风险。按照以下步骤完成本练习:

#### 注意

这里可以找到`german_credit_data.csv`数据集的链接:https://github . com/TrainingByPackt/Data-Science-with-Python/blob/master/chapter 01/Data/german _ credit _ Data . CSV。

1.  Open a Jupyter notebook and add a new cell. Write the following code to import the necessary libraries: pandas, NumPy, matplotlib, and seaborn. Fetch the dataset and load it into the pandas DataFrame. Add the following code to do this:

    ```py
    import pandas as pd
    import numpy as np
    %matplotlib inline  
    import seaborn as sbn
    dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/german_credit_data.csv'
    #reading the data into the dataframe into the object data
    df = pd.read_csv(dataset, header=0)
    ```

    在前面的代码中，`%matplotlib inline`是一个神奇的函数，如果我们想要在笔记本中看到这个情节，它是必不可少的。

2.  This dataset contains an `Age` column. Let's plot a boxplot of the `Age` column. To do so, use the `boxplot()` function from the seaborn library:

    ```py
    sbn.boxplot(df['Age'])
    ```

    上述代码生成以下输出:

    ![Figure 1.22: A box plot of the Age column](image/C13322_01_22.jpg)

    ###### 图 1.22:年龄列的方框图

    我们可以看到一些数据点是箱线图中的异常值。

3.  The boxplot uses the IQR method to display the data and the outliers (the shape of the data). But in order to print an outlier, we use a mathematical formula to retrieve it. Add the following code to find the outliers of the `Age` column using the IQR method:

    ```py
    Q1 = df["Age"].quantile(0.25)
    Q3 = df["Age"].quantile(0.75)
    IQR = Q3 - Q1
    print(IQR)
    >>> 15.0
    ```

    在上述代码中，Q1 是第一个四分位数，Q3 是第三个四分位数。

4.  现在我们通过添加下面的代码找到上围栏和下围栏，并打印上围栏以上和下围栏以下的所有数据。为此，添加以下代码:

    ```py
    Lower_Fence = Q1 - (1.5 * IQR)
    Upper_Fence = Q3 + (1.5 * IQR)
    print(Lower_Fence)
    print(Upper_Fence)
    >>> 4.5
    >>> 64.5
    ```

5.  To print all the data above the upper fence and below the lower fence, add the following code:

    ```py
    df[((df["Age"] < Lower_Fence) |(df["Age"] > Upper_Fence))]
    ```

    上述代码生成以下输出:

    ![Figure 1.23: Outlier data based on the Age column](image/C13322_01_23.jpg)

    ###### 图 1.23:基于年龄列的异常数据

6.  Filter out the outlier data and print only the potential data. To do so, just negate the preceding result using the `~` operator:

    ```py
    df = df[~((df ["Age"] < Lower_Fence) |(df["Age"] > Upper_Fence))]
    df
    ```

    上述代码生成以下输出:

![Figure 1.24: Potential data based on the Age column](image/C13322_01_24.jpg)

###### 图 1.24:基于年龄列的潜在数据

您已经使用 IQR 成功地找到了异常值。在下一节中，我们将探索另一种叫做数据集成的预处理方法。

## 数据整合

到目前为止，我们已经确保去除数据中的杂质，使其变得干净。现在，下一步是将不同来源的数据结合起来，得到一个统一的结构，其中包含更有意义和价值的信息。如果数据被分离到不同的源中，这是最常用的。为了简单起见，让我们假设我们在不同的地方有 CSV 格式的数据，都在谈论同一个场景。假设我们在数据库中有一些关于雇员的数据。我们不能期望关于雇员的所有数据都驻留在同一个表中。员工的个人数据可能位于一个表中，员工的项目历史记录可能位于另一个表中，员工的上班和下班详细信息可能位于另一个表中，等等。因此，如果我们想对雇员做一些分析，我们需要在一个公共的地方得到所有的雇员数据。将数据集中到一个地方的过程称为数据集成。为了进行数据集成，我们可以使用`merge`函数合并多个 pandas 数据帧。

我们来解决一个基于数据整合的练习，以便对它有一个清晰的认识。

### 练习 5:整合数据

在本练习中，我们将合并两个数据集中学生的详细信息，即`student.csv`和`marks.csv`。`student`数据集包含`Age`、`Gender`、`Grade`和`Employed`等列。`marks.csv`数据集包含像`Mark`和`City`这样的列。`Student_id`列在两个数据集之间是公共的。按照以下步骤完成本练习:

#### 注意

`student.csv`数据集可以在这个位置找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/student . CSV。

`marks.csv`数据集可以在这个位置找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/mark . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写下面的代码来导入熊猫并将`student.csv`和`marks.csv`数据集加载到`df1`和`df2`熊猫数据帧:

    ```py
    import pandas as pd
    dataset1 = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv"
    dataset2 = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/mark.csv"
    df1 = pd.read_csv(dataset1, header = 0)
    df2 = pd.read_csv(dataset2, header = 0)
    ```

2.  To print the first five rows of the first DataFrame, add the following code:

    ```py
    df1.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.25: The first five rows of the first DataFrame](image/C13322_01_25.jpg)

    ###### 图 1.25:第一个数据帧的前五行

3.  To print the first five rows of the second DataFrame, add the following code:

    ```py
    df2.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.26: The first five rows of the second DataFrame](image/C13322_01_26.jpg)

    ###### 图 1.26:第二个数据帧的前五行

4.  `Student_id`是两个数据集共有的。使用`pd.merge()`函数对与`Student_id`列相关的两个数据帧进行数据整合，然后打印新数据帧的前 10 个值:

    ```py
    df = pd.merge(df1, df2, on = 'Student_id')
    df.head(10)
    ```

![Figure 1.27: First 10 rows of the merged DataFrame](image/C13322_01_27.jpg)

###### 图 1.27:合并数据帧的前 10 行

这里，`df1`数据帧的数据与`df2`数据帧的数据合并。合并后的数据存储在一个名为`df`的新数据帧中。

我们现在已经学习了如何执行数据集成。在下一节中，我们将探索另一个预处理任务，数据转换。

## 数据转换

之前，我们看到了如何将不同来源的数据合并到一个统一的数据框架中。现在，我们有许多包含不同类型数据的列。我们的目标是将数据转换成机器学习可理解的格式。所有的机器学习算法都是基于数学的。因此，我们需要将所有列转换成数字格式。在此之前，让我们看看我们拥有的所有不同类型的数据。

从更广的角度来看，数据分为数值数据和分类数据:

*   **数值**:顾名思义，这是可以量化的数值数据。
*   **分类**:数据是一个字符串或非数值的数据，本质上是定性的。

数字数据进一步分为以下几类:

*   **离散**:简单来说，任何可数的数值数据都叫做离散，比如一个家庭的人数或者一个班级的学生人数。离散数据只能取某些值(如 1、2、3、4 等)。
*   **连续**:任何可测量的数值数据都称为连续，例如，一个人的身高或到达一个目的地所用的时间。连续数据实际上可以取任何值(例如，1.25、3.8888 和 77.1276)。

分类数据进一步分为以下几类:

*   **有序**:任何具有某种关联顺序的分类数据称为有序分类数据，例如，电影评级(优秀、良好、糟糕、最差)和反馈(开心、不错、不好)。你可以把有序的数据看作是你可以在标尺上标记的东西。
*   **名义**:任何没有顺序的分类数据称为名义分类数据。例子包括性别和国家。

从这些不同类型的数据中，我们将重点放在分类数据上。在下一节中，我们将讨论如何处理分类数据。

### 处理分类数据

有一些算法可以很好地处理分类数据，比如决策树。但是大多数机器学习算法不能直接操作分类数据。这些算法要求输入和输出都是数字形式。如果要预测的输出是分类的，那么在预测之后，我们将它们从数值数据转换回分类数据。让我们讨论一下我们在处理分类数据时面临的一些关键挑战:

*   **高基数**:基数意味着数据的唯一性。在这种情况下，数据列将有许多不同的值。一个很好的例子是用户 ID——在一个包含 500 个不同用户的表中，用户 ID 列有 500 个唯一值。
*   **很少出现**:这些数据列可能有很少出现的变量，因此不会显著到对模型产生影响。
*   **频繁出现**:数据列中可能会有一个类别出现多次，方差很小，不会对模型产生影响。
*   **不符合**:这种未经处理的分类数据不符合我们的模型。

**编码**

为了解决与分类数据相关的问题，我们可以使用编码。这是我们将分类变量转换成数字形式的过程。在这里，我们将看看编码分类数据的三种简单方法。

**更换**

这是一种用数字代替分类数据的技术。这是一个简单的替换，不涉及太多的逻辑处理。让我们看一个练习来更好地理解这一点。

### 练习 6:用数字简单替换分类数据

在本练习中，我们将使用之前看到的`student`数据集。我们将数据加载到 pandas 数据框架中，并简单地用数字替换所有的分类数据。按照以下步骤完成本练习:

#### 注意

`student`数据集可以在这个位置找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/student . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写以下代码来导入 pandas，然后将数据集加载到 pandas dataframe:

    ```py
    import pandas as pd
    import numpy as np
    dataset = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv"
    df = pd.read_csv(dataset, header = 0)
    ```

2.  Find the categorical column and separate it out with a different dataframe. To do so, use the `select_dtypes()` function from pandas:

    ```py
    df_categorical = df.select_dtypes(exclude=[np.number])
    df_categorical
    ```

    上述代码生成以下输出:

    ![Figure 1.28: Categorical columns of the dataframe](image/C13322_01_28.jpg)

    ###### 图 1.28:数据框架的分类列

3.  Find the distinct unique values in the `Grade` column. To do so, use the `unique()` function from pandas with the column name:

    ```py
    df_categorical['Grade'].unique()
    ```

    上述代码生成以下输出:

    ![Figure 1.29: Unique values in the Grade column](image/C13322_01_29.jpg)

    ###### 图 1.29:等级列中的唯一值

4.  Find the frequency distribution of each categorical column. To do so, use the `value_counts()` function on each column. This function returns the counts of unique values in an object:

    ```py
    df_categorical.Grade.value_counts()
    ```

    这一步的输出如下:

    ![Figure 1.30: Total count of each unique value in the Grade column
    ](image/C13322_01_30.jpg)

    ###### 图 1.30:等级列中每个唯一值的总数

5.  For the `Gender` column, write the following code:

    ```py
    df_categorical.Gender.value_counts()
    ```

    这段代码的输出如下:

    ![Figure 1.31: Total count of each unique value in the Gender column
    ](image/C13322_01_31.jpg)

    ###### 图 1.31:性别列中每个唯一值的总数

6.  Similarly, for the `Employed` column, write the following code:

    ```py
    df_categorical.Employed.value_counts()
    ```

    这段代码的输出如下:

    ![Figure 1.32: Total count of each unique value in the Employed column
    ](image/C13322_01_32.jpg)

    ###### 图 1.32:Employed 列中每个唯一值的总数

7.  替换`Grade`栏中的条目。用`1`替换`1st class`，用`2`替换`2nd class`，用`3`替换`3rd class`。为此，使用`replace()`功能:

    ```py
    df_categorical.Grade.replace({"1st Class":1, "2nd Class":2, "3rd Class":3}, inplace= True)
    ```

8.  替换`Gender`栏中的条目。用`0`替换`Male`，用`1`替换`Female`。为此，使用`replace()`功能:

    ```py
    df_categorical.Gender.replace({"Male":0,"Female":1}, inplace= True)
    ```

9.  替换`Employed`栏中的条目。用`0`替换`no`，用`1`替换`yes`。为此，使用`replace()`功能:

    ```py
    df_categorical.Employed.replace({"yes":1,"no":0}, inplace = True)
    ```

10.  一旦三列的替换完成，我们需要打印数据帧。添加以下代码:

    ```py
    df_categorical.head()
    ```

![Figure 1.33: Numerical data after replacement
](image/C13322_01_33.jpg)

###### 图 1.33:替换后的数字数据

您已经使用简单的手动替换方法成功地将分类数据转换为数值数据。我们现在将继续研究编码分类数据的另一种方法。

**标签编码**

在这种技术中，我们用从 0 到 N-1 的数字替换分类列中的每个值。例如，假设我们在一列中有一个雇员姓名列表。执行标签编码后，将为每个员工姓名分配一个数字标签。但这可能并不适用于所有情况，因为模型可能会将数值视为分配给数据的权重。标签编码是用于序数数据的最佳方法。scikit-learn 库提供了`LabelEncoder()`，这有助于标签编码。下一节我们来看一个练习。

### 练习 7:使用标签编码将分类数据转换为数值数据

在本练习中，我们将把`Banking_Marketing.csv`数据集加载到 pandas 数据帧中，并使用标签编码将分类数据转换为数值数据。按照以下步骤完成本练习:

#### 注意

`Banking_Marketing.csv`数据集可以在这里找到:https://github . com/TrainingByPackt/Master-Data-Science-with-Python/blob/Master/Chapter % 201/Data/Banking _ marketing . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写代码来导入 pandas 并将数据集加载到 pandas dataframe:

    ```py
    import pandas as pd
    import numpy as np
    dataset = 'https://github.com/TrainingByPackt/Master-Data-Science-with-Python/blob/master/Chapter%201/Data/Banking_Marketing.csv'
    df = pd.read_csv(dataset, header=0)
    ```

2.  在进行编码之前，删除所有丢失的数据。为此，使用`dropna()`功能:

    ```py
    df = df.dropna()
    ```

3.  Select all the columns that are not numeric using the following code:

    ```py
    data_column_category = df.select_dtypes(exclude=[np.number]).columns
    data_column_category	
    ```

    要了解选择的外观，请参考下面的屏幕截图:

    ![Figure 1.34: Non-numeric columns of the dataframe
    ](image/C13322_01_34.jpg)

    ###### 图 1.34:数据帧的非数字列

4.  Print the first five rows of the new dataframe. Add the following code to do this:

    ```py
    df[data_column_category].head()
    ```

    上述代码生成以下输出:

    ![Figure 1.35: Non-numeric values for the columns 
    ](image/C13322_01_35.jpg)

    ###### 图 1.35:列的非数字值

5.  Iterate through this `category` column and convert it to numeric data using `LabelEncoder()`. To do so, import the `sklearn.preprocessing` package and use the `LabelEncoder()` class to transform the data:

    ```py
    #import the LabelEncoder class
    from sklearn.preprocessing import LabelEncoder
    #Creating the object instance
    label_encoder = LabelEncoder()
    for i in data_column_category:
        df[i] = label_encoder.fit_transform(df[i])
    print("Label Encoded Data: ")
    df.head()
    ```

    上述代码生成以下输出:

![Figure 1.36: Values of non-numeric columns converted into numeric form
](image/C13322_01_36.jpg)

###### 图 1.36:转换成数字形式的非数字列的值

在前面的屏幕截图中，我们可以看到所有的值都已经从分类转换为数字。在这里，原始值已经被转换并替换为新编码的数据。

您已经使用`LabelEncoder` 方法成功地将分类数据转换为数值数据。在下一节中，我们将探索另一种类型的编码:一键编码。

**一键编码**

在标签编码中，分类数据被转换为数值数据，并且这些值被分配标签(例如 1、2 和 3)。使用这些数字数据进行分析的预测模型有时可能会将这些标签误认为某种顺序(例如，模型可能认为标签 3 比标签 1“更好”，这是不正确的)。为了避免这种混乱，我们可以使用一键编码。这里，标签编码的数据被进一步分成 n 列。这里，n 表示在执行标签编码时生成的唯一标签的总数。例如，假设通过标签编码生成了三个新标签。然后，在执行一键编码时，列将被分成三个部分。所以 n 的值是 3。让我们来看一个练习，以获得进一步的澄清。

### 练习 8:使用一键编码将分类数据转换为数值数据

在本练习中，我们将把`Banking_Marketing.csv`数据集加载到 pandas 数据帧中，并使用 one-hot 编码将分类数据转换成数值数据。按照以下步骤完成本练习:

#### 注意

`Banking_Marketing`数据集可以在这里找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Banking _ marketing . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写代码来导入 pandas 并将数据集加载到 pandas dataframe:

    ```py
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import OneHotEncoder
    dataset = 'https://github.com/TrainingByPackt/Master-Data-Science-with-Python/blob/master/Chapter%201/Data/Banking_Marketing.csv'
    #reading the data into the dataframe into the object data
    df = pd.read_csv(dataset, header=0)
    ```

2.  在进行编码之前，删除所有丢失的数据。为此，使用`dropna()`功能:

    ```py
    df = df.dropna()
    ```

3.  Select all the columns that are not numeric using the following code:

    ```py
    data_column_category = df.select_dtypes(exclude=[np.number]).columns
    data_column_category
    ```

    上述代码生成以下输出:

    ![Figure 1.37: Non-numeric columns of the dataframe
    ](image/C13322_01_37.jpg)

    ###### 图 1.37:数据帧的非数字列

4.  Print the first five rows of the new dataframe. Add the following code to do this:

    ```py
    df[data_column_category].head()
    ```

    上述代码生成以下输出:

    ![Figure 1.38: Non-numeric values for the columns
    ](image/C13322_01_38.jpg)

    ###### 图 1.38:列的非数字值

5.  Iterate through these category columns and convert them to numeric data using `OneHotEncoder`. To do so, import the `sklearn.preprocessing` package and avail yourself of the `OneHotEncoder()` class do the transformation. Before performing one-hot encoding, we need to perform label encoding:

    ```py
    #performing label encoding
    from sklearn.preprocessing import LabelEncoder
    label_encoder = LabelEncoder()
    for i in data_column_category:
        df[i] = label_encoder.fit_transform(df[i])
    print("Label Encoded Data: ")
    df.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.39: Values of non-numeric columns converted into numeric data](image/C13322_01_39.jpg)

    ###### 图 1.39:转换成数字数据的非数字列的值

6.  一旦我们执行了标签编码，我们就执行一次热编码。添加以下代码来实现这一点:

    ```py
    #Performing Onehot Encoding
    onehot_encoder = OneHotEncoder(sparse=False)
    onehot_encoded = onehot_encoder.fit_transform(df[data_column_category])
    ```

7.  Now we create a new dataframe with the encoded data and print the first five rows. Add the following code to do this:

    ```py
    #Creating a dataframe with encoded data with new column name
    onehot_encoded_frame = pd.DataFrame(onehot_encoded, columns = onehot_encoder.get_feature_names(data_column_category))
    onehot_encoded_frame.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.40: Columns with one-hot encoded values
    ](image/C13322_01_40.jpg)

    ###### 图 1.40:具有独热编码值的列

8.  Due to one-hot encoding, the number of columns in the new dataframe has increased. In order to view and print all the columns created, use the `columns` attribute:

    ```py
    onehot_encoded_frame.columns
    ```

    上述代码生成以下输出:

    ![Figure 1.41: List of new columns generated after one-hot encoding
    ](image/C13322_01_41.jpg)

    ###### 图 1.41:一键编码后生成的新列列表

9.  For every level or category, a new column is created. In order to prefix the category name with the column name you can use this alternate way to create one-hot encoding. In order to prefix the category name with the column name, write the following code:

    ```py
    df_onehot_getdummies = pd.get_dummies(df[data_column_category], prefix=data_column_category)
    data_onehot_encoded_data = pd.concat([df_onehot_getdummies,df[data_column_number]],axis = 1)
    data_onehot_encoded_data.columns
    ```

    上述代码生成以下输出:

![Figure 1.42: List of new columns containing the categories
](image/C13322_01_42.jpg)

###### 图 1.42:包含类别的新列列表

您已经使用`OneHotEncoder`方法成功地将分类数据转换为数值数据。

我们现在将进入另一个数据预处理步骤——如何处理数据中的大量数据。

## 不同尺度的数据

在现实生活中，数据集中的值可能有各种不同的大小、范围或比例。使用距离作为参数的算法可能不会以相同的方式衡量所有这些。有多种数据转换技术可用于转换数据的要素，以便它们使用相同的比例、大小或范围。这确保了每个特征对模型的预测都有适当的影响。

我们数据中的一些要素可能具有高量值(例如，年薪)，而其他要素可能具有相对较低的值(例如，在公司工作的年数)。仅仅因为一些数据的值较小，并不意味着它的重要性较低。因此，为了确保我们的预测不会因为数据中要素的不同量级而变化，我们可以执行要素缩放、标准化或归一化(这是处理数据中量级问题的三种类似方法)。

### 练习 9:使用 标准缩放器方法实现缩放

在本练习中，我们将把`Wholesale customer's data.csv`数据集加载到 pandas 数据帧中，并使用标准缩放器方法执行缩放。该数据集指的是批发分销商的客户。它包括各种产品类别的年度货币支出。按照以下步骤完成本练习:

#### 注意

`Wholesale customer`数据集可以在这里找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Wholesale % 20 customers % 20 Data . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写代码来导入 pandas 并将数据集加载到 pandas dataframe:

    ```py
    import pandas as pd
    dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv'
    df = pd.read_csv(dataset, header=0)
    ```

2.  Check whether there is any missing data. If there is, drop the missing data:

    ```py
    null_ = df.isna().any()
    dtypes = df.dtypes
    info = pd.concat([null_,dtypes],axis = 1,keys = ['Null','type'])
    print(info)
    ```

    上述代码生成以下输出:

    ![Figure 1.43: Different columns of the dataframe
    ](image/C13322_01_43.jpg)

    ###### 图 1.43:数据框的不同列

    如我们所见，dataframe 中有八列，都是类型`int64`。由于空值是`False`，这意味着任何列中都没有空值。因此，没有必要使用`dropna()`功能。

3.  Now perform standard scaling and print the first five rows of the new dataset. To do so, use the `StandardScaler()` class from `sklearn.preprocessing` and implement the `fit_transorm()` method:

    ```py
    from sklearn import preprocessing
    std_scale = preprocessing.StandardScaler().fit_transform(df)
    scaled_frame = pd.DataFrame(std_scale, columns=df.columns)
    scaled_frame.head()
    ```

    上述代码生成以下输出:

![Figure 1.44: Data of the features scaled into a uniform unit
](image/C13322_01_44.jpg)

###### 图 1.44:换算成统一单位的特征数据

使用`StandardScaler`方法，我们将所有列的数据缩放成一个统一的单位。如上表所示，所有要素的值都已转换为相同比例的统一范围。正因为如此，模型更容易做出预测。

您已经使用`StandardScaler`方法成功缩放了数据。在下一节中，我们将做一个练习，使用`MinMax` scaler 方法实现缩放。

### 练习 10:使用最小最大缩放器方法实现缩放

在本练习中，我们将把`Wholesale customers data.csv`数据集加载到 pandas 数据帧中，并使用`MinMax` scaler 方法执行缩放。按照以下步骤完成本练习:

#### 注意

`Whole customers data.csv`数据集可以在这里找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Wholesale % 20 customers % 20 Data . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写以下代码来导入 pandas 库并将数据集加载到 pandas dataframe:

    ```py
    import pandas as pd
    dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv'
    df = pd.read_csv(dataset, header=0)
    ```

2.  Check whether there is any missing data. If there is, drop the missing data:

    ```py
    null_ = df.isna().any()
    dtypes = df.dtypes
    info = pd.concat([null_,dtypes],axis = 1,keys = ['Null','type'])
    print(info)
    ```

    上述代码生成以下输出:

    ![Figure 1.45: Different columns of the dataframe
    ](image/C13322_01_45.jpg)

    ###### 图 1.45:数据框的不同列

    如我们所见，dataframe 中有八列，都是类型`int64`。由于空值是`False`，这意味着任何列中都没有空值。因此，没有必要使用`dropna()`功能。

3.  Perform `MinMax` scaling and print the initial five values of the new dataset. To do so, use the `MinMaxScaler()` class from `sklearn.preprocessing` and implement the `fit_transorm()` method. Add the following code to implement this:

    ```py
    from sklearn import preprocessing
    minmax_scale = preprocessing.MinMaxScaler().fit_transform(df)
    scaled_frame = pd.DataFrame(minmax_scale,columns=df.columns)
    scaled_frame.head()
    ```

    上述代码生成以下输出:

![Figure 1.46: Data of the features scaled into a uniform unit
](image/C13322_01_46.jpg)

###### 图 1.46:换算成统一单位的特征数据

使用`MinMaxScaler`方法，我们再次将数据缩放到所有列的统一单位中。如上表所示，所有要素的值都已转换为相同比例的统一范围。您已经使用`MinMaxScaler`方法成功缩放了数据。

在下一节中，我们将探索另一个预处理任务:数据离散化。

## 数据离散化

到目前为止，我们已经完成了使用编码的分类数据处理和使用缩放的数值数据处理。

**数据离散化**是通过分组将连续数据转换为离散桶的过程。离散化也因数据的易维护性而闻名。用离散数据训练模型比用连续数据训练模型更快、更有效。尽管连续值数据包含更多信息，但大量数据会降低模型的速度。这里，离散化可以帮助我们在两者之间取得平衡。一些著名的数据离散化方法有**宁滨**和使用直方图。尽管数据离散化很有用，但我们需要有效地选择每个时段的范围，这是一个挑战。

离散化的主要挑战是选择间隔或箱的数量以及如何决定它们的宽度。

这里我们使用了一个名为`pandas.cut()`的函数。此函数对于实现分段数据的存储和排序非常有用。

### 练习 11:连续数据的离散化

在本练习中，我们将加载`Student_bucketing.csv`数据集并执行分桶。数据集由学生的详细信息组成，如`Student_id`、`Age`、`Grade`、`Employed`和`marks`。按照以下步骤完成本练习:

#### 注意

`Student_bucketing.csv`数据集可以在这里找到:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Student _ bucket ing . CSV。

1.  打开 Jupyter 笔记本并添加新单元格。编写以下代码来导入所需的库，并将数据集加载到 pandas dataframe 中:

    ```py
    import pandas as pd
    dataset = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Student_bucketing.csv"
    df = pd.read_csv(dataset, header = 0)
    ```

2.  Once we load the dataframe, display the first five rows of the dataframe. Add the following code to do this:

    ```py
    df.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.47: First five rows of the dataframe
    ](image/C13322_01_47.jpg)

    ###### 图 1.47:数据帧的前五行

3.  Perform bucketing using the `pd.cut()` function on the `marks` column and display the top 10 columns. The `cut()` function takes parameters such as `x`, `bins`, and `labels`. Here, we have used only three parameters. Add the following code to implement this:

    ```py
    df['bucket']=pd.cut(df['marks'],5,labels=['Poor','Below_average','Average','Above_Average','Excellent'])
    df.head(10)
    ```

    上述代码生成以下输出:

![Figure 1.48: Marks column with five discrete buckets
](image/C13322_01_48.jpg)

###### 图 1.48:用五个离散的桶标记列

在前面的代码中，第一个参数表示一个数组。这里，我们从数据帧中选择了`marks`列作为数组。`5`表示要使用的箱数。由于我们已经将 bin 设置为`5`，标签需要相应地填充五个值:`Poor`、`Below_average`、`Average`、`Above_average`和`Excellent`。在上图中，我们可以看到整个连续的**标记**列被放入五个离散的桶中。我们已经学会了如何表演桶装。

我们现在已经介绍了预处理中涉及的所有主要任务。在下一节中，我们将详细研究如何训练和测试您的数据。

## 训练和测试数据

一旦将数据预处理成模型可以使用的格式，就需要将数据分成训练集和测试集。这是因为你的机器学习算法会使用训练集中的数据来学习它需要知道的东西。然后，它将使用所学的知识对测试集中的数据进行预测。然后，您可以将该预测与测试集中的实际目标变量进行比较，以查看您的模型有多准确。下一节中的练习将更清楚地说明这一点。

我们将按比例进行培训/测试。数据分割的较大部分将是训练集，较小部分将是测试集。这将有助于确保您使用足够的数据来准确地训练您的模型。

一般来说，按照帕累托原则，我们以 80:20 的比例进行训练测试分割。帕累托原则指出，“对于许多事件，大约 80%的结果来自 20%的原因。”但是如果你有一个大的数据集，无论是 80:20 分割还是 90:10 或 60:40 分割都没有关系。(如果我们的过程是计算密集型的，对训练集使用较小的分裂集可能会更好，但这可能会导致过度拟合的问题——这将在本书的后面部分讨论。)

### 练习 12:将数据分成训练集和测试集

在本练习中，我们将把`USA_Housing.csv`数据集(您之前看到的)加载到 pandas 数据帧中，并执行训练/测试分割。按照以下步骤完成本练习:

#### 注意

`USA_Housing.csv`数据集可从这里获得:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/USA _ housing . CSV。

1.  打开一个 Jupyter 笔记本，添加一个新单元格来导入 pandas，并将数据集加载到 pandas:

    ```py
    import pandas as pd
    dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv'
    df = pd.read_csv(dataset, header=0)
    ```

2.  Create a variable called `X` to store the independent features. Use the `drop()` function to include all the features, leaving out the dependent or the target variable, which in this case is named `Price`. Then, print out the top five instances of the variable. Add the following code to do this:

    ```py
    X = df.drop('Price', axis=1)
    X.head()
    ```

    上述代码生成以下输出:

    ![Figure 1.49: Dataframe consisting of independent variables
    ](image/C13322_01_49.jpg)

    ###### 图 1.49:由独立变量组成的数据框架

3.  Print the shape of your new created feature matrix using the `X.shape` command:

    ```py
    X.shape
    ```

    上述代码生成以下输出:

    ![Figure 1.50: Shape of the X variable
    ](image/C13322_01_50.jpg)

    ###### 图 1.50:X 变量的形状

    在上图中，第一个值表示数据集中的观测值数量( **5000** )，第二个值表示要素数量( **6** )。

4.  Similarly, we will create a variable called `y` that will store the target values. We will use indexing to grab the target column. Indexing allows us to access a section of a larger element. In this case, we want to grab the column named Price from the `df` dataframe and print out the top 10 values. Add the following code to implement this:

    ```py
    y = df['Price']
    y.head(10)
    ```

    上述代码生成以下输出:

    ![Figure 1.51: Top 10 values of the y variable
    ](image/C13322_01_51.jpg)

    ###### 图 1.51:y 变量的前 10 个值

5.  Print the shape of your new variable using the `y.shape` command:

    ```py
    y.shape
    ```

    上述代码生成以下输出:

    ![Figure 1.52: Shape of the y variable
    ](image/C13322_01_52.jpg)

    ###### 图 1.52:y 变量的形状

    形状应该是一维的，长度等于观察次数( **5000** )。

6.  Make train/test sets with an 80:20 split. To do so, use the `train_test_split()` function from the `sklearn.model_selection` package. Add the following code to do this:

    ```py
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    ```

    在前面的代码中，`test_size`是一个定义测试数据大小的浮点值。如果该值为 0.2，则拆分比例为 80:20。`test_train_split`以随机方式将数组或矩阵分割成训练和测试子集。每次我们运行没有`random_state`的代码，我们都会得到不同的结果。

7.  Print the shape of `X_train`, `X_test`, `y_train`, and `y_test`. Add the following code to do this:

    ```py
    print("X_train : ",X_train.shape)
    print("X_test : ",X_test.shape)
    print("y_train : ",y_train.shape)
    print("y_test : ",y_test.shape)
    ```

    上述代码生成以下输出:

![Figure 1.53: Shape of train and test datasets
](image/C13322_01_53.jpg)

###### 图 1.53:训练和测试数据集的形状

您已经成功地将数据分为训练集和测试集。

在下一节中，您将完成一个活动，在该活动中您将对数据集执行预处理。

### 活动 1:使用银行营销订阅数据集进行预处理

在本活动中，我们将对`Bank Marketing Subscription`数据集执行各种预处理任务。该数据集与一家葡萄牙银行机构的直接营销活动相关。打电话推销新产品，数据集记录每个客户是否订购了该产品。

按照以下步骤完成本活动:

#### 注意

`Bank Marketing Subscription`数据集可从这里获得:https://github . com/training bypackt/Data-Science-with-Python/blob/master/chapter 01/Data/Banking _ marketing . CSV。

1.  从给定的链接中加载数据集到熊猫数据框架中。
2.  通过查找行数和列数、列出所有列、查找所有列的基本统计数据(可以使用`describe().transpose()`函数)、列出列的基本信息(可以使用`info()`函数)来探索数据的特性。
3.  检查是否有任何缺失(或 NULL)值，如果有，找出每列中有多少个缺失值。
4.  删除任何缺失的值。
5.  打印`education`列的频率分布。
6.  数据集的`education`列有很多类别。减少类别以便更好地建模。
7.  为数据选择并执行合适的编码方法。
8.  Split the data into train and test sets. The target data is in the `y` column and the independent data is in the remaining columns. Split the data with 80% for the train set and 20% for the test set.

    #### 注意

    这项活动的解决方案可在第 324 页找到。

现在我们已经讨论了各种数据预处理步骤，让我们更详细地看看数据科学家可用的不同类型的机器学习。

## 监督学习

监督学习是一种使用标记数据(目标变量已知的数据)进行训练的学习系统。该模型学习特征矩阵中的模式如何映射到目标变量。当受过训练的机器被输入新的数据集时，它可以使用它所学到的知识来预测目标变量。这也可以称为预测建模。

监督学习大致分为两类。这些类别如下:

**分类**主要处理分类目标变量。分类算法有助于预测数据点属于哪个组或类。

当预测在两个类别之间时，它被称为二元分类。一个例子是预测客户是否会购买产品(在这种情况下，分类是 yes 和 no)。

如果预测涉及两个以上的目标类，称为多分类；例如，预测客户将购买的所有商品。

**回归**处理数字目标变量。回归算法基于训练数据集预测目标变量的数值。

线性回归测量一个或多个预测变量和一个结果变量之间的联系。例如，线性回归可以帮助列举年龄、性别和饮食(预测变量)对身高(结果变量)的相对影响。

**时间序列分析**顾名思义，处理的是关于时间分布的数据，也就是按时间顺序排列的数据。股票市场预测和客户流失预测是时间序列数据的两个例子。根据要求或需要，时间序列分析可以是回归或分类任务。

## 无监督学习

与监督学习不同，非监督学习过程涉及既未分类也未标记的数据。该算法将在没有指导的情况下对数据进行分析。机器的工作是根据数据的相似性对未分类的信息进行分组。目的是让模型发现数据中的模式，以便对数据告诉我们的内容有所了解，并做出预测。

一个例子是获取大量未标记的客户数据，并使用它来寻找将客户聚类到不同组的模式。不同的产品可以销售给不同的群体，以获得最大的利润。

无监督学习大致分为两种类型:

*   **聚类**:聚类过程有助于发现数据中的内在模式。
*   关联:关联规则是发现与大量数据相关的模式的一种独特方式，例如假设当某人购买产品 1 时，他们也倾向于购买产品 2。

## 强化学习

强化学习是机器学习中的一个广泛领域，其中机器通过查看已经执行的动作的结果来学习在环境中执行下一步。强化学习没有答案，学习代理决定应该做什么来执行指定的任务。它从先前的知识中学习。这种学习包括奖励和惩罚。

无论你使用哪种类型的机器学习，你都希望能够衡量你的模型有多有效。您可以使用各种性能指标来做到这一点。在本书后面的章节中，你会看到它们是如何被使用的，但是这里给出了一些最常见的方法的简要概述。

## 绩效指标

机器学习中有不同的评估指标，这些指标取决于数据的类型和要求。一些指标如下:

*   混淆矩阵
*   精确
*   回忆
*   准确(性)
*   F1 分数

**混淆矩阵**

**混淆矩阵**是用于定义分类模型对已知实际值的测试数据的性能的表格。为了更好地理解这一点，请看下图，其中显示了预测值和实际值:

![](image/C13322_01_54.jpg)

###### 图 1.54:预测值与实际值的对比

让我们详细研究混淆矩阵的概念及其度量标准 TP、TN、FP 和 FN。假设您正在构建一个预测怀孕的模型:

*   **TP** ( **真阳性**):性别是女的而且她确实怀孕了，你的模型也预测到了`True`。
*   **FP** ( **假阳性**):性别是男性，你的模型预测了`True`，这不可能发生。这是一种称为类型 1 错误的错误。
*   **FN** ( **假阴性**):性别为女性，实际怀孕，模型预测`False`，也是错误。这被称为第二类错误。
*   **TN** ( **真阴性**):性别为男性，预测为`False`；那是一个**真否定**。

类型 1 错误比类型 2 错误更危险。根据问题的不同，我们必须弄清楚我们是需要减少第一类错误还是第二类错误。

**精度**

精度是 TP 结果与模型预测的阳性结果总数的比率。精度查看我们的模型有多精确，如下所示:

![](image/C13322_01_55.jpg)

###### 图 1.55:精度方程

**回忆**

Recall 计算我们的模型预测的 TP 结果的比例:

![](image/C13322_01_56.jpg)

###### 图 1.56:回忆等式

**精度**

准确性计算模型做出的正面预测数与做出的预测总数的比率:

![](image/C13322_01_57.jpg)

###### 图 1.57:精度方程

**F1 得分**

F1 分数是另一种准确性衡量标准，但它允许我们在准确性和召回率之间寻求平衡:

![](image/C13322_01_58.jpg)

###### 图 1.58: F1 得分

当考虑模型的性能时，我们必须理解预测误差的另外两个重要概念:偏差和方差。

**什么是偏见？**

**偏差**是预测值与实际值的差距。高偏差意味着模型非常简单，无法捕捉数据的复杂性，导致所谓的拟合不足。

**什么是方差？**

**高方差**是模型在训练数据集上表现太好的时候。这会导致过度拟合，并使模型过于特定于训练数据，这意味着模型在测试数据上表现不佳。

![](image/C13322_01_59.jpg)

###### 图 1.59:高方差

假设您正在构建一个线性回归模型来预测一个国家的汽车市场价格。假设您有一个关于汽车及其价格的大型数据集，但仍有一些汽车的价格需要预测。

当我们用数据集训练我们的模型时，我们希望我们的模型只是在数据集中找到那个模式，仅此而已，因为如果它超出了那个范围，它将开始记忆训练集。

我们可以通过调整超参数来改进我们的模型——这本书后面会有更多的内容。我们通过使用另一个数据集(称为验证集)来最小化错误和最大化准确性。第一个图表显示，模型还没有学习到足够的知识来在测试集中进行良好的预测。第三个图表显示模型已经记住了训练数据集，这意味着准确度分数将是 100，误差为 0。但如果我们根据测试数据进行预测，中间模型的表现将优于第三种。

## 总结

在本章中，我们讲述了数据科学的基础知识，并探索了使用科学方法、流程和算法从数据中提取潜在信息的过程。然后，我们继续进行数据预处理，包括数据清理、数据集成、数据转换和数据离散化。

我们看到了在使用机器学习算法构建模型时，预处理的数据是如何分成训练集和测试集的。我们还讨论了监督、非监督和强化学习算法。

最后，我们讨论了不同的度量标准，包括混淆矩阵、精确度、召回率和准确度。

在下一章，我们将讨论数据可视化。