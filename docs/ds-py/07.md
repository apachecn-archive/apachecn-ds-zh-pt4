<title>C13322_07_Epub_Final_SW</title> <link href="css/epub.css" rel="stylesheet" type="text/css"> 

# *第七章*

# 处理人类语言

## 学习目标

本章结束时，您将能够:

*   为文本数据创建机器学习模型
*   使用 NLTK 库预处理文本
*   利用正则表达式清理和分析字符串
*   使用 Word2Vec 模型创建单词嵌入

本章将涵盖处理人类语言的概念。

## 简介

人工智能(AI)最重要的目标之一是理解人类语言来执行任务。拼写检查、情感分析、问答、聊天机器人和虚拟助手(如 Siri 和谷歌助手)都有一个自然语言处理(NLP)模块。NLP 模块使虚拟助手能够处理人类语言并基于它执行动作。例如，当我们说，“好的谷歌，设置一个早上 7 点的闹钟”，语音首先被转换成文本，然后这个文本被 NLP 模块处理。在该处理之后，虚拟助理调用闹钟/时钟应用的适当 API。处理人类语言有它自己的一系列挑战，因为它是模糊的，单词的意思取决于它们使用的上下文。这是语言对于 AI 最大的痛点。

另一个重要原因是无法获得完整的信息。我们在交流时往往会遗漏大部分信息；常识信息或普遍适用的事物。例如，句子“我看见一个人在山上拿着望远镜”可以根据上下文信息有不同的含义。例如，它可能意味着“我看见一个人在山上拿着望远镜”，但它也可能意味着“我通过望远镜看见一个人在山上。”计算机很难跟踪这些信息，因为大多数信息都是与上下文相关的。由于深度学习的进步，NLP 今天比我们使用传统方法(如聚类和线性模型)时工作得更好。这就是我们将在文本语料库上使用深度学习来解决 NLP 问题的原因。NLP 和其他机器学习问题一样，有两个主要部分，数据处理和模型创建。在下一个主题中，我们将学习如何处理文本数据，稍后，我们将学习如何使用这些处理后的数据来创建机器学习模型，以解决我们的问题。

## 文本数据处理

在我们开始为我们的文本数据建立机器学习模型之前，我们需要处理数据。首先，我们将学习理解数据构成的不同方式。这有助于我们了解数据到底是什么，并决定下一步要使用的预处理技术。接下来，我们将继续学习有助于我们预处理数据的技术。这一步有助于减少数据的大小，从而减少训练时间，也有助于我们将数据转换为机器学习算法更容易从中提取信息的形式。最后，我们将学习如何将文本数据转换为数字，以便机器学习算法可以实际使用它来创建模型。我们使用单词嵌入来实现，很像我们在*第 5 章* : *掌握结构化数据*中执行的实体嵌入。

### 正则表达式

在我们开始处理文本数据之前，我们需要了解正则表达式(RegEx)。RegEx 并不是真正的预处理技术，而是定义字符串中搜索模式的字符序列。在处理文本数据时，正则表达式是一个强大的工具，因为它可以帮助我们在文本集合中找到序列。正则表达式由元字符和常规字符组成。

![Figure 7.1: Tables containing metacharacters used in RegEx, and some examples
](image/C13322_07_01.jpg)

###### 图 7.1:包含正则表达式中使用的元字符的表格，以及一些例子

使用正则表达式，我们可以在文本中搜索复杂的模式。例如，我们可以用它来删除文本中的 URL。我们可以使用 Python 中的`re`模块删除一个 URL，如下所示:

```
re.sub(r"https?\://\S+\s", '', "https://www.asfd.com hello world")
```

`re.sub`接受三个参数:第一个是 RegEx，第二个是要替换匹配模式的表达式，第三个是应该在其中搜索模式的文本。

该命令的输出如下:

![Figure 7.2: Output command 
](image/C13322_07_02.jpg)

###### 图 7.2:输出命令

#### 注意

很难记住所有的正则表达式约定，所以在使用正则表达式时，参考一个备忘单，比如:(http://www.pyregex.com/)。

### 练习 54:使用正则表达式清理字符串

在本练习中，我们将使用 Python 的`re`模块来修改和分析一个字符串。在这个练习中，我们将简单地学习如何使用正则表达式，在下一节中，我们将看到如何使用正则表达式来预处理我们的数据。我们将使用来自 IMDB 电影评论数据集的单个评论(https://github . com/training bypackt/Data-Science-with-Python/tree/master/chapter 07)，我们也将在本章的稍后部分对其进行处理，以创建情感分析模型。此数据集已被处理，一些单词已被删除。在处理预构建数据集时，有时会出现这种情况，因此在开始工作之前分析正在处理的数据集非常重要。

1.  在本练习中，我们将使用来自 IMDB 的电影评论。将评论文本保存到一个变量中，如下面的代码所示。在本练习中，您可以使用文本的任何其他段落:

    ```
    string = "first think another Disney movie, might good, it's kids 	movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!<br /><br />- review Jamie Robert Ward (http://www.invocus.net)"
    ```

2.  Calculate the length of the review to know by how much we should reduce the size. We will use `len(string)` and get the output, as shown in the following code:

    ```
    len(string)
    ```

    输出长度如下:

    ![Figure 7.3: Length of the string
    ](image/C13322_07_03.jpg)

    ###### 图 7.3:字符串的长度

3.  Sometimes, when you scrape data from websites, hyperlinks get recorded as well. Most of the times, hyperlinks do not provide us any information. Remove any hyperlink from the data using a complex regex string, as in "`https?\://\S+`". This selects any substring with `https://` in it:

    ```
    import re
    string = re.sub(r"https?\://\S+", '', string)
    string
    ```

    带有超链接的字符串按如下方式删除:

    ![Figure 7.4: The string with hyperlinks removed
    ](image/C13322_07_04.jpg)

    ###### 图 7.4:移除了超链接的字符串

4.  Next, we will remove the `br` HTML tags from the text, which we observed while reading the string. Sometimes, these HTML tags get added to the scrapped data:

    ```
    string = re.sub(r'<br />', ' ', string)
    string
    ```

    不带`br`标签的字符串如下:

    ![Figure 7.5: The string without br tags
    ](image/C13322_07_05.jpg)

    ###### 图 7.5:没有 br 标签的字符串

5.  Now, we will remove all the digits from the text. This helps us reduce the size of the dataset when digits are of no significance to us:

    ```
    string = re.sub('\d','', string)
    string
    ```

    没有数字的字符串如下所示:

    ![Figure 7.6: The string without digits
    ](image/C13322_07_06.jpg)

    ###### 图 7.6:没有数字的字符串

6.  Next, we will remove all special characters and punctuations. Depending on your problem, these could just be taking up space and not providing relevant information for the machine learning algorithms. So, we remove them with the following regex pattern:

    ```
    string = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', '', string)
    string
    ```

    没有特殊字符和标点符号的字符串如下所示:

    ![Figure 7.7: The string without special characters
    ](image/C13322_07_07.jpg)

    ###### 图 7.7:没有特殊字符的字符串

7.  Now, we will substitute `can't` with `cannot` and `it's` with `it is`. This helps us reduce the training time as the number of unique words reduces:

    ```
    string = re.sub(r"can\'t", "cannot", string)
    string = re.sub(r"it\'s", "it is", string)
    string
    ```

    最后一个字符串如下:

    ![Figure 7.8: The final string
    ](image/C13322_07_08.jpg)

    ###### 图 7.8:最终的字符串

8.  Finally, we will calculate the length of the cleaned string:

    ```
    len(string)
    ```

    字符串的输出大小如下:

    ![Figure 7.9: The length of cleaned string
    ](image/C13322_07_09.jpg)

    ###### 图 7.9:清洗后的管柱长度

    我们将评论的规模缩小了 14%。

9.  Now, we will use RegEx to analyze the data and get all the words that start with a capital letter:

    #### 注意

    `re.findall`将 regex 模式和字符串作为输入，输出所有匹配该模式的子字符串。

    这显示在以下代码中:

    ```
    re.findall(r"[A-Z][a-z]*", string)
    ```

    原话如下:

    ![Figure 7.10: Words starting with capital letters
    ](image/C13322_07_10.jpg)

    ###### 图 7.10:以大写字母开头的单词

10.  To find all the one- and two-letter words in the text, use the following:

    ```
    re.findall(r"\b[A-z]{1,2}\b", string)
    ```

    输出如下所示:

![Figure 7.11: One and two letter words
](image/C13322_07_11.jpg)

###### 图 7.11:一个和两个字母的单词

恭喜你！您使用 RegEx 和`re`模块成功地修改并分析了一个审查字符串。

### 基本特征提取

基本特征提取帮助我们理解我们的数据由什么组成。这有助于我们选择预处理数据集的步骤。基本特征提取包括诸如计算平均字数和特殊字符数的动作。在本节中，我们将使用 IMDB 电影评论数据集作为示例:

```
data = pd.read_csv('movie_reviews.csv', encoding='latin-1')
```

让我们看看我们的数据集由什么组成:

```
data.iloc[0]
```

输出如下所示:

![Figure 7.12: SentimentText data
](image/C13322_07_12.jpg)

###### 图 7.12:感知文本数据

`SentimentText`变量包含实际的评论，而`Sentiment`变量包含评论的观点。`1`代表积极情绪`0`代表消极情绪。让我们打印第一篇评论，以了解我们正在处理的数据:

```
data.SentimentText[0]
```

第一次审查如下:

![Figure 7.13: First review
](image/C13322_07_13.jpg)

###### 图 7.13:第一次审查

现在，我们将通过获取数据集的关键统计数据来了解我们正在处理的数据类型。

**字数**

我们可以用下面的代码得到每次评论的字数:

```
data['word_count'] = data['SentimentText'].apply(lambda x: len(str(x).split(" ")))
```

现在，DataFrame 中的`word_count`变量包含了评论中的总字数。`apply`函数迭代地将`split`函数应用于数据集的每一行。现在，我们可以得到每个类的平均字数，看看正面评论的字数是否比负面评论多。

`mean()`函数计算 pandas 中一列的平均值。对于负面评价，使用以下代码:

```
data.loc[data.Sentiment == 0, 'word_count'].mean()
```

负面情绪的平均字数如下:

![Figure 7.14: Total number of words for negative sentiment
](image/C13322_07_14.jpg)

###### 图 7.14:负面情绪的总字数

对于正面评价，请使用以下内容:

```
data.loc[data.Sentiment == 1, 'word_count'].mean()
```

积极情绪的平均字数如下:

![Figure 7.15: Total number of words for positive sentiment
](image/C13322_07_15.jpg)

###### 图 7.15:积极情绪的总字数

我们可以看到，两个班级的平均单词量没有太大差别。

**停止字**

停用词是语言中最常见的词，例如，“我”、“我”、“我的”、“你的”和“the”大多数情况下，这些单词没有提供关于句子的真实信息，所以我们从数据集中删除这些单词以减小大小。图书馆有一个我们可以访问的英语停用词列表。

```
from nltk.corpus import stopwords
stop = stopwords.words('english')
```

要获得这些停用词的计数，我们可以使用下面的代码:

```
data['stop_count'] = data['SentimentText'].apply(lambda x: len([x for x in x.split() if x in stop]))
```

然后，通过使用以下代码，我们可以看到每个类的平均停用词数量:

```
data.loc[data.Sentiment == 0, 'stop_count'].mean()
```

负面情绪的平均停用词数量如下所示:

![](image/C13322_07_16.jpg)

###### 图 7.16:负面情绪的平均停用词数量

现在，为了获得积极情绪的停用词数量，我们使用以下代码:

```
data.loc[data.Sentiment == 1, 'stop_count'].mean()
```

正面情绪的停用词输出平均数如下所示:

![Figure 7.17: Average number of stop words for a positive sentiment
](image/C13322_07_17.jpg)

###### 图 7.17:积极情绪的平均停用词数量

**特殊字符的数量**

根据您正在处理的问题的类型，您可能希望保留特殊字符，如`@`、`#`、`$`和`*`，或者删除它们。要做到这一点，首先必须弄清楚数据集中出现了多少特殊字符。要获得数据集中的`^`、`&`、`*`、`$`、`@`和`#`的计数，请使用以下代码:

```
data['special_count'] = data['SentimentText'].apply(lambda x: len(re.sub('[^\^&*$@#]+' ,'', x)))
```

### 文本预处理

现在我们知道了我们的数据由什么组成，我们需要对它进行预处理，以便我们的机器学习算法可以轻松地在文本中找到模式。在本节中，我们将回顾一些用于清理和减少我们输入到机器学习算法中的数据维度的技术。

**小写**

我们执行的第一个预处理步骤是将所有数据转换成小写。这可以防止同一个单词的多个副本。您可以使用以下代码轻松地将所有文本转换为小写:

```
data['SentimentText'] = data['SentimentText'].apply(lambda x: " ".join(x.lower() for x in x.split()))
```

apply 函数迭代地将`lower`函数应用于数据集的每一行。

**停止字删除**

如前所述，停用词应该从数据集中删除，因为它们添加的有用信息非常少。停用词不会影响句子的情感。我们执行此步骤以消除停用词可能引入的任何偏差:

```
data['SentimentText'] = data['SentimentText'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
```

**频繁拆词**

停用词是更一般的词，如“a”、“an”和“the”但是，在这一步中，您将从正在使用的数据集中移除最常用的单词。例如，可以从推特数据集中删除的词是`RT`、`@username`和`DM`。首先，找到最常用的词:

```
word_freq = pd.Series(' '.join(data['SentimentText']).split()).value_counts()
word_freq.head()
```

最常见的词是:

![Figure 7.18: Most frequent words in tweet dataset
](image/C13322_07_18.jpg)

###### 图 7.18:tweet 数据集中最常见的单词

从输出中，我们得到了一个提示:文本包含 HTML 标记，可以删除这些标记以大大减小数据集的大小。因此，让我们首先删除所有的`<br />` HTML 标签，然后删除诸如“电影”和“电影”之类的词，这不会对情感检测器产生太大影响:

```
data['SentimentText'] = data['SentimentText'].str.replace(r'<br />','')
data['SentimentText'] = data['SentimentText'].apply(lambda x: " ".join(x for x in x.split() if x not in ['movie', 'film']))
```

**删除标点符号和特殊字符**

接下来，我们从文本中删除所有标点符号和特殊字符，因为它们给文本添加的信息非常少。要删除标点符号和特殊字符，请使用以下正则表达式:

```
punc_special = r"[^A-Za-z0-9\s]+"
data['SentimentText'] = data['SentimentText'].str.replace(punc_special,'')
```

正则表达式选择所有字母数字字符和空格。

**拼写检查**

有时，同一个单词的错误拼写会导致我们拥有同一个单词的副本。这可以通过使用自动更正库执行拼写检查来纠正:

```
from autocorrect import spell
data['SentimentText'] = [' '.join([spell(i) for i in x.split()]) for x in data['SentimentText']]
```

**词干**

**词干化**指的是去掉诸如‘ily’、‘iest’和‘ing’等后缀的做法这有助于我们的模型，因为同一个词根的变体有相同的意思。例如，“快乐”、“幸福地”和“最快乐的”都是同一个意思，所以可以用“快乐”来代替这在情感分析的情况下是有帮助的，在这种情况下，不需要快乐的程度，因为这将减少表示数据所需的维数。为了执行词干提取，我们可以使用`nltk`库:

```
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
data['SentimentText'] = data['SentimentText'].apply(lambda x: " ".join([stemmer.stem(word) for word in x.split()]))
```

#### 注意

根据数据集的大小，拼写检查、词干提取和词汇化可能需要很长时间才能完成，因此请确保您确实需要通过查看数据集来执行第步。

**词汇化**

#### 小费

与词干化相比，您应该更喜欢词汇化，因为它更有效。

**词条化**类似于词干化，但是在这里，我们用单词的词根替换单词，以降低数据集的维度。词汇化通常是比词干化更有效的选择。要执行词汇化，可以使用`nltk`库:

```
lemmatizer = nltk.stem.WordNetLemmatizer()
data['SentimentText'][:5].apply(lambda x: " ".join([lemmatizer.lemmatize(word) for word in x.split()])) 
```

#### 注意

由于“维数灾难”，我们正在努力降低数据集的维数数据集随着其维度(因变量)的增加而变得稀疏。这导致数据科学技术失败。这是因为很难对大量特征(因变量)进行建模以获得正确的输出。随着数据集要素数量的增加，我们需要更多的数据点来对其建模。因此，为了避开高维数据的诅咒，我们需要获得更多的数据，这反过来会增加处理数据所需的时间。

**标记化**

**标记化**是将句子分割成单词序列或者将段落分割成句子序列的过程。我们需要这样做，最终将数据转换成单词的一个热点向量。可以使用`nltk`库执行标记化:

```
import nltk
nltk.word_tokenize("Hello Dr. Ajay. It's nice to meet you.")
```

标记化列表如下所示:

![Figure 7.19: Tokenized list
](image/C13322_07_19.jpg)

###### 图 7.19:标记化列表

如您所见，它将标点符号从单词中分离出来，并检测复杂的单词，如“Dr”

### 练习 55:预处理 IMDB 电影评论数据集

在本练习中，我们将预处理 IMDB 电影评论数据集，使其为任何机器学习算法做好准备。该数据集由 25，000 条电影评论以及评论的情绪(积极或消极)组成。我们希望使用评论来预测情感，所以在执行预处理时，我们需要记住这一点。

1.  使用 pandas 加载 IMDB 电影评论数据集:

    ```
    import pandas as pd
    data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', 	encoding='latin-1')
    ```

2.  首先，我们将把数据集中的所有字符转换成小写:

    ```
    data.SentimentText = data.SentimentText.str.lower()
    ```

3.  Next, we will write a `clean_str` function, in which we will clean the reviews using the `re` module:

    ```
    import re
    def clean_str(string):
         string = re.sub(r"https?\://\S+", '', string)
         string = re.sub(r'\<a href', ' ', string)
         string = re.sub(r'&amp;', 'and', string) 
         string = re.sub(r'<br />', ' ', string)
         string = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', string)
         string = re.sub('\d','', string)
         string = re.sub(r"can\'t", "cannot", string)
         string = re.sub(r"it\'s", "it is", string)
         return string
    ```

    #### 注意

    这个函数首先删除文本中的超链接，然后删除 HTML 标签(`<a>`或`<br>`)。接下来，它将所有的`&amp;`替换为`and,`，然后删除所有的特殊字符、标点符号和数字。最后，它用“T5”代替“T4”，用“T7”代替“T6”。

    ```
    data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))
    ```

    使用 pandas 的应用功能对整个数据集执行检查清理。

4.  Next, check the word distribution in the dataset using the following code:

    ```
    pd.Series(' '.join(data['SentimentText']).split()).value_counts().head(10)
    ```

    前 10 个单词的出现情况如下:

    ![Figure 7.20: Top 10 words
    ](image/C13322_07_20.jpg)

    ###### 图 7.20:前 10 个单词

5.  Remove stop words from the reviews:

    #### 注意

    这将通过首先对评论进行标记，然后从`nltk`库中删除加载的停用词来完成。

6.  我们将“`movie`、“`film`、”和“`time`”添加到停用词中，因为它们在评论中出现得非常频繁，并且对理解评论情绪没有太大帮助:

    ```
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize,sent_tokenize
    stop_words = stopwords.words('english') + ['movie', 'film', 'time']
    stop_words = set(stop_words)
    remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]
    data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)
    ```

7.  接下来，我们将标记转换回句子，并删除所有文本都是停用词的评论:

    ```
    def combine_text(text):    
        try:
            return ' '.join(text[0])
        except:
            return np.nan
    data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))
    data = data.dropna(how='any')
    ```

8.  下一步是将文本转换成记号，然后是数字。我们将使用 Keras Tokenizer，因为它为我们执行这两个步骤:

    ```
    from keras.preprocessing.text import Tokenizer
    tokenizer = Tokenizer(num_words=250)
    tokenizer.fit_on_texts(list(data['SentimentText']))
    sequences = tokenizer.texts_to_sequences(data['SentimentText'])
    ```

9.  To get the size of the vocabulary, use the following:

    ```
    word_index = tokenizer.word_index
    print('Found %s unique tokens.' % len(word_index))
    ```

    唯一令牌的数量如下:

    ![](image/C13322_07_21.jpg)

    ###### 图 7.21:唯一令牌的数量

10.  To reduce the training time of our model, we will cap the length of our reviews at 200 words. You can play around with this number to find out what gives you the best accuracy.

    #### 注意

    字符较少的行将用 0 填充。您可以根据模型的精确度和训练时间来增加或减少该值。

    ```
    from keras.preprocessing.sequence import pad_sequences
    reviews = pad_sequences(sequences, maxlen=200)
    ```

11.  You should save the tokenizer so that you can convert the reviews back to text:

    ```
    import pickle
    with open('tokenizer.pkl', 'wb') as handle:
                pickle.dump(tokenizer, 
                            handle, 
                            protocol=pickle.HIGHEST_PROTOCOL)
    ```

    要预览已清理的审阅，请运行以下命令:

    ```
    data.SentimentText[124]
    ```

    清理后的评论如下所示:

![Figure 7.22: A cleaned review
](image/C13322_07_22.jpg)

###### 图 7.22:一个干净的审查

要获得流程下一步的实际输入，请运行以下命令:

```
reviews[124]
```

`reviews`命令下一步的输入如下所示:

![Figure 7.23: Input for next step to the cleaned review
](image/C13322_07_23.jpg)

###### 图 7.23:已清理评审下一步的输入

恭喜你！您已经成功预处理了您的第一个文本数据集。评论数据现在是 25，000 行(或评论)和 200 列(或单词)的矩阵。接下来，我们将学习如何将这些数据转换为嵌入数据，以便更容易预测情绪。

### 文本处理

现在我们已经清理了数据集，我们将把它转换成机器学习模型可以处理的形式。回想一下*第 5 章*、*掌握结构化数据*，我们在这里讨论了神经网络如何不能处理单词，所以我们需要将单词表示为数字，以便能够处理它们。因此，为了能够执行情感分析等任务，我们需要将文本转换为数字。

所以，我们讨论的第一种方法是一键编码，它在单词的情况下表现很差，因为单词之间有一定的关系，一键编码使得单词被计算时好像它们是相互独立的。例如，假设我们有三个词:“汽车”、“卡车”和“轮船”。现在，就相似性而言，“car”更接近于“truck”，但它仍然与“ship”有一些相似之处。一键编码未能捕捉到这种关系。

单词嵌入也是单词的向量表示，但是它们捕捉每个单词与另一个单词的关系。获得单词嵌入的不同方法将在下一节中解释。

**计数嵌入**

**Count embedding** 是单词的简单向量表示，取决于它在一段文本中出现的次数。假设一个数据集，其中有 *n* 个唯一的单词和 *M* 条不同的记录。为了获得计数嵌入，您创建一个 *N x M* 矩阵，其中每行是一个单词，每列是一条记录。矩阵中任何 *(n，m)* 位置的值将包含一个字 *n* 在记录 *m* 中出现的次数的计数。

**TF-IDF 嵌入**

TF-IDF 是一种获取单词集合或文档中每个单词重要性的方法。它代表术语频率-逆文档频率。在 TF-IDF 中，一个单词的重要性与该单词的出现频率成比例地增加，但是这种重要性被具有该单词的文档的数量所抵消，因此有助于针对更频繁使用的某些单词进行调整。换句话说，使用单词在训练集的一个数据点中的频率来计算单词的重要性。该重要性根据该单词在训练集的其他数据点中的出现而增加或减少。

TF-IDF 生成的权重由两项组成:

*   **词频** ( **TF** ):一个词在文档中出现的频率，如下图所示:

![Figure 7.24: The term frequency equation
](image/C13322_07_24.jpg)

###### 图 7.24:术语频率方程

其中 w 是单词。

*   **逆文档频率**(**IDF**):word 提供的信息量，如下图所示:

![Figure 7.25: The inverse document frequency equation
](image/C13322_07_25.jpg)

###### 图 7.25:逆文档频率方程

权重是这两项的乘积。在 TF-IDF 的情况下，我们在计数嵌入部分使用的 *N x M* 矩阵中用这个权重替换单词的计数。

**连续词袋嵌入**

**连续词袋** ( **CBOW** )利用神经网络工作。当输入是一个单词的周围单词时，它预测这个单词。神经网络的输入是周围单词的一次性向量。使用窗口参数选择输入单词的计数。网络只有一个隐藏层，使用 softmax 激活函数激活网络的输出层以获得概率。层之间的激活函数是线性的，但是更新梯度的方法与普通神经网络相同。

语料库的嵌入矩阵是隐含层和输出层之间的权重。因此，该嵌入矩阵将具有*N×H*维，其中 *N* 是语料库中唯一单词的数量，而 *H* 是隐藏层节点的数量。CBOW 比前面讨论的两种方法工作得更好，因为它的概率性和低内存需求。

![Figure 7.26: A representation of CBOW network
](image/C13322_07_26.jpg)

###### 图 7.26:CBOW 网络的示意图

**跳格嵌入**

skip-gram 使用神经网络预测给定输入单词的周围单词。这里的输入是单词的一键向量，输出是周围单词的概率。输出字数由窗口参数决定。与 CBOW 非常相似，这种方法使用具有单个隐藏层的神经网络，并且除了输出层之外，所有激活都是线性的，在输出层，我们使用 softmax 函数。一个很大的区别是误差是如何计算的:不同的误差是针对不同的预测单词计算的，然后将所有的误差加在一起得到最终的误差。通过用目标独热码向量减去输出概率向量来计算每个单词的误差。

这里的嵌入矩阵是输入层和隐藏层之间的权重矩阵。因此，该嵌入矩阵将具有*H×N*维，其中 *N* 是语料库中唯一单词的数量，而 *H* 是隐藏层节点的数量。对于不常用的单词，Skip-gram 比 CBOW 效果好得多，但通常较慢:

![Figure 7.27: A representation of skip-gram network
](image/C13322_07_27.jpg)

###### 图 7.27:跳格网络的表示

#### 小费

对于单词较少但样本数量较多的数据集，使用 CBOW。当处理包含更多单词和少量样本的数据集时，请使用 skip-gram。

**Word2Vec**

**Word2Vec** 模型是一组用于产生单词嵌入的 CBOW 和 skip-gram。Word2Vec 帮助我们轻松获得一个语料库的单词嵌入。为了实现该模型并获得单词嵌入，我们将利用`gensim`库:

```
model = gensim.models.Word2Vec(
        tokens,
        iter=5
        size=100,
        window=5,
        min_count=5,
        workers=10,
        sg=0)
```

为了训练模型，我们需要将标记化的句子作为参数传递给`gensim`的`Word2Vec`类。`iter`是训练的历元数，`size`是指隐含层的节点数，决定嵌入层的大小。`window`指训练神经网络时考虑的周围单词的数量。`min_count`指一个词被考虑所需的最低频率。`workers`是训练时要使用的线程数，`sg`是要使用的训练算法， *0* 表示 CBOW， *1* 表示 skip-gram。

要获得训练嵌入中唯一单词的数量，可以使用以下方法:

```
vocab = list(model.wv.vocab)
len(vocab)
```

在我们使用这些嵌入之前，我们需要确保它们是正确的。为此，我们找到了类似的词:

```
model.wv.most_similar('fun')
```

输出如下所示:

![Figure 7.28: Similar words
](image/C13322_07_28.jpg)

###### 图 7.28:相似的单词

要将嵌入内容保存到文件中，请使用以下代码:

```
model.wv.save_word2vec_format('movie_embedding.txt', binary=False)
```

要加载预训练嵌入，可以使用此函数:

```
def load_embedding(filename, word_index , num_words, embedding_dim):
    embeddings_index = {}
    file = open(filename, encoding="utf-8")
    for line in file:
        values = line.split()
        word = values[0]
        coef = np.asarray(values[1:])
        embeddings_index[word] = coef
    file.close()

    embedding_matrix = np.zeros((num_words, embedding_dim))
    for word, pos in word_index.items():
        if pos >= num_words:
            continue
        print(num_words)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[pos] = embedding_vector
    return embedding_matrix
```

该函数首先读取嵌入文件`filename`并获取文件中存在的所有嵌入向量。然后，它创建一个嵌入矩阵，将嵌入向量堆叠在一起。`num_words`参数限制了词汇表的大小，在 NLP 算法的训练时间过长的情况下很有帮助。`word_index`是一个字典，以关键字作为语料库的唯一单词，以值作为单词的索引。`embedding_dim`是训练时指定的嵌入向量的大小。

#### 小费

有很多非常好的预训练嵌入可用。一些受欢迎的是手套:https://nlp.stanford.edu/projects/glove/和快速文本:https://fasttext.cc/docs/en/english-vectors.html

### 练习 56:使用 Gensim 创建单词嵌入

在本练习中，我们将使用`gensim`库创建我们自己的 Word2Vec 嵌入。将为我们一直在处理的 IMDB 电影评论数据集创建单词 embedding。我们将从我们在*练习 55* 中离开的地方开始。

1.  The reviews variable has reviews in the token form but they have been converted into numbers. The `gensim` Word2Vec requires tokens in the string form, so we backtrack to where we converted the tokens back to sentences in step 6 of *Exercise 55*.

    ```
    data['SentimentText'] [0]
    ```

    第一次审查的标志如下:

    ![Figure 7.29: Tokens of first review
    ](image/C13322_07_29.jpg)

    ###### 图 7.29:第一次审查的标志

2.  现在，我们使用 pandas 的`apply`函数将每一行中的列表转换成一个列表，使用下面的代码:

    ```
    data['SentimentText'] = data['SentimentText'].apply(lambda x: x[0])
    ```

3.  现在，我们将预处理后的数据输入 Word2Vec，创建单词 embedding:

    ```
    from gensim.models import Word2Vec
    model = Word2Vec(
            data['SentimentText'],
            iter=50,
            size=100,
            window=5,
            min_count=5,
            workers=10)
    ```

4.  Let us check how well the model performs by viewing some similar words:

    ```
    model.wv.most_similar('insight')
    ```

    数据集中与'`insight`'最相似的词是:

    ![Figure 7.30: Similar words to 'insight'
    ](image/C13322_07_30.jpg)

    ###### 图 7.30:与“洞察力”相似的词

5.  要获得两个单词之间的相似度，请使用:

    ```
    model.wv.similarity(w1='violent', w2='brutal')
    ```

6.  The output similarity is shown here:![Figure 7.31: Similarity output
    ](image/C13322_07_31.jpg)

    ###### 图 7.31:相似性输出

    相似度得分范围从`0`到`1`，其中`1`表示两个单词相同，`0`表示两个单词完全不同，没有任何关联。

7.  Plot the embedding on a 2D space to understand what words are found to be similar to each other.

    首先，使用 PCA 将嵌入转换成二维。我们将只绘制前 200 个单词。(喜欢可以多剧情。)

    ```
    from sklearn.decomposition import PCA
    word_limit = 200
    X = model[model.wv.vocab][: word_limit]
    pca = PCA(n_components=2)
    result = pca.fit_transform(X)
    ```

8.  Now, plot the result on a scatter plot using `matplotlib`:

    ```
    import matplotlib.pyplot as plt
    plt.scatter(result[:, 0], result[:, 1])
    words = list(model.wv.vocab)[: word_limit]
    for i, word in enumerate(words):
        plt.annotate(word, xy=(result[i, 0], result[i, 1]))
    plt.show()
    ```

    您的输出应该如下所示:

    ![Figure 7.32: Representation of embedding of first 200 words using PCA
    ](image/C13322_07_32.jpg)

    ###### 图 7.32:使用 PCA 嵌入前 200 个单词的表示

    #### 注意

    在单词嵌入的表示中，轴没有任何意义。该表示简单地显示了不同单词的接近程度。

9.  将嵌入内容保存到文件中，以便以后检索:

    ```
    model.wv.save_word2vec_format('movie_embedding.txt', binary=False)
    ```

恭喜你！你刚刚创造了你的第一个单词嵌入。您可以尝试嵌入并查看不同单词之间的相似性。

### 活动 19:预测电影评论的情绪

在这项活动中，我们将尝试预测电影评论的情绪。数据集(https://github . com/TrainingByPackt/Data-Science-with-Python/tree/master/chapter 07)由 25，000 条来自 IMDB 的电影评论及其情绪(正面或负面)组成。让我们看看下面的场景:你在一家 DVD 租赁公司工作，该公司必须根据评论者对某部电影的看法来预测要制作的 DVD 数量。为此，您可以创建一个机器学习模型，该模型可以分析评论，以了解电影是如何被感知的。

1.  阅读并预处理电影评论。
2.  创建评论的单词嵌入。
3.  Create a fully connected neural network to predict sentiments, much like the neural network models we created in *Chapter 5*: *Mastering Structured Data*. The input will be the word embedding of the reviews and the output of the model will be either `1` (positive sentiment) or `0` (negative sentiment).

    #### 注意

    这项活动的解决方案可在第 378 页找到。

输出有点晦涩，因为停用词和标点符号已被删除，但您仍然可以理解评论的一般意义。

恭喜你！您刚刚创建了第一个 NLP 模块。你会发现这个模型给出了大约 76%的准确率，这是相当低的。这是因为它是基于单个单词来预测情绪的；它没有办法弄清楚审查的背景。例如，当它看到“好”这个词时，它会将“不好”预测为一种积极的情绪。如果它能看到多个单词，它就会明白这是一种负面情绪。在下一节中，我们将学习如何创建能够保留过去信息的神经网络。

## 递归神经网络

到目前为止，我们讨论的问题都没有时间相关性，这意味着预测不仅取决于当前输入，还取决于过去的输入。例如，在狗与猫分类器的例子中，我们只需要狗的图片来将其分类为狗。不需要其他信息或图像。相反，如果你想制作一个分类器来预测一只狗是在走路还是站着，你将需要一个序列或一个视频中的多个图像来判断这只狗在做什么。rnn 就像我们讨论过的全连接网络。唯一的变化是 RNN 有记忆功能，可以将以前的输入信息存储为状态。隐藏层的输出作为下一个输入的输入。

![Figure 7.33: Representation of recurrent neural network
](image/C13322_07_33.jpg)

###### 图 7.33:递归神经网络的表示

从图像中，您可以了解隐藏层的输出如何用作下一个输入的输入。这在神经网络中起着记忆元件的作用。另一件要记住的事情是，正常神经网络的输出是网络的输入和权重的函数。

his 允许我们随机输入任何数据点以获得正确的输出。然而，RNNs 的情况并非如此。在 RNNs 的情况下，我们的输出依赖于之前的输入，所以我们需要以正确的顺序输入。

![Figure 7.34: Representation of recurrent layer
](image/C13322_07_34.jpg)

###### 图 7.34:重现层的表示

在前面的图像，你可以看到一个单一的 RNN 层在左边的“折叠”模型。u 是输入权重，V 是输出权重，W 是与内存输入相关的权重。RNN 的记忆也被称为状态。右侧的“展开”模型显示了 RNN 如何为输入序列[xt-1，xt，xt+1]工作。模型根据应用程序的种类而有所不同。例如，在情感分析的情况下，输入序列最终将只需要一个输出。此问题的展开模型如下所示:

![Figure 7.35: Unfolded representation of a recurrent layer used to perform sentiment analysis
](image/C13322_07_35.jpg)

###### 图 7.35:用于执行情感分析的递归层的展开表示

### lstm

**长短期记忆** ( **LSTM** )细胞是一种特殊的 RNN 细胞，能够长时间保留信息。Hochreiter 和 Schmidhuber 于 1997 年引入了 LSTMs。rnn 受到消失梯度问题的困扰。它们会丢失长时间探测到的信息。例如，如果我们正在对一个文本执行情感分析，并且第一个句子说“我今天很高兴”,然后该文本的其余部分没有任何情感，则 RNN 不会很好地检测出该文本的情感是高兴的。长短期记忆(LSTM)细胞克服了这个问题，它将某些信息储存更长时间而不会忘记。大多数真实世界的递归机器学习实现都是使用 LSTMs 完成的。RNN 细胞和 LSTM 细胞的唯一区别是记忆状态。每个 RNN 层取存储器状态的输入并输出存储器状态，而每个 LSTM 层取长期和短期存储器作为输入并输出长期和短期存储器。长期记忆使网络能够更长时间地保留信息。

LSTM 单元在 Keras 中实现，您可以轻松地将 LSTM 图层添加到您的模型中:

```
model.add(keras.layers.LSTM(units, activation='tanh', dropout=0.0, recurrent_dropout=0.0, return_sequences=False))
```

这里，`units`是层中的节点数，`activation`是用于该层的激活函数。`recurrent_dropout`和`dropout`分别是递归状态和输入的退出概率。`return_sequences`指定输出是否应该包含序列；当您计划在当前层之后使用另一个循环层时，这是`True`。

#### 注意

LSTMs 几乎总是比 rnn 工作得更好。

### 练习 57:使用 LSTM 进行情感分析

在本练习中，我们将修改为上一活动创建的模型，使其使用 LSTM 单元。我们将使用我们一直在使用的同一个 IMDB 电影评论数据集。大部分预处理步骤类似于*活动 19* 中的步骤。

1.  使用 Python 中的熊猫读取 IMDB 电影评论数据集:

    ```
    import pandas as pd
    data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', encoding='latin-1')
    ```

2.  将推文转换成小写，以减少独特的单词数:

    ```
    data.text = data.text.str.lower()
    ```

3.  使用带有`clean_str`函数的正则表达式清理评论:

    ```
    import re
    def clean_str(string):

        string = re.sub(r"https?\://\S+", '', string)
        string = re.sub(r'\<a href', ' ', string)
        string = re.sub(r'&amp;', '', string) 
        string = re.sub(r'<br />', ' ', string)
        string = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', string)
        string = re.sub('\d','', string)
        string = re.sub(r"can\'t", "cannot", string)
        string = re.sub(r"it\'s", "it is", string)
        return string
    data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))
    ```

4.  接下来，从评论中删除停用词和其他经常出现的不必要的词。这一步将字符串转换成记号(这在下一步会很有帮助):

    ```
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize,sent_tokenize
    stop_words = stopwords.words('english') + ['movie', 'film', 'time']
    stop_words = set(stop_words)
    remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]
    data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)
    ```

5.  组合这些标记以获得一个字符串，然后在删除停用词后删除任何不包含任何内容的评论:

    ```
    def combine_text(text):    
        try:
            return ' '.join(text[0])
        except:
            return np.nan
    data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))
    data = data.dropna(how='any')
    ```

6.  使用 Keras Tokenizer 对评论进行标记，并将其转换为数字:

    ```
    from keras.preprocessing.text import Tokenizer
    tokenizer = Tokenizer(num_words=5000)
    tokenizer.fit_on_texts(list(data['SentimentText']))
    sequences = tokenizer.texts_to_sequences(data['SentimentText'])
    word_index = tokenizer.word_index
    ```

7.  最后，将推文填充到最多 100 个字。这将删除超过 100 个单词限制的所有单词，如果单词数少于 100，则添加 0:

    ```
    from keras.preprocessing.sequence import pad_sequences
    reviews = pad_sequences(sequences, maxlen=100)
    ```

8.  使用*文本处理*一节中讨论的`load_embedding`函数，通过使用以下代码，加载先前创建的嵌入以获得嵌入矩阵:

    ```
    import numpy as np
    def load_embedding(filename, word_index , num_words, embedding_dim):
        embeddings_index = {}
        file = open(filename, encoding="utf-8")
        for line in file:
            values = line.split()
            word = values[0]
            coef = np.asarray(values[1:])
            embeddings_index[word] = coef
        file.close()

        embedding_matrix = np.zeros((num_words, embedding_dim))
        for word, pos in word_index.items():
            if pos >= num_words:
                continue
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[pos] = embedding_vector
        return embedding_matrix
    embedding_matrix = load_embedding('movie_embedding.txt', word_index, len(word_index), 16)
    ```

9.  以 80:20 的比例将数据分成训练集和测试集。这可以被修改以找到最佳分割:

    ```
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(reviews, pd.get_dummies(data.Sentiment), test_size=0.2, random_state=9)
    ```

10.  创建和编译一个 LSTM 层的 Keras 模型。您可以试验不同的层和超参数:

    ```
    from keras.models import Model
    from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, LSTM
    inp = Input((100,))
    embedding_layer = Embedding(len(word_index),
                        16,
                        weights=[embedding_matrix],
                        input_length=100,
                        trainable=False)(inp)
    model = Dropout(0.10)(embedding_layer)
    model = LSTM(128, dropout=0.2)(model)
    model = Dense(units=256, activation='relu')(model)
    model = Dense(units=64, activation='relu')(model)
    model = Dropout(0.3)(model)
    predictions = Dense(units=2, activation='softmax')(model)
    model = Model(inputs = inp, outputs = predictions)
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])
    ```

11.  使用以下代码，根据 10 个时期的数据训练模型，以查看它是否比*活动 1* 中的模型执行得更好:

    ```
    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)
    ```

12.  Check the accuracy of the model:

    ```
    from sklearn.metrics import accuracy_score
    preds = model.predict(X_test)
    accuracy_score(np.argmax(preds, 1), np.argmax(y_test.values, 1))
    ```

    LSTM 模型的精确度是:

    ![Figure 7.36: LSTM model accuracy
    ](image/C13322_07_36.jpg)

    ###### 图 7.36: LSTM 模型的准确性

13.  绘制模型的混淆矩阵，以正确理解模型的预测:

    ```
    y_actual = pd.Series(np.argmax(y_test.values, axis=1), name='Actual')
    y_pred = pd.Series(np.argmax(preds, axis=1), name='Predicted')
    pd.crosstab(y_actual, y_pred, margins=True)
    ```

    ![Figure 7.37: Confusion matrix of the model (0 = negative sentiment, 1 = positive sentiment)
    ](image/C13322_07_37.jpg)

    ###### 图 7.37:模型的混淆矩阵(0 =负面情绪,1 =正面情绪)

14.  Check the performance of the model by seeing the sentiment predictions on random reviews using the following code:

    ```
    review_num = 110
    print("Review: \n"+tokenizer.sequences_to_texts([X_test[review_num]])[0])
    sentiment = "Positive" if np.argmax(preds[review_num]) else "Negative"
    print("\nPredicted sentiment = "+ sentiment)
    sentiment = "Positive" if np.argmax(y_test.values[review_num]) else "Negative"
    print("\nActual sentiment = "+ sentiment)
    ```

    输出如下所示:

![Figure 7.38: A negative review from the IMDB dataset
](image/C13322_07_38.jpg)

###### 图 7.38:来自 IMDB 数据集的负面评论

恭喜你！您刚刚实现了一个 RNN 来预测电影评论的观点。这个网络比我们之前创建的网络要好得多。调整网络的架构和超参数，以提高模型的准确性。您还可以尝试使用来自 fastText 或 GloVe 的预训练单词嵌入来提高模型的准确性。

### 活动 20:从推文中预测情绪

在这项活动中，我们将尝试预测一条推文的情绪。提供的数据集(https://github . com/TrainingByPackt/Data-Science-with-Python/tree/master/chapter 07)包含了 150 万条推文及其情绪(正面或负面)。让我们看看下面的场景:你在一家大型消费者组织工作，该组织最近创建了一个 Twitter 帐户。一些与贵公司有过不愉快经历的客户正在 Twitter 上表达他们的情绪，这导致了公司声誉的下降。你的任务是识别这些推文，以便公司可以与他们取得联系，提供更好的支持。你可以通过创建情绪预测器来做到这一点，它可以确定一条推文的情绪是积极的还是消极的。在对关于贵公司的实际 tweets 使用新的情绪预测器之前，您将在提供的 tweets 数据集上测试它。

1.  读取数据并删除所有不必要的信息。
2.  对推文进行清洗，标记化，最后转换成数字。
3.  加载 GloVe Twitter 嵌入并创建嵌入矩阵(https://nlp.stanford.edu/projects/glove/)。
4.  Create an LSTM model to predict the sentiment.

    #### 注意

    这项活动的解决方案可以在第 383 页找到。

恭喜你！你刚刚创建了一个机器学习模块来预测推文中的情绪。现在，您可以使用 Twitter API 部署它，对推文执行实时情感分析。你可以从 GloVe 和 fastText 中尝试不同的嵌入，看看你能在你的模型上得到多少改进。

## 总结

在这一章中，我们学习了计算机如何理解人类语言。我们首先了解了什么是正则表达式，以及它如何帮助数据科学家分析和清理文本数据。接下来，我们学习了停用词，它们是什么，以及为什么从数据中删除它们以减少维数。接下来，我们学习了句子标记化及其重要性，然后是单词嵌入。嵌入是我们在*第 5 章* : *掌握结构化数据*中涉及的一个主题；在这里，我们学习了如何创建单词嵌入来提高 NLP 模型的性能。为了创建更好的模型，我们研究了 RNNs，这是一种特殊类型的神经网络，可以保留过去输入的记忆。最后，我们了解了 LSTM 细胞以及它们如何优于正常的 RNN 细胞。

现在您已经完成了本章，您能够处理文本数据并为 NLP 创建机器学习模型。在下一章，你将学习如何使用迁移学习和一些技巧更快地制作模型。