

# 无监督机器学习

最后，我们到了—我们现在要做一些真正的数据科学。在最后两章中，我将介绍一些最流行的高级数据挖掘和机器学习算法。我将向您展示如何使用它们从您的数据中获得深入的知识。

最常见的算法分离是分成两组:无监督的或无指导的，以及有监督的或有指导的算法。无监督的没有目标变量。你只是试图在你的数据中找到一些有趣的模式，例如，一些独特的案例组。然后你需要分析结果，使解释成为可能。谈论案例组或集群时，你事先并不知道这些集群的标签。一旦确定了它们，就需要检查分类中输入变量的特征，以便理解分类的意义。

在开始学习高级算法之前，我将绕个弯。我将向您展示如何在服务器端为 ML 服务(数据库内)安装额外的 R 和 Python 包。

本章包括以下内容:

*   安装 ML 服务(数据库内)包
*   执行市场篮子分析
*   寻找相似案例的群集
*   主成分分析降维
*   从变量中提取潜在因素



# 安装 ML 服务(数据库内)包

出于安全考虑，您不能只从服务器端的`sys.sp_exacute_external_script`系统过程中调用`install.packages()` R 函数。还有许多其他方法可以做到这一点。您可以在文章*在 SQL Server 上安装新的 R 包*中找到安装 R 包选项的完整列表，该文章位于[https://docs . Microsoft . com/en-us/SQL/advanced-analytics/R/Install-additional-R-packages-on-SQL-Server？view=sql-server-2017](https://docs.microsoft.com/en-us/sql/advanced-analytics/r/install-additional-r-packages-on-sql-server?view=sql-server-2017) 。我将在这里展示一个选项，我在写这本书时使用的选项。我在虚拟机上安装了 SQL Server，我可以为该机器启用 web 连接。那么安装一个额外的 R 包的过程就简单了。您只需要从 ML Services (In-Database)安装中运行 R 控制台`R.exe`，它位于默认实例安装的`C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\R_SERVICES\bin`文件夹中。你需要以管理员身份运行`R.exe`:

![](Images/25df55d9-adc2-49f6-a2c3-6161c08f1185.png)

使用管理权限运行 R.exe

在开始安装软件包之前，我用下面的 T-SQL 代码检查了已安装的软件包:

```
USE AdventureWorksDW2017;
EXECUTE sys.sp_execute_external_script
@language=N'R',
@script =
N'str(OutputDataSet);
instpack <- installed.packages();
NameOnly <- instpack[,1];
OutputDataSet <- as.data.frame(NameOnly);'
WITH RESULT SETS (
 ( PackageName nvarchar(20) ) 
);
GO
```

最初，我安装了 57 个包。然后我使用了`R.exe`中的`install.packages("dplyr")`命令来安装`dplyr`库。安装完成后，我用`q()`功能关闭了`R.exe`控制台。然后，我使用前面的 T-SQL 代码重新检查已安装的包。这次的数字是 65，`dplyr`套餐也在其中。

对于 Python，最简单的安装是从 Visual Studio 2017 开始。当然，你也可以在文档中找到所有可能的选项，在[https://docs . Microsoft . com/en-us/SQL/advanced-analytics/Python/Install-additional-Python-packages-on-SQL-Server？view=sql-server-2017](https://docs.microsoft.com/en-us/sql/advanced-analytics/python/install-additional-python-packages-on-sql-server?view=sql-server-2017) 。在 VS 2017 中，你只需要打开你要修改的 Python 安装的 Python Environments 窗口(默认在屏幕右侧)，在下拉列表中选择 Packages 选项，然后在下拉列表下方的文本框中写下你要安装的包的名称。VS 2017 为你创建 Python 安装命令，`pip install`命令。

你可以在下面的截图中看到这个过程，我正在安装`cntk`包，这是用于 Python 的微软认知工具包包:

![](Images/b618f58a-8d19-4248-81ea-94acba881eb4.png)

安装 Python 包

同样，在运行安装命令之前，我检查了用 T-SQL 安装的 Python 包的数量:

```
EXECUTE sys.sp_execute_external_script 
 @language = N'Python', 
 @script = N'
import pip
import pandas as pd
instpack = pip.get_installed_distributions()
instpacksort = sorted(["%s==%s" % (i.key, i.version)
 for i in instpack])
dfPackages = pd.DataFrame(instpacksort)
OutputDataSet = dfPackages'
WITH RESULT SETS (
 ( PackageNameVersion nvarchar (150) )
);
GO
```

我最初安装了 128 个包，运行`pip install`命令后安装了 129 个。



# 执行市场篮子分析

市场购物篮分析的最简单的实现方式是，在同一个购物篮中找出哪些产品倾向于一起购买。购物篮可以是物理的，如零售店中的购物篮，也可以是虚拟的，如包含一个或多个商品的单个 Web 订单。在`AdventureWorksDW2017`演示数据库中，有一个包含 web 购买交易的`dbo.vAssocSeqLineItems`视图，我使用以下查询来检查其内容:

```
SELECT TOP 3 *
FROM dbo.vAssocSeqLineItems;
```

结果是这样的:

```
OrderNumber LineNumber Model
----------- ---------- ------------
SO61313     1          Road-350-W
SO61313     2          Cycling Cap
SO61313     3          Sport-100
```

`OrderNumber`列定义购物篮，`Model`列标识购物篮中的单个产品。注意，我没有在查询中包含`ORDER BY`子句；因此，您可能会得到与我不同的三行。

第一个分析是单个篮子中的单个**项目**和项目组的计数，或者说是**项目集**。找到流行项目集后，您可以表达**关联规则**，例如*如果客户购买了一辆公路自行车，他们也会购买一顶自行车帽*。一个项集的计数被称为规则的**支持度**。您可以定义一些其他度量:

*   **置信度**:组合的支持度除以规则的条件支持度。例如，*如果客户购买公路自行车，则他们将购买自行车限额*规则可以为项目集公路自行车、自行车限额提供 80 的支持，为项目公路自行车提供 100 的条件支持。这给了我们 80%的信心。

*   **期望置信度**:规则中对结果的支持度除以交易总数。
*   **Lift** :置信度与期望置信度相除的结果。Lift 给出了一个因子，当项目独立时，某个项目集的概率超过同一项目集的概率。

您可以通过一个非常简单的查询来计算单个项目的支持度:

```
SELECT Model, COUNT(*) AS Support
FROM dbo.vAssocSeqLineItems
GROUP BY Model
ORDER BY Support DESC;
```

该查询还根据项目的受欢迎程度对结果进行排序。以下是最受欢迎的商品:

```
Model        Support
------------ -------
Sport-100    3782
Water Bottle 2489
Patch kit    1830
```

对于两个项目的项目集，可以使用`CROSS APPLY`运算符来查找组合:

```
WITH Pairs_CTE AS
(
SELECT t1.OrderNumber,
 t1.Model AS Model1, 
 t2.Model2
FROM dbo.vAssocSeqLineItems AS t1
 CROSS APPLY 
 (SELECT Model AS Model2
 FROM dbo.vAssocSeqLineItems
 WHERE OrderNumber = t1.OrderNumber
 AND Model > t1.Model) AS t2
)
SELECT Model1, Model2, COUNT(*) AS Support
FROM Pairs_CTE
GROUP BY Model1, Model2
ORDER BY Support DESC;
```

以下是三对最受欢迎的单品:

```
Model1               Model2       Support
-------------------- ------------ -------
Mountain Bottle Cage Water Bottle 993
Road Bottle Cage     Water Bottle 892
Mountain Tire Tube   Sport-100    747
```

对于包含三个项目的项目集，添加另一个`CROSS APPLY`:

```
WITH Pairs_CTE AS
(
SELECT t1.OrderNumber,
 t1.Model AS Model1, 
 t2.Model2
FROM dbo.vAssocSeqLineItems AS t1
 CROSS APPLY 
 (SELECT Model AS Model2
 FROM dbo.vAssocSeqLineItems
 WHERE OrderNumber = t1.OrderNumber
 AND Model > t1.Model) AS t2
),
Triples_CTE AS
(
SELECT t2.OrderNumber,
 t2.Model1, 
 t2.Model2,
 t3.Model3
FROM Pairs_CTE AS t2
 CROSS APPLY 
 (SELECT Model AS Model3
 FROM dbo.vAssocSeqLineItems
 WHERE OrderNumber = t2.OrderNumber
 AND Model > t2.Model1
 AND Model > t2.Model2) AS t3
)
SELECT Model1, Model2, Model3, COUNT(*) AS Support
FROM Triples_CTE
GROUP BY Model1, Model2, Model3
ORDER BY Support DESC;
```

以下是排名前三的结果:

```
Model1               Model2       Model3       Support
-------------------- ------------ ------------ -------
Mountain Bottle Cage Mountain-200 Water Bottle 343
Mountain Bottle Cage Sport-100    Water Bottle 281
Road Bottle Cage     Road-750     Water Bottle 278
```

您会注意到查询变得越来越复杂。是时候切换到 r 了，你可以在`arules`包里找到**关联规则**算法。如果需要，以下代码会安装包，然后加载必要的库并从 SQL Server 读取数据:

```
# install.packages("arules")
library(arules)
library(RODBC)
con <- odbcConnect("AWDW", uid = "RUser", pwd = "Pa$$w0rd")
df_AR <- as.data.frame(sqlQuery(con,
 "SELECT OrderNumber, Model
 FROM dbo.vAssocSeqLineItems
 ORDER BY OrderNumber, Model;"
), stringsAsFactors = FALSE)
close(con)
```

下一步是定义交易和项目。我还检查了同一个代码块中的事务:

```
# Defining transactions 
trans <- as(split(df_AR[, "Model"],
 df_AR[, "OrderNumber"]),
 "transactions")
# Transactions info
inspect(trans[6:8])
```

这里有三个交易:

```
    items                                         transactionID
[1] {Fender Set - Mountain,Sport-100}             SO61318 
[2] {LL Road Tire,Patch kit,Road Tire Tube}       SO61319 
[3] {Mountain Bottle Cage,Sport-100,Water Bottle} SO61320 
```

现在我可以使用`apriori()`函数来提取规则。我在函数中使用一些参数来只提取具有两个或更多项目的项目集的规则，并且只提取超过最小支持度和最小置信度的规则:

```
AR <- apriori(trans,
 parameter = list
 (minlen = 2,
 supp = 0.03,
 conf = 0.05,
 target = "rules"))
inspect(AR, ruleSep = "---->", itemSep = " + ")
```

以下是前三条规则的输出:

```
    lhs                        rhs                  support     confidence lift 
[1] {ML Mountain Tire}   ----> {Mountain Tire Tube} 0.03321544  0.6565350   4.8079356
[2] {Mountain Tire Tube} ----> {ML Mountain Tire}   0.03321544  0.2432432   4.8079356
[3] {Touring Tire}       ----> {Touring Tire Tube}  0.03890512  0.8709122  12.6559602
```

您可以根据任何术语、支持、信心或提升对输出进行排序，以找到您的业务最感兴趣的规则。对于`arulesViz`包，关联规则也有一个非常好的图形化表示，如下面的代码所示:

```
# install.packages("arulesViz")
library(arulesViz)
# Rules graph
plot(AR, method = "graph", control = list(type = "items"))
```

以下屏幕截图显示了规则图:

![](Images/46779fa3-482f-45ac-9953-5e2588d347b5.png)

关联规则图

当然，也可以在 Python 中找到关联规则。然而，为了简洁起见，我只向您介绍一个非常有用的库，名为`Orange`，特别是位于[https://orange . readthe docs . io/en/latest/reference/rst/orange . associate . html](https://orange.readthedocs.io/en/latest/reference/rst/Orange.associate.html)的关联规则模块。不用担心；后面会有更多的 Python 代码。



# 寻找相似案例的群集

使用**聚类分析**，您可以根据输入变量的**相似度**来尝试找到特定的案例组。这些组或集群帮助您了解您的案例，例如，您的客户或员工。聚类过程根据变量的值对数据进行分组，因此一个聚类中的案例具有很高的相似性；然而，这些病例与其他组群中的病例非常不同。相似性可以用不同的方法来衡量。几何距离是相似性度量的一个例子。您定义了一个 n 维超空间，其中每个输入变量定义了一个维度或一个轴。变量值定义了这个超空间中的点；当然，这几点是事实。现在，您可以测量每个案例与所有其他案例之间的几何距离。

有许多不同的聚类算法。最流行的是 **K-means** 算法。使用该算法，您可以预先定义 K 个聚类的数量。该算法试图找到 K 个组的 K 个平均位置，或**质心**；因此，你可以看到这个名字的由来。K-means 算法将每个案例准确地分配给一个聚类，即最近的质心。

我将使用微软`RevoScaleR`包中的可伸缩`rxKmeans()`函数进行聚类分析。一如既往，让我们从读取数据开始:

```
con <- odbcConnect("AWDW", uid = "RUser", pwd = "Pa$$w0rd")
TM <-
 sqlQuery(con,
 "SELECT CustomerKey, CommuteDistance,
 TotalChildren, NumberChildrenAtHome, 
 Gender, HouseOwnerFlag,
 NumberCarsOwned, MaritalStatus,
 Age, BikeBuyer, Region,
 YearlyIncome AS Income,
 EnglishEducation AS Education,
 EnglishOccupation AS Occupation
 FROM dbo.vTargetMail")
close(con)
```

`rxKmeans()`函数只接受数字数据。我还想把教育变量作为输入变量。我将从它创建一个额外的整数变量，其中数字顺序将遵循这个因子的级别顺序。因此，我必须正确定义顺序:

```
# Order Education
TM$Education = factor(TM$Education, order = TRUE,
 levels = c("Partial High School",
 "High School", "Partial College",
 "Bachelors", "Graduate Degree"))
# Create integer Education
TM$EducationInt = as.integer(TM$Education)
```

现在我可以加载`RevoScaleR`库并训练模型:

```
library(RevoScaleR)
ThreeClust <- rxKmeans(formula = ~NumberCarsOwned + Income + Age +
 NumberChildrenAtHome + BikeBuyer + EducationInt,
 data = TM, numClusters = 3)
```

我得到了集群；然而，在这个时候，我不知道他们是什么意思。为了分析集群，我将集群添加到原始数据框中，作为名为`ClusterID`的第 16 列。该列的值可以是 1、2 或 3:

```
TMClust <- cbind(TM, ThreeClust$cluster)
names(TMClust)[16] <- "ClusterID"
```

现在，我将做一个图形分析，以便了解这些集群代表哪种客户。下一部分代码相当长，尽管并不太复杂。我将 TM 数据框附加到搜索路径，所以我不需要用数据框名称前缀写所有变量。然后我把标绘区域分成一个 2×2 的矩阵；我将在一个区域显示四个图表。然后我用四个不同的图分析了四个不同变量在集群中的分布。最后，我恢复了 1 x 1 绘图区域，并将数据框从搜索路径中分离出来:

```
# Attach the new data frame
attach(TMClust);
# Saving parameters
oldpar <- par(no.readonly = TRUE);
# Defining a 2x2 graph
par(mfrow = c(2, 2));
# Income and clusters
boxplot(Income ~ ClusterID,
 main = "Yearly Income in Clusters",
 notch = TRUE,
 varwidth = TRUE,
 col = "orange",
 ylab = "Yearly Income",
 xlab = "Cluster Id")
# BikeBuyer and clusters
nc <- table(BikeBuyer, ClusterID)
barplot(nc,
 main = 'Bike buyer and cluster ID',
 xlab = 'Cluster Id', ylab = 'BikeBuyer',
 legend = rownames(nc),
 col = c("blue", "yellow"),
 beside = TRUE)
# Education and clusters
nc <- table(Education, ClusterID)
barplot(nc,
 main = 'Education and cluster ID',
 xlab = 'Cluster Id', ylab = 'Total Children',
 col = c("black", "blue", "green", "red", "yellow"),
 beside = TRUE)
legend("topright", rownames(nc), cex = 0.6,
 fill = c("black", "blue", "green", "red", "yellow"))
# Age and clusters
boxplot(Age ~ ClusterID,
 main = "Age in Clusters",
 notch = TRUE,
 varwidth = TRUE,
 col = "Green",
 ylab = "Yearly Income",
 xlab = "Cluster Id")
# Clean up
par(oldpar)
detach(TMClust)
```

您可以在下面的屏幕截图中看到代码的结果:

![](Images/5c0f24e2-2968-402e-9104-37a8032b6d6a.png)

聚类分析

从图表中可以看出，在第一组中，有一些年轻人，他们受教育程度较低，收入相当低。在第二组中，收入很高的老年人比其他两组稍多一些。在第三组中，收入相当高的中年专业人士占优势。

用 T-SQL 编写完整的算法是没有效率的。不过，我可以用 R 代码。我将首先为结果准备一个临时表:

```
CREATE TABLE #tmp
 (CustomerKey INT NOT NULL,
 NumberChildrenAtHome INT NOT NULL,
 NumberCarsOwned INT NOT NULL, 
 Age INT NOT NULL,
 BikeBuyer INT NOT NULL,
 Income INT NOT NULL,
 ClusterID INT NOT NULL);
GO
```

下一步是在一个`INSERT`语句中调用`sys.sp_execute_external_script`过程。该过程执行 R 脚本，为我查找集群，并返回一个包含原始数据和集群成员的数据框:

```
INSERT INTO #tmp
EXECUTE sys.sp_execute_external_script
 @language = N'R'
 ,@script = N'
 library(RevoScaleR)
 ThreeClust <- rxKmeans(formula = ~NumberCarsOwned + Income + Age +
 NumberChildrenAtHome + BikeBuyer,
 data = TM, numClusters = 3)
 TMClust <- cbind(TM, ThreeClust$cluster);
 names(TMClust)[7] <- "ClusterID";
 '
 ,@input_data_1 = N'
 SELECT CustomerKey, NumberChildrenAtHome, 
 NumberCarsOwned, Age, BikeBuyer, 
 YearlyIncome AS Income
 FROM dbo.vTargetMail;'
 ,@input_data_1_name = N'TM'
 ,@output_data_1_name = N'TMClust';
GO
```

现在我可以用 T-SQL 分析集群了:

```
SELECT ClusterID, AVG(1.0*Income) AS AvgIncome
FROM #tmp
GROUP BY ClusterID
ORDER BY ClusterID;
```

结果如下:

```
ClusterID AvgIncome
--------- -------------
1          68270.108043
2         120641.492265
3          28328.305681
```

请注意，这一次集群 3 的收入最低。因为事先不知道集群的标签，所以每次执行代码时，您都可以获得不同的标签。

同样，我在这一节跳过 Python。然而，在下一节中，我将从 Python 开始，并展示一个将使用找到的主成分的聚类算法。



# 主成分和因子分析

**主成分分析** ( **PCA** )是一种众所周知的无向方法，用于减少进一步分析中使用的变量数量。这也被称为**降维**。在一些项目中，您可能有数百甚至数千个输入变量。将它们全部用作聚类算法的 can 输入会导致训练模型需要大量时间。然而，许多可能一起变化的输入变量可能有某种关联。

PCA 再次从超空间开始，其中每个输入变量定义一个轴。PCA 搜索一组新的轴，一组新的变量，它们应该是线性不相关的，称为主分量。主成分的计算方式是，第一个包含整个输入变量集的最大可能可变性，第二个包含第二大可变性，依此类推。主成分的计算来源于线性代数。主成分实际上是输入变量协方差矩阵的特征向量。这些特征向量的特征值定义了主分量的阶；第一个特征值最大，第二个特征值次之，依此类推。找到主要成分后，可以只保留前几个成分进行进一步分析。当然，你失去了一些可变性。然而，通过选择前几个主成分，损失被最小化，同时维数被大大降低。

正如我所承诺的，我将在这一章中最终使用 Python。首先，我需要导入必要的库并读取数据:

```
# Imports
import numpy as np
import pandas as pd
import pyodbc
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
# Reading the data from SQL Server
con = pyodbc.connect('DSN=AWDW;UID=RUser;PWD=Pa$$w0rd')
query = """SELECT CustomerKey, CommuteDistance,
 TotalChildren, NumberChildrenAtHome, 
 Gender, HouseOwnerFlag,
 NumberCarsOwned, MaritalStatus,
 Age, BikeBuyer, Region,
 YearlyIncome AS Income,
 EnglishEducation AS Education,
 EnglishOccupation AS Occupation
 FROM dbo.vTargetMail"""
TM = pd.read_sql(query, con)
```

下一步，我必须从`sklearn`库中导入`PCA()`函数，并用输入变量或特征矩阵定义数据框:

```
# Import PCA 
from sklearn.decomposition import PCA
# Feature matrix
X = TM[["TotalChildren", "HouseOwnerFlag", "Age", "Income",
 "NumberChildrenAtHome", "NumberCarsOwned", "BikeBuyer"]]
```

在下一步中，我定义模型并训练或拟合它。我还调用了`transform()`方法来获取每种情况下两个主成分值的二维数组:

```
model = PCA(n_components = 2)
model.fit(X)
X_2D = model.transform(X)
```

在下一步中，我将主要成分分配给原始数据框:

```
TM['PCA1'] = X_2D[:, 0]
TM['PCA2'] = X_2D[:, 1]
```

现在我可以试着理解主成分是如何与原始变量联系在一起的。例如，我正在使用 seaborn `lmplot()`函数来绘制一个散点图，该散点图带有由两个主要成分定义的平面中的`NumberCarsOwned`变量分布的回归线:

```
ax = sns.lmplot('PCA1', 'PCA2', hue = 'NumberCarsOwned',
 data = TM, fit_reg = True, legend_out = False,
 palette = ("green", "red", "blue", "yellow", "black"), 
 x_bins = 15, scatter_kws={"s": 100})
ax.set_xlabels(fontsize=16)
ax.set_ylabels(fontsize=16)
ax.set_xticklabels(fontsize=12)
ax.set_yticklabels(fontsize=12)
plt.show()
```

你可以在下面的截图中看到剧情:

![](Images/818ef616-1eea-4a6c-ba92-665a6fbaa554.png)

分析主成分

我提到主成分分析的主要目的是在进一步分析中使用前几个主成分。我将使用两个主成分而不是原始变量来再次找到三个聚类，这一次使用 Python 和使用**高斯混合模型** ( **GMM** )算法。该算法还定义了 K 个质心；但是，它通过用每个质心周围的高斯曲线覆盖案例，以某种概率将每个案例分配给每个质心。这也称为软聚类，以区别于 K-means，后者也称为硬聚类。我正在使用 sklearn 库中的`GaussianMixture()`函数。我还使用了`predict()`方法来获得集群成员向量:

```
# GMM clustering
from sklearn.mixture import GaussianMixture
# Define and train the model
X = TM[["PCA1", "PCA2"]]
model = GaussianMixture(n_components = 3, covariance_type = 'full')
model.fit(X)
# Get the clusters vector
y_gmm = model.predict(X)
```

现在，我可以将聚类成员向量添加到原始数据框中，并检查前几种情况:

```
TM['cluster'] = y_gmm
TM.head()
```

在分析其他变量在集群中的分布之前，我将对`CommuteDistance`和职业变量进行适当排序:

```
# Define CommuteDistance as ordinal
TM['CommuteDistance'] = TM['CommuteDistance'].astype('category')
TM['CommuteDistance'].cat.reorder_categories(
 ["0-1 Miles", 
 "1-2 Miles","2-5 Miles", 
 "5-10 Miles", "10+ Miles"], inplace=True)
# Define Occupation as ordinal
TM['Occupation'] = TM['Occupation'].astype('category')
TM['Occupation'].cat.reorder_categories(
 ["Manual", 
 "Clerical","Skilled Manual", 
 "Professional", "Management"], inplace=True)
```

现在，我可以为通勤距离的聚类分布创建一个漂亮的图表:

```
sns.countplot(x="CommuteDistance", hue="cluster", data=TM);
plt.show()
```

这是图表:

![](Images/ce2e7f62-1155-48d7-bd51-c0d574cf5acf.png)

集群通勤距离分布

我继续分析职业变量:

```
sns.countplot(x="Occupation", hue="cluster", data=TM);
plt.show()
```

这是职业图表:

![](Images/ef4f25d7-ac9b-487e-ac09-7345736ce5a2.png)

集群中的职业分布

您可以清楚地看到两个离散变量在不同集群中的分布差异。因为现在我在单个数据框中拥有了所有变量，包括主成分和聚类成员，所以我可以执行任何我想执行的分析。

通常，你不需要理解你提取的主要成分。然而，您可能希望找到变量的底层结构，就像您可以为具有某种聚类的情况找到它一样。当你想找到输入变量的潜在结构时，这就叫做**因子分析** ( **FA** ，或者有时候叫做**探索性因子分析** ( **EFA** )。提取的第一个主成分，即**因子**，用于解释输入变量的关系。

你可以通过在多维超空间中旋转新的轴，主成分，来更好地理解潜在的变量结构。使用**旋转**，您可以最大化主成分和输入变量的不同子集之间的相关性。因此，一个主成分变得与输入变量的一个子集更相关，而另一个主成分与另一个子集相关。

从这些关联中，可以看出前几个因子代表什么输入变量。旋转可以是**正交**(最广为人知的一种称为 **varimax** )，保持轴或因子之间的正交性。旋转也可以是**倾斜**(一个非常著名的是 **promax** )，它将因子与特定子集更加匹配，但也引入了一些因子之间的关联。

我将切换回 r。我将使用 psych 包——如果需要的话安装它并加载它。在下面的代码中，我也将只提取下面代码中的数值变量:

```
# install.packages("psych")
library(psych)
# Extracting numerical data only
TMFA <- TM[, c("TotalChildren", "NumberChildrenAtHome",
 "HouseOwnerFlag", "NumberCarsOwned",
 "BikeBuyer", "Income", "Age")]
```

下面是使用正交旋转提取两个因子的代码。代码随后显示结果:

```
efaTM_varimax <- fa(TMFA, nfactors = 2, rotate = "varimax")
efaTM_varimax
```

我在这里展示的只是一小部分结果。这部分显示了输入变量和两个因素之间的相关性:

```
                      MR1   MR2
TotalChildren         0.36  0.75
NumberChildrenAtHome  0.77  0.18
HouseOwnerFlag        0.07  0.19
NumberCarsOwned       0.55  0.17
BikeBuyer            -0.07 -0.15
Income                0.59  0.09
Age                  -0.05  0.68
```

现在让我重复因子分析，这次是斜向旋转:

```
efaTM_promax <- fa(TMFA, nfactors = 2, rotate = "promax")
efaTM_promax
```

部分结果如下:

```
                      MR1   MR2
TotalChildren         0.23  0.72
NumberChildrenAtHome  0.77  0.04
HouseOwnerFlag        0.03  0.19
NumberCarsOwned       0.55  0.07
BikeBuyer            -0.05 -0.14
Income                0.60 -0.02
Age                  -0.19  0.72
```

你可以看到，在斜旋转的情况下，输入变量是如何与两个因子联系起来的就更加清楚了。借助于`fa.diagram()`函数，您可以看到这些相关性，包括两个因素之间的相关性。我在同一张图中展示了两种旋转的示意图:

```
par(mfrow = c(1, 2))
fa.diagram(efaTM_varimax, simple = FALSE,
 main = "EFA Varimax", cex = 1.2,
 e.size = .07, rsize = .12)
fa.diagram(efaTM_promax, simple = FALSE,
 main = "EFA Promax", cex = 1.3,
 e.size = .07, rsize = .12)
par(oldpar)
```

下面是前面代码的结果:

![](Images/6680846d-153e-4c10-be93-0b1c74c5dfb4.png)

因素和输入变量之间的相关性

从这个图中，你可以清楚地看到哪些输入变量用哪些因子来表示。注意，在 varimax 旋转中，`TotalChildren`变量与两个因子相关，而在 promax 旋转中，与每个因子相关的变量不重叠。然而，这两个因素之间存在关联。



# 摘要

本章介绍了最流行的无监督数据挖掘和机器学习方法，包括关联规则、不同类型的聚类、主成分分析和因子分析。此外，您还学习了如何将 R 或 Python 包添加到 ML 服务中(数据库内)。经过前三章的热身，工作和分析的复杂性增加了。我将监督方法留到了本书的最后一章，这可能是最令人兴奋的部分——预测分析。