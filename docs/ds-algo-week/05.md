

# 五、聚类成 K 个簇

聚类是一种将数据划分为簇的技术，在同一个簇中具有相同的特征。

在本章中，我们将讨论以下主题:

*   如何使用 k 均值聚类算法，以家庭收入为例
*   如何使用性别分类的示例，通过首先将特征与已知类别一起聚类来对特征进行分类
*   在 k-means 聚类算法的*实现部分，如何用 Python 实现 *k* -means 聚类算法*
*   房屋所有权的示例以及如何为您的分析选择适当数量的聚类
*   如何以房屋所有权为例，通过聚类算法适当地缩放一组给定的数值数据，以提高分类的准确性
*   以文档聚类为例，理解不同数量的聚类如何改变这些聚类之间分界线的含义



# 家庭收入——聚集成 k 个集群

例如，让我们看看年收入为 40，000 美元、55，000 美元、70，000 美元、100，000 美元、115，000 美元、130，000 美元和 135，000 美元的家庭。然后，如果我们将这些家庭组合成两个集群，以他们的收入作为相似性的度量，第一个集群将包括收入为 40 k、55 k 和 70 k 的家庭，而第二个集群将包括收入为 100 k、115 k、130 k 和 135 k 的家庭。

这是因为 40k 和 135k 彼此相距最远，因此，因为我们希望有两个群集，所以它们必须在不同的群集中。55 K 离 40 k 比离 135 k 更近，所以 40 k 和 55 k 会在同一个星团里。同样，130 k 和 135 k 会在同一个集群中。70 K 比 130 K 和 135 k 更接近 40 k 和 55 k，所以 70 k 应该在有 40 k 和 55 k 的簇中，115 K 比包含 40 k、55 k 和 70 k 的第一簇更接近 130 k 和 135 k，所以它会在第二簇中。最后，100 k 更靠近第二个集群，包含 115 k、130 k 和 135 k，因此它将位于那里。因此，第一个群集将包括 40 k、55 k 和 70 户。第二组将包括 100 k、115 k、130 k 和 135 k 家庭。

对具有相似属性的组的特征进行聚类并将聚类分配给一个特征是一种分类形式。由数据科学家来解释聚类的结果及其导致的分类。这里，包含年收入为 4 万美元、5 万 5 千美元和 7 万美元的家庭的群集代表一类低收入家庭。第二类包括年收入 10 万美元、11.5 万美元、13 万美元和 13.5 万美元的家庭，代表一类高收入家庭。

我们根据直觉和常识，以非正式的方式将这些家庭分成两类。有一些聚类算法根据精确的规则对数据进行聚类。这些算法包括模糊 c 均值聚类算法、层次聚类算法、高斯(EM)聚类算法、质量阈值聚类算法和*k*-均值聚类算法，这是本章的重点。



# k-均值聚类算法

*k*-均值聚类算法将给定点分类成 *k* 组，使得同一组成员之间的距离最小化。

*k*-均值聚类算法确定初始的*k*-质心(聚类中心的点)——每个聚类一个。然后，将每个特征分类到质心最接近该特征的聚类中。在对所有特征进行分类后，我们形成了一个初始的 *k* 聚类。

对于每个聚类，我们将质心重新计算为该聚类中点的平均值。在我们移动了质心之后，我们再次重新计算类。类别中的功能可能会改变。在这种情况下，我们必须再次重新计算质心。如果质心不再移动，那么*k*-意味着聚类算法终止。



# 拾取初始 k 形质心

我们可以选择初始的*k*-质心作为要分类的数据中的任何 *k* 特征。但是，理想情况下，我们希望从一开始就选择属于不同集群的点。因此，我们可能希望以某种方式最大化它们的相互距离为目标。为了简化这个过程，我们可以从特征中选择任意一点作为第一个质心。第二个可能是离第一个最远的。第三个可能是离第一个和第二个最远的一个，依此类推。



# 计算给定聚类的质心

聚类的质心就是该聚类中的点的平均值。如果一个群集包含坐标为 *x [1] ，x [2] ，…，x [n]* 的一维点，那么该群集的质心将是![](Images/a1e22e13-4d22-4526-a133-86bca036067f.png)。如果一个聚类包含坐标为 *(x [1] ，y [1] )、(x [2] ，y [2] )、…、(x [n] ，y [ n ] )* 的二维点，那么该聚类的质心的 *x* 坐标的值为 *(1/n)*(x...+x [ n ] )* ，并且*y*-坐标将具有值![](Images/afbc1a34-7864-4496-a187-513dc5e5d8f1.png)。

这种计算很容易推广到更高的维度。如果 *x* 坐标的高维特征值为 *x [1] ，x [2] ，…，x [n]* ，那么质心的 *x* 坐标的值为![](Images/afbc1a34-7864-4496-a187-513dc5e5d8f1.png)。



# 利用 k-means 聚类算法对家庭收入进行实例分析

我们将把*k*-聚类算法应用于家庭收入示例。一开始，我们有收入为 4 万美元、5.5 万美元、7 万美元、10 万美元、11.5 万美元、13 万美元和 13.5 万美元的家庭。

要选取的第一个质心可以是任何要素，例如$70，000。第二个质心应该是离第一个质心最远的特征，即 135 k，因为 135 k 减去 70 k 是 65 k，这是任何其他特征和 70 k 之间的最大差异。因此，70 k 是第一个聚类的质心，而 135 k 是第二个聚类的质心。

现在，通过取差，40 k、55 k、70 k 和 100 k 更接近 70 k 而不是 135 k，所以它们将在第一簇中，而 115 k、130 k 和 135 k 更接近 135 k 而不是 70 k，所以它们将在第二簇中。

在我们根据初始质心对特征进行分类之后，我们重新计算质心。第一个群集的质心如下:

![](Images/ee300cca-5ab2-44aa-ab19-2c686e619d9d.png)

第二个群集的质心如下:

![](Images/179e82f3-1388-4a6c-b804-02c8bd469ee9.png)

使用新的质心，我们对要素进行如下重分类:

*   包含质心 66.25 k 的第一个聚类将包含特征 40 k、55 k 和 70 k
*   包含质心 126.66 k 的第二个聚类将包含特征 100 k、115 k、130 k 和 135 k

我们注意到 100 k 特征从第一个聚类移动到第二个聚类，因为它现在比第一个聚类的质心(*距离|100 k-66.25 k|=33.75 k* )更接近第二个聚类的质心(*距离|100 k-126.66 k|=26.66 k* )。由于聚类中的特征已经改变，我们必须再次重新计算质心。

第一个群集的质心如下:

![](Images/e5779d6d-ccc3-4ba7-ada3-509f3df61c4c.png)

第二个群集的质心如下:

![](Images/28b0ff92-f3a2-4778-82cd-d6d8e878de27.png)

使用这些质心，我们将特征重新分类到聚类中。第一个质心 55 k 将包含特征 40 k、55 k 和 70 k。第二个质心 120 k 将包含特征 100 k、115 k、130 k 和 135 k。因此，它们的质心将保持不变。

因此，算法以两个聚类结束:第一个聚类具有特征 40 k、55 k 和 70 k，第二个聚类具有特征 100 k、115 k、130 k 和 135 k



# 性别分类–聚类分类

以下数据取自性别分类示例，*问题 6* ，第二章，*朴素贝叶斯*:

| **以厘米为单位的高度** | **以千克为单位的重量** | **头发长度** | **性别** |
| one hundred and eighty  | Seventy-five | 短的 | 男性的 |
| One hundred and seventy-four | Seventy-one | 短的 | 男性的 |
| One hundred and eighty-four | Eighty-three | 短的 | 男性的 |
| One hundred and sixty-eight | Sixty-three | 短的 | 男性的 |
| One hundred and seventy-eight | Seventy | 长的 | 男性的 |
| One hundred and seventy | Fifty-nine | 长的 | 女性的 |
| One hundred and sixty-four | Fifty-three | 短的 | 女性的 |
| One hundred and fifty-five | Forty-six | 长的 | 女性的 |
| One hundred and sixty-two | fifty-two | 长的 | 女性的 |
| One hundred and sixty-six | Fifty-five | 长的 | 女性的 |
| One hundred and seventy-two | Sixty | 长的 | ？ |

为了简化问题，我们将删除标题为**头发长度**的列。我们还将删除标题为**性别**的列，因为我们希望根据人们的身高和体重对他们进行分组。我们希望使用聚类来确定表中的第 11 个人更可能是男性还是女性:

| **以厘米为单位的高度** | **以千克为单位的重量** |
| one hundred and eighty  | Seventy-five |
| One hundred and seventy-four | Seventy-one |
| One hundred and eighty-four | Eighty-three |
| One hundred and sixty-eight | Sixty-three |
| One hundred and seventy-eight | Seventy |
| One hundred and seventy | Fifty-nine |
| One hundred and sixty-four | Fifty-three |
| One hundred and fifty-five | Forty-six |
| One hundred and sixty-two | fifty-two |
| One hundred and sixty-six | Fifty-five |
| One hundred and seventy-two | Sixty |



# 分析

我们可以对初始数据进行缩放，但为了简化，我们将在算法中使用未缩放的数据。我们将把现有数据分为两类，因为性别有两种可能性——男性或女性。然后，我们的目标是将一个身高 172 cm，体重 60 kg 的人归类为更有可能是一个男人，如果且仅如果在那个群中有更多的男人。聚类算法是一种非常有效的技术。因此，以这种方式分类非常快，尤其是在有大量要素要分类的情况下。

因此，让我们对我们拥有的数据应用*k*-均值聚类算法。首先，我们选择初始质心。例如，假设第一个质心是一个人，身高 180 cm，体重 75 kg，用向量表示为 *(180，75)* 。离 *(180，75)* 最远的点是 *(155，46)* 。这是第二个质心。

取欧氏距离最接近第一质心 *(180，75)* 的点是 *(180，75)**(174，71)**(184，83)**(168，63)【178，70】**(170，59)**(172，60 所以这些点会在第一个簇中。距离第二质心 *(155，46)* 较近的点有 *(155，46)* 、 *(164，53)* 、 *(162，52)* 、 *(166，55)* 。所以这些点会在第二个集群中。下图显示了这两个集群的当前情况:*

![](Images/2ca5c8e6-7dd7-4d76-83ac-ea0752aebe07.png)

图 5.1:根据身高和体重对人群进行聚类

让我们重新计算星团的质心。具有特征( *180，75)* 、*、【174，71】、*、【184，83】、、*、【168，63】、*、*、【178，70】、【170，59】、*、【172，60】、*的蓝色簇将具有质心*((180+174+171)**

具有特征 *(155，46)* 、*、【164，53】、*、【162，52】、*、【166，55】、*的红色星团将具有质心 *((155+164+162+166)/4、(46+53+52+55)/4)=(161.75，51.5)* 。**

使用新质心对点进行重分类时，点的类别不会改变。蓝色的星团会有 *(180，75)**【174，71】**【184，83】**【168，63】**【178，70】**【170，59】**【172，60】*。红色的簇会有 *(155，46)**(164，53)**(162，52)**【166，55】*。因此，聚类分析算法以聚类结束，如下图所示:

![](Images/736d91cc-3378-4953-9286-8034a22357e1.png)

图 5.2:根据身高和体重对人群进行聚类

现在我们要对实例 *(172，60)* 进行分类，看那个人是男是女。实例 *(172，60)* 在蓝色簇中，因此它与蓝色簇中的特征相似。蓝色星团中的其余特征更可能是男性还是女性？六个特征中有五个是男性，而只有一个是女性。因为大多数特征在蓝色聚类中是男性，并且人 *(172，60)* 也在蓝色聚类中，所以我们将身高 172 cm、体重 60 kg 的人分类为男性。



# k 均值聚类算法的实现

我们现在将实现*k*-均值聚类算法。它以一个 CSV 文件作为输入，每行一个数据项。数据项被转换成点。这些算法将这些点分类到指定数量的簇中。最后，使用`matplotlib`库将聚类可视化在图形上:

```py
# source_code/5/k-means_clustering.py
import math
import imp
import sys
import matplotlib.pyplot as plt
import matplotlib
import sys
sys.path.append('../common')
import common # noqa
matplotlib.style.use('ggplot')

# Returns k initial centroids for the given points.
def choose_init_centroids(points, k):
    centroids = []
    centroids.append(points[0])
    while len(centroids) < k:
        # Find the centroid that with the greatest possible distance
        # to the closest already chosen centroid.
        candidate = points[0]
        candidate_dist = min_dist(points[0], centroids)
        for point in points:
            dist = min_dist(point, centroids)
            if dist > candidate_dist:
                candidate = point
                candidate_dist = dist
        centroids.append(candidate)
    return centroids

# Returns the distance of a point from the closest point in points.
def min_dist(point, points):
    min_dist = euclidean_dist(point, points[0])
    for point2 in points:
        dist = euclidean_dist(point, point2)
        if dist < min_dist:
            min_dist = dist
    return min_dist

# Returns an Euclidean distance of two 2-dimensional points.
def euclidean_dist((x1, y1), (x2, y2)):
    return math.sqrt((x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2))

# PointGroup is a tuple that contains in the first coordinate a 2d point
# and in the second coordinate a group which a point is classified to.
def choose_centroids(point_groups, k):
    centroid_xs = [0] * k
    centroid_ys = [0] * k
    group_counts = [0] * k
    for ((x, y), group) in point_groups:
        centroid_xs[group] += x
        centroid_ys[group] += y
        group_counts[group] += 1
    centroids = []
    for group in range(0, k):
        centroids.append((
            float(centroid_xs[group]) / group_counts[group],
            float(centroid_ys[group]) / group_counts[group]))
    return centroids

# Returns the number of the centroid which is closest to the point.
# This number of the centroid is the number of the group where
# the point belongs to.
def closest_group(point, centroids):
    selected_group = 0
    selected_dist = euclidean_dist(point, centroids[0])
    for i in range(1, len(centroids)):
        dist = euclidean_dist(point, centroids[i])
        if dist < selected_dist:
            selected_group = i
            selected_dist = dist
    return selected_group

# Reassigns the groups to the points according to which centroid
# a point is closest to.
def assign_groups(point_groups, centroids):
    new_point_groups = []
    for (point, group) in point_groups:
        new_point_groups.append(
            (point, closest_group(point, centroids)))
    return new_point_groups

# Returns a list of pointgroups given a list of points.
def points_to_point_groups(points):
    point_groups = []
    for point in points:
        point_groups.append((point, 0))
    return point_groups

# Clusters points into the k groups adding every stage
# of the algorithm to the history which is returned.
def cluster_with_history(points, k):
    history = []
    centroids = choose_init_centroids(points, k)
    point_groups = points_to_point_groups(points)
    while True:
        point_groups = assign_groups(point_groups, centroids)
        history.append((point_groups, centroids))
        new_centroids = choose_centroids(point_groups, k)
        done = True
        for i in range(0, len(centroids)):
            if centroids[i] != new_centroids[i]:
                done = False
                break
        if done:
            return history
        centroids = new_centroids

# Program start
csv_file = sys.argv[1]
k = int(sys.argv[2])
everything = False
# The third argument sys.argv[3] represents the number of the step of the
# algorithm starting from 0 to be shown or "last" for displaying the last
# step and the number of the steps.
if sys.argv[3] == "last":
    everything = True
else:
    step = int(sys.argv[3])

data = common.csv_file_to_list(csv_file)
points = data_to_points(data)  # Represent every data item by a point.
history = cluster_with_history(points, k)
if everything:
    print "The total number of steps:", len(history)
    print "The history of the algorithm:"
    (point_groups, centroids) = history[len(history) - 1]
    # Print all the history.
    print_cluster_history(history)
    # But display the situation graphically at the last step only.
    draw(point_groups, centroids)
else:
    (point_groups, centroids) = history[step]
    print "Data for the step number", step, ":"
    print point_groups, centroids
    draw(point_groups, centroids)
```



# 性别分类的输入数据

我们将性别分类示例中的数据保存到 CSV 文件中，如下所示:

```py
# source_code/5/persons_by_height_and_weight.csv 180,75
174,71
184,83
168,63
178,70
170,59
164,53
155,46
162,52
166,55
172,60
```



# 性别分类数据的程序输出

我们运行程序，对来自性别分类示例的数据实现了 *k* -means 聚类算法。数字参数`2`意味着我们希望将数据分成两个簇，如下面的代码块所示:

```py
$ python k-means_clustering.py persons_by_height_weight.csv 2 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((180.0, 75.0), 0), ((174.0, 71.0), 0), ((184.0, 83.0), 0), ((168.0, 63.0), 0), ((178.0, 70.0), 0), ((170.0, 59.0), 0), ((164.0, 53.0), 1), ((155.0, 46.0), 1), ((162.0, 52.0), 1), ((166.0, 55.0), 1), ((172.0, 60.0), 0)]
centroids = [(180.0, 75.0), (155.0, 46.0)]
Step number 1: point_groups = [((180.0, 75.0), 0), ((174.0, 71.0), 0), ((184.0, 83.0), 0), ((168.0, 63.0), 0), ((178.0, 70.0), 0), ((170.0, 59.0), 0), ((164.0, 53.0), 1), ((155.0, 46.0), 1), ((162.0, 52.0), 1), ((166.0, 55.0), 1), ((172.0, 60.0), 0)]
centroids = [(175.14285714285714, 68.71428571428571), (161.75, 51.5)]
```

该程序还输出一个图表，如图 5.2 中的*所示。*`last`参数意味着我们希望程序执行聚类直到最后一步。如果我们只想显示第一步(步骤 0)，我们可以将最后一步改为`0`以便运行，如以下代码所示:

```py
$ python k-means_clustering.py persons_by_height_weight.csv 2 0
```

在执行程序时，我们会在初始步骤中得到一个聚类图和它们的质心，如图 5.1 中的*所示。*



# 房屋所有权——选择集群数量

让我们以第一章中关于房屋所有权的例子为例:

| **年龄** | **美元年收入** | **房屋所有权状况** |
| Twenty-three | Fifty thousand | 非所有者 |
| Thirty-seven | Thirty-four thousand | 非所有者 |
| Forty-eight | Forty thousand | 物主 |
| fifty-two | Thirty thousand | 非所有者 |
| Twenty-eight | Ninety-five thousand | 物主 |
| Twenty-five | Seventy-eight thousand | 非所有者 |
| Thirty-five | 13,0000 | 物主 |
| Thirty-two | 10,5000 | 物主 |
| Twenty | 10,0000 | 非所有者 |
| Forty | Sixty thousand | 物主 |
| Fifty | Eighty thousand | 彼得（男子名） |

我们希望使用聚类来预测 Peter 是否是房屋所有者。



# 分析

正如在第一章中一样，我们将不得不调整数据，因为*收入*轴明显更大，因此将减少*年龄*轴的影响，这实际上在这类问题中具有良好的预测能力。这是因为与年轻人相比，老年人有更多的时间安顿下来、存钱和买房。

我们使用 K 个最近邻对第 1 章、*分类的[进行同样的重新调整，并获得下表:](e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml)*

| **年龄** | **换算年龄** | **美元年收入** | **换算年收入** | **房屋所有权状况** |
| Twenty-three | 0.09375 | Fifty thousand | Zero point two | 非所有者 |
| Thirty-seven | 0.53125 | Thirty-four thousand | Zero point zero four | 非所有者 |
| Forty-eight | Zero point eight seven five | forty thousand | Zero point one | 物主 |
| fifty-two | one | thirty thousand | Zero | 非所有者 |
| Twenty-eight | Zero point two five | Ninety-five thousand | Zero point six five | 物主 |
| Twenty-five | 0.15625 | Seventy-eight thousand | Zero point four eight | 非所有者 |
| Thirty-five | 0.46875 | One hundred and thirty thousand | one | 物主 |
| Thirty-two | Zero point three seven five | One hundred and five thousand | Zero point seven five | 物主 |
| Twenty | Zero | One hundred thousand | Zero point seven | 非所有者 |
| Forty | Zero point six two five | Sixty thousand | Zero point three | 物主 |
| Fifty | 0.9375 | Eighty thousand | Zero point five | ？ |

给定该表，我们生成算法的输入文件并执行它，将特征聚类成两个聚类。
**输入**:

```py
# source_code/5/house_ownership2.csv 0.09375,0.2
0.53125,0.04
0.875,0.1
1,0
0.25,0.65
0.15625,0.48
0.46875,1
0.375,0.75
0,0.7
0.625,0.3
0.9375,0.5
```

**两个集群的输出**:

```py
$ python k-means_clustering.py house_ownership2.csv 2 last The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 0), ((0.15625, 0.48), 0), ((0.46875, 1.0), 0), ((0.375, 0.75), 0), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.09375, 0.2), (1.0, 0.0)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 0), ((0.15625, 0.48), 0), ((0.46875, 1.0), 0), ((0.375, 0.75), 0), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.26785714285714285, 0.5457142857142857), (0.859375, 0.225)]
Step number 2: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 0), ((0.15625, 0.48), 0), ((0.46875, 1.0), 0), ((0.375, 0.75), 0), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.22395833333333334, 0.63), (0.79375, 0.188)]
```

![](Images/d27bf970-4129-4f73-a650-d9109a755968.png)

蓝色簇包含缩放特征-*(0.09375，0.2)**【0.25，0.65】**【0.15625，0.48】**【0.46875，1】**【0.375，0.75】**(0，0.7)*和未缩放特征-。红色聚类包含缩放特征-*(0.53125，0.04)* 、 *(0.875，0.1)* 、 *(1，0)* 、 *(0.625，0.3)* 、 *(0.9375，0.5)*和未缩放特征 *(37，34000)*

所以，彼得属于红色星团。不算彼得，红簇里的房主比例是多少？红色集群中的 2/4 或 1/2 的人是房屋所有者。因此，彼得所属的红色聚类在确定彼得是否会成为房屋所有者方面似乎没有很高的预测能力。我们可以尝试将数据聚类到更多的聚类中，希望我们可以获得更纯的聚类，从而可以更可靠地预测 Peter 的房屋所有权。因此，让我们试着将数据分成三组。

**三个集群的输出**:

```py
$ python k-means_clustering.py house_ownership2.csv 3 last The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 2), ((0.15625, 0.48), 0), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.09375, 0.2), (1.0, 0.0), (0.46875, 1.0)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 2), ((0.15625, 0.48), 0), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 2), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.1953125, 0.355), (0.859375, 0.225), (0.3645833333333333, 0.7999999999999999)]
Step number 2: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 2), ((0.15625, 0.48), 0), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 2), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.125, 0.33999999999999997), (0.79375, 0.188), (0.2734375, 0.7749999999999999)]
```

![](Images/4e4dbc70-3394-483c-9e88-fcfc3cbcd199.png)

红色星团保持不变。因此，让我们将数据分为四类。

**四个集群的输出**:

```py
$ python k-means_clustering.py house_ownership2.csv 4 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.09375, 0.2), (1.0, 0.0), (0.46875, 1.0), (0.0, 0.7)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.3125, 0.12000000000000001), (0.859375, 0.225), (0.421875, 0.875), (0.13541666666666666, 0.61)]
```

![](Images/39fd3799-c004-48c0-bfa9-e87f0befb50b.png)

现在，彼得所属的红色星团已经改变了。现在红色集群的房屋拥有者比例是多少？如果不算彼得，红色集群中有 2/3 的人拥有自己的房子。当我们分成两个或三个群时，比例只有，这并没有告诉我们彼得是否是一个房子的主人。现在，红色集群中的房屋所有者占大多数，不包括彼得，因此我们更加相信彼得也是房屋所有者。然而，2/3 对于将彼得归类为房屋所有者来说仍然是一个相对较低的置信度。让我们将数据分成五组，看看会发生什么。

**五个集群的输出:**

```py
$ python k-means_clustering.py house_ownership2.csv 5 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 4), ((0.9375, 0.5), 4)]
centroids = [(0.09375, 0.2), (1.0, 0.0), (0.46875, 1.0), (0.0, 0.7), (0.9375, 0.5)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 4), ((0.9375, 0.5), 4)]
centroids = [(0.3125, 0.12000000000000001), (0.9375, 0.05), (0.421875, 0.875), (0.13541666666666666, 0.61), (0.78125, 0.4)]
```

![](Images/51db4dd3-3ecb-4aa4-a035-8dfeb21018fb.png)

现在，红色集群只包含 Peter 和一个非所有者。这种聚类表明 Peter 也更有可能是非所有者。然而，根据之前的聚类，彼得更有可能是一所房子的所有者。所以，彼得有没有房子，可能就不那么清楚了。收集更多的数据会改进我们的分析，并且应该在对这个问题作出明确的分类之前进行。

从我们的分析中，我们注意到不同数量的聚类会导致不同的分类结果，因为单个聚类中成员的性质会发生变化。在收集更多数据后，我们应该执行交叉验证，以确定对数据进行分类的最高准确度的聚类数。



# 文档聚类——了解语义上下文中 k 个聚类的数量

我们从古腾堡计划图书馆的以下 17 本书中获得了关于单词*金钱*和*上帝*的频率计数的以下信息:

| **书号** | **图书名称** | **钱作为%** | **神(s)作为%** |
| one | *吠檀多经典，由*评论*Ramanuja* ，由 Trans。乔治·蒂博特 | Zero | Zero point zero seven |
| Two | 奎师那的《摩诃婆罗多》——德威帕扬那·维阿萨*-阿迪·帕尔瓦*，作者基萨里·莫汉·甘古利 | Zero | Zero point one seven |
| three | 奎师那的《摩诃婆罗多》——道派衍那*Vyasa, Pt. 2* , by Krishna-Dwaipayana Vyasa | Zero point zero one | Zero point one |
| four | 奎师那的《摩诃婆罗多》----------------。*3 点。1* ，作者:克里希纳-德威帕亚那·维亚萨 | Zero | Zero point three two |
| five | 奎师那的《摩诃婆罗多》——德威帕扬那·维阿萨*Bk。4* ，作者基萨里·莫汉·甘古利 | Zero | Zero point zero six |
| six | 奎师那的《摩诃婆罗多》——德威帕扬那·维阿萨*Bk。3 磅。2* ，基萨里·莫汉·甘古利译 | Zero | Zero point two seven |
| seven | *《吠檀多经典》，附评论*作者桑卡斯卡利亚 | Zero | Zero point zero six |
| eight | *詹姆斯国王圣经* | Zero point zero two | Zero point five nine |
| nine | 约翰·弥尔顿的《重返天堂》 | Zero point zero two | Zero point four five |
| Ten | 托马斯·肯比斯的《模仿基督》 | Zero point zero one | Zero point six nine |
| Eleven | 罗德韦尔翻译的《古兰经》 | Zero point zero one | One point seven two |
| Twelve | 汤姆索亚历险记，完成，由马克·吐温(塞缪尔·克莱门斯) | Zero point zero five | Zero point zero one |
| Thirteen | 《哈克贝利·费恩历险记》完成，马克·吐温(塞缪尔·克莱门斯) | Zero point zero eight | Zero |
| Fourteen | 查尔斯·狄更斯的《远大前程》 | Zero point zero four | Zero point zero one |
| Fifteen | 奥斯卡·王尔德的《道林·格雷的画像》 | Zero point zero three | Zero point zero three |
| Sixteen | 阿瑟·柯南·道尔的《夏洛克·福尔摩斯历险记》 | Zero point zero four | Zero point zero three |
| Seventeen | 卡夫卡的《变形记》 *s*由大卫·威利翻译 | Zero point zero six | Zero point zero three |

我们希望基于所选的单词频率计数，根据它们的语义上下文将这个数据集聚类成组。



# 分析

首先，我们将执行重新调整，因为单词 *money* 的最高频率计数是 0.08%，而单词“god(s)”的最高频率计数是 1.72%。因此，我们将货币的频率计数除以 0.08，上帝的频率计数除以 1.72，如下所示:

| **书号** | **货币规模** | **上帝降临了** |
| one | Zero | 0.0406976744 |
| Two | Zero | 0.0988372093 |
| three | Zero point one two five | 0.0581395349 |
| four | Zero | 0.1860465116 |
| five | Zero | 0.0348837209 |
| six | Zero | 0.1569767442 |
| seven | Zero | 0.0348837209 |
| eight | Zero point two five | 0.3430232558 |
| nine | Zero point two five | 0.261627907 |
| Ten | Zero point one two five | 0.4011627907 |
| Eleven | Zero point one two five | one |
| Twelve | Zero point six two five | 0.0058139535 |
| Thirteen | one | Zero |
| Fourteen | Zero point five | 0.0058139535 |
| Fifteen | Zero point three seven five | 0.0174418605 |
| Sixteen | Zero point five | 0.0174418605 |
| Seventeen | Zero point seven five | 0.0174418605 |

现在我们已经重新调整了数据，让我们应用*k*-均值聚类算法，尝试将数据分成不同数量的聚类。

**输入**:

```py
source_code/5/document_clustering/word_frequencies_money_god_scaled.csv 0,0.0406976744
0,0.0988372093
0.125,0.0581395349
0,0.1860465116
0,0.0348837209
0,0.1569767442
0,0.0348837209
0.25,0.3430232558
0.25,0.261627907
0.125,0.4011627907
0.125,1
0.625,0.0058139535
1,0
0.5,0.0058139535
0.375,0.0174418605
0.5,0.0174418605
0.75,0.0174418605
```

**两个集群的输出**:

```py
$ python k-means_clustering.py document_clustering/word_frequencies_money_god_scaled.csv 2 last The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 0), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 0), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.0, 0.0406976744), (1.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 0), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.10416666666666667, 0.21947674418333332), (0.675, 0.0093023256)]
Step number 2: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 0), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.07954545454545454, 0.2378435517909091), (0.625, 0.01065891475)]
```

![](Images/42911ca1-e009-4853-a961-b7eda68a1a91.png)

我们可以观察到，聚类成两个簇将书籍分为宗教类的，即蓝色簇中的书籍，和非宗教类的，即红色簇中的书籍。让我们试着把这些书分成三组，观察算法将如何划分数据。

**三个集群的输出**:

```py
$ python k-means_clustering.py document_clustering/word_frequencies_money_god_scaled.csv 3 last The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 0), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.0, 0.0406976744), (1.0, 0.0), (0.125, 1.0)]
Step number 1: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.10227272727272728, 0.14852008456363636), (0.675, 0.0093023256), (0.125, 1.0)]
Step number 2: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.075, 0.16162790697), (0.625, 0.01065891475), (0.125, 1.0)]
```

![](Images/baabeb91-1761-4629-b5d2-02c942e15b85.png)

这一次，算法将古兰经从宗教书籍中分离出来，变成一个绿色的簇。这是因为，事实上，单词*上帝*是古兰经中第五个最常见的单词。这里的聚类恰好根据书籍的写作风格来划分书籍。聚类成四个聚类将单词 *money* 出现频率相对较高的一本书从非宗教书籍的红色聚类中分离成单独的聚类。现在，让我们看一下如何分成五个集群。

**五个集群的输出**:

```py
$ python k-means_clustering.py word_frequencies_money_god_scaled.csv 5 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 4), ((0.25, 0.261627907), 4), ((0.125, 0.4011627907), 4), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 3), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 3), ((0.375, 0.0174418605), 3), ((0.5, 0.0174418605), 3), ((0.75, 0.0174418605), 3)]
centroids = [(0.0, 0.0406976744), (1.0, 0.0), (0.125, 1.0), (0.5, 0.0174418605), (0.25, 0.3430232558)]
Step number 1: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 4), ((0.25, 0.261627907), 4), ((0.125, 0.4011627907), 4), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 3), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 3), ((0.375, 0.0174418605), 3), ((0.5, 0.0174418605), 3), ((0.75, 0.0174418605), 3)]
centroids = [(0.017857142857142856, 0.08720930231428571), (1.0, 0.0), (0.125, 1.0), (0.55, 0.0127906977), (0.20833333333333334, 0.3352713178333333)]
```

![](Images/ba0f2f79-1c06-45b9-97fa-73b1a7e1ff2b.png)

这种聚类进一步将剩余宗教书籍的蓝色聚类分成印度教书籍的蓝色聚类和基督教书籍的灰色聚类。

我们可以以这种方式使用聚类来对具有相似属性的项目进行分组，然后根据给定的示例快速找到相似的项目。参数 *k* 下的聚类粒度决定了我们可以预期一个组中的项目有多相似。参数越高，聚类中的项目就越相似，尽管数量较少。



# 摘要

在本章中，我们了解了数据聚类是如何非常高效，并可用于通过将某个要素分类为属于该要素聚类中所代表的类来促进新要素的快速分类。通过选择导致最准确分类的一个，可以通过交叉验证来确定适当数量的聚类。

聚类根据相似性对数据进行排序。聚类越多，聚类中的要素之间的相似性就越大，但聚类中的要素就越少。

我们还了解到， *k* -means 算法是一种聚类算法，它试图以一种最小化聚类中各要素之间的相互距离的方式对要素进行聚类。为此，该算法计算每个聚类的质心，一个要素属于质心最接近它的聚类。一旦聚类或它们的质心不再改变，该算法就结束聚类的计算。

在下一章中，我们将使用数学回归分析因变量之间的关系。与分类和聚类算法不同，回归分析将用于估计变量的最可能值，如重量、距离或温度。



# 问题

**问题 1** :计算以下聚类的质心:

a) 2、3、4

(100 美元、400 美元、1000 美元

c) (10，20)，(40，60)，(0，40)

(d)(200，40 公里美元)，(300，60 公里美元)，(500，100 公里美元)，(250，200 公里美元)

e) (1，2，4)，(0，0，3)，(10，20，5)，(4，8，2)，(5，0，1)

**问题 2** :使用*k*-均值聚类算法将以下数据集聚类成两个、三个和四个聚类:

a) 0，2，5，4，8，10，12，11

b) (2，2)、(2，5)、(10，4)、(3，5)、(7，3)、(5，9)、(2，8)、(4，10)、(7，4)、(4，4)、(5，8)、(9，3)

问题 3:我们知道这些夫妇的年龄和他们有多少个孩子:

| **情侣号** | **妻子的年龄** | **丈夫的年龄** | **子女数量** |
| one | 48 | forty-nine | five |
| 2 | Forty | Forty-three | Two |
| 3 | Twenty-four | Twenty-eight | one |
| 4 | forty-nine | forty-two | three |
| 5 | Thirty-two | Thirty-four | Zero |
| 6 | Twenty-four | Twenty-seven | Zero |
| 7 | Twenty-nine | Thirty-two | Two |
| 8 | Thirty-five | Thirty-five | Two |
| 9 | Thirty-three | Thirty-six | one |
| 10 | forty-two | Forty-seven | three |
| 11 | Twenty-two | Twenty-seven | Two |
| 12 | Forty-one | Forty-five | four |
| 13 | Thirty-nine | Forty-three | four |
| 14 | Thirty-six | Thirty-eight | Two |
| 15 | Thirty | Thirty-two | one |
| 16 | Thirty-six | Thirty-eight | Zero |
| 17 | Thirty-six | Thirty-nine | three |
| 18 | Thirty-seven | Thirty-eight | ？ |

我们想使用聚类来猜测丈夫年龄为 37 岁、妻子年龄为 38 岁的夫妇有多少个孩子。



# 分析

**问题 1**:a)**![](Images/dbe2db56-78ad-4fb8-874e-3b405828a540.png)**
b)![](Images/05c3cee4-8cb6-42a3-a46d-1762b351aedd.png)
c)![](Images/e5697e9f-d754-41e6-b0a4-1a9f1fb72978.png)

d)**![](Images/cc01cdbe-b8d3-4609-ace6-a4054f085f4f.png)**

e) ![](Images/135753ed-7971-4a68-998f-e25309588af5.png)

**问题 2** : a)我们为所有特征添加第二个坐标，并将其设置为`0`。这样，特征之间的距离不会改变，我们可以使用本章前面实现的聚类算法。

**输入**:

```py
# source_code/5/problem5_2.csv 0,0
2,0
5,0
4,0
8,0
10,0
12,0
11,0
```

**对于两个集群**:

```py
$ python k-means_clustering.py problem5_2.csv 2 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 0), ((4.0, 0.0), 0), ((8.0, 0.0), 1), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(0.0, 0.0), (12.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 0), ((4.0, 0.0), 0), ((8.0, 0.0), 1), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(2.75, 0.0), (10.25, 0.0)]
```

**对于三个集群**:

```py
$ python k-means_clustering.py problem5_2.csv 3 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 2), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(0.0, 0.0), (12.0, 0.0), (5.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 2), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(1.0, 0.0), (11.0, 0.0), (5.666666666666667, 0.0)]
```

**对于四个集群**:

```py
$ python k-means_clustering.py problem5_2.csv 4 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 3), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(0.0, 0.0), (12.0, 0.0), (5.0, 0.0), (8.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 3), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(1.0, 0.0), (11.0, 0.0), (4.5, 0.0), (8.0, 0.0)]
```

b)我们再次使用实现的算法。

**输入**:

```py
# source_code/5/problem5_2b.csv 2,2
2,5
10,4
3,5
7,3
5,9
2,8
4,10
7,4
4,4
5,8
9,3
```

**两个集群的输出**:

```py
$ python k-means_clustering.py problem5_2b.csv 2 last The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 1), ((2.0, 8.0), 0), ((4.0, 10.0), 0), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 1), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (10.0, 4.0)]
Step number 1: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 0), ((2.0, 8.0), 0), ((4.0, 10.0), 0), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 0), ((9.0, 3.0), 1)]
centroids = [(2.8333333333333335, 5.666666666666667), (7.166666666666667, 5.166666666666667)]
Step number 2: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 0), ((2.0, 8.0), 0), ((4.0, 10.0), 0), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 0), ((9.0, 3.0), 1)]
centroids = [(3.375, 6.375), (8.25, 3.5)]
```

**三个集群的输出**:

```py
$ python k-means_clustering.py problem5_2b.csv 3 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (10.0, 4.0), (4.0, 10.0)]
Step number 1: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.75, 4.0), (8.25, 3.5), (4.0, 8.75)]
```

**四个集群的输出**:

```py
$ python k-means_clustering.py problem5_2b.csv 4 last The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 3), ((10.0, 4.0), 1), ((3.0, 5.0), 3), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 3), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (10.0, 4.0), (4.0, 10.0), (3.0, 5.0)]
Step number 1: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 3), ((10.0, 4.0), 1), ((3.0, 5.0), 3), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 3), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (8.25, 3.5), (4.0, 8.75), (3.0, 4.666666666666667)]
```

问题 3 :我们知道 17 对夫妇的年龄和他们有多少个孩子，我们想知道第 18 对^(夫妇有多少个孩子。我们将使用前 14 对夫妇作为训练数据，接下来的 3 对夫妇用于交叉验证，以确定聚类数 *k* ，我们将使用这些聚类数来找出第 18 对^对夫妇预计有多少个孩子。)

聚类后，我们会说，一对夫妇可能拥有的孩子数量大致是该聚类的平均值。使用交叉验证，我们将选择使实际和预测的孩子数量之间的差异最小化的聚类数量。我们将获取该分类中所有项目的累计差值，作为每对夫妇子女数量差值的平方根。这将最小化与第 18 对^对夫妇的预测孩子数量相关的随机变量的方差。

我们将执行两个、三个、四个和五个集群的聚类。

**输入**:

```py
# source_code/5/couples_children.csv 48,49
40,43
24,28
49,42
32,34
24,27
29,32
35,35
33,36
42,47
22,27
41,45
39,43
36,38
30,32
36,38
36,39
37,38
```

**两个集群的输出**:

为一个集群列出的一对是形式`(couple_number,(wife_age,husband_age))`:

```py
Cluster 0: [(1, (48.0, 49.0)), (2, (40.0, 43.0)), (4, (49.0, 42.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0)), (13, (39.0, 43.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
Cluster 1: [(3, (24.0, 28.0)), (5, (32.0, 34.0)), (6, (24.0, 27.0)), (7, (29.0, 32.0)), (8, (35.0, 35.0)), (9, (33.0, 36.0)), (11, (22.0, 27.0)), (15, (30.0, 32.0))]
```

我们想确定第 15 对^对夫妇 *(30，32)* 的预期子女数，即妻子 30 岁，丈夫 32 岁。 *(30，32)* 在聚类 1 中。聚类 1 中的情侣如下: *(24.0，28.0)* 、 *(32.0，34.0)* 、 *(24.0，27.0)* 、 *(29.0，32.0)* 、 *(35.0，35.0)* 、 *(33.0，36.0)* 。其中，除了用于数据目的的前 14 对夫妇，其余的夫妇是: *(24.0，28.0)* ， *(32.0，34.0)* ， *(24.0，27.0)* ， *(29.0，32.0)*，【35.0】，*， *(33 这些夫妇的平均子女数为 *est15=8/7~1.14* 。这是根据前 14 对夫妇的数据估算出的第 15 对夫妇的孩子数量。**

第 16 对^对夫妇的预计子女数为 *est16=23/7~3.29* 。由于第 16 对^对和第 17 对^对对夫妻属于同一群，第 17 对^对对夫妻的孩子数量估计也是 *est17=23/7~3.29* 。

现在我们将计算估计的孩子数量(例如，对于第 15 对^对夫妇，用 *est15* 表示)和实际的孩子数量(例如，对于第 15 对^对夫妇，用 *act15* 表示)之间的 *E2* 误差(两个聚类两个),如下所示:

![](Images/002cecdc-82e4-40f7-9781-a478866c51c4.png)

![](Images/b2ca6e56-f0de-42e9-9227-cb25a7fd8498.png)

现在，我们已经计算了 *E2* 误差，我们将根据其他聚类的估计来计算误差。我们将选择包含最少误差的聚类数来估计第 18 对^(夫妇的孩子数量。)

**三个集群的输出**:

```py
Cluster 0: [(1, (48.0, 49.0)), (2, (40.0, 43.0)), (4, (49.0, 42.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0)), (13, (39.0, 43.0))]
Cluster 1: [(3, (24.0, 28.0)), (6, (24.0, 27.0)), (7, (29.0, 32.0)), (11, (22.0, 27.0)), (15, (30.0, 32.0))]
Cluster 2: [(5, (32.0, 34.0)), (8, (35.0, 35.0)), (9, (33.0, 36.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
```

现在，第 15 对^对在第 1 组，第 16 对^对在第 2 组，第 17 对^对在第 2 组。所以估计每对夫妇的孩子数是 *5/4=1.25* 。

估算的 *E3* 误差如下:

![](Images/72664168-009d-4b4f-a5e4-833789796640.png)

**四个集群的输出**:

```py
Cluster 0: [(1, (48.0, 49.0)), (4, (49.0, 42.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0))]
Cluster 1: [(3, (24.0, 28.0)), (6, (24.0, 27.0)), (11, (22.0, 27.0))]
Cluster 2: [(2, (40.0, 43.0)), (13, (39.0, 43.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
Cluster 3: [(5, (32.0, 34.0)), (7, (29.0, 32.0)), (8, (35.0, 35.0)), (9, (33.0, 36.0)), (15, (30.0, 32.0))]
```

第 15 对^对在第 3 组，第 16 对^对在第 2 组，第 17 对^对在第 2 组。所以，第 15 对^(夫妇的孩子数量估计为 *5/4=1.25* 。第 16 ^对和第 17 ^对对夫妻的孩子数估计为 8/3=2.67。)

估算的 *E4* 误差如下:

![](Images/f995b78a-8035-4b00-91cc-f4593b3795c8.png)

**五个集群的输出**:

```py
Cluster 0: [(1, (48.0, 49.0)), (4, (49.0, 42.0))]
Cluster 1: [(3, (24.0, 28.0)), (6, (24.0, 27.0)), (11, (22.0, 27.0))]
Cluster 2: [(8, (35.0, 35.0)), (9, (33.0, 36.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
Cluster 3: [(5, (32.0, 34.0)), (7, (29.0, 32.0)), (15, (30.0, 32.0))]
Cluster 4: [(2, (40.0, 43.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0)), (13, (39.0, 43.0))]
```

第 15 对^对在第 3 组，第 16 对^对在第 2 组，第 17 对^对在第 2 组。所以，第 15 对夫妇的孩子数量估计是 1 个。第 16 ^对和第 17 ^对对夫妻的子女数估计为 5/3=1.67。

估算的 *E5* 误差如下:

![](Images/9781e3d2-892b-419f-8121-85ece972c9e4.png)

**使用交叉验证确定结果**

我们使用 14 对作为估计的训练数据，另外三对用于交叉验证，以在值 2、3、4 和 5 中找到 *k* 聚类的最佳参数。我们可以尝试分成更多的集群，但是由于我们的数据量相对较少，所以最多分成五个集群就足够了。让我们总结一下估算产生的误差:

| **星团数量** | **错误率** |
| Two | 3.3 |
| three | 2.17 |
| four | 2.7 |
| five | 2.13 |

对于 **3** 和 **5** 簇，错误率最低。对于 **4** 簇，错误率上升，然后对于 **5** 簇，错误率再次下降，这一事实可能表明我们没有足够的数据来做出良好的估计。自然的期望是对于大于 2 的 k 值没有局部最大误差。而且用 **3** 和 **5** 聚类的错误率差别很小，并且 **5** 中的一个聚类小于 **3** 中的一个聚类。出于这个原因，我们选择了 3 个集群而不是 5 个来估计第 18 对夫妇的孩子数量。

当聚类成三个聚类时，第 18 对^对对在聚类 **2** 中。因此，第 18 对^(夫妇的孩子数量估计为 1.25 个。)