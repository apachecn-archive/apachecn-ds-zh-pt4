<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 决策树

决策树是数据在树结构中的排列，其中在每个节点上，数据根据节点上的属性值被分成不同的分支。

为了构建决策树，我们将使用标准的 ID3 学习算法，该算法选择一个属性，以尽可能最好的方式对数据样本进行分类，以最大化信息增益，这是一种基于信息熵的度量。

在本章中，我们将讨论以下主题:

*   什么是决策树，以及如何通过 swim preference 示例在决策树中表示数据
*   信息熵和信息增益的概念，首先是理论上的，然后在*信息论*部分中实际应用游泳偏好的例子
*   如何使用 ID3 算法从训练数据中构造决策树，及其在 Python 中的实现
*   如何通过游泳偏好示例使用构建的决策树对新数据项进行分类
*   如何使用决策树对第 2 章中的国际象棋博弈问题进行替代分析，以及两种算法的结果有何不同
*   您将在*问题*部分验证您的理解，并了解何时使用决策树作为分析方法
*   以*购物*为例如何处理决策树构建过程中的数据不一致

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 游泳偏好——使用决策树表示数据

我们可能有某些偏好来决定我们是否会游泳。这些可以记录在一个表格中，如下所示:

| **游泳衣** | **水温** | **游泳偏好** |
| 没有人 | 寒冷 | 不 |
| 没有人 | 温暖的 | 不 |
| 小的 | 寒冷 | 不 |
| 小的 | 温暖的 | 不 |
| 好的 | 寒冷 | 不 |
| 好的 | 温暖的 | 是 |

此表中的数据也可以显示在以下决策树中:

![](Images/b974fd83-2b36-4328-8c77-02a7deccd42d.png)

图 3.1:游泳偏好示例的决策树

在根节点，我们问这个问题——你有游泳衣吗？对问题的回答将可用数据分成三组，每组两行。如果属性为`swimming suit = none`，那么两行的游泳偏好属性为`no`。因此，没有必要询问水温，因为所有具有`swimming suit = none`属性的样本都将被归类为`no`。对于`swimming suit = small`属性也是如此。在`swimming suit = good`的情况下，剩下的两行可以分为两类——`no`和`yes`。

如果没有进一步的信息，我们将无法正确地对每一行进行分类。幸运的是，对于正确分类的每一行，还有一个问题可以问。对于具有`water=cold`属性的行，游泳偏好是`no`。对于具有`water=warm`属性的行，游泳偏好是`yes`。

总之，从根节点开始，我们在每个节点问一个问题，根据答案，我们沿着树向下移动，直到到达一个叶节点，在那里我们找到对应于这些答案的数据项的类。

这就是我们如何使用现成的决策树对数据样本进行分类。但是知道如何从数据中构建决策树也很重要。

哪个属性在哪个节点有问题？这如何体现在决策树的构造上？如果我们改变属性的顺序，得到的决策树会比另一棵树分类得更好吗？

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 信息论

信息论研究信息的量化、存储和交流。我们引入信息熵和信息增益的概念，使用 ID3 算法构造决策树。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 信息熵

任何给定数据的信息熵是从该数据中表示数据项所需的最小信息量的度量。信息熵的单位很常见——比特、字节、千字节等等。信息熵越低，数据越规则，数据中出现的模式越多，因此，表示数据所需的信息量就越少。这就是计算机上的压缩工具如何将大型文本文件压缩到更小的大小，因为单词和单词表达式不断重复出现，形成一种模式。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 抛硬币

想象我们抛一枚不带偏见的硬币。我们想知道结果是正面还是反面。我们需要多少信息来表示结果？头和尾这两个字都由四个字符组成，如果我们用一个字节(8 位)来表示一个字符，这是 ASCII 表中的标准，那么我们需要四个字节或 32 位来表示结果。

但是信息熵是表示结果所需的最小数据量。我们知道只有两种可能的结果——正面或反面。如果我们同意用 0 表示头部，用 1 表示尾部，那么 1 位就足以有效地传达结果。在这里，数据是投掷硬币的结果的可能性的空间。可以表示为集合`{0,1}`的是集合`{head,tail}`。实际结果是这个集合中的一个数据项。原来集合的熵是 1。这是因为头和尾都是 50%的概率。

现在想象一下，硬币是有偏向的，25%的时间是正面，75%的时间是反面。这次概率空间`{0,1}`的熵会是多少？我们当然可以用一点信息来表示结果。但是我们能做得更好吗？当然，一个比特是不可分割的，但是也许我们可以把信息的概念推广到不确定的数量。

在前面的例子中，除非我们看硬币，否则我们对之前的硬币投掷结果一无所知。但是在有偏向的硬币的例子中，我们知道结果更可能是反面。如果我们在一个表示正面为 0，反面为 1 的文件中记录了 *n* 次掷硬币的结果，那么其中大约 75%的位的值为 1，25%的位的值为 0。这样一个文件的大小将是 *n* 位。但是因为它更规则(1 的模式在其中占优势)，一个好的压缩工具应该能够将它压缩到少于 *n* 比特。

为了了解压缩背后的理论以及表示一个数据项需要多少信息，让我们来看看信息熵的精确定义。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 信息熵的定义

假设给我们一个概率空间， *S* ，元素 *1，2，...，n* 。从概率空间中选择元素 *i* 的概率是*p[I]。概率空间的信息熵定义如下:*

![](Images/7e5fdefb-a6bf-44f2-8797-bb12161e973b.png)

上式中，*log[2]为二进制对数。*

因此，无偏硬币投掷的概率空间的信息熵如下:

![](Images/25c77162-1f29-42fb-9d8a-3aacab40d80e.png)

当硬币有偏向时，正面有 25%的机会，反面有 75%的机会，那么这样一个空间的信息熵如下:

![](Images/cd1627da-fe47-48b2-8b52-dde0f8070f10.png)

这小于 1。因此，例如，如果我们有一个大约 25%的 0 位和 75%的 1 位的大文件，一个好的压缩工具应该能够将其压缩到其大小的 81.12%。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 信息增益

信息增益是作为某个过程的结果而获得的信息熵的量。例如，如果我们想知道三个公平硬币的结果，那么信息熵是 3。但是如果我们看第三个硬币，那么剩下的两个硬币的信息熵将会是 2。因此，通过观察第三枚硬币，我们获得了一位信息，所以信息增益是 1。

我们也可以通过将整个集合 *S* 划分成集合，按照相似的模式将它们分组来获得信息熵。如果我们根据元素的属性值 *A* 对元素进行分组，那么我们将信息增益定义如下:

![](Images/f5d65d98-1431-4e05-9704-64d62f7dc1a5.png)

这里，*S[v]是一个集合，其中 *S* 的元素的值为 *v* ，属性为 *A* 。*

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 游泳偏好-信息增益计算

让我们通过将一个`swimming suit`作为属性来计算 swim preference 示例中六行的信息增益。因为我们感兴趣的是，在回答您是否应该去游泳的问题时，给定的数据行是被分类为`no`还是`yes`，所以我们将使用游泳偏好来计算熵和信息增益。我们使用属性`swimming suit`对集合 *S* 进行划分:

*S [无] ={(无，冷，无)，(无，暖，无)}*

*S [小] ={(小，冷，没有)，(小，暖，没有)}*

*S [好] ={(好，冷，否)，(好，暖，是)}*

*S* 的信息熵为:

*E(S)= ![](Images/36bd8ff7-5288-4b68-aa7e-e2ae36444bce.png)*

分区的信息熵是:

*E(S[none])=-(2/2)* log[2](2/2)=-log[2](1)= 0，*因为所有实例都有类`no`。

同理，E(S [小] )=0 。

*E(S [好的] )= ![](Images/c6aad04d-c94c-4ac3-8bcb-009ec62772bc.png)*

因此，信息增益为:

*IG(S，游泳衣)=E(S)-[(2/6)*E(S [无] )+(2/6)*E(S [小] )+(2/6)*E(S [好] )]*

*= 0.65002242164-(1/3)= 0.3166890883*

如果我们选择`water temperature`属性对集合进行划分， *S* ，那么信息增益， *IG(S，水温)*会是多少？水温将 *S* 分成以下几组:

*S [冷] ={(无，冷，无)，(小，冷，无)，(好，冷，无)}*

*S [暖] ={(无，暖，否)，(小，暖，否)，(好，暖，是)}*

它们的熵如下:

*E(S [cold] )=0，*因为所有实例都被分类为否

*E(S [暖])=-(2/3)* log[2](2/3)-(1/3)* log[2](1/3)~ 0.91829583405*

因此，使用*水温*属性对集合 *S* 进行分区的信息增益如下:

*IG(S，水温)=E(S)-[(1/2)*E(S [冷] )+(1/2)*E(S [暖] )]*

*= 0.65002242164-0.5 * 0.91829583405 = 0.19087450461*

这还不到 *IG(S，游泳衣)*。因此，通过按照`swimming suit`属性而不是`water temperature`属性对集合进行划分，我们可以获得关于集合 *S* (其实例的分类)的更多信息。这一发现将是 ID3 算法在下一节构建决策树时的基础。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# ID3 算法–决策树构造

ID3 算法基于信息增益从数据构建决策树。一开始，我们从集合 *S* 开始。集合中的数据项 *S* 具有各种属性，根据这些属性我们可以对集合 *S* 进行划分。如果属性 *A* 具有值 *{v [1] ，...，v [n] }* ，然后我们将集合 *S* 划分为集合 *S [1]* ，...， *S [n]* ，其中集合 S [i] ，是集合的子集， *S* ，其中元素有值， *v [i]* ，为属性， *A* 。

如果集合中的每个元素 *S* 都具有属性 *A [1] ，...，A [m]* ，那么我们可以根据任何可能的属性对集合 *S* 进行划分。ID3 算法根据产生最高信息增益的属性来划分集合 *S* 。现在假设它有属性，*A[1]。然后，对于集合 *S* ，我们有分区 *S [1] ，...，S [n]* ，其中 *A [1]* 有可能值 *{v [1] ，...，v [n] }* 。*

因为我们还没有构造树，所以我们首先放置一个根节点。对于 *S* 的每个分区，我们从根开始放置一个新的分支。每个分支代表所选属性的一个值。一个分支包含具有相同属性值的数据样本。对于每个新分支，我们可以定义一个新节点，该节点将拥有来自其祖先分支的数据样本。

一旦我们定义了一个新节点，我们就为该节点上的数据选择另一个具有最高信息增益的剩余属性，以在该节点上进一步划分数据，然后定义新的分支和节点。这个过程可以重复进行，直到我们用完了节点的所有属性，或者甚至更早，直到节点上的所有数据都与我们感兴趣的类相同。在游泳偏好示例的情况下，对于游泳偏好只有两个可能的类别——类别`no`和类别`yes`。最后一个节点称为**叶节点**，从数据中决定数据项的类别。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 游泳偏好–通过 ID3 算法构建决策树

在这里，我们一步一步地描述 ID3 算法如何从游泳偏好示例中的给定数据样本构建决策树。初始集合由六个数据样本组成:

```
S={(none,cold,no),(small,cold,no),(good,cold,no),(none,warm,no),(small,warm,no),(good,warm,yes)}
```

在前面的章节中，我们计算了两个属性以及唯一的未分类属性`swimming suit`和`water temperature`的信息增益，如下所示:

```
IG(S,swimming suit)=0.3166890883
IG(S,water temperature)=0.19087450461
```

因此，我们将选择`swimming suit`属性，因为它具有更高的信息增益。还没有画出树，所以我们从根节点开始。由于`swimming suit`属性有三个可能的值——`{none, small, good}`，我们为每个值绘制了三个可能的分支。每个分支将从分区集 *S: S [none]* 、 *S [small]* 和 *S [good]* 中选择一个分区。我们将节点添加到分支的末端。*S*[*none*]数据样本具有相同的游泳偏好 class = `no`，因此我们不需要用进一步的属性来分支该节点并划分集合。因此，带有数据的节点*S[none]已经是一个叶节点。有数据的节点也是如此， *S [小]。**

但是具有数据的节点， *S [good]* ，对于游泳偏好有两个可能的类别。因此，我们将进一步分支该节点。只剩下一个非分类属性— `water temperature`。所以不需要用数据计算那个属性的信息增益， *S [好]* 。从节点中， *S [好的]，我们会有两个分支，每个分支都有一个分区从集合， *S [好的]。其中一个分支会有数据样本的集合， *S [good，]T20cold = {(good，cold，no)}；*其他分支会有分区， *S [好，]暖 ={(好，暖，是)}* 。这两个分支都将以一个节点结束。每个节点将是一个叶节点，因为每个节点都有相同值的数据样本用于分类游泳偏好属性。**

生成的决策树有四个叶节点，是*图 3.1-游泳偏好示例的决策树*中的树。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 履行

我们实现了一个 ID3 算法，为 CSV 文件中给出的数据构建一个决策树。所有资料都在章节目录中。这里给出了源代码中最重要的部分:

```
# source_code/3/construct_decision_tree.py
# Constructs a decision tree from data specified in a CSV file.
# Format of a CSV file:
# Each data item is written on one line, with its variables separated
# by a comma. The last variable is used as a decision variable to
# branch a node and construct the decision tree.

import math
# anytree module is used to visualize the decision tree constructed by
# this ID3 algorithm.
from anytree import Node, RenderTree
import sys
sys.path.append('../common')
import common
import decision_tree

# Program start csv_file_name = sys.argv[1]
verbose = int(sys.argv[2])  # verbosity level, 0 - only decision tree

# Define the enquired column to be the last one.
# I.e. a column defining the decision variable.
(heading, complete_data, incomplete_data,
 enquired_column) = common.csv_file_to_ordered_data(csv_file_name)

tree = decision_tree.constuct_decision_tree(
    verbose, heading, complete_data, enquired_column)
decision_tree.display_tree(tree)
```

```
# source_code/common/decision_tree.py
# ***Decision Tree library ***
# Used to construct a decision tree and a random forest.
import math
import random
import common
from anytree import Node, RenderTree
from common import printfv

# Node for the construction of a decision tree.
class TreeNode: 
    def __init__(self, var=None, val=None):
        self.children = []
        self.var = var
        self.val = val

    def add_child(self, child):
        self.children.append(child)

    def get_children(self):
        return self.children

    def get_var(self):
        return self.var

    def get_val(self):
        return self.val

    def is_root(self):
        return self.var is None and self.val is None

    def is_leaf(self):
        return len(self.children) == 0

    def name(self):
        if self.is_root():
            return "[root]"
        return "[" + self.var + "=" + self.val + "]"

# Constructs a decision tree where heading is the heading of the table
# with the data, i.e. the names of the attributes.
# complete_data are data samples with a known value for every attribute.
# enquired_column is the index of the column (starting from zero) which
# holds the classifying attribute.
def construct_decision_tree(verbose, heading, complete_data, enquired_column):
    return construct_general_tree(verbose, heading, complete_data,
                                  enquired_column, len(heading))

# m is the number of the classifying variables that should be at most
# considered at each node. *m needed only for a random forest.*
def construct_general_tree(verbose, heading, complete_data,
                           enquired_column, m):
    available_columns = []
    for col in range(0, len(heading)):
        if col != enquired_column:
            available_columns.append(col)
    tree = TreeNode()
    printfv(2, verbose, "We start the construction with the root node" +
                        " to create the first node of the tree.\n")
    add_children_to_node(verbose, tree, heading, complete_data,
                         available_columns, enquired_column, m)
    return tree

# Splits the data samples into the groups with each having a different
# value for the attribute at the column col.
def split_data_by_col(data, col):
    data_groups = {}
    for data_item in data:
        if data_groups.get(data_item[col]) is None:
            data_groups[data_item[col]] = []
        data_groups[data_item[col]].append(data_item)
    return data_groups

# Adds a leaf node to node.
def add_leaf(verbose, node, heading, complete_data, enquired_column):
    leaf_node = TreeNode(heading[enquired_column],
                         complete_data[0][enquired_column])
    printfv(2, verbose,
            "We add the leaf node " + leaf_node.name() + ".\n")
    node.add_child(leaf_node)

# Adds all the descendants to the node.
def add_children_to_node(verbose, node, heading, complete_data,
                         available_columns, enquired_column, m):
    if len(available_columns) == 0:
        printfv(2, verbose, "We do not have any available variables " +
                "on which we could split the node further, therefore " +
                "we add a leaf node to the current branch of the tree. ")
        add_leaf(verbose, node, heading, complete_data, enquired_column)
        return -1

    printfv(2, verbose, "We would like to add children to the node " +
            node.name() + ".\n")

    selected_col = select_col(
        verbose, heading, complete_data, available_columns,
        enquired_column, m)
    for i in range(0, len(available_columns)):
        if available_columns[i] == selected_col:
            available_columns.pop(i)
            break

    data_groups = split_data_by_col(complete_data, selected_col)
    if (len(data_groups.items()) == 1):
        printfv(2, verbose, "For the chosen variable " +
                heading[selected_col] +
                " all the remaining features have the same value " +
                complete_data[0][selected_col] + ". " +
                "Thus we close the branch with a leaf node. ")
        add_leaf(verbose, node, heading, complete_data, enquired_column)
        return -1

    if verbose >= 2:
        printfv(2, verbose, "Using the variable " +
                heading[selected_col] +
                " we partition the data in the current node, where" +
                " each partition of the data will be for one of the " +
                "new branches from the current node " + node.name() +
                ". " + "We have the following partitions:\n")
        for child_group, child_data in data_groups.items():
            printfv(2, verbose, "Partition for " +
                    str(heading[selected_col]) + "=" +
                    str(child_data[0][selected_col]) + ": " +
                    str(child_data) + "\n")
        printfv(
            2, verbose, "Now, given the partitions, let us form the " +
                        "branches and the child nodes.\n")
    for child_group, child_data in data_groups.items():
        child = TreeNode(heading[selected_col], child_group)
        printfv(2, verbose, "\nWe add a child node " + child.name() +
                " to the node " + node.name() + ". " +
                "This branch classifies %d feature(s): " +
                str(child_data) + "\n", len(child_data))
        add_children_to_node(verbose, child, heading, child_data, list(
            available_columns), enquired_column, m)
        node.add_child(child)
    printfv(2, verbose,
            "\nNow, we have added all the children nodes for the " +
            "node " + node.name() + ".\n")

# Selects an available column/attribute with the highest
# information gain.
def select_col(verbose, heading, complete_data, available_columns,
               enquired_column, m):
    # Consider only a subset of the available columns of size m.
    printfv(2, verbose,
            "The available variables that we have still left are " +
            str(numbers_to_strings(available_columns, heading)) + ". ")
    if len(available_columns) < m:
        printfv(
            2, verbose, "As there are fewer of them than the " +
                        "parameter m=%d, we consider all of them. ", m)
        sample_columns = available_columns
    else:
        sample_columns = random.sample(available_columns, m)
        printfv(2, verbose,
                "We choose a subset of them of size m to be " +
                str(numbers_to_strings(available_columns, heading)) +
                ".")

    selected_col = -1
    selected_col_information_gain = -1
    for col in sample_columns:
        current_information_gain = col_information_gain(
            complete_data, col, enquired_column)
        # print len(complete_data),col,current_information_gain
        if current_information_gain > selected_col_information_gain:
            selected_col = col
            selected_col_information_gain = current_information_gain
    printfv(2, verbose,
            "Out of these variables, the variable with " +
            "the highest information gain is the variable " +
            heading[selected_col] +
            ". Thus we will branch the node further on this " +
            "variable. " +
            "We also remove this variable from the list of the " +
            "available variables for the children of the current node. ")
    return selected_col

# Calculates the information gain when partitioning complete_data
# according to the attribute at the column col and classifying by the
# attribute at enquired_column.
def col_information_gain(complete_data, col, enquired_column):
    data_groups = split_data_by_col(complete_data, col)
    information_gain = entropy(complete_data, enquired_column)
    for _, data_group in data_groups.items():
        information_gain -= (float(len(data_group)) / len(complete_data)
                             ) * entropy(data_group, enquired_column)
    return information_gain

# Calculates the entropy of the data classified by the attribute
# at the enquired_column.
def entropy(data, enquired_column):
    value_counts = {}
    for data_item in data:
        if value_counts.get(data_item[enquired_column]) is None:
            value_counts[data_item[enquired_column]] = 0
        value_counts[data_item[enquired_column]] += 1
    entropy = 0
    for _, count in value_counts.items():
        probability = float(count) / len(data)
        entropy -= probability * math.log(probability, 2)
    return entropy
```

**程序输入**:

我们将游泳偏好示例中的数据输入到程序中，以构建决策树:

```
# source_code/3/swim.csv
swimming_suit,water_temperature,swim
None,Cold,No
None,Warm,No
Small,Cold,No
Small,Warm,No
Good,Cold,No
Good,Warm,Yes 
```

**程序输出**:

我们从`swim.csv`数据文件构建一个决策树，其中详细度设置为`0`。建议读者将详细度设置为`2`,以查看关于决策树构造方式的详细解释:

```
$ python construct_decision_tree.py swim.csv 0 Root
├── [swimming_suit=Small]
│ ├── [water_temperature=Cold]
│ │ └── [swim=No]
│ └── [water_temperature=Warm]
│   └── [swim=No]
├── [swimming_suit=None]
│ ├── [water_temperature=Cold]
│ │ └── [swim=No]
│ └── [water_temperature=Warm]
│   └── [swim=No]
└── [swimming_suit=Good]
    ├── [water_temperature=Cold]
    │   └── [swim=No]
    └── [water_temperature=Warm]
        └── [swim=Yes]
```

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 用决策树分类

一旦我们从具有属性 *A [1] 的数据中构建了决策树，...，A[m]和类 *{c [1] ，...，c [k] }* ，我们可以用这个决策树对一个新的数据项进行分类，属性为 *A [1] ，...，A[m]进入其中一个类 *{c [1] ，...，c [k] }* 。**

给定一个我们想要分类的新数据项，我们可以将包括根节点在内的每个节点视为数据样本的一个问题:*该数据样本对于所选属性 A [i] 有什么值？*然后，根据答案，我们选择决策树的一个分支，继续前进到下一个节点。然后，回答关于数据样本的另一个问题，并且另一个问题，直到数据样本到达叶节点。叶节点具有类别 *{c [1] 之一，...，c [k] }* 与之相关联；比如*c[I]。然后，决策树算法会将数据样本分类到类别中，*c[I]。**

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 用游泳偏好决策树对数据样本进行分类

让我们使用 ID3 算法为游泳偏好示例构建一个决策树。现在我们已经构造了决策树，我们想用它来分类一个数据样本，*(好，冷，？)*进入*{否，是}* 中的两个班之一。

从树根处的数据样本开始。从根分叉出来的第一个属性是`swimming suit`，所以我们要求得到样本*的`swimming suit`属性的值(好的，冷的，？)*。我们知道属性的值是`swimming suit=good`；因此，向下移动最右边的分支，将该值作为其数据样本。我们到达具有`water temperature`属性的节点，并提出问题:*数据样本的水温属性的值是多少(好，冷？)?*我们知道，对于那个数据样本，我们有`water temperature=cold`；因此，我们将左边的分支移到叶节点。这个叶子与`swimming preference=no`类相关联。因此，决策树会对数据样本*(好的，冷的，？)*在游泳偏好类中；换句话说，要完成它(通过用确切的数据代替问号)到数据样本 *(* *好，冷，不)*。

因此，决策树说，如果你有一件很好的泳衣，但是水温很低，那么根据表中收集的数据，你仍然不想游泳。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 下棋——决策树分析

让我们再举一个来自第二章、*朴素贝叶斯*的例子:

| **温度** | **风** | **阳光** | **播放** |
| 寒冷 | 强烈的 | 多云的 | 不 |
| 寒冷 | 强烈的 | 多云的 | 不 |
| 温暖的 | 没有人 | 快活的 | 是 |
| 热的 | 没有人 | 快活的 | 不 |
| 热的 | 微风 | 多云的 | 是 |
| 温暖的 | 微风 | 快活的 | 是 |
| 寒冷 | 微风 | 多云的 | 不 |
| 寒冷 | 没有人 | 快活的 | 是 |
| 热的 | 强烈的 | 多云的 | 是 |
| 温暖的 | 没有人 | 多云的 | 是 |
| 温暖的 | 强烈的 | 快活的 | ？ |

我们想知道我们的朋友是否愿意和我们一起去公园下棋。但是这一次，我们想用决策树来寻找答案。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 分析

我们有数据样本的初始集合 *S* ，如下所示:

```
S={(Cold,Strong,Cloudy,No),(Warm,Strong,Cloudy,No),(Warm,None,Sunny,Yes), (Hot,None,Sunny,No),(Hot,Breeze,Cloudy,Yes),(Warm,Breeze,Sunny,Yes),(Cold,Breeze,Cloudy,No),(Cold,None,Sunny,Yes),(Hot,Strong,Cloudy,Yes),(Warm,None,Cloudy,Yes)}
```

首先，我们确定三个未分类属性的信息增益:`temperature`、`wind`和`sunshine`。`temperature`的可能值为`Cold`、`Warm`和`Hot`。因此，我们将集合 *S* 划分为三个集合:

```
Scold={(Cold,Strong,Cloudy,No),(Cold,Breeze,Cloudy,No),(Cold,None,Sunny,Yes)}
Swarm={(Warm,Strong,Cloudy,No),(Warm,None,Sunny,Yes),(Warm,Breeze,Sunny,Yes),(Warm,None,Cloudy,Yes)}
Shot={(Hot,None,Sunny,No),(Hot,Breeze,Cloudy,Yes),(Hot,Strong,Cloudy,Yes)}
```

我们首先计算集合的信息熵:

![](Images/b6d23d69-5c62-455a-808f-83f5f3a7ba2b.png)

![](Images/9d5fdd96-20e6-42be-a66f-168df9b6fba5.png)

![](Images/73ac61fc-b892-4050-9542-6cae5c95cf0d.png)

![](Images/55f6d338-236f-4860-bcfc-a6afa7c374e3.png)

![](Images/04be8669-816b-4abf-b8a9-84f08fa5d441.png)

![](Images/1bc93be0-e80e-4478-a7eb-7770febac121.png)

![](Images/5cc2ba99-5f78-406c-9384-04a1f5897775.png)

`wind`属性的可能值是`None`、`Breeze`和`Strong`。因此，我们将把集合 *S* 分成三个分区:

![](Images/913e09cf-1f4a-4838-a44d-7e386857de33.png)

![](Images/21c1f8f6-ef95-446f-8f5b-0ef3181db4af.png)

![](Images/050cedb3-7fc0-4d70-aced-ccd53cc97b6a.png)

集合的信息熵如下:

![](Images/29b98f94-6d23-4d88-be49-85cb9528fa23.png)

![](Images/25ec3d85-19a3-4732-9cbe-b70e8bc0361b.png)

![](Images/03d4f9db-4723-4d5f-b1c6-603be3bcef37.png)

![](Images/66ec446d-ffd5-4028-b7ed-485fabe7551b.png)

![](Images/7f61d2d0-fb0c-4ac0-a6bf-c9dd108c8bf0.png)

![](Images/c30c697d-c7f2-4338-86c5-74c992d534b1.png)

最后，第三个属性`Sunshine`，有两个可能的值`Cloudy`和`Sunny`。因此，它将集合 *S* 分成两个集合:

![](Images/db8b17d8-8329-4731-9ca0-520393b753d8.png)

![](Images/9f9c929a-dd35-49a6-8e98-3646356ffa2b.png)

集合的熵如下:

![](Images/74517c8e-9378-421b-841a-ba76bc3fbd31.png)

![](Images/0013e3ef-8416-470d-a331-41774c28103f.png)

![](Images/b022dcbf-097b-4106-b005-385de8861495.png)

![](Images/b5b3caaa-c2e7-4042-849b-82707ebfb928.png)

*IG(S，风)**IG(S，温度)*大于 *IG(S，阳光)*。两者是平等的；因此，我们可以选择任何属性来形成这三个分支；比如第一个，`Temperatur` `e`。在这种情况下，三个分支中的每一个都将具有数据样本 *S [冷]* 、 *S [暖]* 和 *S [热]* 。在这些分支上，我们可以进一步应用算法来形成决策树的其余部分。相反，我们将使用程序来完成它:

**输入**:

```
source_code/3/chess.csv
Temperature,Wind,Sunshine,Play
Cold,Strong,Cloudy,No
Warm,Strong,Cloudy,No
Warm,None,Sunny,Yes
Hot,None,Sunny,No
Hot,Breeze,Cloudy,Yes
Warm,Breeze,Sunny,Yes
Cold,Breeze,Cloudy,No
Cold,None,Sunny,Yes
Hot,Strong,Cloudy,Yes
Warm,None,Cloudy,Yes
```

**输出**:

```
$ python construct_decision_tree.py chess.csv 0
Root
├── [Temperature=Cold]
│ ├── [Wind=Breeze]
│ │ └── [Play=No]
│ ├── [Wind=Strong]
│ │ └── [Play=No]
│ └── [Wind=None]
│   └── [Play=Yes]
├── [Temperature=Warm]
│ ├── [Wind=Breeze]
│ │ └── [Play=Yes]
│ ├── [Wind=None]
│ │ ├── [Sunshine=Sunny]
│ │ │ └── [Play=Yes]
│ │ └── [Sunshine=Cloudy]
│ │   └── [Play=Yes]
│ └── [Wind=Strong]
│   └── [Play=No]
└── [Temperature=Hot]
    ├── [Wind=Strong]
    │ └── [Play=Yes]
    ├── [Wind=None]
    │ └── [Play=No]
    └── [Wind=Breeze]
        └── [Play=Yes]
```

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 分类

现在我们已经构建了决策树，我们想用它来分类一个数据样本*(温暖，强壮，阳光，？)*进入*{否，是}* 中的两个班之一。

我们从根开始。在该实例中,`temperature`属性有什么值？`Warm`，所以我们去中间分支。在这种情况下，`wind`属性有什么值呢？`Strong`，所以实例将属于类`No`，因为我们已经到达了叶节点。

因此，根据决策树分类算法，我们的朋友不会想和我们在公园里下棋。请注意，朴素贝叶斯算法另有说明。要选择最好的方法，需要了解这个问题。在其他时候，更准确的方法是考虑几个算法或几个分类器的结果，如在第 4 章[、*随机森林*中的随机森林算法。](0a6e5d42-ab32-49c4-934f-7f1954eb1a25.xhtml)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 购物——处理数据不一致

关于我们的朋友 Jane 的购物偏好，我们有以下数据:

| **温度** | **下雨** | **购物** |
| 寒冷 | 没有人 | 是 |
| 温暖的 | 没有人 | 不 |
| 寒冷 | 强烈的 | 是 |
| 寒冷 | 没有人 | 不 |
| 温暖的 | 强烈的 | 不 |
| 温暖的 | 没有人 | 是 |
| 寒冷 | 没有人 | ？ |

我们想使用决策树来找出，如果外面的温度很冷并且没有下雨，简是否会去购物。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 分析

这里，我们应该小心，因为有些数据的实例对于相同的属性具有相同的值，但是属于不同的类；也就是`(cold,none,yes)`和`(cold,none,no)`。我们制作的程序将形成下面的决策树:

```
    Root
    ├── [Temperature=Cold]
    │    ├──[Rain=None]
    │    │    └──[Shopping=Yes]
    │    └──[Rain=Strong]
    │         └──[Shopping=Yes]
    └── [Temperature=Warm]
         ├──[Rain=None]
         │    └──[Shopping=No]
         └── [Rain=Strong]
              └── [Shopping=No]

```

但是在具有父节点`[Temperature=Cold]`的叶节点`[Rain=None]`处，有两个具有两个类的数据样本`no`和`yes`。因此，我们无法准确地对一个实例`(cold,none,?)`进行分类。为了让决策树算法更好地工作，我们必须在叶节点提供一个权重最大的类，也就是多数类。更好的方法是为数据样本收集更多属性的值，以便我们可以更准确地做出决策。

因此，根据现有的数据，我们不确定简是否会去购物。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 摘要

在本章中，我们了解了决策树 ID3 算法如何首先从输入数据构建决策树，然后使用构建的树对新的数据实例进行分类。决策树是通过选择具有最高信息增益的分支属性来构建的。我们研究了信息增益是如何根据信息熵的增益来衡量可以学习的信息量的。

我们还了解到决策树算法可以获得与其他算法不同的结果，例如朴素贝叶斯算法。

在下一章中，我们将学习如何将各种算法或分类器组合成一个决策森林(称为**随机森林**)，以便获得更准确的结果。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 问题

**问题 1** :以下多重集的信息熵是多少？
a) {1，2}，b) {1，2，3}，c) {1，2，3，4}，d) {1，1，2，2}，e) {1，1，2，3}

**问题二**:偏向硬币以 10%的概率显示正面，以 90%的概率显示反面，诱导出的概率空间的信息熵是多少？

**问题 3** :再举一个来自[第二章](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml)、*朴素贝叶斯*的下棋例子:

a)表中每个未分类属性的信息增益是多少？

b)从给定的表中构造的决策树是什么？

c)如何根据构建的决策树对数据样本`(Warm,Strong,Spring,?)`进行分类？

| **温度** | **风** | **赛季** | **播放** |
| 寒冷 | 强烈的 | 冬天 | 不 |
| 温暖的 | 强烈的 | 秋天 | 不 |
| 温暖的 | 没有人 | 夏天 | 是 |
| 热的 | 没有人 | 春天 | 不 |
| 热的 | 微风 | 秋天 | 是 |
| 温暖的 | 微风 | 春天 | 是 |
| 寒冷 | 微风 | 冬天 | 不 |
| 寒冷 | 没有人 | 春天 | 是 |
| 热的 | 强烈的 | 夏天 | 是 |
| 温暖的 | 没有人 | 秋天 | 是 |
| 温暖的 | 强烈的 | 春天 | ？ |

**问题 4** : **玛丽和温度偏好**:让我们以[第一章](e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml)，*使用 K 最近邻分类*为例，关于玛丽的温度偏好:

| **摄氏度温度** | **以公里/小时为单位的风速** | **玛丽的感知** |
| Ten | Zero | 寒冷 |
| Twenty-five | Zero | 温暖的 |
| Fifteen | five | 寒冷 |
| Twenty | three | 温暖的 |
| Eighteen | seven | 寒冷 |
| Twenty | Ten | 寒冷 |
| Twenty-two | five | 温暖的 |
| Twenty-four | six | 温暖的 |

我们希望使用决策树来判断我们的朋友 Mary 在一个温度为 16°C、风速为 3 km/h 的房间里会感到温暖还是寒冷。

你能解释一下如何在这里使用决策树算法，以及在这个例子中使用它有多大好处吗？

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 分析

**问题 1** :这里是多重集的熵:

a) ![](Images/7f1d570b-442a-4461-bcf2-a3969e95bac8.png)

b) ![](Images/cdb58fb4-3f36-4ea9-84ae-99a1d32285d3.png)

c) ![](Images/7912ec03-379f-4fc1-9d4d-d9e98db19757.png)

d) ![](Images/20279a9c-d349-4c0b-bf66-d277eb2ff08c.png)

e) ![](Images/4b287548-1003-4188-aced-deabb5645e5b.png)

请注意，具有两个以上类的多重集的信息熵大于 1，因此我们需要不止一位的信息来表示结果。但是，对于每一个包含两类以上元素的多重集来说，情况都是这样吗？

**问题二** : *![](Images/bcd665ff-2e2f-444c-9b5f-bc9e17a3eae3.png)*

**问题 3** : a)三个属性的信息增益如下:

![](Images/7a94df6b-1916-46b4-a830-dc6ce3356092.png)

![](Images/43db9692-58e3-423a-9431-040c14eee8ac.png)

![](Images/a1267b6c-2d2e-450c-a578-38aec73de774.png)

b)因此，我们将选择`season`属性从根节点分支，因为它具有最高的信息增益。或者，我们可以将所有输入数据输入到程序中来构建一个决策树，如下所示:

```
    Root
    ├── [Season=Autumn]
    │    ├──[Wind=Breeze]
    │    │    └──[Play=Yes]
    │    ├──[Wind=Strong]
    │    │    └──[Play=No]
    │    └──[Wind=None]
    │         └──[Play=Yes]
    ├── [Season=Summer]
    │    ├──[Temperature=Hot]
    │    │    └──[Play=Yes]
    │    └──[Temperature=Warm]
    │         └──[Play=Yes]
    ├── [Season=Winter]
    │    └──[Play=No]
    └── [Season=Spring]
         ├── [Temperature=Hot]
         │    └──[Play=No]
         ├── [Temperature=Warm]
         │    └──[Play=Yes]
         └── [Temperature=Cold]
              └── [Play=Yes]

```

c)根据构建的决策树，我们将通过从根节点到最底层分支，然后通过中间分支到达叶节点，将数据样本`(warm,strong,spring,?)`分类到类别`Play=Yes`。

**问题 4** :这里，如果不对数据进行任何处理，决策树算法可能不会执行得那么好。如果我们考虑每一类温度，那么 25°C 仍然不会出现在决策树中，因为它不在输入数据中，所以我们无法对 Mary 在 16°C 和 3 km/h 风速下的感觉进行分类。

我们也可以将温度和风速划分为不同的区间，以减少类别，这样得到的决策树就可以对输入实例进行分类。但正是这种划分，即 25°C 和 3 km/h 应划分的区间，是这类问题分析程序的基本部分。因此，没有任何重大修改的决策树不能很好地分析问题。