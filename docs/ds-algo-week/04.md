

# 四、随机森林

随机森林是一组随机决策树(类似于第 3 章、*决策树*中描述的那些)，每个决策树都是在随机数据子集上生成的。随机森林对属于由大多数随机决策树投票选出的类别的要素进行分类。与决策树相比，随机森林倾向于提供更准确的特征分类，因为它们降低了偏差和方差。

在本章中，我们将讨论以下主题:

*   作为随机森林构建的一部分的树打包(或引导聚集)技术，但也可以扩展到数据科学中的其他算法和方法，以减少偏差和方差，从而提高准确性
*   如何构建随机森林并使用通过游泳偏好示例构建的随机森林对数据项进行分类
*   如何用 Python 实现一个构建随机森林的算法
*   用朴素贝叶斯算法分析问题的区别，用决策树和随机森林下棋的例子
*   随机森林算法如何克服决策树算法的缺点，并以购物为例超越决策树算法
*   以购物为例，随机森林如何表达其要素分类的置信度
*   在*问题*部分，减少分类器的方差如何产生更准确的结果



# 随机森林算法简介

一般来说，为了构建一个随机森林，首先我们必须选择它将包含的树的数量。随机森林不会过度拟合(除非数据非常嘈杂)，因此选择许多决策树不会降低预测的准确性。随机森林不会过度拟合(除非数据非常嘈杂)，因此拥有更多数量的决策树不会降低预测的准确性。拥有足够数量的决策树非常重要，这样当随机选择构建决策树时，更多的数据将用于分类目的。另一方面，决策树越多，需要的计算能力就越大。此外，增加决策树的数量并不能显著提高分类的准确性。

在实践中，您可以在特定数量的决策树上运行该算法，增加它们的数量，并比较较小和较大森林的分类结果。如果结果非常相似，那么没有理由增加树的数量。

为了简化演示，在本书中，我们将在随机森林中使用少量决策树。



# 随机森林构造概述

我们将描述每棵树是如何随机构造的。我们通过随机选择 *N* 个训练特征来构建决策树。这种随机选择数据并替换每棵树的过程被称为**引导聚集**，或**树打包**。自举聚合的目的是减少分类结果中的方差和偏差。

假设一个特征有 *M 个*变量，这些变量用于使用决策树对该特征进行分类。在 ID3 算法中，当我们必须在一个节点做出分支决策时，我们选择导致最高信息增益的变量。这里，在随机决策树中，在每个节点，我们最多只考虑 *m* 个变量。我们不考虑那些已经被选择的，没有从给定的 *M* 变量中进行任何替换的随机抽样。然后，在这些 *m* 变量中，我们选择导致最高信息增益的一个。

构建随机决策树的其余部分就像在[第 3 章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)、*决策树*中的决策树一样进行。



# 游泳偏好——涉及随机森林的分析

我们将使用[第 3 章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)、*决策树*中关于游泳偏好的例子。我们有相同的数据表，如下:

| **游泳衣** | **水温** | **游泳偏好** |
| 没有人 | 寒冷 | 不 |
| 没有人 | 温暖的 | 不 |
| 小的 | 寒冷 | 不 |
| 小的 | 温暖的 | 不 |
| 好的 | 寒冷 | 不 |
| 好的 | 温暖的 | 是 |

我们希望从这些数据中构建一个随机森林，并使用它来对项目`(Good,Cold,?)`进行分类。



# 分析

给我们 *M=3* 个变量，根据这些变量可以对一个特征进行分类。在随机森林算法中，我们通常不使用所有三个变量在每个节点形成树分支。我们只使用来自 *M* 的变量的子集( *m* )。所以我们选择 *m* 使得 *m* 小于或者等于 *M* 。 *m* 越大，每个构建的树中的分类器越强。然而，如前所述，更多的数据导致更多的偏见。但是，因为我们使用多个树(具有较低的 *m* )，即使每个构建的树都是弱分类器，它们的组合分类精度也很高。由于我们想要减少随机森林中的偏差，我们可能想要考虑选择比 *M* 稍小的 *m* 参数。

因此，我们选择在节点上考虑的变量的最大数量: *m=min(M，math . ceil(2 * math . sqrt(M)))= min(M，math . ceil(2 * math . sqrt(3)))= 3*。

我们被赋予以下特征:

```py
[['None', 'Cold', 'No'], ['None', 'Warm', 'No'], ['Small', 'Cold', 'No'], ['Small', 'Warm', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Warm', 'Yes']]
```

当构建随机决策树作为随机森林的一部分时，我们将随机选择这些特征的一个子集，以及它们的替换。



# 随机森林构建

我们将构建一个由两个随机决策树组成的随机森林。



# 0 号随机决策树的构造

我们有六个特征作为输入数据。在这些特征中，我们随机选择六个特征，并替换构建这个随机决策树:

```py
[['None', 'Warm', 'No'], ['None', 'Warm', 'No'], ['Small', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]
```

我们从根节点开始构建，创建树的第一个节点。我们希望向[根]节点添加子节点。

我们有以下可用的变量:`['swimming_suit', 'water_temperature']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。在这些变量中，信息增益最高的是游泳衣。

因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用`swimming_suit`变量，我们将当前节点中的数据划分如下:

*   `swimming_suit=Small: [['Small', 'Cold', 'No']]`的分区
*   `swimming_suit=None: [['None', 'Warm', 'No'], ['None', 'Warm', 'No']]`的分区
*   `swimming_suit=Good: [['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]`的分区

使用前面的分区，我们创建分支和子节点。

我们现在向`[root]`节点添加一个子节点`[swimming_suit=Small]`。这个分支对一个单一特征进行分类:`[['Small', 'Cold', 'No']]`。

我们希望向`[swimming_suit=Small]`节点添加子节点。

我们有以下可用的变量:`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`water_temperature`，所有剩余的特性都具有相同的值:`Cold`。所以，我们用一个叶子节点结束分支，加上`[swim=No]`。

我们现在向`[root]`节点添加一个子节点`[swimming_suit=None]`。这个分支划分了两个特征:`[['None', 'Warm', 'No'], ['None', 'Warm', 'No']]`。

我们希望向`[swimming_suit=None]`节点添加子节点。

我们有以下可用的变量:`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`water_temperature`，所有剩余的特性都具有相同的值:`Warm`。所以，我们用一个叶子节点结束分支，加上`[swim=No]`。

我们现在向`[root]`节点添加一个子节点`[swimming_suit=Good]`。这个分支分为三个特征:`[['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]`

我们希望向`[swimming_suit=Good]`节点添加子节点。

我们有以下可用的变量:`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`water_temperature`，所有剩余的特性都具有相同的值:`Cold`。所以，我们用一个叶子节点结束分支，加上`[swim=No]`。

现在，我们已经将所有子节点添加到了`[root]`节点。



# 1 号随机决策树的构造

我们有六个特征作为输入数据。在这些特征中，我们随机选择六个特征，并用替换来构建这个随机决策树:

```py
[['Good', 'Warm', 'Yes'], ['None', 'Warm', 'No'], ['Good', 'Cold', 'No'], ['None', 'Cold', 'No'], ['None', 'Warm', 'No'], ['Small', 'Warm', 'No']]
```

构建 1 号随机决策树的其余部分类似于之前的 0 号随机决策树的构建。唯一的区别是，该树是使用初始数据的一个不同的随机生成的子集(如前所述)构建的。

我们从根节点开始构建，创建树的第一个节点。我们希望向`[root]`节点添加子节点。

我们有以下可用的变量:`['swimming_suit', 'water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。在这些变量中，信息增益最高的是`swimming_suit`变量。

因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用`swimming_suit`变量，我们将当前节点中的数据划分如下:

*   `swimming_suit=Small: [['Small', 'Warm', 'No']]`的分区
*   `swimming_suit=None: [['None', 'Warm', 'No'], ['None', 'Cold', 'No'], ['None', 'Warm', 'No']]`的分区
*   `swimming_suit=Good: [['Good', 'Warm', 'Yes'], ['Good', 'Cold', 'No']]`的分区

现在，给定分区，让我们创建分支和子节点。我们向`[root]`节点添加一个子节点`[swimming_suit=Small]`。这个分支对一个单一特征进行分类:`[['Small', 'Warm', 'No']]`。

我们希望向`[swimming_suit=Small]`节点添加子节点。

我们有以下可用的变量:`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`water_temperature`，所有剩余的特性都具有相同的值:`Warm`。所以，我们用一个叶子节点结束分支，加上`[swim=No]`。

我们向`[root]`节点添加一个子节点`[swimming_suit=None]`。这个分支分为三个特征:`[['None', 'Warm', 'No']`、`['None', 'Cold', 'No']`和`['None', 'Warm', 'No']]`。

我们希望向`[swimming_suit=None]`节点添加子节点。

我们有以下可用的变量:`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用`water temperature`变量，我们将当前节点中的数据划分如下:

*   `water_temperature=Cold: [['None', 'Cold', 'No']]`的分区
*   分区为`water_temperature=Warm: [['None', 'Warm', 'No'], ['None', 'Warm', 'No']]`；现在，给定分区，让我们创建分支和子节点

我们向`[swimming_suit=None]`节点添加一个子节点`[water_temperature=Cold]`。这个分支对一个单一特征进行分类:`[['None', 'Cold', 'No']]`。

我们没有任何可用的变量来进一步分割节点；因此，我们向树的当前分支添加一个叶节点。我们添加了`[swim=No]`叶节点。我们向`[swimming_suit=None]`节点添加一个子节点`[water_temperature=Warm]`。这个分支分为两个特征:`[['None', 'Warm', 'No']`和`['None', 'Warm', 'No']]`。

我们没有任何可用的变量来进一步分割节点；因此，我们向树的当前分支添加一个叶节点。我们添加了`[swim=No]`叶节点。

现在，我们已经将所有子节点添加到了`[swimming_suit=None]`节点。

我们向`[root]`节点添加一个子节点`[swimming_suit=Good]`。这个分支分为两个特征:`[['Good', 'Warm', 'Yes']`和`['Good', 'Cold', 'No']]`

我们希望向`[swimming_suit=Good]`节点添加子节点。

我们有以下可用的变量:`['water_temperature']`。由于这里只有一个变量，它小于`m=3`参数，我们将考虑这个变量。信息增益最高的是`water_temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用`water temperature`变量，我们将当前节点中的数据划分如下:

*   `water_temperature=Cold: [['Good', 'Cold', 'No']]`的分区
*   `water_temperature=Warm: [['Good', 'Warm', 'Yes']]`的分区

现在，给定分区，让我们创建分支和子节点。

我们向`[swimming_suit=Good]`节点添加一个子节点`[water_temperature=Cold]`。该分支对单一特征进行分类:`[['Good', 'Cold', 'No']]`

我们没有任何可用的变量来进一步分割节点；因此，我们向树的当前分支添加一个叶节点。我们添加了`[swim=No]`叶节点。

我们向`[swimming_suit=Good]`节点添加一个子节点`[water_temperature=Warm]`。这个分支对一个单一特征进行分类:`[['Good', 'Warm', 'Yes']]`。

我们没有任何可用的变量来进一步分割节点；因此，我们向树的当前分支添加一个叶节点。我们添加了`[swim=Yes]`叶节点。

现在，我们已经添加了`[swimming_suit=Good]`节点的所有子节点。

我们还向`[root]`节点添加了所有的子节点。



# 构造随机森林

我们已经完成了由两个随机决策树组成的随机森林的构建，如下面的代码块所示:

```py
Tree 0:
    Root
    ├── [swimming_suit=Small]
    │ └── [swim=No]
    ├── [swimming_suit=None]
    │ └── [swim=No]
    └── [swimming_suit=Good]
      └── [swim=No]
Tree 1:
    Root
    ├── [swimming_suit=Small]
    │ └── [swim=No]
    ├── [swimming_suit=None]
    │ ├── [water_temperature=Cold]
    │ │ └── [swim=No]
    │ └──[water_temperature=Warm]
    │   └── [swim=No]
    └── [swimming_suit=Good]
      ├──  [water_temperature=Cold] 
      │ └── [swim=No]
      └──  [water_temperature=Warm]
        └── [swim=Yes]
The total number of trees in the random forest=2.
The maximum number of the variables considered at the node is m=3.
```



# 使用随机森林分类

因为我们仅使用原始数据的子集来构建随机决策树，所以我们可能没有足够的特征来形成能够对每个特征进行分类的完整树。在这种情况下，树不会为应该分类的要素返回任何类。因此，我们将只考虑对特定类的特征进行分类的树。

我们要分类的特征是`['Good', 'Cold', '?']`。随机决策树投票选出给定特征的类别，使用与[第 3 章](a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml)、*决策树*中相同的特征分类方法。树 0 票投给了`No`类。树 1 票投给了`No`阶层。票数最多的班级是`No`。因此，构建的随机森林根据类别`No`对特征`['Good', 'Cold', '?']`进行分类。



# 随机森林算法的实现

我们使用上一章中改进的决策树算法实现了一个随机森林算法。我们还添加了一个选项来设置程序中的详细模式，该模式可以描述算法如何对特定输入进行操作的整个过程——如何使用随机决策树构建随机森林，以及如何使用构建的随机森林对其他特征进行分类。

建议您参考上一章的`decision_tree.construct_general_tree`功能:

```py
# source_code/4/random_forest.py import mathimport randomimport sys
sys.path.append('../common')
import common # noqa
import decision_tree # noqa
from common import printfv # noqa

#Random forest construction
def sample_with_replacement(population, size):
    sample = []
    for i in range(0, size):
        sample.append(population[random.randint(0, len(population) - 1)])
    return sample

def construct_random_forest(verbose, heading, complete_data,
 enquired_column, m, tree_count):
    printfv(2, verbose, "*** Random Forest construction ***\n")
    printfv(2, verbose, "We construct a random forest that will " +
            "consist of %d random decision trees.\n", tree_count)
    random_forest = []
    for i in range(0, tree_count):
        printfv(2, verbose, "\nConstruction of a random " +
                "decision tree number %d:\n", i)
        random_forest.append(construct_random_decision_tree(
            verbose, heading, complete_data, enquired_column, m))
    printfv(2, verbose, "\nTherefore we have completed the " +
            "construction of the random forest consisting of %d " +
            "random decision trees.\n", tree_count)
    return random_forest

def construct_random_decision_tree(verbose, heading, complete_data,
 enquired_column, m):
    sample = sample_with_replacement(complete_data, len(complete_data))
    printfv(2, verbose, "We are given %d features as the input data. " +
            "Out of these, we choose randomly %d features with the " +
            "replacement that we will use for the construction of " +
            "this particular random decision tree:\n" +
            str(sample) + "\n", len(complete_data),
            len(complete_data))
# The function construct_general_tree from the module decision_tree
# is written in the implementation section in the previous chapter
# on decision trees.
    return decision_tree.construct_general_tree(verbose, heading,
                                                sample,
                                                enquired_column, m)

# M is the given number of the decision variables, i.e. properties
# of one feature.
def choose_m(verbose, M):
    m = int(min(M, math.ceil(2 * math.sqrt(M))))
    printfv(2, verbose, "We are given M=" + str(M) +
            " variables according to which a feature can be " +
            "classified. ")
    printfv(3, verbose, "In random forest algorithm we usually do " +
            "not use all " + str(M) + " variables to form tree " +
            "branches at each node. ")
    printfv(3, verbose, "We use only m variables out of M. ")
    printfv(3, verbose, "So we choose m such that m is less than or " +
            "equal to M. ")
    printfv(3, verbose, "The greater m is, a stronger classifier an " +
            "individual tree constructed is. However, it is more " +
            "susceptible to a bias as more of the data is considered. " +
            "Since we in the end use multiple trees, even if each may " +
            "be a weak classifier, their combined classification " +
            "accuracy is strong. Therefore as we want to reduce a " +
            "bias in a random forest, we may want to consider to " +
            "choose a parameter m to be slightly less than M.\n")
    printfv(2, verbose, "Thus we choose the maximum number of the " +
            "variables considered at the node to be " +
            "m=min(M,math.ceil(2*math.sqrt(M)))" +
            "=min(M,math.ceil(2*math.sqrt(%d)))=%d.\n", M, m)
    return m

#Classification
def display_classification(verbose, random_forest, heading,
 enquired_column, incomplete_data):
    if len(incomplete_data) == 0:
        printfv(0, verbose, "No data to classify.\n")
    else:
        for incomplete_feature in incomplete_data:
            printfv(0, verbose, "\nFeature: " +
                    str(incomplete_feature) + "\n")
            display_classification_for_feature(
                verbose, random_forest, heading,
                enquired_column, incomplete_feature)

def display_classification_for_feature(verbose, random_forest, heading,
 enquired_column, feature):
    classification = {}
    for i in range(0, len(random_forest)):
        group = decision_tree.classify_by_tree(
            random_forest[i], heading, enquired_column, feature)
        common.dic_inc(classification, group)
        printfv(0, verbose, "Tree " + str(i) +
                " votes for the class: " + str(group) + "\n")
    printfv(0, verbose, "The class with the maximum number of votes " +
            "is '" + str(common.dic_key_max_count(classification)) +
            "'. Thus the constructed random forest classifies the " +
            "feature " + str(feature) + " into the class '" +
            str(common.dic_key_max_count(classification)) + "'.\n")

# Program start
csv_file_name = sys.argv[1]
tree_count = int(sys.argv[2])
verbose = int(sys.argv[3])

(heading, complete_data, incomplete_data,
 enquired_column) = common.csv_file_to_ordered_data(csv_file_name)
m = choose_m(verbose, len(heading))
random_forest = construct_random_forest(
    verbose, heading, complete_data, enquired_column, m, tree_count)
display_forest(verbose, random_forest)
display_classification(verbose, random_forest, heading,
                       enquired_column, incomplete_data)
```

**输入**:

作为实现的算法的输入文件，我们提供来自游泳偏好示例的数据:

```py
# source_code/4/swim.csv
swimming_suit,water_temperature,swim
None,Cold,No
None,Warm,No
Small,Cold,No
Small,Warm,No
Good,Cold,No
Good,Warm,Yes
Good,Cold,?
```

**输出**:

我们在命令行中键入以下命令来获得输出:

```py
$ python random_forest.py swim.csv 2 3 > swim.out
```

`2`表示我们要构造两个决策树，`3`是程序的详细程度，包括随机森林的构造、特征的分类以及随机森林的图形的详细说明。最后一部分，`> swim.out`，意味着输出被写到`swim.out`文件中。这个文件可以在`source_code/4`章节目录中找到。这个程序的输出以前被用来写游泳偏好问题的分析。



# 下棋的例子

我们将再次使用来自第 2 章、*朴素贝叶斯、*和[第 3 章](4f3fafce-e9a1-4593-bd9d-847a94cde2bf.xhtml)、*决策树*的示例，如下所示:

| **温度** | **风** | **阳光** | **播放** |
| 寒冷 | 强烈的 | 多云的 | 不 |
| 温暖的 | 强烈的 | 多云的 | 不 |
| 温暖的 | 没有人 | 快活的 | 是 |
| 热的 | 没有人 | 快活的 | 不 |
| 热的 | 微风 | 多云的 | 是 |
| 温暖的 | 微风 | 快活的 | 是 |
| 寒冷 | 微风 | 多云的 | 不 |
| 寒冷 | 没有人 | 快活的 | 是 |
| 热的 | 强烈的 | 多云的 | 是 |
| 温暖的 | 没有人 | 多云的 | 是 |
| 温暖的 | 强烈的 | 快活的 | ？ |

然而，我们希望使用由四个随机决策树组成的随机森林来找到分类的结果。



# 分析

我们被给定 *M=4* 个变量，从中可以对一个特征进行分类。因此，我们选择节点上考虑的变量的最大数量:

![](Images/67f7de1d-b134-48f2-9de2-fc61f30107c0.png)

我们被赋予以下特征:

```py
[['Cold', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'None', 'Sunny',
'Yes'], ['Hot', 'None', 'Sunny', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Warm', 'Breeze',
'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Hot', 'Strong', 'Cloudy', 'Yes'], ['Warm', 'None', 'Cloudy', 'Yes']]
```

当构建随机决策树作为随机森林的一部分时，我们将随机选择这些特征的一个子集，以及它们的替换。



# 随机森林构建

我们将构建一个由四个随机决策树组成的随机森林。

**构造 0 号随机决策树**

我们有 10 个特征作为输入数据。在这些特征中，我们随机选择所有特征及其替换特征来构建随机决策树:

```py
[['Warm', 'Strong', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Warm', 'Breeze', 'Sunny', 'Yes']]
```

我们从根节点开始构建，创建树的第一个节点。我们希望向`[root]`节点添加子节点。

我们有以下可用的变量:`['Temperature', 'Wind', 'Sunshine']`。因为它们比`m=4`参数少，所以我们考虑所有的参数。在这些变量中，信息增益最高的是`Temperature`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用`Temperature`变量，我们将当前节点中的数据划分如下:

*   `Temperature=Cold: [['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No']]`的分区
*   `Temperature=Warm: [['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Breeze', 'Sunny', 'Yes']]`的分区
*   `Temperature=Hot: [['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes']]`的分区

现在，给定分区，让我们创建分支和子节点。

我们向`[root]`节点添加一个子节点`[Temperature=Cold]`。这个分支分为四个特征:`[['Cold', 'Breeze', 'Cloudy', 'No']`、`['Cold', 'None', 'Sunny', 'Yes']`、`['Cold', 'Breeze', 'Cloudy', 'No'],`和` ['Cold', 'Breeze', 'Cloudy', 'No']]`。

我们希望向`[Temperature=Cold]`节点添加子节点。

我们有以下可用的变量:`['Wind', 'Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Wind`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用 water `Wind`变量，我们将当前节点中的数据划分如下:

*   `Wind=None: [['Cold', 'None', 'Sunny', 'Yes']]`的分区
*   `Wind=Breeze: [['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No']]`的分区

现在，给定分区，让我们创建分支和子节点。

我们向`[Temperature=Cold]`节点添加一个子节点`[Wind=None]`。这个分支对一个单一特征进行分类:`[['Cold', 'None', 'Sunny', 'Yes']]`。

我们希望向`[Wind=None]`节点添加子节点。

我们有以下变量:`available['Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Sunshine`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`Sunshine`，所有剩余的特性都具有相同的值:`Sunny`。所以，我们用一个叶子节点结束分支，加上`[Play=Yes]`。

我们向`[Temperature=Cold]`节点添加一个子节点`[Wind=Breeze]`。这个分支分为三个特征:`[['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'],`和` ['Cold', 'Breeze', 'Cloudy', 'No']]`。

我们希望向`[Wind=Breeze]`节点添加子节点。

我们有以下可用的变量:`['Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Sunshine`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选的变量“阳光”,所有剩余的要素都具有相同的值:多云。所以，我们用一个叶节点结束分支，加上[Play=No]。

现在，我们已经为`[Temperature=Cold]`节点添加了所有的子节点。

我们向`[root]`节点添加一个子节点`[Temperature=Warm]`。这个分支分为三个特征:`[['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], `和` ['Warm', 'Breeze', 'Sunny', 'Yes']]`。

我们希望向`[Temperature=Warm]`节点添加子节点。

我们剩下的可用变量是`['Wind', 'Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Wind`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。使用`Wind`变量，我们对当前节点中的数据进行分区，其中数据的每个分区将用于来自当前节点`[Temperature=Warm]`的一个新分支。我们有以下分区:

*   `Wind=Breeze: [['Warm', 'Breeze', 'Sunny', 'Yes']]`的分区
*   `Wind=Strong: [['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No']]`的分区

现在，给定分区，让我们形成分支和子节点。

我们向`[Temperature=Warm]`节点添加一个子节点`[Wind=Breeze]`。这个分支对一个单一特征进行分类:`[['Warm', 'Breeze', 'Sunny', 'Yes']]`。

我们希望向`[Wind=Breeze]`节点添加子节点。

我们有以下可用的变量:`['Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Sunshine`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`Sunshine`，所有剩余的特性都具有相同的值:`Sunny`。所以，我们用一个叶子节点结束分支，加上`[Play=Yes]`。

我们向`[Temperature=Warm]`节点添加一个子节点`[Wind=Strong]`。这个分支分为两个特征:`[['Warm', 'Strong', 'Cloudy', 'No'],`和` ['Warm', 'Strong', 'Cloudy', 'No']]`

我们希望向`[Wind=Strong]`节点添加子节点。

我们有以下可用的变量:`['Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Sunshine`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`Sunshine`，所有剩余的特性都具有相同的值:`Cloudy`。所以，我们用一个叶子节点结束分支，加上`[Play=No]`。

现在，我们已经将所有子节点添加到了`[Temperature=Warm]`节点。

我们向`[root]`节点添加一个子节点`[Temperature=Hot]`。这个分支分为三个特征:`[['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'],`和` ['Hot', 'Breeze', 'Cloudy', 'Yes']]`。

我们希望向`[Temperature=Hot]`节点添加子节点。

我们有以下可用的变量:`['Wind', 'Sunshine']`。因为这些参数比`m=3`参数少，所以我们两个都考虑。信息增益最高的是`Wind`变量。因此，我们将在这个变量上进一步分支节点。我们还将从当前节点的子节点的可用变量列表中删除该变量。对于所选择的变量`Wind`，所有剩余的特性都具有相同的值:`Breeze`。所以，我们用一个叶子节点结束分支，加上`[Play=Yes]`。

现在，我们已经将所有子节点添加到了`[root]`节点。

**构建 1 号、2 号和 3 号随机决策树**

我们以类似的方式构建接下来的三棵树。我们应该注意到，由于这种构造是随机的，读者进行另一种正确的构造可能会得到不同的构造。然而，如果在随机森林中有足够数量的随机决策树，那么在所有随机构造中分类的结果应该是非常相似的。

完整的结构可以在程序输出的`source_code/4/chess.out`文件中找到。

**构建随机森林**:

```py
Tree 0:
Root
├── [Temperature=Cold]
│ ├── [Wind=None]
│ │ └── [Play=Yes]
│ └── [Wind=Breeze]
│   └── [Play=No]
├── [Temperature=Warm]
│ ├── [Wind=Breeze]
│ │ └── [Play=Yes]
│ └── [Wind=Strong]
│   └── [Play=No]
└── [Temperature=Hot]
  └── [Play=Yes]

Tree 1:
Root
├── [Wind=Breeze]
│ └── [Play=No]
├── [Wind=None]
│ ├── [Temperature=Cold]
│ │ └── [Play=Yes]
│ ├── [Temperature=Warm]
│ │ ├── [Sunshine=Sunny]
│ │ │ └── [Play=Yes]
│ │ └── [Sunshine=Cloudy]
│ │   └── [Play=Yes]
│ └── [Temperature=Hot]
│   └── [Play=No]
└── [Wind=Strong]
    ├── [Temperature=Cold]
    │ └── [Play=No]
    └── [Temperature=Warm]
        └── [Play=No]

Tree 2: 
    Root
    ├── [Wind=Strong]
    │ └── [Play=No]
    ├── [Wind=None]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=Yes]
    └── [Wind=Breeze]
      ├── [Temperature=Hot]
      │ └── [Play=Yes]
      └── [Temperature=Warm]
        └── [Play=Yes]

Tree 3:
    Root
    ├── [Temperature=Cold]
    │ └── [Play=No]
    ├── [Temperature=Warm]
    │ ├──[Wind=Strong]
    │ │ └──[Play=No]
    │ ├── [Wind=None]
    │ │ └── [Play=Yes]
    │ └──[Wind=Breeze]
    │   └── [Play=Yes]
    └── [Temperature=Hot]
      ├── [Wind=Strong]
      │ └── [Play=Yes]
      └── [Wind=Breeze]
        └── [Play=Yes]
The total number of trees in the random forest=4.
The maximum number of the variables considered at the node is m=4.
```



# 分类

给定构建的随机森林，我们对`['Warm', 'Strong', 'Sunny', '?']`特征进行分类:

*   **树 0 票为阶级** : `No`
*   **树 1 票为阶级** : `No`
*   **树类 2 票** : `No`
*   **树类 3 票** : `No`

票数最多的班级是`No`。因此，构建的随机森林将特征`['Warm', 'Strong', 'Sunny', '?']`分类到类别`No`。

**输入**:

为了执行前面的分析，我们使用本章前面实现的程序。首先，我们将表中的数据插入到以下 CSV 文件中:

```py
# source_code/4/chess.csv Temperature,Wind,Sunshine,Play  
Cold,Strong,Cloudy,No  
Warm,Strong,Cloudy,No  
Warm,None,Sunny,Yes  
Hot,None,Sunny,No  
Hot,Breeze,Cloudy,Yes  
Warm,Breeze,Sunny,Yes  
Cold,Breeze,Cloudy,No  
Cold,None,Sunny,Yes  
Hot,Strong,Cloudy,Yes  
Warm,None,Cloudy,Yes  
Warm,Strong,Sunny,? 
```

**输出**:

我们通过执行以下命令行来生成输出:

```py
$ python random_forest.py chess.csv 4 2 > chess.out
```

这里的数字`4`意味着我们想要构建四个决策树，而`2`是程序的详细程度，包括如何构建一个树的解释。最后一部分，`> chess.out`，意味着输出被写到`chess.out`文件中。这个文件可以在`source_code/4`章节目录中找到。我们不会把所有的输出都放在这里，因为它非常大而且重复。相反，它的一部分包含在前面的分析和随机森林的构建中。



# 去购物——克服随机数据的不一致性，并衡量信心水平

我们把前一章的问题接过来。我们有以下与我们的朋友 Jane 的购物偏好相关的数据:

| **温度** | **下雨** | **购物** |
| 寒冷 | 没有人 | 是 |
| 温暖的 | 没有人 | 不 |
| 寒冷 | 强烈的 | 是 |
| 寒冷 | 没有人 | 不 |
| 温暖的 | 强烈的 | 不 |
| 温暖的 | 没有人 | 是 |
| 寒冷 | 没有人 | ？ |

在前一章中，决策树无法对特征`(Cold, None)`进行分类。因此，这一次，我们将使用随机森林算法来确定如果温度很低并且没有下雨，Jane 是否会去购物。



# 分析

为了使用随机森林算法执行分析，我们使用实现的程序。

**输入**:

我们将表中的数据插入以下 CSV 文件:

```py
# source_code/4/shopping.csv Temperature,Rain,Shopping  
Cold,None,Yes  
Warm,None,No  
Cold,Strong,Yes  
Cold,None,No  
Warm,Strong,No  
Warm,None,Yes 
Cold,None,? 
```

**输出**:

为了获得更准确的结果，我们希望使用比前面的例子和解释中稍多的树。我们想用 20 棵树构建一个随机森林，输出为 0 级的低冗余度。因此，我们在终端中执行:

```py
$ python random_forest.py shopping.csv 20 0
***Classification***
Feature: ['Cold', 'None', '?'] 
Tree 0 votes for the class: Yes 
Tree 1 votes for the class: No 
Tree 2 votes for the class: No 
Tree 3 votes for the class: No 
Tree 4 votes for the class: No 
Tree 5 votes for the class: Yes 
Tree 6 votes for the class: Yes 
Tree 7 votes for the class: Yes 
Tree 8 votes for the class: No 
Tree 9 votes for the class: Yes 
Tree 10 votes for the class: Yes 
Tree 11 votes for the class: Yes 
Tree 12 votes for the class: Yes 
Tree 13 votes for the class: Yes 
Tree 14 votes for the class: Yes 
Tree 15 votes for the class: Yes 
Tree 16 votes for the class: Yes 
Tree 17 votes for the class: No 
Tree 18 votes for the class: No 
Tree 19 votes for the class: No
The class with the maximum number of votes is 'Yes'. Thus the constructed random forest classifies the feature ['Cold', 'None', '?'] into the class 'Yes'.

```

但是，我们要注意，20 棵树中只有 12 棵树投了答案`Yes`。因此，虽然我们有一个明确的答案，但它可能不是那么确定，类似于我们用普通决策树得到的结果。然而，不像在决策树中，由于数据不一致而没有产生答案，在这里，我们有一个答案。

此外，通过测量每个个体类别的投票能力的强度，我们可以测量答案正确的置信度。在这种情况下，特征`['Cold', 'None', '?']`属于置信度为 12/20 或 60%的`*Yes*`类。为了更精确地确定分类的确定性水平，将需要更大的随机决策树集合。



# 摘要

在本章中，我们了解到随机森林是一组决策树，其中每棵树都是从初始数据中随机选择的样本构建而成的。这个过程称为**引导聚合**。其目的是减少随机森林分类中的差异和偏差。在构建决策树的过程中，通过只考虑树的每个分支的变量的随机子集，进一步减少了偏差。

我们还了解到，一旦构建了随机森林，随机森林的分类结果就是随机森林中所有树的多数投票。多数人的水平也决定了答案正确的信心水平。

因为随机森林由决策树组成，所以在决策树是一个好选择的情况下，对每个问题使用它们是很好的。由于随机森林减少了决策树分类器中存在的偏差和方差，因此它们优于决策树算法。

在下一章，我们将学习将数据聚类成相似簇的技术。我们还将利用这种技术将数据分类到一个创建的集群中。



# 问题

**问题**1:我们从第二章、*朴素贝叶斯*来举下棋的例子。你会如何根据随机森林算法对一个`(Warm,Strong,Spring,?)`数据样本进行分类？

| **温度** | **风** | **赛季** | **播放** |
| 寒冷 | 强烈的 | 冬天 | 不 |
| 温暖的 | 强烈的 | 秋天 | 不 |
| 温暖的 | 没有人 | 夏天 | 是 |
| 热的 | 没有人 | 春天 | 不 |
| 热的 | 微风 | 秋天 | 是 |
| 温暖的 | 微风 | 春天 | 是 |
| 寒冷 | 微风 | 冬天 | 不 |
| 寒冷 | 没有人 | 春天 | 是 |
| 热的 | 强烈的 | 夏天 | 是 |
| 温暖的 | 没有人 | 秋天 | 是 |
| 温暖的 | 强烈的 | 春天 | ？ |

**问题 2** :只用一棵树和一个随机森林会是个好主意吗？证明你的答案。

**问题 3** :交叉验证能否改善随机森林分类的结果？证明你的答案。



# 分析

**问题 1:** 我们运行程序构建随机森林并对特征进行分类(`Warm, Strong, Spring`)。

**输入**:

```py
source_code/4/chess_with_seasons.csv Temperature,Wind,Season,Play  
Cold,Strong,Winter,No  
Warm,Strong,Autumn,No  
Warm,None,Summer,Yes  
Hot,None,Spring,No  
Hot,Breeze,Autumn,Yes  
Warm,Breeze,Spring,Yes  
Cold,Breeze,Winter,No  
Cold,None,Spring,Yes  
Hot,Strong,Summer,Yes  
Warm,None,Autumn,Yes  
Warm,Strong,Spring,? 
```

**输出**:

我们在随机森林中构建四棵树，如下所示:

```py
$ python random_forest.py chess_with_seasons.csv 4 2 > chess_with_seasons.out
```

整个构造和分析存储在`source_code/4/chess_with_seasons.out`文件中。由于随机因素的影响，你的结构可能会有所不同。从输出中，我们提取随机森林图，它由随机决策树组成，给定我们运行期间生成的随机数。

再次执行前面的命令很可能会导致不同的输出和不同的随机森林图。然而，由于随机决策树的多重性和它们的投票能力相结合，分类的结果很有可能是相似的。一个随机决策树的分类可能会有很大的差异。然而，多数投票结合了所有树的分类，从而减少了差异。为了验证您的理解，您可以将您的分类结果与以下随机森林图的分类进行比较。

**随机森林图及分类:**

让我们看看随机森林图的输出和特征的分类:

```py
Tree 0:
    Root
    ├── [Wind=None]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   ├── [Season=Autumn]
    │   │ └── [Play=Yes]
    │   └── [Season=Summer]
    │     └── [Play=Yes]
    └── [Wind=Strong]
      ├── [Temperature=Cold]
      │ └── [Play=No]
      └── [Temperature=Warm]
        └── [Play=No]
Tree 1:
    Root
    ├── [Season=Autumn]
    │ ├──[Wind=Strong]
    │ │ └──[Play=No]
    │ ├── [Wind=None]
    │ │ └── [Play=Yes]
    │ └──[Wind=Breeze]
    │   └── [Play=Yes]
    ├── [Season=Summer]
    │   └── [Play=Yes]
    ├── [Season=Winter]
    │   └── [Play=No]
    └── [Season=Spring]
      ├── [Temperature=Cold]
      │ └── [Play=Yes]
      └── [Temperature=Warm]
        └── [Play=Yes]

Tree 2:
    Root
    ├── [Season=Autumn]
    │ ├── [Temperature=Hot]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=No]
    ├── [Season=Spring]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=Yes]
    ├── [Season=Winter]
    │ └── [Play=No]
    └── [Season=Summer]
      ├── [Temperature=Hot]
      │ └── [Play=Yes]
      └── [Temperature=Warm]
        └── [Play=Yes]
Tree 3:
    Root
    ├── [Season=Autumn]
    │ ├──[Wind=Breeze]
    │ │ └── [Play=Yes]
    │ ├── [Wind=None]
    │ │ └── [Play=Yes]
    │ └──[Wind=Strong]
    │   └── [Play=No]
    ├── [Season=Spring]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=Yes]
    ├── [Season=Winter]
    │ └── [Play=No]
    └── [Season=Summer]
      └── [Play=Yes]
The total number of trees in the random forest=4.
The maximum number of the variables considered at the node is m=4.

Classication
Feature: ['Warm', 'Strong', 'Spring', '?']
Tree 0 votes for the class: No
Tree 1 votes for the class: Yes
Tree 2 votes for the class: Yes
Tree 3 votes for the class: Yes
The class with the maximum number of votes is 'Yes'. Thus the constructed random forest classifies the feature ['Warm', 'Strong', 'Spring', '?'] into the class 'Yes'.
```

**问题 2** :当我们在一个随机的森林中构建一棵树时，我们只使用数据的一个随机子集，并进行替换。这是为了消除分类器对某些特征的偏见。然而，如果我们仅使用一个树，则该树可能碰巧包含有偏差的特征，并且可能丢失使其能够提供准确分类的重要特征。因此，带有一个决策树的随机森林分类器可能会导致非常差的分类。因此，我们应该在随机森林中构建更多的决策树，以受益于分类中减少的偏差和方差。

**问题 3** :在交叉验证的时候，我们把数据分为训练和测试数据。训练数据用于训练分类器，测试数据用于评估哪些参数或方法最适合改进分类。交叉验证的另一个优势是减少偏差，因为我们只使用部分数据，从而减少了过度拟合特定数据集的机会。

然而，在决策森林中，我们以另一种方式解决交叉验证解决的问题。每个随机决策树只在数据的子集上构建，减少了过度拟合的机会。最后，分类是这些树的结果的组合。最终，最好的决定不是通过调整测试数据集上的参数来做出的，而是通过在减少偏差的情况下对所有树进行多数投票来做出的。

因此，决策森林算法的交叉验证没有多大用处，因为它已经是算法中固有的了。