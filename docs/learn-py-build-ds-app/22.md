        

# Serverless API Using Chalice

In the previous chapter, we created a REST API that served our prediction model forecasts by managing our own server and application. While that approach is by far the most popular, there is another that is also very useful for specific tasks—using serverless applications.

In this chapter, we will use the Chalice Python package to build an API that's similar to the one we built in [Chapter 18](e1d8b121-e8db-45da-9dbf-a034664926fa.xhtml), *Serving Models with a RESTful API*, but it will run in the cloud as a serverless application. Along the way, we will discuss along the way all the pros and cons of this approach.

In this chapter, we will learn about the following:

*   What a serverless application is
*   How to build a simple application using the Chalice package
*   How to mitigate Chalice's limitations
*   Scheduling a serverless process
*   Deploying a larger-than-limit application with Zappa

        

# Technical requirements

The code for this chapter requires the `chalice` package, version 1.9.1\. We recommend installing Chalice from `conda-forge` and use this specific version for reproductivity: `conda install -c conda-forge chalice=1.9.1`.

As usual, the code for this chapter is in the GitHub repository, under the `Chapter19` folder ([https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications))[.](https://github.com/PacktPublishing/Python-Programming-Projects-Learn-Python-3.7-by-building-applications/tree/master/Chapter18)

        

# Understanding serverless

The word "serverless" might be somewhat misleading—serverless applications still do run on servers. There is a major difference is responsibility zones, though. With serverless, we don't rent computers and deploy our own APIs; instead, we send Python (or JavaScript, or Go, or whatever else) functions, along with our requirements, to a provider (which could be **Amazon Web Services** (**AWS**), Google Cloud Platform, or something else), and they execute those functions on their servers when triggered to do so. We don't need to think about configuring servers, turning them on and off, or scaling—the functions we trigger will work when needed on the scale that is needed (the providers will add computers, if required, behind the scenes). The best part? We'll only pay for the fact of execution—if a function wasn't triggered, we'll pay nothing.

Because of that, serverless applications can be a great alternative to running your own servers in the following cases:

*   If requests are infrequent and you would otherwise have to pay for a server with little to no payload. Providers such as AWS provide solutions to trigger serverless applications automatically on specific events, such as a new file being uploaded to an S3 bucket. A serverless application could also serve as an API endpoint, running when you request it to. 
*   If the requests rate is hard to predict or very sporadic, so it would be hard to scale and adjust the number of workers yourself.
*   If the function in question itself is relatively simple and fast.
*   If you only need a few simple and stateless endpoints (serverless functions can invoke other services and serve their outcomes, such as serving the frontend for a large and complex **machine learning** (**ML**) model).

Of course, serverless has its own limitations and caveats as well:

*   Code needs to be pre-packaged in a specific way.
*   There is a cap of 50 MB on the memory required to upload your code and img/, including all the dependencies, except a few default ones.
*   The problem of cold starts: if an endpoint has not been used for a certain time, the virtual server will be stopped and resumed on the next request. This means that the first few requests after that period of having been stopped may take longer to execute. If that is critical, we could ping the endpoint once in a while so that the server won't stop—but that would mean some additional costs. 

Building a serverless application manually requires some time and can be a tedious process. Everything should be isolated and zipped, and the archive should meet certain criteria. Luckily, there are a few Python packages that help to prepare and deploy code as a serverless application.

To summarize, serverless applications are services that allow you to focus on the code for your specific task; running the code, server configuration, scheduling, scaling, and so on are taken care of by the service provider. You only pay for the fact of execution. The code for the application needs to be prepackaged and has some requirements to meet. Luckily, there are frameworks that help us to simplify the workflow. 

Now that we know what serverless is, in the next section, we'll build our own serverless application using Chalice.

        

# Getting started with Chalice

Let's try replicating the API endpoint we did for 311 in the previous chapter as a serverless application. For that, we'll use a framework called **Chalice**.

Chalice is a Python package for serverless applications on AWS, and is itself developed by Amazon. It can take care of an application, from its template all the way to deployment. It's also great for testing, as it emulates deployment with no fee, authentication, or even internet connection required.

Before we start working on our serverless application, let's ask Chalice to generate a template. In your terminal, type this:

```jl
chalice new-project
```

After this, type the name of the project: `311estimate`. This will generate a new folder with a few files:

```jl
311estimate/ ...
```

        

# Setting up a simple model

Similar to how we built a REST API in [Chapter 18](e1d8b121-e8db-45da-9dbf-a034664926fa.xhtml), *Serving Models with a RESTful API*, let's start by serving median values from a JSON file. This will help us to set the model for working with Chalice:

1.  First of all, we need to load the JSON object:

```jl
import json

with open('./model.json', 'r') as f:
    model = json.load(f)
```

2.  Now we will rename the route and define the last resource to map to the complaint type (in the same way we would for FastAPI, again!). We will also have to import a `Response` object:

```jl
from chalice import Response

@app.route('/predict/{complaint_type}', methods=['GET'])
def predict(complaint_type:str) -> Response:
```

3.  Finally, finalize the function by adding simple lookup logic; here, we decided to be nice and let our user know if they pass a wrong complaint type:

```jl
@app.route('/predict/{complaint_type}', methods=['GET'])
def predict(complaint_type:str) -> Response:

    if complaint_type in model:
        return Response(status_code=200,
                        headers={'Content-Type': 
                                 'application/json'},
                        body={'status': 'success',
                              'complaint_type': complaint_type,
                              'estimated_time': 
                                      model[complaint_type]})
    else:
        return Response(status_code=400,
                        headers={'Content-Type': 
                                 'application/json'},
                        body={'status': 'failure',
                              'problem': 'Complaint type is 
                                          not in database',
                              'complaint_type': complaint_type})
```

4.  Then, we deploy it locally, as we did before:

```jl
$ chalice local
Serving on 127.0.0.1:8000
```

5.  Now let's try it out from another terminal:

```jl
$ curl -X GET http://localhost:8000/predict/appliance
{"status":"success","complaint_type":"appliance","estimated_time":63.53}
```

6.  Let's see what happens if we ask for a wrong complaint type: 

```jl
$ curl -X GET http://localhost:8000/predict/wrongtype
{"status":"failure","problem":"Complaint type is not in database","complaint_type":"wrongtype"}
```

As you can see, it will return an error, with the message we passed. Let's now refactor our code so that the data will be stored outside of the Python script.

        

# Externalizing medians

This was quite a simple script so far, but you probably would want to update the JSON constantly—without needing to re-deploy the function every time. Let's do exactly that. We'll move the JSON to some public bucket (or not a *public* bucket—just make sure to use credentials with the rights to access it), and let a function download it upon request or upon deployment. We'll use the built-in `urllib` package, not `requests`, as we don't want to spend our 50 MB memory on one more dependency package. Delete the local JSON to keep the code slim, and add this code:

```jl
import json, urllib.requesturl = 'https://raw.githubusercontent.com/PacktPublishing/Python-Programming-Projects-Learn-Python-3.7-by-building-applications/master/Chapter18/data/model.json' ...
```

        

# Building a serverless API for an ML model

Getting public access to data in 10 lines of code is useful. But let's now do something more complex than that—say, serving an actual ML model.

Let's create one more app—`311predictions`. As before, we would need to call `chalice new-project` and type our new project's name.

Now, for the previous application, we didn't need any dependencies; in order to serve the ML model we used in the previous chapter, we need to have `pandas` and `sklearn`. The problem is that both of them cannot fit into the 50 MB limitation. In fact, until recently, there was no easy way to fit either of them there—normal `pip install` requires all the source code to be downloaded and compiled on the machine. Luckily, now a pre-compiled version can be installed, and `chalice` will explicitly look for a pre-compiled binary, generated for a Linux machine we'll be running.

Still, we have to decide how to shrink our memory usage. In this particular case, there are not many options. We definitely can't serve the model without `sklearn`, so we'll have to get rid of `pandas`. Let's add `sklearn` to the requirements (note that it has a different name in `pip`):

```jl
scikit-learn==0.21.2
```

Now let's recreate the API, using only `sklearn` and NumPy (which is a dependency of `sklearn`, anyway). First, let's load our model from S3:

```jl
import boto3

BUCKET, KEY = 'philipp-packt', 'model.pkl'

def _load_pickle(bucket, key):
    S3 = boto3.client('s3', region_name='us-east-1')
    response = S3.get_object(Bucket=bucket, Key=key)

    body = response['Body'].read()
    return pickle.loads(body)

model = _load_pickle(BUCKET, KEY)
```

Note that this model will be preloaded on each deployment, not for each request. To shave even more time off the request itself, let's predefine a NumPy singleton object (a one-row matrix that will be populated with data from the request:

```jl
singleton = np.empty(shape=(1, 4), dtype='object')
```

Unlike FastAPI, there is no built-in parsing mechanism, so we'll have to parse values on our own. Let's predefine a parsing mechanism for each parameter, except `complaint_type`, which does not need to be parsed:

```jl
dtypes = {
    'lon': float,
    'lat': float,
    'date': np.datetime64
}
```

Now we can start writing the endpoint itself. In the following code, we use the `app.current_request.query_params` dictionary, which stores all the parameters in the URL. There is a similar dictionary for the body of the request.

Another useful command is `app.log.debug`. If lambda is running in debug mode, this will spill the values out to the logs:

```jl
@app.route('/predict/{complaint_type}', methods=['GET'])
def index(complaint_type:str):
    try:
        app.log.debug(app.current_request.query_params)
        singleton[0, 0] = complaint_type

        for i, col in enumerate(dtypes.keys(), 1):
            singleton[0, i] = dtypes[col](app.current_request.query_params.get(col, np.nan))

        app.log.debug(singleton.astype(str).tolist())
```

Finally, we can add the prediction itself and define the responses—both for failure and success cases. This part looks pretty similar to the FastAPI version. Here is the whole function:

```jl
@app.route('/predict/{complaint_type}', methods=['GET'])
def index(complaint_type:str):
    try:
        app.log.debug(app.current_request.query_params)
        singleton[0, 0] = complaint_type

        for i, col in enumerate(dtypes.keys(), 1):
            singleton[0, i] = dtypes[col](app.current_request.query_params.get(col, np.nan))

        app.log.debug(singleton.astype(str).tolist())

        prediction = model.predict(singleton)[0]
        app.log.debug(prediction)

        return Response(status_code=200,
                        headers={'Content-Type': 'application/json'},
                        body={'status': 'success',
                              'estimated_time': prediction})
    except Exception as e:
        return Response(status_code=400,
                        headers={'Content-Type': 'application/json'},
                        body={'status': 'failure',
                              'error message': str(e)})
```

This is all the code we need. If you try running it, however, it will throw an error—there are still two issues we need to figure out.

First, the pickled model stores the objects, but not its dependencies (such as external libraries). To make it even worse, it expects to find them exactly where they were imported at the time of serialization, relative to the loaded pickle. For example, it expects `TimeTransformer` to be not only available but available as part of the `ml` namespace. Theoretically, we could hack the namespace, but it seems easier to copy the `ml.py` file and import the object. Indeed, this is a better option—except that `chalice` won't push any Python files from the folder, except `app.py`. To push our `ml` file, we need to create a folder called `vendor`, then create another folder called `ml` and treat it as a package, moving the code there. As we need to match the exact `ml.TimeTransformer`, we'll create an `__init__.py` file—in fact, to keep it simple, we can just move `ml.py` there and rename it. 

The second issue is within `TimeTransformer` itself: it uses `pandas`, which we can't ship—it's just too large. How can we drop `pandas` from the dependencies?

As you may remember, this object transforms any given array of dates into an array of three numeric features: the time of the day, the day of the year, and the day of the week. The first part of the question is how to execute that logic with NumPy. Luckily, every `DateTime` object is just a large integer of seconds that have passed since midnight January 1, 1970, UTC. Therefore, all those operations can be viewed as a combination of subtraction, division, and getting a reminder. We can use a hack of converting `DateTime` values to different units and back. Consider the following code:

```jl
def day_of_week_num(dts):
    return (dts.astype('datetime64[D]').view('int64') - 4) % 7

def day_of_year_num(dts):
    return (dts.astype('datetime64[D]').view('int64') - dts.astype('datetime64[Y]').astype('datetime64[D]').view('int64'))

def time_of_day_num(dts):
    return dts.astype('datetime64[s]').view('int64') - dts.astype('datetime64[D]').astype('datetime64[s]').view('int64')
```

Similar to `pandas` implementations, all three functions will return the corresponding values as numbers.

Great! But the `TimeTransformer` object still relies on `pandas`. One solution would be to get rid of it completely—just use the preceding functions both in the application and in the model training; after all, those operations are independent of the training set. This solution is totally fine.

Alternatively, you might notice that `TimeTransformer` needs `pandas` to find `DateTime` columns and operate on them. This means that we can store indices of the columns in the same way we store column names and then use them if we get a NumPy array instead of a DataFrame. Indeed, this is totally doable (see the repository for the full code). The best part? We don't even need to retrain our model again—while our pipeline uses the `TimeTransform` object under the hood, it does not require it to be identical to the one used for training, as long as it has the same name and behaves similarly.

Finally, our ML model is ready to be served! Let's deploy the model and test it live, using a URL like this:

```jl
<deployment_url>/predict/commercial?lat=40.636626&lon=-73.951694&date=2019-06-08 00:00:09
```

We get a timeout. It seems that inference requires more operation memory. We could change that via the AWS web console (there are tons of other options, too, such as the use of environmental variables), or we could add a corresponding `config` for `lambda_memory_size` to `.chalice/config.json`:

```jl
# .chalice/config.json

{
  "version": "2.0",
  "app_name": "311predictions-v2",
  "stages": {
    "dev": {
      "api_gateway_stage": "api", 
      "lambda_memory_size": 520
    },
  ""
  }
}
```

Redeploy it one more time. Now it works! See here:

```jl
$ <deployment_url>/predict/commercial?lat=40.636626&lon=-73.951694&date=2019-06-08 00:00:09
{
 "status": "success",
 "estimated_time": 1.36
}
```

        

# When we're still out of memory

We were able to shrink the size to just under the memory limit. However, it is not always possible or desirable to do this. If our dependencies are complex and our files are large, we have two more options:

*   First, the lambda can be just an entry point—it could invoke a Docker image (an isolated virtual environment running somewhere else in the cloud), pass the parameters there, and communicate the results back.
*   Alternatively—and this is a bit of a hack—we could download all the dependencies in the same way that we downloaded our model: during runtime. This data will be lost once the server is down, which will happen if a serverless function is not triggered by anything. Redeployment can take a significant amount ...

        

# Building a serverless function as a data pipeline

So far, we have only used serverless functions as API endpoints, but they can serve in many other ways as well. For example, they can be triggered to run for each new file uploaded to a specific folder on S3, or scheduled to run at a specific time. 

Let's create one more application for data collection. We can specify that we need the `requests` library in `requirements.txt`. We can also copy and paste the `_get_data` function from [Chapter 15](eb2254cc-485d-4603-9d17-7a442aa5e3b8.xhtml), *Packaging and Testing with Poetry and PyTest*, along with the resource and time columns. One part of the code that we are still missing is that for uploading data to S3\. Here is the code:

```jl
def _upload_json(obj, filename, bucket, key):
    S3 = boto3.client('s3', region_name='us-east-1')
    key += ('/' + filename)

    S3.Object(Bucket=bucket, Key=key).put(Body=json.dumps(obj))
```

Having all the necessary pieces, let's pull them together as one function. Here is the code:

```jl
from datetime import date, timedelta

def get_data(event):
    yesterday = date.today() - timedelta(days=1)

    data = _get_data(resource, time_col, yesterday, offset=0)
    _upload_json(data, f'{yesterday:%Y-%m-%d}.json', bucket=BUCKET, key=KEY)
```

Finally, all we need now is to add a `chalice` decorator, `@app.schedule('rate(1 day)')`. Here, instead of the `app.route` decorator, we use `app.schedule` and define a corresponding frequency. Here is how it will look as a whole:

```jl
import json
from datetime import date, timedelta
BUCKET, FOLDER = 'philipp-packt', '311/raw_data'
resource = 'fhrw-4uyv'
time_col = 'Created Date'
app = Chalice(app_name='collect-311')

def _upload_json(obj, filename, bucket, key):
    S3 = boto3.client('s3', region_name='us-east-1')
    key += ('/' + filename)

    S3.Object(Bucket=bucket, Key=key).put(Body=json.dumps(obj))

def _get_data(resource, time_col, date, offset=0):
    '''collect data from NYC open data
    '''

    Q = f"where=created_date between '{date:%Y-%m-%d}' AND '{date:%Y-%m-%d}T23:59:59.000'"
    url = f'https://data.cityofnewyork.us/resource/{resource}.json?$limit=50000&$offset={offset}&${Q}'
    r = rq.get(url)
    r.raise_for_status()

    data = r.json()
    if len(data) == 50_000:
        offset2 = offset + 50000
        data2 = _get_data(resource, time_col, date, offset=offset2)
        data.extend(data2)

    return data

@app.schedule('rate(1 day)')
def get_data(event):
    yesterday = date.today() - timedelta(days=1)
    data = _get_data(resource, time_col, yesterday, offset=0)
    _upload_json(data, f'{yesterday:%Y-%m-%d}.json', bucket=BUCKET, key=FOLDER)
```

Once deployed, this function will run every day, collecting data for the previous date and storing it as JSON in our S3 bucket. But can we go further and prepare medians for our prediction model, automatically? Let's find out in the next section.

        

# S3-triggered events

So far, we have written a lambda function that's driven by API requests or is scheduled to be triggered. Now let's complete our application's data cycle by adding an S3-triggered task that will compute the medians for each category every time a new dataset is collected—in other words, once the scheduled task is finished.

In order to do so, we'll add one more operation to the same data collection application that we've been working on:

1.  First, let's redefine the median computation. The original code also used `pandas`, so we have two options: use NumPy as we just did for the ML part, or use vanilla Python. As our raw data collection doesn't need NumPy, let's stick with the second option. Here is the no-dependency code for ...

        

# Summary

In this chapter, we introduced you to serverless functions—a different approach to APIs and computation in general. Serverless functions don't need maintenance, scale automatically, are secure, and are simple to write. They may be a great option for operations that don't need a huge amount of requests, or for when demand spikes unpredictably. In addition to serving as APIs, lambdas can be scheduled with one line of code or triggered by an external event, such as a new file landing in an S3 bucket. The downside of serverless applications is that they have strict memory limitations that could be a serious barrier for certain tasks. The response time could also be longer for the first time after a long break—but there are ways to solve that issue to some extent.

As a practice exercise, we were able to recreate our 311 API endpoints as serverless applications. In addition, we wrote two more functions for scheduled data collection and the computation of medians. In other words, we used lambdas to recreate the functionality we achieved in [Chapter 17](a0274de8-bfb3-43a0-82f2-b3a67c088ffc.xhtml), *Let's Build a Dashboard*, and [Chapter 18](e1d8b121-e8db-45da-9dbf-a034664926fa.xhtml), *Serving Models with a RESTful API*, together—all with serverless applications.

In the next chapter, we will learn about some best practices for using Python, a few issues with using Python, and the performance of Python.

        

# Questions

1.  What does a serverless application mean?
2.  What are the limitations of the serverless approach?
3.  What are the benefits of serverless APIs?
4.  What role does Chalice play in the development of a serverless application?

        

# Further reading

*Serverless Architectures with AWS* by Mohit Gupta, published by Packt Publishing ([https://www.packtpub.com/networking-and-servers/serverless-architectures-aws](https://www.packtpub.com/networking-and-servers/serverless-architectures-aws)).