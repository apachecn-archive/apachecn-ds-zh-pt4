        

# Data Cleaning and Manipulation

Before we dive into data analysis, data needs to be properly prepared and structured. Some datasets, for example, structured computer logs, are ready to go from the start, but, most of the time, the majority of the time is spent preparing data properly. This process inevitably requires certain decisions that depend on the specifics of the task.

In this chapter, we will learn how to prepare the data with `pandas`, using the dataset we collected from Wikipedia in [Chapter 7](232fe2da-7fa8-4d76-b5fc-d4bf80535e86.xhtml), *Scraping Data from the Web with Beautiful Soup 4*, as an example. 

We will cover the following topics in the chapter:

*   Quick start with `pandas`
*   Working with real data
*   Regular expressions
*   Using custom functions with `pandas` dataframes
*   Writing the file

        

# Technical requirements

The code for this chapter makes use of two packages: `pandas`, which is included in the default Anaconda distribution, and `missingo`, which we included in the `environment.yml` file. If you skipped the step of `conda` environment creation, just install `missingo` using the `pip` or `conda` package managers. As always, all the notebooks are stored in the repository, in the `Chapter11` folder ([https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications)).

        

# Getting started with pandas

Pandas is *the tool* for data manipulation in Python—it combines speed and convenience, allowing the rapid processing and manipulation of data. Let's first overview a number of basic operations: `pandas` is simple and intuitive to use, but it is still a learning curve. 

`pandas` does have two main data structures:

1.  `Series` is a one-dimensional array of one data type that also has an index. The index could be numeric, categorical, a string, or datetime.
2.  `DataFrame` is a two-dimensional table consisting of a set of columns—each of one single data type. `Dataframe` has two indexes—index and columns. Columns of `Dataframe` can be thought of as `Series`. Rows can be retrieved as `Series` but, in this case, data in the cells will likely be converted to one shared data type *object *(more on that later).

Most of the time, we get our data from external sources: a database, a link, or a file. To do that in `pandas`, just use one of the many `pd.read_...` functions, including, but not limited to, the following:

*   `pd.read_csv` and `pd.read_excel` for CSV and Excel formats. There is also `pd.read_html` to read tables for a given HTML page.
*   `pd.read_sql` and `pd.read_sql_table` for SQL databases (the first expects a query as an argument, while the second will try to collect the whole table).
*   `pd.read_pickle`, `pd.read_feather`, `pd.read_hdfs`, `pd.read_parque` and so on for different binary formats.

Be aware that most of those functions support (where applicable) not only local paths, but also web URLs. In addition, many of them can detect and unarchive archived files—for example, the `./data.csv.gz` file will be unzipped and read in memory, seamlessly. 

All of the preceding functions have plenty of arguments. For example, for `read_excel`, you can specify which sheet to use, and, for `read_csv `, which row to treat as column names (if any), and how many rows to skip at the beginning and drop at the end of the file. You can also specify which columns to parse as a date or time values and whether those should be stitched together; for example, if one column defines the date, and the other the time. 

Writing data to a file or database is as easy as reading. `pandas` support all the same formats for data write—available via multiple `df.to_csv`, `df.to_excel`, and many more. As with reading, you can add an archive extension to the end of the path, and `pandas` will automatically detect those and use the corresponding archiving algorithm. 

We now know how to read and write data using `pandas`. Let's now talk about how to select and edit values.

        

# Selection – by columns, indices, or both

Now, let's learn how to access and edit specific values in `pandas` data structures. We'll start with a toy example—here, I will generate a dataframe from a dictionary of lists:

```jl
import pandas as pddata = {'x':[1,2,3], 'y':['a', 'b', 'c'], 'z': [False, True, False]}df = pd.DataFrame(data)
```

Now, we can take a look at the data we just stored:

```jl
>>> df x   y   z 0   1   a   False1   2   b   True2   3   c   False
```

As you can see, this frame has three rows and two columns. Let's see how it works:

1.  First, let's start selecting columns. Any column can be selected using indexing via square brackets with the column name. As we're asking for one column, it will be returned as a `pandas Series` object:

```jl
>>> df['x'] 1 2 3Name: x, dtype: int ...
```

        

# Masking

Now, both `loc` and simple square brackets accept masks. Mask can be represented by a `Series`, a NumPy array, or a simple list of Boolean values of the same length as the number of rows in the dataframe. If given, this collection will be interpreted as a **mask***—*essentially, an explanation of which rows to return. For example, we can use our third column, `z`, as a mask to filter on. Because we only have a `True` value in the first row, a dataframe of one row will be returned:

```jl
>>> df[df['z']]
 x   y   z    new_column
1   2   b   True         -1
```

This is a very important technique, which we'll be using all the time! Such a mask can be generated using any logic operations, for example, an equality operator. Take a look: here, we are creating a mask by checking whether the values in column `x` are equal to `2`:

```jl
>>> mask = df['x'] == 2
>>> mask
0 False
1 True
2 False
Name:x, dtype:bool
```

This mask can now be used to filter rows in our dataframe or any other one with the same indices. Only the second row will be retrieved—as only the second value in the masking series is true:

```jl
>>> df.loc[mask, 'y']1 b
Name: y, dtype: object
```

        

# Data types and data conversion

As you may notice, when we print out a `Series` object, its data type will be declared in the last line. An alternative way is to call `dtype` for each `Series`, or `.dtypes` for the entire dataframe (it will return a `Series` object). Those data types are defined in C, and not Python. The majority of them largely match Python ones; for example, integers, floats, and Booleans. There are, however, a few caveats to be aware of regarding the data types:

*   First, there is no existing data type for strings. As you may notice in the last code block, all strings are defined as *objects*, that is, an arbitrary Python object. This type is the last resort, the type that suits any Python value but does not give any computation benefits.  ...

        

# Math

Of course, mathematical operations are well present in `pandas`, which actively leverages NumPy's functionality and supports an extra-wide specter of math and statistical functionality. To get a sum, mean, median, max/min, or percentile of a numerical column, just call it as a column's method:

```jl
>>> N = pd.Series([1,2,3,10])

>>> N.mean()
4.0

>>> N.median()
2.5

>>> N.sum()
16

>>> N.max()
10
```

It also supports operations such as correlation (just call it on another numeric column of the same length), and many more. Most of the time, you can run the very same functions on the dataframes—in this case, axis (direction of operation) will be used as an argument. The default, all operations are run vertically—for example, for `df.sum()` you will get a series of sums, one for each column in the original dataframe. The very same operations with `axis=1` will summarize every row, so you will get `Series` with a cell for each row in the dataframe.

        

# Merging

On occasion, we need to join multiple dataframes together. There could be different ways to do that—let's take a look.

First and foremost, if you have multiple dataframes with the same columns, and you want to join them—never do that iteratively—try to do that once, by passing a list of all of them to the `pd.concat` function with the `axis=0` and `sort=False` arguments (unless you need to sort them):

```jl
>>> df.shape(3, 4)>>> double = pd.concat([df, df], axis=0, sort=False)>>> double.shape(6, 4)
```

Similarly, `pd.concat` can merge multiple dataframes horizontally if `axis=1`:

```jl
>>> pd.concat([df, df], axis=1) x y z     new_column x y z     new_column0 1 a False -1         1 a False -11 2 b True  -1         2 b True  -12 3 c False -1         3 c False -1
```

In this example, we group two ...

        

# Working with real data

Let's now try using `pandas` on real data. In `Chapter 7`, *Scraping Data from the Web with Beautiful Soup 4*, we collected a huge dataset of WWII battles and operations—including casualties, armies, dates, and locations. We never explored what is inside the dataset, though, and usually, this kind of data requires intensive processing. Now, let's see what we'll be able to do with this data.

As you may recall, we stored the dataset as a nested `.json` file. `pandas` can read from JSON files of different structures, but it won't understand nested data points. At this point, the task for us is straightforward (you may think of writing a recursive function, for example), so we won't discuss this much. If you want, you can check the `0_json_to_table.ipynb` notebook in this chapter's folder on GitHub at the following link: [https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications/tree/master/Chapter11](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications/tree/master/Chapter11). The only new operation there is the `pandas.io.json.json_normalize` function, which expects an array of dictionaries, representing rows, and flattens their nested properties, concatenating keys (in our case, nested belligerents, casualties, strengths, and leader elements). We stored the resulting data as a set of CSVs, representing different theaters of war (see `Chapter11/data/...csv` in the repository). Note that no additional processing, with the exception of unnesting, was undertaken.

With this done, we can now look closer at the data we collected. Let's dive into one of the CSV files and see what we're working with:

```jl
df = pd.read_csv('./data/Eastern Front.csv')
```

This will read the report and present the data.

        

# Initial exploration

Before anything else, we need to take a look at the data itself, as well as its columns and rows. It's reasonable to start data exploration by understanding the following:

1.  How do specific values look like, for example, using `df.head(N)`, `df.tail(N)` , or `df.sample(N)` to retrieve (and print) the first N, last N, or random N rows from the dataset? As regards heads and tails, by default, *N = 5*. For our sample, it is 1 (one row). Alternatively, the sample method can take a `frac` argument, which will return a fraction of records—for example, `df.sample(frac=0.25)` will return 25% of the initial dataset. Note that printing will omit some columns in the middle if there are too many of them.
2.  The overall shape of the dataset—the number ...

        

# Defining the scope of work to be done

Before we dive into the process of data cleaning, which might be very time-consuming, it is always useful to define the scope of work—which columns and rows we actually need to clean. For this chapter, let's restrict the scope to the lowest level of the hierarchy—specific battles (`level=100`—pages for events with no children). We can use the equality operator to generate a Boolean mask, and then use this mask to filter the dataset:

```jl
>>> battles = data[data.level == 100] 
>>> battles.shape
(147, 23)
```

There are many columns in the dataset—enough for `pandas` to omit the middle part when printing. As we'll be mostly focused on time, geolocation, names, and casualties of each side, let's define those columns of interest in a list and investigate them more closely:

```jl
columns = ['Location', 'name', 'Date', 'Result', 'Belligerents.allies', 'Belligerents.axis']
battles[columns].head(3)
```

As a result of this code, we'll get the following table:

|  | `Location` | `Name` | `Date` | `Results` | `Belligerents.allies` | `Belligerents.axis` | `Casualties and losses.allies` | `Casualties and losses.axis` |
| `0` | `Westerplatte, harbor of Free City of Danzig54°...` | `Battle of Westerplatte` | `1–7 September 1939` | `German victory` | `Poland` | `Germany Danzig` | `15 dead at least 40 wounded Remainder captured` | `50 dead at least 150 wounded` |
| `1` | `Mokra, Kielce Voivodeship, Poland` | `Battle of Mokra` | `September 1, 1939` | `Polish victory` | `Germany` | `Poland` | `800 killed, missing, captured, or wounded 50 tanks` | `500 killed, missing or wounded 300 horses sever a...` |
| `2` | `Near Mława, Warsaw Voivodeship, Poland` | `Battle of Mlawa` | `1–3 September 1939` | `German victory` | `Germany` | `Poland` | `1,800 killed 3,000 wounded 1,000 missing 72 tanks..` | `1,200 killed 1,500 wounded` |
| `3` | `Near Tuchola Forest, Pomeranian Voivodeship, P...` | `Battle of Tuchola Forest` | `1–5 September 1939` | `German victory` | `Germany` | `Poland` | `506 killed \n\n743 wounded` | `1600 killed 750 wounded Unknown number cap...` |
| `4` | `Jordanów, Kraków Voivodeship, Poland` | `Battle of Jordanów` | `1–3 September 1939` | `Pyrrhic German victory` | `Poland` | `Germany` | `3+ tanks` | `70+ tanks and AFVs` |

Now, let's investigate the missing values in the data if the particular column is mostly empty. It makes no sense to spend time cleaning and processing it. The best way to explore the missing values is to make a plot. With the help of the `missingno` library, it is an easy task. Take a look at the code:

```jl
import missingno as msno
msno.matrix(battles, labels=True, sparkline=False)
```

As a result, the following chart will be plotted:

![](img/5a32d19b-0176-4e4e-92d9-e69fac4ef5c3.png)

Here, the black rectangles represent non-empty values. As you can see, a few auto-generated columns (level, name, parent, and URL) don't have any misses. Some others, on the other hand, do have just a few non-empty ones (for example, all the columns related to the *third party*). What is even more important is the fact that there is a clear correlation between the missing values on some of the columns—it seems that rows with missing data in `Belligerents` also lack values for `Date` and `Location`. Let's first investigate those columns:

```jl
>>> mask = battles[['Date', 'Location']].isnull().all(1)
>>> battles.loc[mask, ['name', 'url']]
 name                                                url
39   Pripyat swamps (punitive operation)  https://en.wikipedia.org/wiki/Pripyat_swamps_(...
42    Bombing of Tallinn in World War II  https://en.wikipedia.org/wiki/Bombing_of_Talli...
46                       Operation Wotan  https://en.wikipedia.org/w/index.php?title=Ope...
47                      Nevsky Pyatachok     https://en.wikipedia.org/wiki/Nevsky_Pyatachok
48            Operation Nordlicht (1942)  https://en.wikipedia.org/wiki/Operation_Nordli...
61                      Operation Büffel  https://en.wikipedia.org/wiki/Operation_B%C3%B...
67                     Operation Kremlin    https://en.wikipedia.org/wiki/Operation_Kremlin
68                Operation Braunschweig  https://en.wikipedia.org/wiki/Operation_Brauns...
70                         Malaya Zemlya        https://en.wikipedia.org/wiki/Malaya_Zemlya
96                   Concert (operation)  https://en.wikipedia.org/wiki/Concert_(operation)
97          Zhitomir–Berdichev Offensive  https://en.wikipedia.org/wiki/Zhitomir%E2%80%9...
152      Operation Nordlicht (1944-1945)  https://en.wikipedia.org/wiki/Operation_Nordli...
157                     Operation Konrad     https://en.wikipedia.org/wiki/Operation_Konrad
175                 Operation Margarethe  https://en.wikipedia.org/wiki/Operation_Margar...
```

From the outcome, it seems that the web pages actually lack this kind of information. Moreover, many of them are not exactly standard battle pages, so perhaps we'd be better off without them—let's throw them out for good:

```jl
battles=battles.dropna(subset=['Date', 'Location'])
```

Now that we're done with missing values, let's get back to the table we printed. As you can see, there are a few serious issues, including an incorrectly stated axis and `allies` belligerents (refer to rows 3 and 4 of the preceding example), and `Date`, `Location`, and `Casualties` (among others) values stored in an unstructured way. Those issues have to be taken care of before we can move on to analysis. In other words, we need to correct the sides, parse dates, convert locations into coordinates, and parse multiple types of casualties as numbers. Unfortunately, there is no one silver bullet here. To process all those records accurately would require a lot of time. Usually, our time is limited, so we'll have to find some sort of compromise, depending on our end goals. 

In this section, we explored the dataset in general, which allowed us to throw away what we won't use, and identify issues with the data that we'll have to fix in the next sections.

But first, how do we even approach data cleaning and parsing? The former is simple – just use masks, filters, and/or imputation strategies. The latter, however, will require us to use yet another technological trick—regular expressions.

        

# Getting to know regular expressions

Strings that store data usually have certain patterns, which can be leveraged to retrieve actual data values in a unified fashion. For example, some location cells have distinctive coordinates, and numbers and symbols of degrees, minutes, and seconds. To extract those values, we could write a custom Python code, but this will be verbose and time-consuming.

This problem – extracting values from text by defining a pattern – sounds like something quite general and useful in many situations. When a problem can be stated as something universal, it usually means that it is, and someone has a solution! This is, by the way, a good approach for programming in general.

Indeed, there is a universal solution, called ...

        

# Parsing locations

Let's start with the location column. As you remember, data in this column is supposed to represent the location where the battle took place. In many cases, the value was stored as Wikipedia GeoMarker, which includes latitude/longitude coordinates. Here is what the *raw* value of this marker looks like:

```jl
>>> battles['Location'].iloc[10]
'Warsaw, Poland52°13′48″N 21°00′39″E\ufeff / \ufeff52.23000°N 21.01083°E\ufeff / 52.23000; 21.01083Coordinates: 52°13′48″N 21°00′39″E\ufeff / \ufeff52.23000°N 21.01083°E\ufeff / 52.23000; 21.01083'
```

Note that this geotag has both a *nice* latitude/longitude pair (with minutes and seconds), as well as its float representation, which is easier to use. In fact, the very same coordinates are repeated at the very end in their most simple form—and that's what we'll extract.

Let's write our first pattern. Usually, it is easiest to write a draft pattern, which will match our example string, and then work from there—adopting, relaxing, and tightening the pattern, where needed. Here is our attempt—a slash, and then two groups, each containing either numeric characters or a period (which we have to escape with a slash):

```jl
pattern = r'/ ([\d|\.]+); ([\d|\.]+)'
```

It is usually easier to tailor the pattern in an interactive way. Our favorite tool for the job is Pythex ([https://pythex.org/](https://pythex.org/)), an online console for interactive regex testing, tailored specifically for Python-flavored regex (yes, there are some differences).

Let's test this pattern:

```jl
battles.head(10).Location.str.extract(pattern)
```

It works! You may want to go over addresses and check that ones with no numbers extracted indeed do not have it. We can store the results in two new columns:

```jl
battles[['Latitude', 'Longitude']] = battles.Location.str.extract(pattern)
```

Note that both columns are still strings, but now they can be converted into floats:

```jl
for col in  'Latitude', 'Longitude':
    battles[col] =  battles[col].astype(float)
```

Still, many locations did not have coordinates to start with. But how many? Let's check the percentage of empty cells in `Latitude`:

```jl
>>> 100 * (battles['Lattitude'].isnull().sum() / len(battles))
78.2312925170068
```

That is, 78% of our locations are empty—too many! Other cells don't have any coordinates, but most of them do have an address as a string. Let's try to geocode them using the `nominatim_geocode` function that we wrote in earlier in this book.

        

# Geocoding

As you should recall, geocoding is the process of converting address as a text into latitude and longitude coordinates. As the task is quite complex and requires large datasets, it is usually handled by web-based services, using their API. in [Chapter 6](232fe2da-7fa8-4d76-b5fc-d4bf80535e86.xhtml), *First Script – Geocoding with Web API*, we wrote a Python function that communicates with such an API, allowing us to send addresses and get latitude/longitude pairs back.

To use it, let's first import the function (we copied the file to the local folder). We will also use `tqdm` to see how the process goes—it has a solution for `pandas`—a progress bar will appear on any `progress_apply` method execution once we register `tqdm` with `pandas`:

```jl
from geocode import nominatim_geocodefrom tqdm import ...
```

        

# Time

Another column is time. Now, `pandas` has a built-in `DateTime` parser and a very good one! Just use `pd.to_datetime()` on your scalar value or a collection. In this case, however, it won't work, and neither will any external packages that usually help (`dateparser` is our favorite). And all that because cells describe a time range, and not just one specific date.

Again, let's (at least, for now) see whether we can make our life simpler. Indeed, we probably don't care about specific dates—all we need is the month and year. Luckily, all months are properly stated and uniform—and `pd.to_datetime` can parse them. So, all we need is to correctly extract two month-year pairs from each.

Now, it seems hard to define one regular expression that will work here. Instead, we can try to get all years (we know all of them are four-digit numbers, starting with 19) and all months (there are just 12 variants). Then, we can combine them, using the year twice if there is only one value.

Let's try it out! First, we define the patterns:

```jl
d = ('January', 'February', 'March', 'April', 'May', 
     'June', "July",' August', 'September', 'October', 'November', 'December')

month_pattern = r'(' + "|".join(d) + ')'
year_pattern = r'(19\d\d)' 
```

Now, instead of `str.extract`, we will use the `str.extractall` method—it will try to retrieve *ALL* occurrences of the pattern in the string. As a result, it will create `multiindex`—an index with multiple levels. In this case, the first level will be the original one, taken from the argument. The second one will represent the number of occurrences within the string. Here, we should use the `.unstack()` function, which will rotate `Series` into `DataFrame`, so that the first level will be its index, and the second its columns.

As you may have guessed, there is an opposite function, `stack()`, which converts a dataframe into a series with a multilevel index.

In the following code, we run a regex to extract two values—the start and end of the column:

```jl
year_extracted = battles['Date'].str.extractall(year_pattern).unstack()
```

Notice that there are four, not two, columns here. Those are empty for most of the columns, but its mere existence means that there is at least one row where this last column is not empty. In the following code block, we mask our dataframe to show only records where the last column is not null:

```jl
>>> year_extracted[year_extracted.iloc[:, -1].notnull()]
 0
match 0    1    2    3
94    1943 1943 1943 1943
```

It seems that only one record has a value in there. Let's take a look at the corresponding raw value:

```jl
>>> battles.loc[94, 'Date']
'3 November 1943 – 13 November 1943(Offensive operation) 13 November 1943 – 22 December 1943(Defensive operation)'
```

The corresponding record indeed has four-year values, but all of them are the same. Another row has three values, but again, all of them are the same—so there is no harm in dropping all but the first two columns:

```jl
year_extracted = year_extracted.iloc[:, :2]
```

We can also fill empty cells of the second row with values from the first one, using the `fillna()` function. This function can fill empty cells in a series with a given scalar value, or corresponding values from another series of the same length (our case), or even from the series itself, using one of a few methods (for example, using the value in the previous cell). The following code does precisely that in that it fills the empty second column with the corresponding values from the first one:

```jl
year_extracted.iloc[:, 1].fillna(year_extracted.iloc[:, 0], inplace=True)
```

Now, let's do the same with `Months` except that this time, we'll use the `fillna` from left to right, and use the first and the last columns (as we require the beginning and end of the event):

```jl
month_extracted = battles['Date'].str.extractall(month_pattern).unstack()

for i in range(2, month_extracted.shape[1]+1):
    month_extracted.iloc[:, -1].fillna(month_extracted.iloc[:, -i], inplace=True)

month_extracted = month_extracted.iloc[:, [0, -1]]
```

Finally, we need to combine the two. Let's rename the columns so that we can use `.loc`, and then just loop over them:

```jl
year_extracted.columns = month_extracted.columns = ['start', 'end']
I = battles.index

for col in 'start', 'end':
    combined = month_extracted.loc[I, col] + ' ' + year_extracted.loc[I, i]
    battles[col] = pd.to_datetime(combined)
```

Yay! We're done with our second column – time. It wasn't easy, but we were able to convert the text into datetime values so that we can analyze them in the future. Next in line is belligerents.

        

# Belligerents

Lastly, as we noticed, in some rows, the `axis` and `allies` parties are swapped. It is slightly confusing for this specific dataset. For example, in this dual model, we'll have to mark `Soviets` as `axis` when they attacked Poland during the initial stages of the war. Let's take a look at all the possible combinations: 

```jl
battles['Belligerents.allies'].value_counts()
```

Here, `value_counts()` calculates a number of occurrences of each value. Hence, the index of those series represents unique values. There is a more intuitive alternative – the `unique()` function (which is also faster). However, this is a NumPy function and it returns a NumPy array, which Jupyter prints badly—that's the only reason we prefer to use `value_counts`.

From the examination, ...

        

# Understanding casualties

Casualties are probably the most verbose and non-structured columns of the dataset. It will be extremely hard to make use of all the nuances of information here, so again—perhaps we can simplify the task, getting only the things we really want to use. Perhaps we can use code words to extract any digit preceding them; for example, `([\d|,]+)\s*dead` should extract any consecutive digits or commas before the word `'dead'`. We can define similar patterns for all types of casualties and loop over all of them, testing the patterns. There are, unfortunately, many keywords that mean the same thing (`'captured'`, `'prisoners'`, and many more), so we have to make them optional, similar to the preceding month expression:

```jl
digit_pattern = '([\d|\,]+)(?:\[\d+\])?\s*(?:{words})'

keywords = { 'killed': ['dead', 'killed', 'men'], 
             'wounded': ['wounded', 'sick'], 
             'captured': ['captured', 'prisoners'],
             'tanks': ['tanks'],
             'airplane': ['airplane'],
             'guns': ['artillery', 'gun'],
             'ships': ['warships', 'boats'],
             'submarines': ['submarines']
}
```

Now, for each keyword, we can generate a custom regular expression and extract all their cells with multiple occurrences (casualties from the different countries involved). In this case, however, we can preemptively convert them into numbers and summarize. By itself this is easy—but before we do that, we need to remove commas, filter empty cells, and convert strings to integers. There is probably a way to do some of that using regex, but it seems easier in this particular case to write a custom pure-Python function (note—it may or may not be the slowest part of the timeline):

```jl
def _shy_convert_numeric(v):
    if pd.isnull(v) or v == ',':
        return 0
    return int(v.replace(',', ''))
```

This function can be applied to every cell via `applymap`. After that, we can finally summarize every row. The result can be viewed as follows:

```jl
results = {
    'allies' : pd.DataFrame(index=battles.index), # empty dataframes with the same index
    'axis' : pd.DataFrame(index=battles.index)
}

for name, edf in results.items():
    column = battles[f'Casualties and losses.{name}']

    for tp, keys in keywords.items():
        pattern = digit_pattern.format(words="|".join(keys))
        extracted = column.str.extractall(pattern).unstack()
        edf[tp] = extracted.applymap(_shy_convert_numeric).sum(1)
    results[name] = edf.fillna(0).astype(int)
```

Let's now see the result of `results['axis'].head(5)`:

|  | `killed` | `wounded` | `captured` | `tanks` | `airplane` | `guns` | `ships` | `submarines` |
| `0` | `50` | `150` | `0` | `0` | `0` | `0` | `0` | `0` |
| `1` | `500` | `0` | `0` | `1` | `0` | `0` | `0` | `0` |
| `2` | `1200` | `1500` | `0` | `0` | `0` | `0` | `0` | `0` |
| `3` | `1600` | `750` | `0` | `0` | `0` | `0` | `0` | `0` |
| `4` | `0` | `0` | `0` | `0` | `0` | `0` | `0` | `0` |

Note, that there is a caveat to our casualties parsing approach that we'll have to keep in mind—due to the pattern we use, in all cases where a range of casualties is stated, we take the last number mentioned. It will be the maximum digit in the range (for example, the *100-150 killed* pattern will return `150`) and the minimum in other cases (for example, the *10+ tanks* pattern will return `10`). 

Finally, let's reconnect both of those new dataframes to the original one. This time, let's create a multilevel column structure of our own so that we can select casualties for `axis/allies` without the need for the long column name. We're going to use the `pd.concat` function, which can join dataframes both vertically or horizontally. Our `allies/axis` casualties data is already in the proper dictionary format; we just need to add the rest of the data to the dictionary and then join the datasets together:

```jl
results['old_metrics'] = battles
new_dataset = pd.concat(results, axis=1)
```

As a result, we now have a clean, numeric dataframe of casualties for both sides in the conflict, divided by the type of casualty—be it a warship, plane, tank, or soldiers.

        

# Multilevel slicing

The good part is that given a dictionary of dataframes, `pd.concat` will create a multilevel column index, which will come in handy in a bit. This means, however, that it's not enough now to pass the column name as a string; we need to use multilevel slicing. Let's use an alias:

```jl
idx = pd.IndexSlice
```

Now, if we want to get a specific column in this dataframe, we have to use `.loc` with this indexing object for columns. The `IndexSlice` interface is very similar to `loc`. For one column, we'll use it like this:

```jl
df.loc[:, idx['old_metrics', 'url']]
```

Note that because we defined a specific value on all the levels, the result will be `pandas Series`. We can, however, relax our query, by using colons: for example, `df.loc[:, idx[:, 'killed']] ...`

        

# Quality assurance

I know we have spent a lot of time cleaning the data, but there is still one last task we need to perform – quality assurance. Proper quality assurance is a very important practice. In a nutshell, you need to define certain assumptions about the dataset (for example, minimum and maximum values, the acceptable number of missing values, standard deviation, medians, the number of unique values, and many more). The key is to start with something that is somewhat reasonable, and then run tests to check whether the data fits your assumptions. If not, investigate specific data points to check whether your assumptions were incorrect (and update them), or whether there are still some issues with the data. It just gets a little more tricky for the multilevel columns. Consider the following code:

```jl
assumptions = {
 'killed': [0, 1_500_000],
 'wounded': [0, 1_000_000],
 'tanks': [0, 1_000],
 'airplane': [0, 1_000],
 'guns': [0, 10_000],
 ('start', 'end'): [pd.to_datetime(el) for el in ('1939-01-01', '1945-12-31')]
}

def _check_assumptions(data, assumptions):
    for k, (min_, max_) in assumptions.items():
        df = data.loc[:, idx[:, k]]
        for i in range(df.shape[1]):
            assert df.iloc[:, i].between(min_, max_).all(), (df.iloc[:, i].name, df.iloc[:, i].describe())

_check_assumptions(data, assumptions)
```

Here, we use a dictionary to describe our assumptions—a key representing the column, and a value being minimum and maximum values. Using multilevel slicing, we can treat the key as the lowest column name—hence, testing both `allies` and `axis` casualties in the same pass. The `describe()` method returns a series of descriptive statistics for the column (in this case) or the entire dataframe—minimum and maximum values, most frequent value, and many more.

Note that the preceding assumptions will not hold. Feel free to run them and investigate which battles go beyond your expectations and whether their values are correct. The QA checkup process usually does require some back-and-forth on the first try, as you usually have to relax your requirements somewhat. This is a valuable process on its own—even here, you're usually learning some new information about your data. 

Finally, let's write our resulting clean dataset so that we can use it in our next section.

        

# Writing the file

Finally, we have all the data we wanted, in a more-or-less good condition. Let's store it in CSV format. We can always use other formats instead. For example, the pickle format, by definition, preserves all the data types and properties of the dataframe (we won't need to convert dates from strings again), but can't be read manually (it also has a number of security risks). CSV, on the other hand, can be opened manually or with something like Excel, edited, and then stored again if you observed that there are factual errors in the data or something that is easier to correct manually.

In the following code block, we export our CSV file into a dataframe just to specify a relative path to the file we want it to be. The `index=None ...`

        

# Summary

In this chapter, we spent time cleaning the data we acquired in [Chapter 6](ca8361ef-be7b-4ada-9b74-67c692791316.xhtml), *First Script –* *Geocoding with Web APIs*. Unless data was carefully prepared for the exact purpose of analysis, the chances are that cleaning will take a lot of time and effort. Here, we learned the basics of pandas, and how to filter and mask the data. We discussed how to investigate missing values, saw how to use regular expressions to extract specific values from non-structured text, creating data of a proper structure and type, and learned how to apply custom functions to each cell in the entire `Series` or `DataFrame` and then used that information to geocode locations where we lacked coordinates.

Finally, we stored all the data we processed, along with the original values, in another CSV file, ready to be explored in our next chapter.

        

# Questions

1.  Why, if there is an empty cell in the Pandas column, are integer values in this column converted into floats?
2.  What is the benefit of plotting missing values?
3.  What is `regex`? Is it a separate language?
4.  How can we use `regex` in Python?

5.  How is a `regex` pattern defined? How can we combine and modify patterns dynamically within the code?
6.  Is it a good idea to run ordinary Python functions on dataframe cells? What are the pros and cons of that approach? Should we use loops for that?

        

# Further reading

*   *Pandas Cookbook*, by Packt ([https://www.packtpub.com/big-data-and-business-intelligence/pandas-cookbook](https://www.packtpub.com/big-data-and-business-intelligence/pandas-cookbook))
*   *Python Regular Expressions*, by Packt ([https://www.packtpub.com/application-development/mastering-python-regular-expressions](https://www.packtpub.com/application-development/mastering-python-regular-expressions))
*   *2018 Python Regular Expressions – Real-World Projects*, by Packt ([https://www.packtpub.com/big-data-and-business-intelligence/2018-python-regular-expressions-real-world-projects-video](https://www.packtpub.com/big-data-and-business-intelligence/2018-python-regular-expressions-real-world-projects-video))