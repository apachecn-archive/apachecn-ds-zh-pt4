        

# 七、使用 BeaultifulSoup4 从网上抓取数据

在前一章中，我们写了一段代码，它与 Nominatim web 服务通信以收集信息。然而，通常没有适当的 API，数据可能分散在数百个网页中，或者更糟的是，具有复杂结构的文件(pdf)。在这一章中，我们将探索另一种数据收集途径——抓取原始 HTML 页面。为了做到这一点，我们将使用另一个库，Beautiful Soup 4，它可以将原始 HTML 文件解析为对象，并帮助我们筛选它们，提取信息。使用这个工具，我们将收集一个相对较大的二战历史战役数据集，在接下来的章节中，我们将对其进行处理、清理和分析。

在本章中，我们将讨论以下主题:

*   当没有 API 时
*   第二次世界大战
*   超越 BeaultifulSoup

        

# 技术要求

在本章中，我们将利用`requests`和`BeautifulSoup`库——它们都包含在 Anaconda 发行版中。如果您不使用 Anaconda，请确保安装了它们。鉴于你将从网上搜集数据，互联网连接也是必需的。像往常一样，这一章的代码存储在 GitHub 库的`Chapter07`文件夹中，[https://GitHub . com/packt publishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications)。

        

# 当没有 API 时

与 API 服务一样，网页也有自己的所有者，他们可能会也可能不会接受抓取数据的想法。如果有现成的 API，这总是优于抓取，原因如下:

*   首先，它通常使用起来更好更简单，而且有很多保证，API 所有者将保留它的结构，或者至少让你提前知道即将到来的变化。对于 HTML 网页，没有任何保证；网站会经常改变，他们不会提前告诉你，所以期待很多突发的变化吧！
*   第二，作为一个好公民，从计算角度来说，提供原始数据比一个成熟的 HTML 页面要便宜得多，所以服务所有者会心存感激。
*   最后，一些数据(例如，历史变化)将无法通过网页获得。

然而，有很多没有 API 的网页的例子。有些信息不是为了共享(例如，电子商店对其价格被跟踪不感兴趣)，许多组织根本没有能力维护这些信息。

作为一个实践练习，我们现在将从维基百科收集数据，这是一个非常好和非常坏的网站收集的例子。这是一个很好的例子，因为维基百科完全支持抓取；事实上，它公开共享整个数据集，所以我们可以下载并使用它。与此同时，这是一个糟糕的例子，因为就其本质而言，维基百科没有严格的模板和数据结构——每个页面的结构和布局都是独一无二的，即使是原始数据集也不会改变这一点。大多数网站都是使用相同的模板预先生成的，所以平均而言，数据采集更容易。但总是脏脏的，半手工的辛苦活。

但是在我们深入研究编码之前，让我们讨论一下什么是 HTML 以及如何使用它。

        

# 简单地说 HTML

你可能知道网页是用三种主要语言编写的——HTML、CSS 和 JavaScript，其中只有后者是真正的语言。在这个三元组中，CSS 用于可视化地设置对象的样式，例如，设置元素的颜色或字体。JavaScript 是一种在客户端机器上运行的语言，允许在网站上进行基本的交互，例如，将信息发送回服务器，以及从下拉菜单中选择元素。

页面的主体是用 HTML 描述的。它的目标是呈现页面的层次和布局。HTML 是 XML 的子集，通用标记语言，专门为网页设计。像 XML 一样，HTML 通过嵌套系列来描述文档...

        

# 刮痧配 BeaultifulSoup 4

任何可公开访问的 HTTP 都可以用一个`requests`库拉出来。正如您所记得的，如果结果值存储为 JSON，`requests`有一个内置的解析方法。对于 HTML，情况就不同了:解析 HTML 不是一件简单的任务。它比你普通的 JSON 复杂得多；HTML 文件很大，可能是无效的(浏览器通常仍然会“修复”并呈现它们)。

为了做到这一点，我们将使用**Beautiful Soup 4**(**BS4**)，它是解析 HTML 的两个主要库之一，与 **LXML** 一起使用。BeaultifulSoup 还知道如何解析 HTML，甚至可以修复无效文件。一旦文档有了 Pythonic 表示，我们就可以通过使用元素 ID、类、CSS 属性、它们的顺序等等的组合，使用 CSS 选择器或 XPath 小型语言，深入并检索我们感兴趣的特定元素。

        

# CSS 和 XPath 选择器

如前所述，Beautiful Soup 将 HTML 从字符串解析为 Python 对象。即使经过解析，这种结构也不是一件容易导航的事情。由于 web 的动态特性，当我们一次操作多个页面时，对于批量检索来说尤其如此。即使是同一个页面也可能不断变化，添加或删除一些元素，更不用说不同的页面，即使是那些结构明显相同的页面。这是您开始欣赏定义良好且稳定的 API 的时刻！

为了导航 HTML 文档结构，也称为**文档对象模型** ( **DOMs** ，使用了两种常见且广泛采用的技术。第一个是 CSS 选择器，它是一种模式语言，用于处理 HTML 和...

        

# 开发者控制台

在我们提取页面的特定元素之前，我们首先需要知道在哪里搜索它们。为了理解页面结构，我们将使用 Chrome 浏览器内置的 Chrome 开发者控制台。火狐有一个类似的工具——火狐开发者工具。

要查看任何页面的 HTML 结构，只需在 Chrome 中打开它，将鼠标悬停在页面的任何特定元素上，右键单击鼠标按钮，然后选择 Inspect——将出现一个新窗口，显示页面的 HTML，以您刚刚悬停的元素为中心(参见下图):

![](img/f802e082-7469-4f09-8c3d-af84a59466dd.png)

请注意，当您将鼠标悬停在这个新窗口中的其他元素上时，页面上相应的元素会高亮显示。现在，转到您感兴趣的元素，在这个新窗口中找到它。注意窗口后面的一行——它描述了从文档的根目录到您看到的元素的路径。这条路径对我们来说非常重要——我们将使用它从 Python 的路径中提取特定的值。

现在让我们看看所有这些在实践中是如何工作的。

        

# 第二次世界大战

这一章的目标是从维基百科收集二战中所有战役的信息。提供了相应的列表:[https://en.wikipedia.org/wiki/List_of_World_War_II_battles](https://en.wikipedia.org/wiki/List_of_World_War_II_battles)。正如你所看到的，它包含了大量页面的链接，每个页面对应一场战斗、一次行动和一次战役。此外，列表是结构化的，所以战斗是根据战役或行动分组的，而战役或行动又是根据战区分组的——保留这种层次结构会很好！列表中的大多数元素也有日期。我们一会儿会处理这些列表。

现在，如果你查看几页特定的战斗，你可能会注意到它们有相似的结构。对他们中的大多数人来说...

        

# 第一步——收集战斗列表

先从刮主页面开始。为此，让我们经历几个步骤。

首先，我们需要使用`requests` 库以字符串形式收集主页面——与我们通过库的`get`方法使用 HTTP `GET`请求从 nomim 中提取信息的方式相同:

```py
import requests as rq

base_url = 'https://en.wikipedia.org/wiki/List_of_World_War_II_battles'
response = rq.get(url)
```

我们可以通过`response.content`访问页面的原始内容。

接下来，我们需要使用 BS4 将这个字符串解析成页面的 Python 表示:

```py
from bs4 import BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')
```

完美！为了构建代码，我们创建了一个新的函数，`get_dom` ( **DOM** 代表**文档对象模型**，它包含了前面所有的代码:

```py
def get_dom(url):
    response = rq.get(url)
    response.raise_for_status()
    return BeautifulSoup(response.content, 'html.parser')
```

接下来，一旦我们有了经过解析的 DOM，指定我们将要工作的页面区域——主要内容元素是一个好主意。使用 Chrome 开发人员控制台，我们可以看到主文本存储在带有`mw-parser-output`类的`div`元素中，并被包装到另一个具有`mw-content-text` ID 的`div`元素中。现在让我们使用 BS4，用 Python 来获取这些信息。

Beautiful Soup 有三种不同的方式来搜索元素——`find`、`find_all`和`select`。第一种和第二种方法希望您传递一个对象类型和可选的属性。递归参数定义搜索是否应该是递归的(比中的一个级别更深)。这两种方法之间的区别很微妙——第一种方法只检索第一次出现的内容。相反，第二个函数总是返回一个包含所有元素的列表。最后，`select` 也将返回一个列表——并期望您传递一个 CSS 选择器字符串。因此，更容易指定要检索的嵌套元素:

```py
content = soup.select('div#mw-content-text > div.mw-parser-output', limit=1)[0]
```

它们返回的对象类似于根 BS4 对象，但是只覆盖页面的相应部分。

在下一步中，我们需要为每个前端收集相应的元素，在我们的例子中是`h2`头。所有的前端都被组织成部分——每个部分都有一个标题(元素 T1 ),但是在层次上，标题并不嵌套在部分中；所有部分内容都位于相应标题的正下方。还有最后一个我们不关心的标题——引文和注释。一种过滤方法是删除最后一个元素。或者，我们可以使用 CSS 选择器技巧，使用下面的谓词: **`:not(:last-of-type)`** 。为了保持简单和可读性，我们将删除列表中的最后一个元素:

```py
fronts = content.select('div.mw-parser-output>h2')[:-1]
```

这里，我们在内容部分搜索所有的`h2`头，没有递归。我们收集了所有正确的标题吗？让我们检查一下！要从标题中删除`[edit]`元素，我们只需删除最后六个字符:

```py
>>> for el in fronts:
>>>     print(el.text[:-6])
African Front
Mediterranean Front
Western Front
Atlantic Ocean
Eastern Front
Indian Ocean
Pacific Theatre
China Front
Southeast Asia Front
```

这看起来是正确的。现在我们有了所有的正面！

但是我们怎样才能得到每个头对应的`ul`列表呢？这似乎是一个问题——我们需要检索基于相同级别上的其他元素并与之相关联的元素。幸运的是，BS4 可以使用`find_next_siblings`方法解决这个问题。它的工作方式与`find_all`完全一样，除了它不会在元素内部而是在紧随其后的总体文档中查找。换句话说，BS4 对象不仅仅存储给定 HTML 元素的信息——它们知道整个 HTML 文档。

看看下面的代码片段，我们通过使用`0`作为索引来获取第一个标题，然后我们运行该方法，传递查询属性——它应该是第一个`div`中的一个`ul`元素，有三个类:`div-col`、`columns`和`column-width`:

```py
>>> fronts[0].find_next_siblings("div", "div-col columns column-width")[0].ul
<ul><li><a href="/wiki/North_African_Campaign" title="North African Campaign">North African Campaign</a>.....
```

成功了！我们要求 BS4 在每个头之后找到所有带有`"div-col columns column-width"`类的`div`元素。它们有很多，但是只有第一个`div`元素与头相关。因此，我们需要检索第一个元素并获得底层的`ul`元素。这就是我们在下一节要做的事情。

        

# 无序列表

但是我们如何同时从嵌套列表中收集所有信息呢？这似乎是一个适合递归的任务——对于列表中的每个元素，我们将存储链接和日期(如果存在的话),如果它有一些嵌套元素，我们也将对它们运行相同的函数。为了跟踪嵌套层次，我们还添加了一个`level` 属性。考虑下面的例子。乍一看，这似乎过于复杂，但是所有这些`try` / `else`和`if`语句使代码在一些值丢失的情况下仍能工作——如果没有日期、链接或嵌套元素，函数仍能工作。我们还使用了`next`，因为我们不想获取所有的文本元素(并在它们上面浪费时间和内存)；我们只需要前两个:...

        

# 步骤 2-从维基页面抓取信息

因为我们需要从某个地方开始，所以让我们选择一个战斗页面来测试我们的代码——斯科尔皮恩行动([en.wikipedia.org//wiki/Operation_Skorpion](http://en.wikipedia.org//wiki/Operation_Skorpion))。以下是该页面的屏幕截图，我们感兴趣的区域以红色突出显示:

![](img/2bd8e62e-6763-46a1-9269-bf65220e50ad.png)

本章这一部分的代码存储在`B_Scraping_part_2`笔记本中。

首先，我们可以使用我们在上一节中编写的函数来收集页面作为一个`dom`对象:

```py
url = 'https://en.wikipedia.org/wiki/Opeation_Scorpion'
dom = get_dom(url)
```

接下来，和上一个例子一样，我们选择存储我们感兴趣的所有信息的较大容器——在这个例子中，是 info card(屏幕截图上较大的红色虚线矩形)。我们感兴趣的所有信息(日期、地点、结果、交战方和伤亡)都在地图下方:

```py
table = dom.find('table','infobox vevent')
```

现在我们有了卡片，我们需要做的就是从中提取所有的信息，这将在下一节中进行。

        

# 关键信息

关键信息存储在地图的正下方，包括日期、位置和结果。让我们把这部分称为主要部分。正如您在开发人员控制台上看到的，这个部分被设计成一个有两列的表——第一列代表一个键(指标名称),第二列代表相应的值。事实上，这与字典的结构非常相似，所以让我们编写一个从这个两列表到字典的通用转换器。看看下面的片段。这里，我们遍历行，将第一列的值作为键添加到字典中，将第二列的值作为值添加到字典中:

```py
def _table_to_dict(table):    result = {}    for row in table.find_all('tr'): result[row.th.text] = row.td.get_text().strip() ...
```

        

# 附加说明

页面上有许多我们希望包含在数据集中的附加信息，如交战者、领导者、优势和伤亡人数等部分(请参考*步骤 2-从 Wiki 页面*收集信息部分的截图下方的矩形，名为补充信息)。如您所见，这里的每个标题都是表格中的一行——所有实际信息都位于其下——类似于上一任务中标题后的列表。您还将观察到大多数部分被水平分割成两个单元格，第一个在 allies 上，第二个在 axis 上。事实上，在某些情况下(参考维尔纽斯攻击性页面)，维基百科会为事件中涉及的第三方添加第三栏。对于 Skorpion 行动，一些部分只有一列，代表总体结果(例如，总伤亡人数)，而对于其他部分(古德伍德行动—【https://en.wikipedia.org/wiki/Operation_Goodwood】T2)，伤亡人数在所涉双方之间分摊。这也需要解决。

所有的标题在所有的页面上都是一致的。因此，我们可以搜索特定的标题——如果有，从每个标题下的部分获取相应的数据。下面是这样做的代码片段；它会寻找一个标头，如果有，则获取下一部分:

```py
def _find_row_by_header(table, string):
    header = table.tbody.find('tr', text=string)

    if header is not None:
        return header.next_sibling
```

现在我们知道了如何获取每一部分，让我们解析它们。下面的代码检查表中的行数。如果只有一个，则存储为`total`。如果有两个或更多，它们将接收相应的列—`allies`、`axis`和`third party`:

```py
def _parse_row(row, names=('allies', 'axis', 'third party')):
    '''parse secondory info row
    as dict of info points
    '''
    cells = row.find_all('td', recursive=False)
    if len(cells) == 1:
        return {'total':cells[0].get_text().strip()}

    return {name:cell.get_text().strip() for name, cell in zip(names, cells)}
```

在同一页中，交战方的顺序假定是一致的。不幸的是，每一页边的顺序都不一样，我们找不到任何合理的解释。因此，我们称之为`axis`的列实际上可能指的是`allies`端——我们必须在[第 11 章](99046ac4-efac-4f29-9561-41f1dde49bc4.xhtml)、*数据清理和操作*中的数据清理过程中解决这个问题。

现在，让我们收集所有附加信息，使用一组预定义的标题来查找:

```py
def _additional(table):

    keywords = (
        'Belligerents',
        'Commanders and leaders',
        'Strength',
        'Casualties and losses',
    )

    result = {}
    for keyword in keywords:
        try:
            data = _find_row_by_header(table, keyword)
            if data:
                result[keyword] = _parse_row(data)
        except Exception as e:
            raise Exception(keyword, e)

    return result
```

请注意，这里的异常用于显示引发问题的关键字，从而有助于调试。

最后，让我们将页面的所有代码封装到一个函数中，我们将在所有链接上运行该函数:

```py
def parse_battle_page(url):
    ''' main function to parse battle urls from wikipedia
    '''
    try:
        dom = _default_collect(url) # dom
    except Exception as e:
        warnings.warn(str(e))
        return {}

    table = dom.find('table','infobox vevent') # info table
    if table is None: # some campaigns don't have table
        return {} 

    data = _get_main_info(table)
    data['url'] = url 

    additional = _additional(table)
    data.update(additional)
    return data
```

如果没有这样的页面，前面的`try` / `except`子句有助于捕捉异常——我们的数据库中有一个断开的链接。你可以在初始页面找到它；它以红色突出显示。

现在，作为一个额外的奖励，所有的操作和活动页面都有相同的结构，所以我们使用相同的代码从它们那里收集所有的信息。不错！

为了确保我们已经解决了大部分问题，在一组链接上测试代码，理想情况下，是最多样化和最奇特的链接。由于前面的代码将在另一个笔记本中使用，因此将它复制到一个专用的`.py`文件中是有意义的。

        

# 步骤 3–整体抓取数据

最后，我们有了所有的链接和代码，可以从每个链接和代码中获取信息。

本小节的代码可在`C_Scraping_part3`笔记本中找到。

首先，让我们导入我们将需要的所有代码，并阅读带有链接的文件:

```py
import jsonfrom wiki import parse_battle_pageimport timewith open('./all_battles.json', 'r') as f:    campaigns = json.load(f)
```

同样，我们需要编写一个递归函数，为给定事件抓取数据，并为所有嵌套事件调用自身:

```py
def _parse_in_depth(element, name):    '''attempts to scrape data for every    element with url attribute – and all the children    if there are any'''        if 'children' in element:        for k, child in element['children'].items(): parsed ...
```

        

# 质量管理

正如我们已经提到的，这种数据有很多问题，因为网页在结构上非常不同，并且提供不同的信息集，格式也不同。代码中有很多问题——清理所有这些问题将需要另一个章节(事实上，这就是我们将在[第 11 章](99046ac4-efac-4f29-9561-41f1dde49bc4.xhtml)、*数据清理和操作*中要做的)。然而，执行少量的基本质量控制，验证所有的页面都具有一些最小的、必需的属性，并且它们不为空，这是一个很好的实践。我们还可以添加一些其他检查，例如，确保附加字段不为空，至少对于相当多的页面是如此。

我们将使用双重方法。首先，我们将尝试定义一个值列表，我们假设每个记录都需要这些值。第二，我们已经知道某些页面会缺少某些信息，所以至少让我们计算一下。为此，我们定义了一个字典来存储所有信息。开始时，它将只包含零。考虑下面的例子。在这里，我们涵盖了我们将要检查的所有战斗(`total`)，记录了丢失的地点、结果和领土部分。此外，我们将计算一些只包含`total`部分中`Casualties`、`Commanders`和`Strength`的总值的记录。同样，我们将检查有多少记录没有这些部分:

```py
STATISTICS = {
    'battles_checked':0,
    'location_null':0,
    'result_null':0,
    'territorial_null': 0,
    'total': {
        'Casualties and losses':0,
        'Commanders and leaders':0,
        'Strength':0
    },
    'none': {
        'Casualties and losses':0,
        'Commanders and leaders':0,
        'Strength':0
    }
}
```

一旦定义了数据结构，让我们写一个检查函数。这是一个相当简单的问题，与其他问题保持一致。它是递归的，因为它在给定记录的所有子记录上调用自己。

注意，我们从一开始就检查所需的值。最后，我们只保留了`level`值作为必需属性:

```py
def qa(battle, name='Unknown'):
    required = (
        # 'Location'
        # 'url', 
        'level',
    )

    for el in required:
        assert el in battle and battle[el] is not None, (name, el)

    STATISTICS['battles_checked'] +=1

    for el in 'Location', 'Result', 'Territorial':
        if el not in battle or battle[el] is None:
            STATISTICS[f'{el.lower()}_null'] += 1

    for el in 'Casualties and losses', 'Commanders and leaders', 
    'Strength':
        if el not in battle:
            STATISTICS['none'][el] += 1
            continue

        if 'total' in battle[el]:
            STATISTICS['total'][el] += 1

    if 'children' in battle:
         for name, child in battle['children'].items():
                qa(child, name)
```

使用这个函数，我们现在可以循环查看记录并检查我们的统计数据:

```py
for _, front in campaigns_parsed.items():
    for name, campaign in front.items():
        qa(campaign, name)
```

前面定义的函数通过了所有测试。但是为什么我们从 required 中删除了`url`和`Location` ？有些记录确实漏掉了它们——例如，`Battle of Lang Son`根本没有链接，而其他一些记录(例如，法属西非—[https://en . Wikipedia . org/wiki/French _ West _ Africa _ in _ World _ War _ II](https://en.wikipedia.org/wiki/French_West_Africa_in_World_War_II)page)漏掉了`Location`和`Date`。在这种情况下，我们决定放松我们的要求，但是在那些丢失的记录上添加一个注释。请随意修改测试——这将让您深入了解一些不同类型的问题，我们将在未来使用该数据集来缓解这些问题。

一旦测试结束，我们可以检查统计数据:

```py
>>> STATISTICS

{'battles_checked': 624,
 'location_null': 37,
 'result_null': 40,
 'territorial_null': 553,
 'total': {'Casualties and losses': 7,
 'Commanders and leaders': 3,
 'Strength': 2},
 'none': {'Casualties and losses': 83,
 'Commanders and leaders': 44,
 'Strength': 109}}
```

大多数记录都缺少地域部分——相当多的记录没有关于整体实力的任何信息。同样，为将来收集这些信息是一个好主意。现在，让我们将获得的数据集存储到另一个 JSON:

```py
with open('_all_battles_parsed.json', 'w') as f:
    json.dump(campaigns_parsed, f)
```

我们完事了。本节中的三个步骤将帮助您收集数据并相应地呈现出来。

        

# 超越 BeaultifulSoup

在这个例子中，我们使用 BS4 库来解析静态 HTML。Beautiful Soup 是处理偶尔出现的混乱 HTML 的无价库，但是对于大规模和动态页面，它根本不够用。对于大量的生产报废，也许是定期的，使用 Scrapy([https://scrapy.org/](https://scrapy.org/))包是个好主意。Scrapy 是一个完整的框架，用于下载 HTML，解析数据，提取数据，然后存储数据。它的杀手锏之一是它可以异步运行——例如，当它等待一个页面加载时，它可以自动切换到处理另一个页面。正因为如此，Scrapy 的抓取工具在大型网站列表上的速度要快得多。...

        

# 摘要

在这一章中，我们学习了通过使用漂亮的 Soup 4 库从 HTML 页面抓取数据的艰苦工作。使用它，我们能够从一个页面收集所有链接，保留层次结构，并检索每个收集的链接的信息。这项技能是非常宝贵的，因为它允许你从互联网上收集信息，用于研究、商业或作为个人爱好。

我们还提到了 Selenium，它模拟了一个成熟的浏览器，可以与页面交互并执行 JavaScript，为我们提供了静态内容之外的访问。

在下一章，我们将清理和使用我们收集的数据，创建一个交互式的战争可视化。

        

# 问题

1.  在这个上下文中，术语“网络搜集”是什么意思？
2.  抓取和使用 API 的最大区别是什么？有哪些挑战？
3.  BeaultifulSoup 到底是做什么的？没有它我们能刮吗？
4.  为什么我们在这里使用递归？
5.  在刮除过程中应该清理数据吗？
6.  处理丢失数据或断开链接的正确方法是什么？

        

# 进一步阅读

*   *理查德·劳森和凯瑟琳·贾穆尔撰写的《Python 网页抓取-第二版*，由 Packt 出版([https://www . packtpub . com/big-data-and-business-intelligence/Python-Web-Scraping-第二版](https://www.packtpub.com/big-data-and-business-intelligence/python-web-scraping-second-edition))
*   *Python 网络抓取食谱*，作者 Michael Heydt，由 Packt 出版([https://www . packtpub . com/big-data-and-business-intelligence/Python-Web-Scraping-Cookbook](https://www.packtpub.com/big-data-and-business-intelligence/python-web-scraping-cookbook))