

# 十二、特征工程

概述

到本章结束时，你将能够合并多个数据集在一起；分类变量和数值变量；对数据执行聚合；并使用`pandas`操作日期。

本章将向您介绍在现有数据集上创建新变量的一些关键技术。

# 简介

在前面的章节中，我们学习了如何分析和准备数据集以提高其质量水平。在本章中，我们将向您介绍另一个有趣的主题:创建新功能，也称为功能工程。你已经在*第 3 章*、*二进制分类*中看到了其中的一些概念，但是我们将在本章中更深入地探讨。

特征工程的目标是为您正在执行的分析或您将要训练的机器学习算法提供更多信息。添加更多信息将有助于您获得更好、更准确的结果。

新功能可以来自内部数据源，例如来自数据库或不同系统的另一个表。例如，您可能希望将公司使用的 CRM 工具中的数据链接到营销工具中的数据。添加的功能也可以来自外部来源，例如来自合作伙伴或提供商的开源数据或共享数据。例如，您可能希望将销售量与天气 API 或政府人口普查数据相关联。但它也可以通过从现有变量创建新变量来自原始数据集。

让我们暂停一秒钟，理解为什么特征工程对于训练机器学习算法如此重要。我们都知道，这些算法近年来在从数据中发现极其复杂的模式方面取得了令人难以置信的成果。但是它们的主要局限性在于，它们只能在作为输入提供的数据中分析和找到有意义的模式。如果数据不正确、不完整或缺少重要特征，算法将无法正确执行。

另一方面，我们人类倾向于理解更广泛的背景，并很容易看到更大的画面。例如，如果您的任务是分析客户流失，甚至在查看现有数据之前，您就会期望它具有一些描述客户属性的特性，例如人口统计、订阅的服务或产品以及订阅日期。一旦我们收到数据，我们就可以突出显示我们认为重要但在数据集中缺失的特征。这就是为什么拥有专业知识和经验的数据科学家需要考虑更多信息，以帮助算法从丰富的数据中理解和检测更有意义的模式。事不宜迟，让我们开始吧。

# 合并数据集

大多数组织将其数据存储在数据库、数据仓库或数据湖等数据存储中。信息流可以来自不同的系统或工具。大多数情况下，数据存储在由多个表组成的关系数据库中，而不是存储在单个表中，表之间有明确的关系。

例如，一个在线商店可以有多个表来记录在其平台上进行的所有购买。一个表可能包含与现有客户相关的信息，另一个表可能列出目录中所有现有和过去的产品，第三个表可能包含所有发生的交易，等等。

如果您正在为一个电子商务平台(如 Amazon)向客户推荐产品，那么您可能只获得了 transactions 表中的数据。在这种情况下，您希望获得每个产品和客户的一些属性，并要求提取您需要的这些附加表，然后在构建您的推荐系统之前将这三个表合并在一起。

让我们用一个真实的例子来看看如何合并多个数据源:我们在前一章中使用的在线零售数据集。我们将添加关于交易是否发生在英国公共假日的新信息。这些额外的数据可能有助于模型了解销售额与一些公共假日(如圣诞节或女王生日，这是澳大利亚等国家的假日)之间是否存在一些相关性。

注意

英国的公共假期列表将从该网站摘录:[https://packt.live/2twsFVR](https://packt.live/2twsFVR)。

首先，我们需要将在线零售数据集导入到`pandas`数据框架中:

```py
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter12/Dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
df.head()
```

您应该得到以下输出。

![Figure 12.01: First five rows of the Online Retail dataset
](img/B15019_12_01.jpg)

图 12.01:在线零售数据集的前五行

接下来，我们将把英国所有的公共假日加载到另一个`pandas`数据帧中。从*第十章*、*分析一个数据集*我们知道这个数据集的记录只有 2010 年和 2011 年的。因此，我们将提取这两年的公共假期，但我们需要分两个不同的步骤来完成，因为由`date.nager`提供的 API 仅被拆分为一年。

让我们首先关注 2010 年:

```py
uk_holidays_2010 = pd.read_csv('https://date.nager.at/PublicHoliday/Country/GB/2010/CSV')
```

我们可以打印它的形状，看看它有多少行和列:

```py
uk_holidays_2010.shape
```

您应该得到以下输出。

```py
(13, 8)
```

我们可以看到那一年有`13`个公共假日，有`8`个不同的栏目。

让我们打印该数据帧的前五行:

```py
uk_holidays_2010.head()
```

您应该得到以下输出:

![Figure 12.02: First five rows of the UK 2010 public holidays DataFrame
](img/B15019_12_02.jpg)

图 12.02:英国 2010 年公共假日数据框架的前五行

现在我们已经有了 2010 年的公共假日列表，让我们提取 2011 年的公共假日:

```py
uk_holidays_2011 = pd.read_csv('https://date.nager.at/PublicHoliday/Country/GB/2011/CSV')
uk_holidays_2011.shape
```

您应该得到以下输出。

```py
(15, 8)
```

2011 年有`15`个公共假期。现在我们需要合并这两个数据帧的记录。我们将使用来自`pandas`的`.append()`方法，并将结果分配到一个新的数据帧中:

```py
uk_holidays = uk_holidays_2010.append(uk_holidays_2011)
```

让我们检查追加两个数据帧后的行数是否正确:

```py
uk_holidays.shape
```

您应该得到以下输出:

```py
(28, 8)
```

我们得到了`28`的记录，这与 2010 年和 2011 年的公共假期总数相对应。

为了将两个数据帧合并在一起，我们需要在它们之间至少有一个公共列，这意味着两个数据帧应该至少有一个包含相同类型信息的列。在我们的示例中，我们将使用`Date`列合并这个数据框架和`InvoiceDate`列上的在线零售数据框架。我们可以看到这两列的数据格式是不同的:一列是日期(`yyyy-mm-dd`)，另一列是日期时间(`yyyy-mm-dd hh:mm:ss`)。

因此，我们需要将`InvoiceDate`列转换成日期格式(`yyyy-mm-dd`)。一种方法是(我们将在本章后面看到另一种方法)将该列转换成文本，然后使用`.str.slice()`方法提取每个单元格的前 10 个字符。

例如，日期 2010-12-01 08:26:00 将首先被转换为字符串，然后我们将只保留前 10 个字符，即 2010-12-01。我们将这些结果保存到一个名为`InvoiceDay`的新列中:

```py
df['InvoiceDay'] = df['InvoiceDate'].astype(str).str.slice(stop=10)
df.head()
```

![Figure 12.03: First five rows after creating InvoiceDay
](img/B15019_12_03.jpg)

图 12.03:创建 InvoiceDay 后的前五行

现在，来自在线零售数据框架的`InvoiceDay`和来自英国公共假日数据框架的`Date`具有相似的信息，因此我们可以使用来自`pandas`的`.merge()`将这两个数据框架合并在一起。

有多种方法可以将两个表连接在一起:

*   左侧连接
*   正确的连接
*   内部联接
*   外部联接

### 左边加入

左连接将保留第一个数据帧中的所有行，即*在线零售*数据集(左侧)，并将其连接到第二个数据帧中的匹配行，即*英国公共假日*数据集(右侧)，如图*图 12.04* 所示:

![Figure 12.04: Venn diagram for left join
](img/B15019_12_04.jpg)

图 12.04:左连接的维恩图

要执行左连接，我们需要指定。merge()方法的以下参数:

*   `how = 'left'`对于左连接
*   `left_on = InvoiceDay`从左侧指定用于合并的列(这里是来自在线零售数据框的`Invoiceday`列)
*   `right_on = Date`从右侧指定用于合并的列(这里是英国公共假日数据框架中的`Date`列)

这些参数组合在一起，如下面的代码片段所示:

```py
df_left = pd.merge(df, uk_holidays, left_on='InvoiceDay', right_on='Date', how='left')
df_left.shape
```

您应该得到以下输出:

```py
(541909, 17)
```

我们得到了与原始在线零售数据框架完全相同的行数，这是一个左连接。让我们看看前五行:

```py
df_left.head()
```

您应该得到以下输出:

![Figure 12.05: First five rows of the left-merged DataFrame
](img/B15019_12_05.jpg)

图 12.05:左合并数据帧的前五行

我们可以看到，公共假日数据框架中的八列已被合并到原始数据框架中。如果第二个数据帧(本例中为公共假日数据帧)中没有匹配的行，`pandas`将用缺失值(`NaT`或`NaN`)填充所有单元格，如图*图 12.05* 所示。

### 右加入

右连接与左连接相似，只是它会保留第二个数据帧(右侧)的所有行，并尝试将其与第一个数据帧(左侧)匹配，如图*图 12.06* 所示:

![Figure 12.06: Venn diagram for right join
](img/B15019_12_06.jpg)

图 12.06:右连接的维恩图

我们只需要指定参数:

*   `how` `= 'right`'到`.merge()`方法来执行这种类型的连接。
*   我们将使用与上一个示例完全相同的用于合并的列，对于在线零售数据框架是`InvoiceDay`,对于英国公共假日数据框架是`Date`。

这些参数组合在一起，如下面的代码片段所示:

```py
df_right = df.merge(uk_holidays, left_on='InvoiceDay', right_on='Date', how='right')
df_right.shape
```

您应该得到以下输出:

```py
(9602, 17)
```

我们可以看到，由于正确的连接，行数减少了，但是它没有获得与公共假日数据帧相同的数量。这是因为在线零售数据框架中有多行与公共假日数据框架中的一个日期相匹配。

例如，查看合并的数据帧的第一行，我们可以看到在 2011 年 1 月 4 日有多个购买，所以它们都与相应的公共假日匹配。看看下面的代码片段:

```py
df_right.head()
```

您应该得到以下输出:

![Figure 12.07: First five rows of the right-merged DataFrame
](img/B15019_12_07.jpg)

图 12.07:右合并数据帧的前五行

还有另外两种类型的合并:内部合并和外部合并。

内部联接将只保留两个表之间匹配的行:

![Figure 12.08: Venn diagram for inner join
](img/B15019_12_08.jpg)

图 12.08:内部连接的维恩图

你只需要在`.merge()`方法中指定`how = 'inner'`参数。

这些参数组合在一起，如下面的代码片段所示:

```py
df_inner = df.merge(uk_holidays, left_on='InvoiceDay', right_on='Date', how='inner')
df_inner.shape
```

您应该得到以下输出:

```py
(9579, 17)
```

我们可以看到，在英国的公共假日期间，只有 9579 次观察。

外部连接将保留两个表中的所有行(匹配和不匹配)，如图*图 12.09* 所示:

![Figure 12.09: Venn diagram for outer join
](img/B15019_12_09.jpg)

图 12.09:外部连接的维恩图

正如您可能已经猜到的，您只需要在`.merge()`方法中指定`how == 'outer'`参数:

```py
df_outer = df.merge(uk_holidays, left_on='InvoiceDay', right_on='Date', how='outer')
df_outer.shape
```

您应该得到以下输出:

```py
(541932, 17)
```

在合并两个表之前，知道你的重点是什么是极其重要的。如果您的目标是通过添加另一个数据集中的列来扩展原始数据集中的要素数量，那么您可能会使用左连接或右连接。但是请注意，由于两个表之间潜在的多个匹配，您可能会得到更多的观察结果。另一方面，如果您想知道两个表中哪些观察值匹配或不匹配，您可以使用内部或外部连接。

## 练习 12.01:合并 ATO 数据集和邮政编码数据

在本练习中，我们将把 ATO 数据集(28 列)与邮政编码数据集(150 列)合并，以获得一个包含更多列的更丰富的数据集。

注意

澳大利亚税务局(ATO)数据集可以在 Packt GitHub 存储库中找到:[https://packt.live/39B146q](https://packt.live/39B146q)。

邮政编码数据集可以在这里找到:[https://packt.live/2sHAPLc](https://packt.live/2sHAPLc)。

数据集的来源如下:

澳大利亚税务局(ATO):https://packt.live/361i1p3。

邮政编码数据集:[https://packt.live/2umIn6u](https://packt.live/2umIn6u)。

以下步骤将帮助您完成练习:

1.  打开一个新的 Colab 笔记本。
2.  现在，从`pandas`包的`import`开始:

    ```py
    import pandas as pd
    ```

3.  将 ATO 数据集的链接分配给一个名为`file_url` :

    ```py
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter12/Dataset/taxstats2015.csv'
    ```

    的变量
4.  使用`pandas`包中的`.read_csv()`方法，将数据集加载到一个名为`df` :

    ```py
    df = pd.read_csv(file_url)
    ```

    的新数据框架中
5.  Display the dimensions of this DataFrame using the `.shape` attribute:

    ```py
    df.shape
    ```

    您应该得到以下输出:

    ```py
    (2473, 28)
    ```

    ATO 数据集包含`2471`行和`28`列。

6.  Display the first five rows of the ATO DataFrame using the `.head()` method:

    ```py
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.10: First five rows of the ATO dataset
    ](img/B15019_12_10.jpg)

    图 12.10:ATO 数据集的前五行

    两个数据帧都有一个名为`Postcode`的包含邮政编码的列，所以我们将使用它将它们合并在一起。

    注意

    邮政编码是澳大利亚邮政编码的名称。它是邮政区域的标识符。

    我们有兴趣了解更多关于这些邮政编码的信息。让我们确保它们在这个数据集中都是唯一的。

7.  Display the number of unique values for the `Postcode` variable using the `.nunique()` method:

    ```py
    df['Postcode'].nunique()
    ```

    您应该得到以下输出:

    ```py
    2473
    ```

    该列中有`2473`个唯一值，而数据帧有`2473`行，所以我们确信`Postcode`变量只包含唯一值。

    现在，将第二个邮政编码数据集的链接分配给一个名为`postcode_df`的变量:

    ```py
    postcode_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter12/Dataset/taxstats2016individual06taxablestatusstateterritorypostcodetaxableincome%20(2).xlsx?raw=true'
    ```

8.  Load the second Postcode dataset into a new DataFrame called `postcode_df` using the `.read_excel()` method.

    我们将只加载*个人表 6B* 表，因为这是数据所在的位置，所以我们需要向`sheet_name`参数提供这个名称。此外，该电子表格中的标题行(包含变量的名称)位于第三行，因此我们需要将其指定给 header 参数。

    注意

    别忘了 Python 中的`index`是以 0 开头的。

    看看下面的代码片段:

    ```py
    postcode_df = pd.read_excel(postcode_url, sheet_name='Individuals Table 6B', header=2)
    ```

9.  Print the dimensions of `postcode_df` using the `.shape` attribute:

    ```py
    postcode_df.shape
    ```

    您应该得到以下输出:

    ```py
    (2567, 150)
    ```

    该数据帧包含用于`150`列的`2567`行。通过将其与 ATO 数据集合并，我们将获得每个邮政编码的附加信息。

10.  Print the first five rows of `postcode_df` using the `.head()` method:

    ```py
    postcode_df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.11: First five rows of the Postcode dataset
    ](img/B15019_12_11.jpg)

    图 12.11:邮政编码数据集的前五行

    我们可以看到第二列包含邮政编码值，这是我们将用来与 ATO 数据集合并的值。让我们检查它们是否是唯一的。

11.  Print the number of unique values in this column using the `.nunique()` method as shown in the following code snippet:

    ```py
    postcode_df['Postcode'].nunique()
    ```

    您应该得到以下输出:

    ```py
    2567
    ```

    有`2567`个唯一值，这正好对应于这个数据帧的行数，所以我们绝对确定这个列包含唯一值。这也意味着合并两个表后，将只有一对一的匹配。我们不会遇到这样的情况:从一个数据集中得到多行，而另一个数据集中只有一行匹配。例如，ATO 数据帧中的邮政编码`2029`在第二个邮政编码数据帧中只有一个匹配。

12.  使用`.merge()`方法对两个数据帧执行左连接，并将结果保存到名为`merged_df`的新数据帧中。指定`how='left'`和`on='Postcode'`参数:

    ```py
    merged_df = pd.merge(df, postcode_df, how='left', on='Postcode')
    ```

13.  Print the dimensions of the new merged DataFrame using the `.shape` attribute:

    ```py
    merged_df.shape
    ```

    您应该得到以下输出:

    ```py
    (2473, 177)
    ```

    合并后我们得到了确切的`2473`行，这是我们所期望的，因为我们使用了左连接，并且在两个原始数据帧的`Postcode`列上有一对一的匹配。此外，我们现在有了`177`列，这是本练习的目标。但是在得出结论之前，我们想看看两个数据集之间是否有任何不匹配的邮政编码。为此，我们将查看右侧数据帧(邮政编码数据集)中的一列，看看是否有任何丢失的值。

14.  Print the total number of missing values from the `'State/Territory1'` column by combining the `.isna()` and `.sum()` methods:

    ```py
    merged_df['State/ Territory1'].isna().sum()
    ```

    您应该得到以下输出:

    ```py
    4
    ```

    ATO 数据集中有四个邮政编码与邮政编码不匹配。

    让我们看看他们是哪一个。

15.  Print the missing postcodes using the `.iloc()` method, as shown in the following code snippet:

    ```py
    merged_df.loc[merged_df['State/ Territory1'].isna(), 'Postcode']
    ```

    您应该得到以下输出:

    ![Figure 12.12: List of unmatched postcodes
    ](img/B15019_12_12.jpg)

图 12.12:不匹配的邮政编码列表

邮政编码数据集中缺失的邮政编码是`3010`、`4462`、`6068`和`6758`。在一个真实的项目中，你必须与你的利益相关者或者数据团队取得联系，看看你是否能够得到这些数据。

我们已经成功地合并了两个感兴趣的数据集，并将特征的数量从`28`扩展到`177`。我们现在有了更丰富的数据集，并将能够对其进行更详细的分析。

在下一个主题中，将向您介绍宁滨变量。

# 宁滨的变化

如前所述，特征工程不仅仅是获取数据集中不存在的信息。通常，您必须从现有的特性中创建新的特性。例如，将现有列中的值合并到新的值列表中。

例如，数据集中的某些分类列可能有大量的唯一值，比如说每个变量有 1000 多个值。这实际上是相当多的信息，算法需要额外的计算能力来处理和学习模式。如果您使用云计算服务，这可能会对项目成本或项目交付时间产生重大影响。

一个可能的解决方案是不使用这些列并删除它们，但是在这种情况下，您可能会丢失一些对业务非常重要和关键的信息。另一个解决方案是通过将唯一值的数量减少到一个较小的数字，比如 100，来创建这些列的一个更统一的版本。这将大大加快算法的训练过程，而不会丢失太多信息。这种转换被称为宁滨，传统上，它指的是数字变量，但同样的逻辑也可以应用于分类变量。

让我们看看如何在在线零售数据集上实现这一点。首先，我们需要加载数据:

```py
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter12/Dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

在*第 10 章*、*分析数据集*中，我们了解到`Country`列包含`38`个不同的唯一值:

```py
df['Country'].unique()
```

您应该得到以下输出:

![Figure 12.13: List of unique values for the Country column
](img/B15019_12_13.jpg)

图 12.13:国家列的唯一值列表

我们将把一些国家分为亚洲、中东和美洲等地区。我们将让欧洲国家保持现状。

首先，让我们通过复制`Country`列来创建一个名为`Country_bin`的新列:

```py
df['Country_bin'] = df['Country']
```

然后，我们将为`Country`列创建一个名为`asian_countries`的列表，其中包含来自唯一值列表的亚洲国家的名称:

```py
asian_countries = ['Japan', 'Hong Kong', 'Singapore']
```

最后，使用来自`pandas`的`.loc()`和`.isin()`方法，我们将把`asian_countries`列表中所有国家的`Country_bin`值改为`Asia`:

```py
df.loc[df['Country'].isin(asian_countries), 'Country_bin'] = 'Asia'
```

现在，如果我们打印这个新列的唯一值的列表，我们将看到三个亚洲国家(`Japan`、`Hong Kong`和`Singapore`)已经被值`Asia`替换:

```py
df['Country_bin'].unique()
```

您应该得到以下输出:

![Figure 12.14: List of unique values for the Country_bin column after binning Asian countries
](img/B15019_12_14.jpg)

图 12.14:宁滨亚洲国家之后的 Country_bin 列的唯一值列表

让我们对中东国家执行同样的过程:

```py
m_east_countries = ['Israel', 'Bahrain', 'Lebanon', 'United Arab Emirates', 'Saudi Arabia']
df.loc[df['Country'].isin(m_east_countries), 'Country_bin'] = 'Middle East'
df['Country_bin'].unique()
```

您应该得到以下输出:

![Figure 12.15: List of unique values for the Country_bin column after binning Middle Eastern countries
](img/B15019_12_15.jpg)

图 12.15:宁滨中东国家之后的 Country_bin 列的唯一值列表

最后，让我们将北美和南美的所有国家归为一组:

```py
american_countries = ['Canada', 'Brazil', 'USA']
df.loc[df['Country'].isin(american_countries), 'Country_bin'] = 'America'
df['Country_bin'].unique()
```

您应该得到以下输出:

![Figure 12.16: List of unique values for the Country_bin column after binning countries from North and South America
](img/B15019_12_16.jpg)

图 12.16:北美和南美宁滨国家之后的 Country_bin 列的唯一值列表

```py
df['Country_bin'].nunique()
```

您应该得到以下输出:

```py
30
```

`30`是`Country_bin`列的唯一值的数量。因此，我们将该列中唯一值的数量从`38`减少到了`30`:

我们刚刚看到了如何对分类值进行分组，但是同样的过程也可以应用于数值。例如，将人们的年龄分为 20 岁(20 到 29 岁)、30 岁(30 到 39 岁)等等是很常见的。

看看*习题 12.02* 。

## 练习 12.02: 宁滨来自艾姆斯住宅数据集中的年建变量

在本练习中，我们将通过宁滨现有的数字列创建一个新特征，以便将唯一值的数量从`112`减少到`15`。

注意

我们将在这个练习中使用的数据集是 Ames Housing 数据集，它可以在我们的 GitHub 存储库中找到:[https://packt.live/35r2ahN](https://packt.live/35r2ahN)。

这个数据集是由迪安·德·科克编辑的:[https://packt.live/2uojqHR](https://packt.live/2uojqHR)。

该数据集包含 2010 年至 2016 年爱荷华州埃姆斯市的住宅销售列表。

关于每个变量的更多信息可以在这里找到:[https://packt.live/2sT88L4](https://packt.live/2sT88L4)。

1.  打开一个新的 Colab 笔记本。
2.  导入`pandas`和`altair`包:

    ```py
    import pandas as pd
    import altair as alt
    ```

3.  将数据集的链接分配给一个名为`file_url` :

    ```py
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter12/Dataset/ames_iowa_housing.csv'
    ```

    的变量
4.  使用`pandas`包中的`.read_csv()`方法，将数据集加载到一个名为`df` :

    ```py
    df = pd.read_csv(file_url)
    ```

    的新数据框架中
5.  Display the first five rows using the `.head()` method:

    ```py
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.17: First five rows of the AMES housing DataFrame
    ](img/B15019_12_17.jpg)

    图 12.17:AMES 房屋数据框的前五行

6.  Display the number of unique values on the column using `.nunique()`:

    ```py
    df['YearBuilt'].nunique()
    ```

    您应该得到以下输出:

    ```py
    112
    ```

    在`YearBuilt`列中有`112`个不同或唯一的值:

7.  Print a scatter plot using `altair` to visualize the number of records built per year. Specify `YearBuilt:O` as the x-axis and `count()` as the y-axis in the `.encode()` method:

    ```py
    alt.Chart(df).mark_circle().encode(alt.X('YearBuilt:O'), y='count()')
    ```

    您应该得到以下输出:

    ![Figure 12.18: First five rows of the AMES housing DataFrame
    ](img/B15019_12_18.jpg)

    图 12.18:AMES 房屋数据框的前五行

    注意

    由于其局限性，GitHub 上没有显示输出。如果您在 Colab 文件上运行此命令，将会显示图表。

    有几年没有卖出多少房产。所以，你可以把他们按年代分组(10 年一组)。

8.  创建一个名为`year_built`的列表，包含`YearBuilt` 列中的所有唯一值:

    ```py
    year_built = df['YearBuilt'].unique()
    ```

9.  Create another list that will compute the decade for each year in `year_built`. Use list comprehension to loop through each year and apply the following formula: `year - (year % 10)`.

    例如，应用于 2015 年的公式将得出 2015 - (2015 % 10)，即 2015–5 等于 2010。

    注意

    %对应于模运算符，并将返回每年的最后一位数字。

    看看下面的代码片段:

    ```py
    decade_list = [year - (year % 10) for year in year_built]
    ```

10.  从`decade_list`创建唯一值的排序列表，并将结果保存到名为`decade_built`的新变量中。为此，将`decade_list`转换成一个集合(这将排除所有的重复项)，然后使用`sorted()`函数，如下面的代码片段所示:

    ```py
    decade_built = sorted(set(decade_list))
    ```

11.  Print the values of `decade_built`:

    ```py
    decade_built
    ```

    您应该得到以下输出:

    ![Figure 12.19: List of decades
    ](img/B15019_12_19.jpg)

    图 12.19:十年列表

    现在我们有了一个列表，我们将把它放入`YearBuilt`栏。

12.  在`df`数据帧上创建一个名为`DecadeBuilt`的新列，它将把来自`YearBuilt`的每个值绑定到一个十年中。您将使用来自`pandas`的`.cut()`方法，并指定`bins=decade_built`参数:

    ```py
    df['DecadeBuilt'] = pd.cut(df['YearBuilt'], bins=decade_built)
    ```

13.  Print the first five rows of the DataFrame but only for the `'YearBuilt'` and `'DecadeBuilt'` columns:

    ```py
    df[['YearBuilt', 'DecadeBuilt']].head()
    ```

    您应该得到以下输出:

    ![Figure 12.20: First five rows after binning
    ](img/B15019_12_20.jpg)

图 12.20:宁滨之后的前五行

我们可以看到每一年都被恰当地分配到相应的十年。

我们已经成功地从宁滨的`YearBuilt`列创建了一个新的特征，它的值分成几十个组。我们已经将唯一值的数量从`112`减少到`15`。

# 操纵日期

在您将要处理的大多数数据集中，会有一个或多个包含日期信息的列。通常，你不会将这种类型的信息直接作为机器学习算法的输入。原因是您不希望它学习非常具体的模式，例如客户 A 在 2012 年 8 月 3 日上午 08:11 购买了产品 X。在这种情况下，模型将会过度拟合，并且无法推广到未来的数据。

你真正想要的是学习模式的模型，比如有小孩的顾客倾向于在 12 月购买独角兽玩具。您不需要提供原始日期，而是希望提取一些周期性特征，比如一年中的月份、一周中的日期等等。在本节中，我们将看到使用`pandas`包获得这类信息是多么容易。

注意

这条经验法则有一个例外。如果您正在执行时间序列分析，这种算法需要一个日期列作为输入特性，但这超出了本书的范围。

在*第 10 章*、*分析数据集*中，你已经了解了`pandas`中数据类型的概念。当时，我们主要关注数字变量和范畴变量，但还有一个重要的变量:`datetime`。让我们再来看看在线零售数据集中每一列的类型:

```py
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter12/Dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
df.dtypes
```

您应该得到以下输出:

![Figure 12.21: Data types for the variables in the Online Retail dataset
](img/B15019_12_21.jpg)

图 12.21:在线零售数据集中变量的数据类型

我们可以看到，`pandas`自动检测到`InvoiceDate`的类型是`datetime`。但是对于其他一些数据集，它可能无法正确识别日期。在这种情况下，您必须使用`.to_datetime()`方法手动转换它们:

```py
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
```

一旦将列转换为`datetime`，pandas 提供了很多属性和方法来提取与时间相关的信息。例如，如果您想获得日期的年份，您可以使用`.dt.year`属性:

```py
df['InvoiceDate'].dt.year
```

您应该得到以下输出:

![Figure 12.22: Extracted year for each row for the InvoiceDate column
](img/B15019_12_22.jpg)

图 12.22:为 InvoiceDate 列的每一行提取年份

您可能已经猜到了，提取日期的月份和日期的属性分别是:`.dt.month`和`.dt.day`。您可以使用`.dt.dayofweek`属性从日期中获取星期几:

```py
df['InvoiceDate'].dt.dayofweek
```

您应该得到以下输出。

![Figure 12.23: Extracted day of the week for each row for the InvoiceDate column
](img/B15019_12_23.jpg)

图 12.23:为 InvoiceDate 列的每一行提取的星期几

注意

你可以在这里找到可用属性的完整列表:[https://packt.live/2ZUe02R](https://packt.live/2ZUe02R)。

使用日期时间列，您还可以执行一些数学运算。例如，我们可以使用 pandas 时序偏移对象`pd.tseries.offsets.Day(3)`为每个日期添加`3`天:

```py
df['InvoiceDate'] + pd.tseries.offsets.Day(3)
```

您应该得到以下输出:

![Figure 12.24: InvoiceDate column offset by three days
](img/B15019_12_24.jpg)

图 12.24: InvoiceDate 列偏移三天

您也可以使用`pd.tseries.offsets.BusinessDay()`通过工作日来抵消天数。例如，如果我们想得到以前的营业日，我们这样做:

```py
df['InvoiceDate'] + pd.tseries.offsets.BusinessDay(-1) 
```

您应该得到以下输出:

![Figure 12.25: InvoiceDate column offset by -1 business day
](img/B15019_12_25.jpg)

图 12.25: InvoiceDate 列偏移了-1 个工作日

另一个有趣的日期操作是使用`pd.Timedelta()`应用特定的时间频率。例如，如果您想从日期中获取一个月的第一天，您可以:

```py
df['InvoiceDate'] + pd.Timedelta(1, unit='MS')
```

您应该得到以下输出:

![Figure 12.26: InvoiceDate column transformed to the start of the month
](img/B15019_12_26.jpg)

图 12.26: InvoiceDate 列转换为月初

要获取月末日期，只需将参数单位改为`M`:

```py
df['InvoiceDate'] + pd.Timedelta(1, unit='M')
```

您应该得到以下输出:

![Figure 12.27: InvoiceDate column transformed to the end of the month
](img/B15019_12_27.jpg)

图 12.27: InvoiceDate 列转换到月末

注意

你会在这里找到所有支持的频率:[https://packt.live/2QuV33X](https://packt.live/2QuV33X)。

正如您在本节中看到的，`pandas`包提供了许多不同的 API 来操作日期。你已经学会了如何使用一些最流行的。你现在可以自己探索其他的了。

## 练习 12.03:金融服务消费者投诉的数据处理

在本练习中，我们将学习如何使用`pandas`从两个现有的日期列中提取与时间相关的信息，以便创建六个新列:

注意

我们将在本练习中使用的数据集是金融服务客户投诉数据集，它可以在我们的 GitHub 存储库中找到:[https://packt.live/2ZYm9Dp](https://packt.live/2ZYm9Dp)。

原始数据集可以在这里找到:[https://packt.live/35mFhMw](https://packt.live/35mFhMw)。

1.  打开一个新的 Colab 笔记本。
2.  导入`pandas`包:

    ```py
    import pandas as pd
    ```

3.  将数据集的链接分配给一个名为`file_url` :

    ```py
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter12/Dataset/Consumer_Complaints.csv'
    ```

    的变量
4.  使用`pandas`包中的`.read_csv()`方法，将数据集加载到名为`df` :

    ```py
    df = pd.read_csv(file_url)
    ```

    的新数据帧中
5.  Display the first five rows using the `.head()` method:

    ```py
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.28: First five rows of the Customer Complaint DataFrame
    ](img/B15019_12_28.jpg)

    图 12.28:客户投诉数据框的前五行

6.  Print out the data types for each column using the `.dtypes` attribute:

    ```py
    df.dtypes
    ```

    您应该得到以下输出:

    ![Figure 12.29: Data types for the Customer Complaint DataFrame
    ](img/B15019_12_29.jpg)

    图 12.29:客户投诉数据框架的数据类型

    `Date received`和`Date sent to company`列尚未被识别为日期时间，因此我们需要手动转换它们。

7.  使用`pd.to_datetime()`方法:

    ```py
    df['Date received'] = pd.to_datetime(df['Date received'])
    df['Date sent to company'] = pd.to_datetime(df['Date sent to company'])
    ```

    将`Date received`和`Date sent to company`列转换为日期时间
8.  Print out the data types for each column using the `.dtypes` attribute:

    ```py
    df.dtypes
    ```

    您应该得到以下输出:

    ![Figure 12.30: Data types for the Customer Complaint DataFrame after conversion
    ](img/B15019_12_30.jpg)

    图 12.30:转换后客户投诉数据帧的数据类型

    现在这两列有了正确的数据类型。现在让我们从这两个日期创建一些新的特性。

9.  使用`.dt.year`属性:

    ```py
    df['YearReceived'] = df['Date received'].dt.year
    ```

    创建一个名为`YearReceived`的新列，它将包含来自`Date Received`列的每个日期的年份
10.  创建一个名为`MonthReceived`的新列，它将包含使用`.dt.month`属性的每个日期的月份:

    ```py
    df['MonthReceived'] = df['Date received'].dt.month
    ```

11.  创建一个名为`DayReceived`的新列，它将使用`.dt.day`属性:

    ```py
    df['DomReceived'] = df['Date received'].dt.day
    ```

    包含每个日期中的某一天
12.  创建一个名为`DowReceived`的新列，它将使用`.dt.dayofweek`属性:

    ```py
    df['DowReceived'] = df['Date received'].dt.dayofweek
    ```

    包含每个日期的星期几
13.  Display the first five rows using the `.head()` method:

    ```py
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.31: First five rows of the Customer Complaint DataFrame after creating four new features
    ](img/B15019_12_31.jpg)

    图 12.31:创建四个新功能后，客户投诉数据框的前五行

    我们可以看到我们已经成功地创建了四个新特性:`YearReceived`、`MonthReceived`、`DayReceived`和`DowReceived`。现在让我们创建另一个指示日期是否在周末的例子。

14.  创建一个名为`IsWeekendReceived`的新列，该列包含二进制值，表示`DowReceived`列是否大于或等于`5` ( `0`对应星期一，`5`和`6`分别对应星期六和星期天):

    ```py
    df['IsWeekendReceived'] = df['DowReceived'] >= 5
    ```

15.  Display the first `5` rows using the `.head()` method:

    ```py
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.32: First five rows of the Customer Complaint DataFrame after creating the weekend feature 
    ](img/B15019_12_32.jpg)

    图 12.32:创建周末功能后客户投诉数据框的前五行

    我们创建了一个新功能，说明每个投诉是否在周末收到。现在，我们将为 engineer 添加一个新列，显示从`Date sent to company`到`Date received`之间的天数。

16.  创建一个名为`RoutingDays`的新列，它将包含`Date sent to company`和`Date received`之间的差异:

    ```py
    df['RoutingDays'] = df['Date sent to company'] - df['Date received']
    ```

17.  Print out the data type of the new `'RoutingDays'` column using the `.dtype` attribute:

    ```py
    df['RoutingDays'].dtype
    ```

    您应该得到以下输出:

    ![Figure 12.33: Data type of the RoutingDays column
    ](img/B15019_12_33.jpg)

    图 12.33:routing days 列的数据类型

    减去两个日期时间列的结果是一个新的日期时间列(`dtype('<M8[ns]'`)，它是`numpy`包的特定日期时间类型。我们需要将这种数据类型转换成一个`int`，以获得这两天之间的天数。

18.  使用`.dt.days`属性:

    ```py
    df['RoutingDays'] = df['RoutingDays'].dt.days
    ```

    转换`RoutingDays`列
19.  Display the first five rows using the `.head()` method:

    ```py
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 12.34: First five rows of the Customer Complaint DataFrame after creating RoutingDays
    ](img/B15019_12_34.jpg)

图 12.34:创建路线日期后客户投诉数据框的前五行

在本练习中，您将实践不同的技术来对真实数据集上的日期时间列中的新变量进行特征工程。从两个`Date sent to company`和`Date received`专栏中，您成功地创建了六个新特性，它们将提供额外的有价值的信息。

例如，我们能够发现这样的模式，例如投诉的数量在 11 月或周五往往较高。我们还发现，在周末收到投诉时，路由投诉需要更多时间，这可能是因为一周中那个时间的员工数量有限。

# 执行数据聚合

好的，我们正接近这一章的结尾。但是在我们结束之前，还有一种技术可以用来创建新特性:数据聚合。其背后的想法是从另一个列中为特定的组汇总一个数字列。我们已经在*第 5 章*、*执行您的第一次聚类分析*中看到了一个示例，说明如何使用`.pivot_table()`方法从 ATO 数据集中为 k-means 找到的每个聚类聚合两个数值变量(平均净税收和平均总扣除额)。但在那个时候，我们聚集数据不是为了创造新的特征，而是为了了解这些集群之间的差异。

您可能想知道在什么情况下您会希望使用数据聚合来执行特征工程。如果已经有了一个包含每条记录值的数字列，为什么还需要对它进行汇总并将这些信息添加回 DataFrame 呢？感觉我们只是添加了相同的信息，但细节更少。但是实际上使用这种技术有很多好的理由。

一个潜在的原因可能是您希望使用此聚合来规范化另一个数字列。例如，如果您正在处理一个零售商的数据集，该数据集包含世界各地每家商店的所有销售额，则一个国家的销售额可能会与另一个国家的销售额相差很大，因为它们没有相同的人口。在这种情况下，不是使用每个商店的原始销售数字，而是计算一个商店的销售额除以该国总销售额的比率(或百分比)。有了这一新的比率功能，一些商店看起来表现不佳，因为它们的原始销售量没有其他国家高，实际上可能比其所在国家的平均水平好得多。

在`pandas`中，执行数据聚合相当容易。我们只需要依次组合以下方法:`.groupby()`和`.agg()`。

我们需要指定将被分组到`.groupby()`方法中的列的列表。如果您熟悉 Excel 中的数据透视表，这对应于`Rows`字段。

`.agg()`方法需要一个字典，将列名作为键，将聚合函数作为值，比如`{'column_name': 'aggregation_function'}`。在 Excel 数据透视表中，聚合列被称为`values`。

让我们看看如何在在线零售数据集上实现这一点。首先，我们需要导入数据:

```py
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter12/Dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

让我们计算一下每个国家销售的商品总量。我们将指定`Country`列作为分组列:

```py
df.groupby('Country').agg({'Quantity': 'sum'})
```

您应该得到以下输出:

![Figure 12.35: Sum of Quantity per Country
](img/B15019_12_35.jpg)

图 12.35:每个国家的数量总和

这个结果给出了每个国家销售的商品总量。我们可以看到，澳大利亚销售的商品几乎是比利时的四倍。这个级别的信息可能太高级了，我们可能需要更详细的信息。让我们执行同样的聚合，但是这次我们将在两列上分组:`Country`和`StockCode`。我们只需要将这些列的名称作为列表提供给`.groupby()`方法:

```py
df.groupby(['Country', 'StockCode']).agg({'Quantity': 'sum'})
```

您应该得到以下输出:

![Figure 12.36: Sum of Quantity per Country and StockCode
](img/B15019_12_36.jpg)

图 12.36:每个国家和股票代码的数量总和

我们可以看到每个国家售出了多少件商品。我们可以注意到澳大利亚已经销售了相同数量的产品`20675`、`20676`和`20677` ( `216`)。这可能表明这些产品总是一起出售。

我们可以再添加一层信息，并获得每个国家、产品和日期的售出商品数量。为此，我们首先需要创建一个新的特性来提取`InvoiceDate`的日期部分(我们在上一节中刚刚学习了如何做):

```py
df['Invoice_Date'] = df['InvoiceDate'].dt.date
```

然后，我们可以在`.groupby()`方法中添加这个新列:

```py
df.groupby(['Country', 'StockCode', 'Invoice_Date']).agg({'Quantity': 'sum'})
```

您应该得到以下输出:

![Figure 12.37: Sum of Quantity per Country, StockCode, and Invoice_Date
](img/B15019_12_37.jpg)

图 12.37:每个国家的数量、库存代码和发票日期的总和

我们已经生成了一个新的数据框架，其中包含每个国家销售的商品总量、商品 ID 和日期。我们可以看到带有`StockCode 15036`的商品在`Australia`的`2011-05-17`很受欢迎——有`600`售出商品。另一方面，在`Australia`只有`Stockcode` `20665`的`6`物品在`2011-03-24`售出。

我们现在可以将这些附加信息合并回原始数据帧中。但是在此之前，还需要一个额外的数据转换步骤:重置列索引。默认情况下，`pandas`包在数据聚合后创建一个多级索引。您可以认为列名存储在多行中，而不是只有一行。要将其改回单个级别，您需要调用`.reset_index()`方法:

```py
df_agg = df.groupby(['Country', 'StockCode', 'Invoice_Date']).agg({'Quantity': 'sum'}).reset_index()
df_agg.head()
```

您应该得到以下输出:

![Figure 12.38: DataFrame containing data aggregation information
](img/B15019_12_38.jpg)

图 12.38:包含数据聚集信息的数据帧

现在我们可以使用本章前面提到的`.merge()`方法将新数据帧合并到原始数据帧中:

```py
df_merged = pd.merge(df, df_agg, how='left', on = ['Country', 'StockCode', 'Invoice_Date'])
df_merged
```

您应该得到以下输出:

![Figure 12.39: Merged DataFrame
](img/B15019_12_39.jpg)

图 12.39:合并的数据帧

我们可以看到有两列叫做`Quantity_x`和`Quantity_y`而不是`Quantity`。

原因是，合并后，有两个不同的列有完全相同的名称(`Quantity`)，所以默认情况下，pandas 会添加一个后缀来区分它们。我们可以通过在合并前替换这两列中的一列的名称或者在合并后替换这两列来解决这个问题。要替换列名，我们可以使用来自`pandas`的`.rename()`方法，提供一个字典，旧名称作为键，新名称作为值，比如`{'old_name': 'new_name'}`。

让我们用`Quantity`和`DailyQuantity`替换合并后的列名:

```py
df_merged.rename(columns={"Quantity_x": "Quantity", "Quantity_y": "DailyQuantity"}, inplace=True)
df_merged
```

您应该得到以下输出:

![Figure 12.40: Renamed DataFrame
](img/B15019_12_40.jpg)

图 12.40:重命名的数据帧

现在，我们可以创建一个新的特性来计算在相应国家/地区售出的商品与每日售出商品总量之间的比率:

```py
df_merged['QuantityRatio'] = df_merged['Quantity'] / df_merged['DailyQuantity']
df_merged
```

您应该得到以下输出:

![Figure 12.41: Final DataFrame with new QuantityRatio feature
](img/B15019_12_41.jpg)

图 12.41:具有新 QuantityRatio 特征的最终数据帧

在本节中，我们了解了执行数据聚合如何通过计算每个感兴趣分组的比率或百分比来帮助我们创建新要素。查看第一行和第二行，我们可以看到在`StockCode`交易`84123A`和`71053`中出售了`6`件商品。但如果我们看看新创建的`DailyQuantity`栏，我们可以看到`StockCode` `84123A`更受欢迎:在那天(`2010-12-01`)，商店售出了`454`台，但只有`StockCode` `71053`的`33`。`QuantityRatio`向我们展示的是`StockCode` `84406B`的`8`商品的第三笔交易，这一笔交易占该商品当天销售额的 20%。通过执行数据聚合，我们获得了每条记录的额外信息，并从数据集中获得了原始信息。

## 练习 12.04:在 AMES Housing 数据集上使用数据聚合的特征工程

在本练习中，我们将使用数据聚合创建新要素。首先，我们将通过`YrSold`计算每个邻域的最大值`SalePrice`和`LotArea`。然后，我们将把这些信息添加回数据集中，最后，我们将计算这两个最大值所对应的每笔售出房产的比率:

注意

我们将在这个练习中使用的数据集是 Ames Housing 数据集，它可以在我们的 GitHub 存储库中找到:[https://packt.live/35r2ahN](https://packt.live/35r2ahN)。

1.  打开一个新的 Colab 笔记本。
2.  导入`pandas`和`altair`包:

    ```py
    import pandas as pd
    ```

3.  将数据集的链接分配给一个名为`file_url` :

    ```py
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter12/Dataset/ames_iowa_housing.csv'
    ```

    的变量
4.  使用`pandas`包中的`.read_csv()`方法，将数据集加载到一个名为`df` :

    ```py
    df = pd.read_csv(file_url)
    ```

    的新数据框架中
5.  使用`.groupby.agg()`方法执行数据聚合，找到每个`Neighborhood`和`YrSold`的最大值`SalePrice`，并将结果保存在一个名为`df_agg` :

    ```py
    df_agg = df.groupby(['Neighborhood', 'YrSold']).agg({'SalePrice': 'max'}).reset_index()
    ```

    的新数据帧中
6.  将 df_agg 列重命名为 Neighborhood、YrSold 和 SalePriceMax:

    ```py
    df_agg.columns = ['Neighborhood', 'YrSold', 'SalePriceMax']
    ```

7.  Print out the first five rows of `df_agg`:

    ```py
    df_agg.head()
    ```

    您应该得到以下输出:

    ![Figure 12.42: First five rows of the aggregated DataFrame
    ](img/B15019_12_42.jpg)

    图 12.42:聚集数据帧的前五行

8.  使用`merge()`方法在`Neighborhood`和`YrSold`列上使用左连接(`how='left'`，将原始数据帧`df`合并到`df_agg`，并将结果保存到名为`df_new` :

    ```py
    df_new = pd.merge(df, df_agg, how='left', on=['Neighborhood', 'YrSold'])
    ```

    的新数据帧中
9.  Print out the first five rows of `df_new`:

    ```py
    df_new.head()
    ```

    您应该得到以下输出:

    ![Figure 12.43: First five rows of df_new
    ](img/B15019_12_43.jpg)

    图 12.43:df _ new 的前五行

10.  用`SalePrice`除以`SalePriceMax` :

    ```py
    df_new['SalePriceRatio'] = df_new['SalePrice'] / df_new['SalePriceMax']
    ```

    创建一个名为`SalePriceRatio`的新列
11.  Print out the first five rows of `df_new`:

    ```py
    df_new.head()
    ```

    您应该得到以下输出:

    ![Figure 12.44: First five rows of df_new after feature engineering
    ](img/B15019_12_44.jpg)

    图 12.44:特征工程后 df_new 的前五行

12.  使用`.groupby.agg()`方法执行数据聚合，找到每个`Neighborhood`和`YrSold`的最大值`LotArea`，并将结果保存在名为`df_agg2` :

    ```py
    df_agg2 = df.groupby(['Neighborhood', 'YrSold']).agg({'LotArea': 'max'}).reset_index()
    ```

    的新数据帧中
13.  Rename the column of `df_agg2` to `Neighborhood`, `YrSold`, and `LotAreaMax` and print out the first five columns:

    ```py
    df_agg2.columns = ['Neighborhood', 'YrSold', 'LotAreaMax']
    df_agg2.head()
    ```

    您应该得到以下输出:

    ![Figure 12.45: First five rows of the aggregated DataFrame
    ](img/B15019_12_45.jpg)

    图 12.45:聚集数据帧的前五行

14.  Merge the original DataFrame, `df`, to `df_agg2` using a left join (`how='left'`) on the `Neighborhood` and `YrSold` columns using the `merge()` method and save the results into a new DataFrame called `df_final`:

    ```py
    df_final = pd.merge(df_new, df_agg2, how='left', on=['Neighborhood', 'YrSold'])
    ```

    用`LotArea`除以`LotAreaMax`创建一个名为`LotAreaRatio`的新列:

    ```py
    df_final['LotAreaRatio'] = df_final['LotArea'] / df_final['LotAreaMax']
    ```

15.  Print out the first five rows of df_final for the following columns: Id, Neighborhood, YrSold, SalePrice, SalePriceMax, SalePriceRatio, LotArea, LotAreaMax, LotAreaRatio

    ```py
    df_final[['Id', 'Neighborhood', 'YrSold', 'SalePrice', 'SalePriceMax', 'SalePriceRatio', 'LotArea', 'LotAreaMax', 'LotAreaRatio']].head()
    ```

    您应该得到以下输出:

![Figure 12.46: First five rows of the final DataFrame
](img/B15019_12_46.jpg)

图 12.46:最终数据帧的前五行

这就是了。我们刚刚创建了两个新特性，它们给出了一处房产的`SalePrice`和`LotArea`相对于同一年同一社区售出的最高房产的比率。我们现在可以容易而公平地比较这些特性。例如，从上一步的输出中，我们可以注意到第五个房产大小(`Id` `5`和`LotArea` `14260`)几乎与同一地区同一年份售出的最大房产(`LotArea` `14303`)一样接近(`LotAreaRatio` `0.996994`)。但其售价(`SalePrice` `250000`)明显低于最高价(`SalePrice``350000`)(`SalePriceRatio`是`0.714286`)。这表明房产的其他特征对销售价格有影响。

## 活动 12.01:金融数据集的特征工程

您在捷克共和国的一家大型银行工作，您的任务是分析现有客户的交易。数据团队已经从他们的数据库中提取了他们认为对您分析数据集有用的所有表。您需要将这些表中的数据整合到一个数据框架中，并创建新的要素，以获得丰富的数据集，从而能够对客户的银行交易进行深入分析。

您将只使用以下四个表:

*   `account`:给定分行的客户银行账户的特征
*   `client`:我行客户相关个人信息
*   `disp`:将账户链接到客户的表格
*   `trans`: A list of all historical transactions by account

    注意

    如果您想了解更多关于这些表的信息，可以查看这个数据集的数据字典:[https://packt.live/2QSev9F](https://packt.live/2QSev9F)。

以下步骤将帮助您完成此活动:

1.  将该数据集中的不同表下载并加载到 Python 中。
2.  用`.shape`和`.head()`方法分析每个表。
3.  根据*步骤 2* 的分析，找出将用于合并的表之间的公共/相似列。
4.  应该有四个常用表。使用`pd.merge()`将四个表格合并在一起。
5.  用`.rename()`合并后重命名列名。
6.  检查与`.duplicated()`和`.sum()`合并后没有重复。
7.  使用`.to_datetime()`转换日期列的数据类型。
8.  Create two separate features from `birth_number` to get the date of birth and sex for each customer.

    注意

    这是用于对该列中与生日和性别相关的数据进行编码的规则:对于男性，数字采用 YYMMDD 格式，对于女性，数字采用 yymmm+50DD 格式，其中 YYMMDD 是出生日期。

9.  用`.isna()`修复数据质量问题。
10.  Create a new feature that will calculate customers' ages when they opened an account using date operations:

    注意

    该数据集最初由 Berka，Petr 共享，用于探索挑战 PKDD ' 99:[https://packt.live/2ZVaG7J](https://packt.live/2ZVaG7J)。

    您将在本活动中使用的数据集可以在我们的 GitHub 资源库中找到:

    [https://packt.live/2QpUOXC](https://packt.live/2QpUOXC)。

    [https://packt.live/36sN2BR](https://packt.live/36sN2BR)。

    [https://packt.live/2MZLzLB](https://packt.live/2MZLzLB)。

    [https://packt.live/2rW9hkE](https://packt.live/2rW9hkE)。

    CSV 版本可以在这里找到:[https://packt.live/2N150nn](https://packt.live/2N150nn)。

**预期产出:**

![Figure 12.47: Expected output with the merged rows
](img/B15019_12_47.jpg)

图 12.47:合并行的预期输出

注意

这个活动的解决方案可以在以下地址找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

# 总结

我们首先学习了如何分析数据 et，并使用数据汇总和数据可视化很好地理解其数据。这对于找出数据集的局限性和识别数据质量问题非常有用。我们看到了如何使用`pandas`API 处理和修复一些最常见的问题(重复行、类型转换、值替换和缺少值)。

最后，在本章中，我们学习了几种特征工程技术。不可能涵盖创建特征的所有现有技术。本章的目的是向您介绍可以显著提高分析质量和模型性能的关键步骤。但是请记住，在过于剧烈地转换数据之前，要定期与业务或数据工程团队取得联系，以获得确认。准备数据集并不总是意味着拥有尽可能干净的数据集，而是获得最接近企业感兴趣的真实信息的数据集。否则，您可能会发现不正确或无意义的模式。正如我们所说的，权力越大，责任越大。

下一章打开了这本书的一个新部分，端到端地展示了数据科学用例。*第 13 章*、*处理不平衡数据集*，将带你看一个不平衡数据集的例子，以及如何处理这种情况。