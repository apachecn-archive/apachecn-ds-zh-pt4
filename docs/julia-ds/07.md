

# 七、无监督机器学习

在前一章中，我们学习了监督机器学习算法，以及如何在现实世界中使用它们。

无监督学习有点不同，也更难。目的是让系统学习一些东西，但是我们自己不知道要学什么。无监督学习有两种方法。

一种方法是找到数据集中的相似性/模式。然后我们可以创建这些相似点的集群。我们假设我们发现的聚类可以被分类，并且可以被提供一个标签。

算法本身不能分配名称，因为它没有任何名称。它只能找到基于相似性的聚类，仅此而已。为了真正能够找到有意义的聚类，需要一个大小合适的数据集。

它广泛应用于寻找相似用户、推荐系统、文本分类等等。

我们将详细讨论各种聚类算法。在本章中，我们将学习:

*   使用未标记的数据。
*   什么是无监督学习？
*   什么是集群？
*   不同类型的聚类。
*   K-means 算法和二等分 K-Means。它的优点和缺点。
*   分层聚类。
*   凝聚集群。它的优点和缺点。
*   DBSCAN 算法。

在开始深入研究集群之前，我们还应该讨论第二种方法。它将告诉我们集群与这种方法和用例有什么不同。第二种方法是一种强化学习。这包括奖励，以表明算法的成功。没有进行明确的分类。这种类型的算法最适合现实世界的算法。在这种算法中，系统根据以前的奖励或惩罚进行操作。这种学习可能是强大的，因为没有偏见，也没有预先分类的观察。

这计算了每一个动作的可能性，并且预先知道什么动作会导致什么样的结果。

这种试错法计算量大，耗费大量时间。让我们讨论一下不基于试错法的聚类方法。

# 了解聚类

聚类是一种将数据分成有用且有意义的组(簇)的技术。聚类是通过捕获数据的自然结构而形成的，这些数据彼此之间具有有意义的关系。这也可能仅用于其他算法或进一步分析的准备或汇总阶段。聚类分析在许多领域都有作用，如生物学、模式识别、信息检索等。

聚类可应用于不同的领域:

*   **Information retrieval**: To segregate the information into particular clusters is an important step in searching and retrieving information from the numerous sources or a big pool of data. Let's use the example of news aggregating websites. They create clusters of similar types of news making it easier for the user to go through the interesting sections.

    这些新闻类型还可以有子类来创建分层视图。例如，在体育新闻区，我们可以看到足球、板球、网球和其他体育项目。

*   生物学:聚类在生物学中有很大的用途。经过多年的研究，生物学家已经将大多数生物按等级分类。利用这些类别的特征，可以对未知进行分类。此外，现有的数据可以用来寻找相似之处和有趣的模式。
*   **营销**:公司使用客户和销售数据来创建相似用户或细分市场的集群，在这些集群中可以开展有针对性的促销/活动，以获得最大的投资回报。
*   **天气**:聚类分析广泛应用于气候和天气分析。气象站产生大量的数据。聚类用于生成对这些数据的洞察，并找出模式和重要信息。

## 集群是如何形成的？

形成集群的方法有很多。让我们讨论一些创建集群的基本方法:

*   首先对数据对象进行分组。这种分组应该只基于描述对象的数据。
*   相似的对象被组合在一起。他们可能会表现出彼此之间的关系。
*   不同的对象保存在其他集群中。

![How are clusters formed?](graphics/image_07_001.jpg)

*   前面的图清楚地向我们展示了一些不同的集群，当集群中的不同数据对象之间有更多的相似性，而与其他集群中的数据对象有更多的不相似性时，就会形成这些集群。

![How are clusters formed?](graphics/image_07_002.jpg)

但是在这个数据点的特殊表示中，我们可以看到没有可以形成的明确的聚类。这是指不同集群的数据对象之间存在某种相似性。

## 聚类的类型

根据不同的因素，有不同类型的聚类机制:

*   嵌套或非嵌套—分层或分区
*   重叠、排他和模糊
*   部分还是全部

### 层次聚类

如果聚类不形成子集，则该聚类被称为非嵌套的。因此，分区聚类被定义为创建明确定义的、彼此不重叠的聚类。在这样的群集中，数据点仅位于一个群集中。

如果聚类中有子聚类，则称之为层次聚类。

![Hierarchical clustering](graphics/B05321_07_03.jpg)

上图显示了一个分层集群。分层集群是组织成树的集群。

这里，每个集群都有自己的子集群。每个节点也可以被认为是一个独立的系统，通过分区拥有自己的集群。

### 重叠、排他和模糊聚类

导致创建不同类型的集群的技术可以分为三种方法:

*   **独占集群**:在版块中，集群是如何形成的？我们看到了代表两种不同类型星团的两幅图像。在第一幅图像中，我们看到集群被很好地定义，并且它们之间有很好的分离。这些被称为排他性集群。在这些聚类中，数据点与其他聚类的数据点有明显的不同。
*   **重叠星团**:在第二张图中，我们看到没有这样明确的边界来分隔两个星团。这里，一些数据点可以存在于任何集群中。当没有这样的特征来将数据点区分到任何聚类中时，就会出现这种情况。
*   **模糊聚类**:模糊聚类是一个独特的概念。这里，数据点属于每个聚类，并且它的关系由介于 1(专门属于)到 0(不属于)之间的权重来定义。因此，聚类被认为是模糊集。通过概率规则，添加了所有数据点的权重之和应该等于 1 的约束。

模糊聚类也称为概率聚类。通常，为了具有明确的关系，数据点与具有最高隶属权重的聚类相关联。

### 部分聚类与完全聚类之间的差异

在完全聚类中，所有数据点都被分配给一个聚类，因为它们准确地表示了该聚类的特征。这些类型的簇称为完全簇。

可能有一些数据点不属于任何聚类。这是当这些数据点代表噪声或者是聚类的异常值时。这样的数据点不在任何聚类中，这被称为部分聚类。



# K-均值聚类

K-means 是最流行的聚类技术，因为它易于使用和实现。它还有一个搭档叫 K-medoid。这些分区方法创建数据集的一级分区。下面详细讨论 K-means。

## K-means 算法

k-表示从原型开始。它从数据集中提取数据点的质心。这种技术用于位于 n 维空间中的对象。

该技术包括选择 K 个质心。该 K 由用户指定，并考虑各种因素进行选择。它定义了我们需要多少个集群。因此，选择高于或低于所需的 K 值可能会导致不良结果。

现在向前，每个点被分配到它最近的质心。当许多点与一个特定的质心相关联时，就形成了一个聚类。质心可以根据属于当前聚类的点进行更新。

这个过程重复进行，直到质心变得恒定。

### K-means 算法

理解 K-means 算法将使我们更好地了解如何处理这个问题。让我们一步一步地理解 K-means 算法:

1.  根据定义的 K，选择质心的数量。
2.  将数据点分配给最近的质心。这一步将形成集群。
3.  再次计算群集的质心。
4.  重复步骤 2 和 3，直到质心保持不变。

在第一步中，我们使用平均值作为质心。

步骤 4 表示重复算法的早期步骤。这有时会导致很少变化的大量迭代。因此，我们通常只在新计算的质心变化超过 1%时才重复步骤 2 和 3。

### 将数据点与最近的质心相关联

我们如何测量计算出的质心和数据点之间的距离？

我们使用欧几里得(L2)距离作为度量，并且我们假设数据点在欧几里得空间中。如果需要，我们还可以使用不同的邻近度量，例如，曼哈顿(L1)也可以用于欧几里得空间。

当算法处理不同数据点的相似性时，最好只具有所需的数据点特征集。对于更高维度的数据，计算量急剧增加，因为它必须对每个维度进行迭代计算。

有一些可以使用的距离度量的选择:

*   **曼哈顿(L1)** :取一个中间值作为质心。它的作用是最小化一个物体到其聚类中心的 L1 距离的总和。
*   **平方欧几里德(L2^2)** :取平均值作为质心。它对函数进行处理，以最小化一个对象到群集质心的 L2 距离的平方和。
*   **余弦**:以平均值为质心。它对函数进行处理，以最大化对象与聚类质心的余弦相似性之和。
*   **Bregman 散度**:以平均值为质心。它最小化了一个对象从群集的质心的 Bregman 发散的总和。

### 如何选择初始质心？

这是 K-means 算法中非常重要的一步。我们从随机选择初始质心开始。这通常会导致非常差的聚类。即使这些质心分布得很好，我们也无法接近所需的簇。

有一种技术可以解决这个问题——用不同的初始质心进行多次运行。此后，选择具有最小**平方和误差** ( **SSE** )的聚类集合。由于数据集的大小和所需的计算能力，这可能不总是有效的，也不总是可行的。

在重复随机初始化的过程中，质心可能无法解决这个问题，但是我们可以使用其他技术:

*   使用层次聚类，可以从取一些样本点开始，用层次聚类做一个聚类。现在我们可以从这个聚类中取出 K 个簇，用这些簇的质心作为初始质心。这种方法有一些限制:

    *   样本数据不能太大(计算量大)。
    *   用期望的集群数，K 应该很小。

*   另一种技术是获得所有点的质心。从这个质心，我们找到最大分离点。我们按照这个过程来获得最大距离的质心，这也是随机选择的。但是这种方法有一些问题:

    *   找出最远点的计算量很大。
    *   当数据集中存在离群值时，这种方法有时会产生不良结果。因此，我们可能得不到所需的密集区域。

### K 均值算法的时空复杂度

K-means 不需要那么多空间，因为我们只需要存储数据点和质心。

K-means 算法的存储要求 O((m+K)n) ，其中:

*   *m* 是点数
*   *n* 是属性数量

K-means 算法的时间要求可能不同，但一般来说也是适度的。时间随着数据点的数量线性增加。

K-means 算法的时间要求: *O(I*K*m*n)* ，其中:

*   I 是收敛到质心所需的迭代次数

如果所需的聚类数明显小于 K 均值与之成正比的数据点的数量，则 K 均值效果最佳。

## K 均值的问题

基本的 K 均值聚类算法存在一些问题。我们来详细讨论一下这些问题。

### K-means 中的空簇

可能有这样一种情况，我们得到的是空簇。这是在点被分配的阶段期间没有点被分配给特定的给定聚类的时候。这可以通过以下方式解决:

1.  我们选择一个不同于当前选择的质心。如果不这样做，误差的平方将远大于阈值。
2.  要选择一个不同的质心，我们遵循相同的方法来寻找离当前质心最远的点。这通常消除了对平方误差有贡献的点。
3.  如果我们得到多个空簇，那么我们必须重复这个过程几次。

### 数据集中的异常值

当我们处理平方误差时，异常值可能是决定性因素，并可能影响所形成的聚类。这意味着当数据集中存在异常值时，我们可能无法实现所需的聚类，或者真正代表分组数据点的聚类可能没有相似的特征。

这也导致更高的误差平方和。因此，通常的做法是在应用聚类算法之前移除离群值。

也有一些情况下，我们可能不想删除离群值。一些要点，如网上不寻常的活动、过度信贷等等，对企业来说是有趣和重要的。

## 不同类型的集群

K 均值有局限性。K-means 最常见的限制是它在识别自然聚类时面临困难。我们所说的自然集群是指:

*   非球形/圆形
*   不同大小的集群
*   不同密度的集群

### Tip

如果有几个较密集的聚类和一个不太密集的聚类，K-means 可能会失败。

下图显示了不同大小的集群:

![Different types of cluster](graphics/image_07_004.jpg)

上图有两个图像。在第一幅图像中，我们有原始点，在第二幅图像中，我们有三个 K 均值聚类。我们可以看到这些都是不准确的。当簇的大小不同时会发生这种情况。

下面是不同密度集群的示意图:

![Different types of cluster](graphics/image_07_005.jpg)

上图有两个图像。在第一幅图像中，我们有原始点，在第二幅图像中，我们有三个 K 均值聚类。这些簇具有不同的密度。

这是非球状星团的示意图:

![Different types of cluster](graphics/image_07_006.jpg)

上图有两个图像。在第一幅图像中，我们有原始点，在第二幅图像中，我们有两个 K 均值聚类。这些簇本质上是非圆形或非球形的，K-means 算法不能正确地检测它们。

### K 均值——优势和劣势

K-means 有很多优点和一些缺点。让我们先讨论一下优势:

*   k 均值可以用于各种类型的数据。
*   理解和实现起来很简单。
*   即使是重复和多次迭代，它也是高效的。
*   平分 K-means 是简单 K-means 的一种变体，更有效。我们稍后将更详细地讨论这一点。

K 均值聚类的一些弱点或缺点包括:

*   它并不适用于每种类型的数据。
*   正如在前面的例子中所看到的，对于不同密度、大小的星团或非球状星团来说，这种方法并不适用。
*   当数据集中存在异常值时，就会出现问题。
*   K-means 有一个很大的限制，因为它通过计算中心来形成聚类。因此，我们的数据应该是这样的，可以有一个“中心”。

## 对分 K-means 算法

平分 K-means 是简单 K-means 算法的扩展。这里，我们通过将所有点的集合分成两个聚类来找出 K 个聚类。然后我们从这些集群中取出一个，再次分裂。该过程继续进行，直到形成 K 个簇。

平分 K-means 的算法是:

1.  首先，我们需要初始化聚类列表，该列表将包含所有数据点的聚类。
2.  Repeat the following:

    1.  现在我们从集群列表中删除一个集群
    2.  我们现在多次尝试将集群一分为二
    3.  对于前一步骤中 n=1 的试验次数

3.  The cluster is bisected using K-means:

    *   从具有最低平方和误差的结果中选择两个聚类
    *   这两个集群被添加到集群列表中

4.  执行前面的步骤，直到列表中有 K 个聚类。

有几种方法可以拆分集群:

*   最大集群
*   具有最大误差平方和的聚类
*   两者

在本例中，我们将使用 RDatasets 中的 iris 数据集:

![Bisecting K-means algorithm](graphics/image_07_007.jpg)

这是著名的 iris 数据集的一个简单例子。我们使用`PetalLength`和`PetalWidth`对数据点进行聚类。

结果如下:

![Bisecting K-means algorithm](graphics/image_07_008.jpg)

## 深入了解分层聚类

这是仅次于 K-means 的第二种最常用的聚类技术。让我们再举一个同样的例子:

![Getting deep into hierarchical clustering](graphics/B05321_07_03.jpg)

这里，最上面的根代表所有的数据点或一个集群。现在我们有三个子集群，由节点表示。所有这三个集群都有两个子集群。并且这些子集群中有进一步的子集群。这些子聚类有助于找到纯粹的聚类——也就是说，那些共享大多数特征的聚类。

有两种方法可以实现层次聚类:

*   **凝聚的**:这是基于集群接近度的概念。我们首先将每个点视为单独的聚类，然后逐步合并最接近的点对。
*   **分裂**:这里我们从一个包含所有数据点的聚类开始，然后我们开始分裂它，直到剩下具有单独点的聚类。在这种情况下，我们决定如何进行拆分。

分层聚类表示为树状图，也称为树状图。这用于表示聚类-子聚类关系以及聚类如何合并或分裂(凝聚或分裂)。

## 凝聚层次聚类

这是自底向上的分层聚类方法。在这里，每个观察被视为一个单独的集群。基于相似性，这些聚类对被合并在一起，并且我们向上移动。

这些聚类基于最小距离合并在一起。当这两个集群合并时，它们被视为一个新的集群。当数据点池中只剩下一个聚类时，重复这些步骤。

凝聚层次聚类的算法是:

1.  首先，计算邻近矩阵。
2.  两个最接近的聚类被合并。
3.  在第一步中创建的邻近矩阵在两个聚类合并之后被更新。
4.  重复步骤 2 和步骤 3，直到只剩下一个簇。

### 如何计算接近度

前面算法中的步骤 3 是非常重要的一步。这是两个集群之间的接近度。

对此有多种定义方式:

*   **MIN** :不同聚类的两个最近点定义了这些聚类的接近度。这是最短的距离。
*   **MAX** :与 MIN 相反，MAX 取聚类中最远的点，计算这两个点之间的接近度，作为这些聚类的接近度。
*   **平均**:另一种方法是取不同聚类的所有数据点的平均值，并根据这些点计算接近度。

![How proximity is computed](graphics/B05321_07_10.jpg)

上图描述了使用 MIN 的邻近度测量。

![How proximity is computed](graphics/B05321_07_11.jpg)

上图描述了使用 MAX 的邻近度测量。

![How proximity is computed](graphics/B05321_07_12.jpg)

上图描述了使用平均值的邻近度测量。

这些方法也称为:

*   **单联动**:最小
*   **完成联动**:最大
*   **平均联动**:平均

还有另一种方法被称为质心方法。

在质心方法中，使用聚类的两个平均向量来计算邻近距离。在每一阶段，根据哪一个具有最小的质心距离来组合两个聚类。

让我们举下面的例子:

![How proximity is computed](graphics/B05321_07_13.jpg)

上图显示了 x-y 平面上的七个点。如果我们开始进行凝聚层次聚类，过程如下:

1.  {1},{2},{3},{4},{5},{6},{7}.
2.  {1},{2,3},{4},{5},{6},{7}.
3.  {1,7},{2,3},{4},{5},{6},{7}.
4.  {1,7},{2,3},{4,5},{6}.
5.  {1,7},{2,3,6},{4,5}.
6.  {1,7},{2,3,4,5,6}.
7.  {1,2,3,4,5,6,7}.

这被分解成七个步骤来完成整个集群。

这也可以通过下面的树形图显示出来:

![How proximity is computed](graphics/image_07_014.jpg)

这代表了凝聚层次聚类的前七个步骤。

### 层次聚类的优势和劣势

前面讨论的层次聚类有时或多或少适合于给定的问题。通过理解分层聚类的优点和缺点，我们将能够理解这一点:

*   凝聚聚类缺乏全局目标函数。这种算法的好处是没有局部最小值，并且在选择初始点时没有问题。
*   凝聚聚类可以很好地处理不同大小的聚类。
*   人们认为凝聚聚类产生质量更好的聚类。
*   凝聚聚类通常计算量很大，并且不适用于高维数据。

## 了解 DBSCAN 技术

**DBSCAN** 指的是带有噪声的应用的**基于密度的空间聚类。它是一种数据聚类算法，使用基于密度的种子(起始)点扩展来查找聚类。**

它定位高密度区域，并使用它们之间的低密度将它们与其他区域分开。

### 那么，什么是密度？

在基于中心的方法中，通过使用指定半径内的点数来计算数据集中特定点的密度。这很容易实现，并且点的密度取决于指定的半径。

例如，大半径对应于点 *m* 处的密度，其中 m 是半径内数据点的数量。如果半径很小，那么密度可以是 1，因为只有一个点存在。

### 如何使用基于中心的密度对点进行分类

*   **核心点**:位于基于密度的聚类内部的点就是核心点。这些位于致密区域的内部。
*   **边界点**:这些点位于簇内，但不是核心点。它们位于核心点的附近。这些位于致密区域的边界或边缘。
*   **噪声点**:不是核心点或者边界点的点就是噪声点。

### DBSCAN 算法

彼此非常接近的点被放在同一个簇中。靠近这些点的点也放在一起。非常远的点(噪声点)被丢弃。

DBSCAN 的算法由下式给出:

1.  点被标记为核心点、边界点或噪点。
2.  噪声点被消除。
3.  使用特殊半径在核心点之间形成边缘。
4.  这些核心点组成一个集群。
5.  与这些核心点相关联的边界点被分配给这些聚类。

### DBS can 算法的优缺点

前面讨论的层次聚类有时或多或少适合于给定的问题。通过理解层次聚类的优点和缺点，我们将能够理解这一点。

*   DBSCAN 可以处理不同形状和大小的集群。它能够做到这一点，因为它使用密度创建了群集的定义。
*   它能抵抗噪音。在发现更多的聚类方面，它能够比 K-means 执行得更好。
*   DBSCAN 面临着不同密度数据集的问题。
*   此外，它还存在处理高维数据的问题，因为很难找到这些数据的密度。
*   计算最近邻时计算量很大。

## 聚类验证

聚类验证很重要，因为它告诉我们生成的聚类是否相关。处理群集验证时要考虑的要点包括:

*   它有能力区分数据中是否存在非随机结构
*   它有能力确定集群的实际数量
*   它能够评估数据是否适合集群
*   它应该能够比较两组集群，找出哪个集群更好

## 举例

我们将在凝聚层次聚类和 DBSCAN 的例子中使用`ScikitLearn.jl`。

如前所述，`ScikitLearn.jl`旨在为 Python 提供一个类似的库，比如实际的 scikit-learn。

我们首先将所需的包添加到我们的环境中:

```jl
julia> Pkg.update() 
julia> Pkg.add("ScikitLearn") 
julia> Pkg.add("PyPlot") 

```

这也要求我们在 Python 环境中拥有 scikit-learn。如果尚未安装，我们可以使用以下命令进行安装:

```jl
$ conda install scikit-learn 

```

在这之后，我们可以从我们的例子开始。我们将在`ScikitLearn.jl`中尝试不同的聚类算法。这在`ScikitLearn.jl`的示例中提供:

```jl
julia> @sk_import datasets: (make_circles, make_moons, make_blobs) 
julia> @sk_import cluster: (estimate_bandwidth, MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering) 

julia> @sk_import cluster: (DBSCAN, AffinityPropagation, Birch) 
julia> @sk_import preprocessing: StandardScaler 
julia> @sk_import neighbors: kneighbors_graph 

```

我们从官方 scikit-learn 库中导入数据集和聚类算法。由于其中一些依赖于邻居的距离度量，我们还导入了 kNN:

```jl
julia> srand(33) 

julia> # Generate datasets. 

julia> n_samples = 1500 
julia> noisy_circles = make_circles(n_samples=n_samples, factor=.5, noise=.05) 
julia> noisy_moons = make_moons(n_samples=n_samples, noise=.05) 
julia> blobs = make_blobs(n_samples=n_samples, random_state=8) 
julia> no_structure = rand(n_samples, 2), nothing 

```

这个特定的代码片段将生成所需的数据集。生成的数据集将足够大，可以测试这些不同的算法:

```jl
julia> colors0 = collect("bgrcmykbgrcmykbgrcmykbgrcmyk") 
julia> colors = vcat(fill(colors0, 20)...) 

julia> clustering_names = [ 
    "MiniBatchKMeans", "AffinityPropagation", "MeanShift", 
    "SpectralClustering", "Ward", "AgglomerativeClustering", 
    "DBSCAN", "Birch"]; 

```

我们给这些算法命名，并用颜色填充图像:

```jl
julia> figure(figsize=(length(clustering_names) * 2 + 3, 9.5)) 
julia> subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01) 

julia> plot_num = 1 

julia> datasets = [noisy_circles, noisy_moons, blobs, no_structure] 

```

现在，我们指定如何为不同的算法和数据集形成图像:

![Example](graphics/B05321_07_15.jpg)

在这里，我们正在标准化数据集以方便选择参数，并为需要距离测量的算法初始化`kneighbors_graph`:

![Example](graphics/B05321_07_16.jpg)

这里，我们正在创建聚类估计器，算法需要这些估计器根据用例进行相应的操作:

![Example](graphics/B05321_07_17.jpg)

不同算法的相似估计量。

之后，我们在数据集上使用这些算法:

```jl
for (name, algorithm) in zip(clustering_names, clustering_algorithms) 
    fit!(algorithm, X) 
    y_pred = nothing 
    try 
        y_pred = predict(algorithm, X) 
    catch e 
        if isa(e, KeyError) 
            y_pred = map(Int, algorithm[:labels_]) 
            clamp!(y_pred, 0, 27) # not sure why some algorithms return -1 
        else rethrow() end 
    end 
    subplot(4, length(clustering_algorithms), plot_num) 
    if i_dataset == 1 
        title(name, size=18) 
    end 

    for y_val in unique(y_pred) 
        selected = y_pred.==y_val 
        scatter(X[selected, 1], X[selected, 2], color=string(colors0[y_val+1]), s=10) 
    end 

    xlim(-2, 2) 
    ylim(-2, 2) 
    xticks(()) 
    yticks(()) 
    plot_num += 1 
end 

```

获得的结果如下:

![Example](graphics/image_07_018.jpg)

*   我们可以看到凝聚聚类和 DBSCAN 在前两个数据集中表现得非常好
*   凝聚聚类在第三个数据集中表现不佳，而 DBSCAN 表现良好
*   凝聚聚类和 DBSCAN 在第四个数据集上都表现不佳



# 总结

在这一章中，我们学习了无监督学习以及它与监督学习的不同之处。我们讨论了使用无监督学习的各种用例。

我们讨论了不同的无监督学习算法，并讨论了它们的算法，以及彼此的优缺点。

我们讨论了各种集群技术以及集群是如何形成的。我们了解了聚类算法之间的差异，以及它们如何适用于特定的用例。

我们学习了 K-means、层次聚类和 DBSCAN。

在下一章，我们将学习集成学习。



# 参考文献

*   [https://github.com/JuliaLang/julia](https://github.com/JuliaLang/julia)
*   [https://github.com/JuliaStats/Clustering.jl](https://github.com/JuliaStats/Clustering.jl)
*   [http://julistats . github . io/](http://juliastats.github.io/)
*   [https://github.com/stevengj/PyCall.jl](https://github.com/stevengj/PyCall.jl)
*   [https://github . com/cstjean/scikitlearn . JL](https://github.com/cstjean/ScikitLearn.jl)