

# 三、数据可视化和预测

做出预测通常是不确定的。然而，有一些已经被使用的方法可以为你的结果提供一些信心。在 Jupyter 下，我们可以使用 Python 和/或 R 进行具有现成功能的预测。



# 使用 scikit-learn 进行预测

scikit-learn 是一个使用 Python 构建的机器学习工具集。该包的一部分是监督学习，其中样本数据点具有允许您将数据点分配到单独的类中的属性。我们使用一个估计器，它将一个数据点分配给一个类，并对具有相似属性的其他数据点进行预测。在 scikit-learn 中，估计器提供了两个函数，`fit()`和`predict()`，分别提供了对数据点进行分类和预测其他数据点类别的机制。

例如，我们将使用 https://uci.edu/的住房数据(我认为这是波斯顿地区的数据)。有许多因素，包括价格因素。

我们将采取以下步骤:

*   我们将把数据集分成训练集和测试集
*   从训练集中，我们将产生一个模型
*   然后，我们将根据测试集使用该模型，并评估我们的模型与预测房价的实际数据的拟合程度

数据集中的属性(在我们的数据框中按相应的顺序排列)是:

| **卷曲** | **城镇人均犯罪率** |
| `ZN` | 面积超过 25，000 平方英尺的住宅用地比例 |
| `INDUS` | 每个城镇非零售商业用地比例 |
| `CHAS` | 查尔斯河虚拟变量(`= 1`)如果区域界定河流；`0`否则) |
| `NOX` | 氮氧化物浓度(百万分之一) |
| `RM` | 每所住宅的平均房间数 |
| `AGE` | 1940 年以前建造的自有住房比例 |
| `DIS` | 到五个波士顿就业中心的加权距离 |
| `RAD` | 放射状公路可达性指数 |
| `TAX` | 每万美元的全价值财产税税率 |
| `PTRATIO` | 按城镇分列的师生比例 |
| `B` | 1000(Bk - 0.63)^2 其中 *Bk* 是按城镇划分的黑人居民比例 |
| `LSTAT` | %较低的人口地位 |
| `MEDV` | 以 1，000 美元为单位的自有住房的中值 |

下面的编码后面是对所用算法和结果的讨论:

```jl
#define all the imports we are using import matplotlib.pyplot as plt import numpy as np import pandas as pd import random from sklearn import datasets, linear_model from sklearn.cross_validation import train_test_split # load the data set df = pd.read_table('http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', sep='\s+') # add column names df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', \
 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PRATIO',\ 'B', 'LSTAT', 'MDEV'] #produce basic statistics to make sure things are lined up df.head()
```

`head()`函数的结果是数据帧的前几行:

![](../images/00051.jpeg)

```jl
df.describe()  
```

类似地，前面的`describe`语句为我们提供了一些关于数据帧的快速统计数据:

![](../images/00052.jpeg)

当在训练集和测试集之间划分数据集时，我们在两者之间使用随机分配。这给了我们一组公正的数据。但是，为了重现这里显示的结果，您需要使用相同的随机种子/起始值。这就是打`random.seed()`电话的原因。实际上，您应该放弃这个方法调用:

```jl
#we are going to be splitting up the data set 'randomly', #however we need to reproduce results so set the seed random.seed(3277) #split the data into training and testing (25% for testing) training, testing = train_test_split(df, test_size = 0.25) #need this step to create an instance of the lreg model regr = linear_model.LinearRegression() # Train the model using the training set (MDEV=target) training_data = training.drop('MDEV', axis=1) training_test = training.iloc[:,-1] #training.loc[:,['MDEV']] #look at coefficients in the model to validate regr.fit(training_data,training_test) print('Coefficients: \n', regr.coef_) 'Coefficients: \n', array([
 -1.18763385e-01,   4.19752612e-02,  -1.18584543e-02, 5.53125252e-01,  -1.19774970e+01,   3.80050180e+00, -8.55663104e-03,  -1.46613256e+00,   3.86772585e-01, -1.53024705e-02,  -9.55933426e-01,   1.31347272e-02, -5.28183554e-01])) 
```

大多数都是小数字，除了房间数与#6 呈 3.8 的正相关，与#8 呈-1.5 的负相关。有趣的是，人们如此看重离工作近:

```jl
#split up our test set testing_data = testing.loc[:,['CRIM', 'ZN', 'INDUS', 'CHAS',\ 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PRATIO', 'B',\ 'LSTAT']] testing_test = testing[['MDEV']].as_matrix() #make our prediction prediction_of_test = regr.predict(testing_data) # compute MSE # would usually use the built-in mse function, # but the test_test and prediction have diff # cols sum = 0 rows = len(testing_test) for i in range(rows):
 test = testing_test[i] prediction = prediction_of_test[i] diff = (test - prediction) ** 2 sum = sum + diff mse = sum / rows print("MSE ", mse) ('MSE ', array([ 23.1571225]))
```

有一个 23 的 MSE，与正在处理的数字大小相比，它似乎非常低。现在，让我们用图表显示我们的结果，以便更好地了解正在发生的情况:

```jl
%matplotlib inline #this preceding line is needed to display inline on Jupyter
#plot the tests and predictions plt.scatter(testing_test, prediction_of_test, color='black')
#draw a line through the middle showing the fit x0 = min(testing_test) x1 = max(testing_test) y0 = min(prediction_of_test) y1 = max(prediction_of_test) plt.plot([x0,x1],[y0,y1], color="red")
#add labels plt.xlabel("Actual Price") plt.ylabel("Predicted Price") plt.title("Actual Price vs Predicted Price")
plt.show()
```

从视觉上看，我们似乎很合适。大多数数据点与绘制的轴对齐。一如既往，有一些明显的异常值，例如在 *20* 、 *50* :

![](../images/00053.jpeg)![](../images/00054.jpeg)

# 使用 R 进行预测

我们可以在笔记本上用 R 进行同样的分析。不同的语言有不同的功能，但是功能非常接近。

我们使用相同的算法:

*   加载数据集
*   将数据集拆分为定型分区和测试分区
*   基于训练分区开发模型
*   使用模型从测试分区进行预测
*   比较预测测试和实际测试

编码如下:

```jl
#load in the data set from uci.edu (slightly different from other housing model)
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")

#assign column names
colnames(housing) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX",
 "RM", "AGE", "DIS", "RAD", "TAX", "PRATIO",
 "B", "LSTAT", "MDEV")
#make sure we have the right data being loaded
summary(housing)
 CRIM                ZN             INDUS            CHAS 
 Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000 
 1st Qu.: 0.08204   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000 
 Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000 
 Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917 
 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000 
 Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000 
 NOX               RM             AGE              DIS 
 Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130 
 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100 
 Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207 
 Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795 
 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188 
 Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127 
...  
```

确保数据集在我们的建模中处于正确的顺序。

```jl
housing <- housing[order(housing$MDEV),]

#check if there are any relationships between the data items
plot(housing)  
```

如下所示的数据显示将数据集中的每个变量与其他变量进行对比。我想看看是否有任何漂亮的 45 度“线”显示变量之间的对称性，也许我们应该删除一个，因为另一个足以作为一个贡献因素。有趣的项目有:

*   `CHAS`:查尔斯河访问，但那是二进制值。
*   `LSTAT`(地位较低的人群)和`MDEV`(价格)有一个相反的关系——但是价格不会是一个因素。
*   `NOX`(雾霾)和`DIST`(上班距离)成反比关系。我想我们希望如此。
*   否则，数据项之间似乎没有任何关系:

![](../images/00055.jpeg)

像以前一样，我们开始迫使种子能够繁殖结果。然后，我们将数据分成用`createDataPartitions`函数制作的训练和测试分区。然后，我们可以训练我们的模型并测试结果模型以进行验证:

```jl
#force the random seed so we can reproduce results
set.seed(133)

#caret package has function to partition data set
library(caret)
trainingIndices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE)
#break out the training vs testing data sets
housingTraining <- housing[trainingIndices,]
housingTesting <- housing[-trainingIndices,]
#note their sizes
nrow(housingTraining)
nrow(housingTesting)
#note there may be warning messages to update packages
381
125

#build a linear model
linearModel <- lm(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE +
 DIS + RAD + TAX + PRATIO + B + LSTAT, data=housingTraining)
summary(linearModel)
Call:
lm(formula = MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + 
 DIS + RAD + TAX + PRATIO + B + LSTAT, data = housingTraining)

Residuals:
 Min       1Q   Median       3Q      Max 
-15.8448  -2.7961  -0.5602   2.0667  25.2312 

Coefficients:
 Estimate Std. Error t value Pr(>|t|) 
(Intercept)  36.636334   5.929753   6.178 1.72e-09 ***
CRIM         -0.134361   0.039634  -3.390 0.000775 ***
ZN            0.041861   0.016379   2.556 0.010997 * 
INDUS         0.029561   0.068790   0.430 0.667640 
CHAS          3.046626   1.008721   3.020 0.002702 ** 
NOX         -17.620245   4.610893  -3.821 0.000156 ***
RM            3.777475   0.484884   7.790 6.92e-14 ***
AGE           0.003492   0.016413   0.213 0.831648 
DIS          -1.390157   0.235793  -5.896 8.47e-09 ***
RAD           0.309546   0.078496   3.943 9.62e-05 ***
TAX          -0.012216   0.004323  -2.826 0.004969 ** 
PRATIO       -0.998417   0.155341  -6.427 4.04e-10 ***
B             0.009745   0.003300   2.953 0.003350 ** 
LSTAT        -0.518531   0.060614  -8.555 3.26e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.867 on 367 degrees of freedom
Multiple R-squared:  0.7327,  Adjusted R-squared:  0.7233 
F-statistic:  77.4 on 13 and 367 DF,  p-value: < 2.2e-16  
```

有趣的是，这种模式也拿起了高溢价查尔斯河的意见影响价格。同样，像这样，这个模型提供了`p-value`(对模型的良好信心):

```jl
# now that we have a model, make a prediction
predicted <- predict(linearModel,newdata=housingTesting)
summary(predicted)

#visually compare prediction to actual
plot(predicted, housingTesting$MDEV)  
```

这看起来是一个非常好的相关性，非常接近 45 度映射。例外情况是预测值略高于实际值:

![](../images/00056.jpeg)

# 交互式可视化

有一个 Python 包 Bokeh，它可以用来在你的笔记本中生成一个图形，用户可以在其中进行交互并更改图形。

在本例中，我使用了本章后面的直方图示例中的相同数据(也包含在本章的文件集中)来显示交互式散景直方图。

编码如下:

```jl
from bokeh.io import show, output_notebook from bokeh.charts import Histogram import numpy as np import pandas as pd # this step is necessary to have display inline in a notebook output_notebook() # load the counts from other histogram example from_counts = np.load("from_counts.npy") # convert array to a dataframe for Histogram df = pd.DataFrame({'Votes':from_counts}) # make sure dataframe is working correctly print(df.head())
 Votes 0     23 1     29 2     23 3    302 4     24 # display the Bokeh histogram hist = Histogram(from_counts, \ title="How Many Votes Made By Users", \ bins=12) show(hist) 
```

我们可以看到直方图显示如下。几乎没有自动清理图表的工作，比如移动计数器或不感兴趣的轴标签。我假设`Histogram`函数有一些选项允许进一步的修改:

![](../images/00057.jpeg)

请注意图像顶部的小部件:

*   左侧是一个散景图标
*   右侧是以下图标:
    *   将图像移动到屏幕的另一部分
    *   放大的
    *   调整大小
    *   滚轮缩放-滑动滚轮放大/缩小
    *   将图像保存到磁盘
    *   刷新图像
    *   关于散景功能的交互式帮助



# 使用 Plotly 绘图

Plotly 是一个有趣的组合。这是一个订阅网站，提供重要的数据分析图形功能。您可以使用该软件的免费版本，但您仍然需要使用凭据登录才能使用它。从 Node.js 到 Python 等各种语言都提供了图形函数。

此外，生成的图形在 Plotly 和您的本地笔记本中可用。如果将图形标记为公共，则可以从笔记本中访问它，就像通过 internet 访问任何其他图形一样。类似地，作为 web 图形，您可以从显示中进行选择，并根据需要保存到本地。

在这个例子中，我们再次使用投票直方图，但是使用 Plotly 的功能。

该脚本如下所示:

```jl
import plotly
import plotly.graph_objs as go
import plotly.plotly as py
import pandas as pd
import numpy as np

#once you set credentials they are stored in local space and referenced automatically
#you need to subscribe to the site to get the credentials
#the api key would need to be replaced with your key
#plotly.tools.set_credentials_file(username='DemoAccount', api_key='lr17zw81')

#we are generating a graphic that anyone can see it
plotly.tools.set_config_file(world_readable=True, sharing='public')

# load voting summary from other project
from_counts = np.load("from_counts.npy")
print(from_counts.shape)
(6110,)

#plotly expects a list in a data block
from_count_list = []
for from_count in from_counts:
 from_count_list.append(from_count)

data = [go.Histogram(x=from_count_list)]

# plot on plot.ly site
py.iplot(data, filename='basic histogram')  
```

我认为这是我见过的使用开箱即用选项/设置的直方图的较好的渲染之一。我们有与之前看到的相同的直方图，只是使用了更吸引眼球的属性:

![](../images/00058.jpeg)

# 创建人类密度图

我原本计划制作一个世界范围的人口密度图，但是现有的图形不允许设置每个国家的颜色。所以，我为美国做了一个密度图。

算法是:

1.  获得每个状态的图形形状。
2.  获得每个状态的密度。
3.  确定一个颜色范围，将最低密度应用于该范围的一端，将最高密度应用于另一端。
4.  对于每个状态:

    *   确定它的密度
    *   在该范围内查找密度值并选择一种颜色
    *   画出州名

这是用以下代码编写的(随着代码的进行，注释被嵌入):

```jl
%matplotlib inline import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap from matplotlib.patches import Polygon import pandas as pd import numpy as np import matplotlib # create the map map = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,
 projection='lcc',lat_1=33,lat_2=45,lon_0=-95) # load the shapefile, use the name 'states' # download from https://github.com/matplotlib/basemap/tree/master/examples/st99_d00.dbf,shx,shp map.readshapefile('st99_d00', name='states', drawbounds=True) # collect the state names from the shapefile attributes so we can # look up the shape obect for a state by it's name state_names = [] for shape_dict in map.states_info:
 state_names.append(shape_dict['NAME']) ax = plt.gca() # get current axes instance # load density data drawn from # https://en.wikipedia.org/wiki/List_of_U.S._states_by_population_density df = pd.read_csv('states.csv') print(df.head()) State        rank density/mi2  density/km2  pop_rank   2015_pop New Jersey      1       1,218          470        11  8,958,013 Rhode Island    2       1,021          394        43  1,056,298 Massachusetts   3         871          336        15  6,794,422 Connecticut     4         741          286        29  3,590,886 Maryland        5         618          238        19  6,006,401  
land_rank area_mi2   area_km2 0         46    7,354  19,046.80 1         50    1,034   2,678.00 2         45    7,800  20,201.90 3         48    4,842  12,540.70 4         42    9,707  25,141.00 # determine the range of density values max_density = -1.0 min_density = -1.0 for index, row in df.iterrows():
 d = row['density/mi2'] density = float(d.replace(',' , '')) if (max_density==-1.0) or (max_density<density): max_density = density if (min_density==-1.0) or (min_density>density): min_density = density print('max',max_density) print('min',min_density) range_density = max_density - min_density print(range_density) ('max', 1218.0) ('min', 1.0) 1217.0 # we pick a color for the state density out of color map cmap = matplotlib.cm.get_cmap('Spectral') # for each state get the color for it's density for index, row in df.iterrows():
 state_name = row['State'] d = row['density/mi2'] density = float(d.replace(',' , '')) color = cmap((density - min_density)/range_density) seg = map.states[state_names.index(state_name)] poly = Polygon(seg, facecolor=color, edgecolor=color) ax.add_patch(poly) plt.show()
```

我们在下图中看到一个彩色编码的密度图。我不确定为什么明尼苏达州和威斯康星州与数据不符(它们在地图上没有显示密度的颜色)。数据文件看起来是正确的，并且确实映射到图像点。

本例中使用的软件包需要安装，因为它们不是发布的标准集的一部分:

![](../images/00059.jpeg)



# 绘制社交数据的直方图

有各种各样的社交网站产生数据集。在本例中，我们将收集其中一个数据集，并根据数据生成直方图。具体数据集是来自 https://snap.stanford.edu/data/wiki-Vote.html[的维基上的投票行为。每个数据项显示用户号`N`投票给用户号`X`。因此，我们在直方图中生成一些统计数据，通过以下方式分析投票行为:](https://snap.stanford.edu/data/wiki-Vote.html)

*   收集所有发生的投票
*   对于每一票:
    *   递增一个显示投票者的计数器
    *   增加一个计数器，显示谁被投票
    *   按摩数据，以便我们可以在两个直方图中显示它

编码如下:

```jl
%matplotlib inline # import all packages being used import matplotlib.pyplot as plt import pandas as pd import numpy as np import matplotlib
# load voting data drawn from https://snap.stanford.edu/data/wiki-Vote.html df = pd.read_table('wiki-Vote.txt', sep=r"\s+", index_col=0)
# produce standard summary info to validate print(df.head()) print(df.describe())
```

Python 会自动将第一列指定为表中的索引，而不管该索引是否被重用(就像这种情况)。您可以看到在`describe()`结果中只提到了`ToNodeId`列:

```jl
 ToNodeId FromNodeId 30              1412 30              3352 30              5254 30              5543 30              7478
 ToNodeId count  103689.000000 mean     3580.347018 std      2204.045658 min         3.000000 25%      1746.000000 50%      3260.000000 75%      5301.000000 max      8297.000000
```

接下来，我们根据一个人的投票数和一个人的投票数生成分组总数。我假设有一个内置函数可以更好地完成这项工作，但是我没有找到:

```jl
from_counter = {} to_counter = {} for index, row in df.iterrows():
 ton = row['ToNodeId'] fromn = index    #add the from entry
 if from_counter.has_key(fromn): # bump entry from_counter[fromn] = from_counter.get(fromn) + 1 else: # create entry from_counter[fromn] = 1    #add the to entry
 if to_counter.has_key(ton): # bump entry to_counter[ton] = to_counter.get(ton) + 1 else: # create entry to_counter[ton] = 1print(from_counter) print(to_counter) {3: 23, 4: 29, 5: 23, 6: 302, 7: 24, 8: 182, 9: 81, 10: 86, 11: 743,…
```

我们已经可以看到一些大的数字，比如:

```jl
#extract the count values from_counts = from_counter.values() to_counts = to_counter.values()
print("Most votes by a user",max(from_counts)) print("Most voted for",max(to_counts)) ('Most votes by a user', 893) ('Most voted for', 457)
#make histogram of number of references made by a user plt.hist(from_counts) plt.title("How Many Votes Made By Users") plt.xlabel("Value") plt.ylabel("Frequency") plt.show()
```

我们看到下面的情节，现在熟悉的用户投票显示。我认为这是我见过的最简单的布局之一:

![](../images/00060.jpeg)

现在我们用下面的代码为一个用户制作一个引用直方图:

```jl
#make histogram of number of references made for a user plt.hist(to_counts) plt.title("How Many Votes Made for User") plt.xlabel("Value") plt.ylabel("Frequency") plt.show()
```

我们看到用户投票图如下。我没有预料到如此不平衡的结果:只有少数人投了很多票，只有少数人获得了重要的选票；

![](../images/00061.jpeg)



# 绘制 3D 数据

许多数据分析包(R、Python 等)都具有强大的数据可视化功能。一个有趣的是以三维方式显示数据。通常，当使用三维时，会出现意想不到的可视化效果。

对于这个例子，我们使用的是来自[https://uci.edu/](https://uci.edu/)的汽车数据集。这是一个很常用的数据集，它有几个车辆属性，例如，`mpg`、`weight`和`acceleration`。如果我们将这些数据属性中的三个绘制在一起，看看我们是否能识别出任何明显的规则，会怎么样？

涉及的编码如下:

```jl
%matplotlib inline # import tools we are using import pandas as pd import numpy as np from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt # read in the car 'table' – not a csv, so we need # to add in the column names column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name'] df = pd.read_table('http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data', \
 sep=r"\s+", index_col=0, header=None, names = column_names) print(df.head())
 cylinders  displacement horsepower  weight  acceleration  year  origin  \ mpg 18.0          8         307.0  130.0  3504.0          12.0    70       1 15.0          8         350.0  165.0  3693.0          11.5    70       1 18.0          8         318.0  150.0  3436.0          11.0    70       1 16.0          8         304\.   150.0  3433.0          12.0    70       1 17.0          8         302\.   140.0  3449.0          10.5    70       1  
mpg                        name 18.0  chevrolet chevelle malibu 15.0          buick skylark 320 18.0         plymouth satellite 16.0              amc rebel sst 17.0                ford torino 
```

在下面的代码中，我们根据三个看似重要的因素(重量、每加仑英里数和发动机中的气缸数)绘制出数据:

```jl
#start out plotting (uses a subplot as that can be 3d)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d') # pull out the 3 columns that we want
xs = []
ys = []
zs = []
for index, row in df.iterrows():
 xs.append(row['weight'])
 ys.append(index) #read_table uses first column as index
 zs.append(row['cylinders']) # based on our data, set the extents of the axes
plt.xlim(min(xs), max(xs))
plt.ylim(min(ys), max(ys))
ax.set_zlim(min(zs), max(zs)) # standard scatter diagram (except it is 3d)
ax.scatter(xs, ys, zs) ax.set_xlabel('Weight')
ax.set_ylabel('MPG')
ax.set_zlabel('Cylinders') plt.show()
```

出乎意料的是，不管权重如何，通过明显的三行数据点，似乎存在三个水平:

*   六缸发动机，每加仑油耗更高
*   低 mpg 四缸发动机
*   四缸车的 mpg 更高

我本以为重量会有更大的影响:

>![](../images/00062.jpeg)



# 摘要

在本章中，我们使用了来自 Python 和 Jupyter 下的 R 的预测模型。我们使用 Matplotlib 进行数据可视化。我们使用了交互式绘图(在 Python 下)。我们还介绍了 Jupyter 中几种可用的绘图技术。我们用 SciPy 创建了一个密度图。我们使用直方图来可视化社会数据。最后，我们在 Jupyter 下生成了一个 3D 绘图。

在下一章，我们将看看在 Jupyter 下以不同的方式访问数据。