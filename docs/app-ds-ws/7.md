

# 六、将 Jupyter 笔记本用于网页抓取

概观

在这一章中，你将学习 HTTP 请求和从 HTML 解析数据。与前几章一样，您将继续获得使用 Python 处理数据集的实践经验，包括合并表和为分析做准备。本章结束时，你将能够使用 Python 发出 HTTP 请求，如 API 调用，并创建管道从网页中提取数据。

# 简介

到目前为止，在本书中，我们一直关注使用 Jupyter 来构建可重复的数据分析和建模工作流。在这一章中，我们将继续采用类似的方法，但重点是数据采集。特别是，我们将向您展示如何使用 HTTP 请求从 web 获取数据。这将包括发出 API 请求和通过解析 HTML 抓取网页。除了这些新主题，我们将继续使用 pandas 来构建和转换我们的数据集。

在我们介绍 HTTP 请求以及如何在 Python 中使用它们之前，我们将讨论从 web 收集数据的重要性。网上的数据量是巨大的，而且还在以惊人的速度持续增长。此外，它对于推动业务增长变得越来越重要。比如，想想正在进行的从报纸、杂志和电视等技术向在线内容的全球转移。有了手机上随时可用的定制新闻源，以及脸书、Reddit、Twitter 和 YouTube 等实时新闻源，就很容易明白为什么历史替代品会继续失去市场份额。

# 互联网数据来源

作为数据科学家，互联网帮助我们与任何我们能想象的数据集联系起来。例如，世界各地的政府都会发布信息丰富的公共数据集。同样，一些公司将某些数据集公之于众，这在给定的行业中可能具有巨大的价值。这方面的一个例子是拼车业务 *Lyft* ，该公司发布了开源数据，这些数据可能有利于训练自动驾驶汽车。

除了在线数据集，**应用编程接口** ( **API** )服务也存在，它们以编程方式提供相关的、新鲜的数据。例如，一个依赖天气的企业可能需要一个 API 来提供给定地区的当前情况，以及更新的预测。可以设置流程来每天查询该 API，并更新连接到仪表板的内部数据库，以便向业务利益相关者提供该数据和其他相关数据。

网络搜集是使用计算机程序从一个网页或一组网页中提取信息的过程。这对于从无法通过其他方式(即结构化数据集或 API)轻松访问其数据的网站获取信息非常有用。

虽然 API 旨在以编程方式使用，但 web 抓取涉及使用计算机来摄取旨在供人类查看和理解的数据。API 数据是以机器可读的格式(如 XML 或 JSON)交付的，而 web 页面通常使用 HTML 以人类可读的格式呈现数据。

因此，web 抓取通常涉及从 HTML 文本元素中提取结构化数据的能力。

web 抓取的确切过程将取决于您使用的工具、特定的网页和所需的内容。在本书中，我们将通过请求和解析维基百科中的数据来了解 Python 可以使用的网络抓取技术。随着我们的进展，通过关注底层概念，我们将了解从 HTML 页面中提取所需内容是多么容易。

web 抓取的一个更困难的方面是请求 HTML 本身。可以理解为什么公司不希望任意程序能够从他们的网站请求页面，因为向用户发送网页需要他们的资源。与以合理的速度浏览网站的人不同，计算机能够在很短的时间内请求许多网页。再者，程序化流量对一个网站没有任何价值。例如，计算机不受广告的影响，也不会对购买感兴趣。

注意

根据您所在的地区以及网站的条款和条件，网络抓取可能存在法律问题。在以编程方式请求网页之前(也就是说，在进行本章中的练习和活动之前)，请研究与您当地管辖区相关的规则和法规。为了减轻道德顾虑，请遵循这里提到的指导原则。

这给我们带来了一个关于网络抓取道德的重要观点。虽然这在行业中并不少见，并且可以产生非常有价值的数据，但是不应该不加考虑就这样做。例如，您应该意识到网络抓取会导致企业代表您承担费用，并且在短时间内请求许多页面，您正在耗尽原本打算供实际用户浏览网站的资源。为了减轻 web 抓取的这些不良后果，您应该执行以下操作:

*   Limit the rate at which you make requests.

    例如，在连续的 HTTP 请求之间，您可能会让脚本休眠至少几秒钟。

*   Use a descriptive user agent to identify yourself to the website.

    例如，Python 的`requests`库的默认用户代理将是`python-requests/2.22.0`(对于`requests`的 2.22.0 版本)。

    检查一个`robots.txt`文件，该文件位于网站的个人文件夹中(例如，`www.website.com/robots.txt`)。遵守此文件的规则，不要抓取相关用户代理的不允许通配符下列出的任何页面。

虽然网络抓取如果做得不道德会有负面的后果，但是以编程方式从网站请求和解析信息的能力也会有非常积极的后果。例如，这就是谷歌等搜索引擎能够索引网站的方式，使它们可以通过搜索来访问。此外，网络抓取对于测试网站非常重要，以确保事情按预期运行，并且用户在网站上看到适当的信息。

虽然其中一些概念现在看起来很抽象，比如发出 HTTP 请求或使用 web APIs，但是随着我们在下一节继续讨论它们，它们会变得很清楚。特别是，你可以通过 Python 来学习，这也是我们在接下来的练习和活动中要做的。

# HTTP 请求简介

超文本传输协议，简称为 T2 HTTP T3，是互联网数据通信的基础。它定义了页面应该如何被请求以及响应应该是什么样子。例如，客户可以请求销售笔记本的亚马逊页面，本地餐馆的谷歌搜索，或者他们的脸书 feed。除了 URL，请求还将在请求头的内容中包含用户代理和可用的浏览 cookies。

用户代理告诉服务器客户机正在使用什么浏览器和设备，这通常用于提供网页响应的最用户友好的版本。也许他们最近登录过该网页；此类信息将存储在一个 cookie 中，该 cookie 可用于自动让用户登录。

多亏了 web 浏览器，HTTP 请求和响应的这些细节被处理了。幸运的是，今天，当使用 Python 等高级语言发出请求时，情况也是如此。出于许多目的，请求头的内容可以被忽略。除非另有说明，否则在请求 URL 时，这些都是在 Python 中自动生成的。尽管如此，为了排除故障和理解我们的请求所产生的响应，对 HTTP 有一个基本的了解是很有用的。

HTTP 方法有很多种，比如`GET`、`HEAD`、`POST`和`PUT`。前两个用于请求将数据从服务器发送到客户端，而后两个用于将数据发送到服务器。

这些 HTTP 方法可以总结如下:

*   `GET`:从指定的 URL 中检索信息
*   `HEAD`:从指定 URL 的 HTTP 头中检索元信息
*   `POST`:发送附加到指定 URL 的资源的附加信息
*   `PUT`:发送替换指定 URL 上的资源的附加信息

每当我们在浏览器中键入网页地址并按下*回车*时，就会发送一个`GET`请求。对于网络抓取，这通常是我们唯一感兴趣的 HTTP 方法，也是我们在本章中使用的唯一方法。

一旦发送了请求，服务器就会返回各种类型的响应。这些用 100 级到 500 级代码标记，其中代码中的第一个数字代表响应类别。这些可以描述如下:

*   **1xx** :信息响应；例如，服务器正在处理一个请求。很少看到这种情况。
*   **2xx** :成功；例如，页面已经正确加载。
*   **3xx** :重定向；例如，请求的资源已经被移动，我们被重定向到一个新的 URL。
*   **4xx** :客户端错误；例如，请求的资源不存在。
*   **5xx** :服务器错误；例如，网站服务器接收的流量过大，无法满足请求。

出于 web 抓取的目的，我们通常只关心响应类，即响应代码的第一个数字。然而，每个类别中的响应的子类别提供了更多关于正在发生的事情的粒度。例如，`401`代码表示未授权响应，而`404`代码表示未找到页面响应。这个区别是值得注意的，因为`404`表示我们请求了一个不存在的页面，而`401`告诉我们需要登录才能查看特定的资源。

在下面的练习中，我们将看到如何使用 Jupyter 笔记本在 Python 中完成 HTTP 请求。

## 使用 Python 发出 HTTP 请求

既然我们已经讨论了 HTTP 请求是如何工作的以及我们应该期待什么类型的响应，那么让我们看看如何在 Python 中实现这一点。我们将使用一个名为`requests`的库，它是 Python 最流行的(如果不是最流行的)外部库之一。不使用`requests`，也可以使用 Python 的内置工具，比如`urllib`，来发出 HTTP 请求，但是`requests`更直观，通常是最佳选择，只要您愿意将它作为项目的依赖项添加进来。

允许对标题、cookies 和授权进行各种定制。它跟踪重定向并提供返回特定页面内容的方法，如 JSON(我们将在 API 练习中看到，*练习 6.02* ，*用 Python 和 Jupyter 笔记本进行 API 调用*，稍后)。

`requests`的一个缺点是它不在客户端(也就是在您的机器上)呈现 JavaScript。当请求页面时，服务器通常会返回包含 JavaScript 代码片段的 HTML。如果您使用的是 web 浏览器(Chrome、Firefox 等)，这些代码片段会在页面加载期间自动在您的计算机上运行。然而，当使用 Python 通过`requests`请求内容时，这个 JavaScript 代码将不会被执行。因此，这样做可能会改变或创建的任何元素都将丢失。

通常，缺少 JavaScript 呈现不会影响从页面获取所需信息的能力。如果您发现您的用例需要呈现 JavaScript，那么您可以考虑用 Selenium 这样的库来做这件事。它有一个类似于`requests`库的 API，但是支持使用 web 驱动程序呈现 JavaScript。它甚至可以在动态页面上运行 JavaScript 命令，例如，改变文本颜色或滚动到页面底部。正如你所想象的，这对于网站测试非常有用。

让我们在 Jupyter 笔记本上做一个关于使用 Python 的`requests`库的练习。

## 练习 6.01:使用 Python 和 Jupyter 笔记本进行 HTTP 请求

学习了 HTTP 请求的理论之后，让我们用 Python 和 Jupyter 笔记本来应用它。在本练习中，您将学习如何在笔记本中发出 HTTP 请求，以及如何解释和处理 HTML 响应数据。按照以下步骤完成本练习:

1.  创建一个 Jupyter 笔记本并加载以下库:

    ```py
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

2.  首先向您展示一些使用 Python 进行 HTTP 请求的方法。导入`requests`库，如下:

    ```py
    import requests
    ```

3.  Then, prepare a request by running the following code:

    ```py
    url = 'https://jupyter.org/'
    req = requests.Request('GET', url)
    req = req.prepare()
    ```

    您使用`Request`类准备一个对`jupyter.org`主页的`GET`请求。

4.  Print the docstring for the `req` prepared request by running `req?` in the next cell. You will get the following output:![Figure 6.1: Printing the docstring for a PreparedRequest object
    ](img/B15916_06_01.jpg)

    图 6.1:打印 PreparedRequest 对象的文档字符串

    通过查看它的用法，您可以看到如何使用会话发送请求。这类似于打开 web 浏览器(启动会话)然后请求 URL。

5.  Make the request and store the response in a variable named `resp` by running the following code:

    ```py
    with requests.Session() as sess:
        resp = sess.send(req)
    ```

    前面的代码返回 HTTP 响应，由 page 变量引用。通过使用`with`语句，您可以初始化一个会话，其范围仅限于缩进的代码块。这意味着您不必担心显式关闭会话，因为这是自动完成的。

6.  Run the `resp` and `resp.status_code` commands to investigate the response. The string representation of the page should indicate a `200` status code response.

    `resp`命令的输出如下:

    ```py
    <Response [200]>
    ```

    `resp.status_code`命令的输出是`200`。

7.  Assign the response text to the `page_html` variable and take a look at the first 1,000 characters of the string with the following command:

    ```py
    page_html = resp.text
    page_html[:1000]
    ```

    以下是显示前 100 个字符的输出:

    ![Figure 6.2: Printing the response HTML content
    ](img/B15916_06_02.jpg)

    图 6.2:打印响应 HTML 内容

    正如所料，响应是 HTML。

    在`BeautifulSoup`的帮助下，您可以更好地格式化这个输出，这个库将在本节后面的 HTML 解析中广泛使用。

8.  Print the `head` of the formatted HTML by running the following code:

    ```py
    from bs4 import BeautifulSoup
    print(BeautifulSoup(page_html, 'html.parser').prettify()[:1000])
    ```

    这将显示以下输出:

    ![Figure 6.3: Displaying the response HTML content with indentation
    ](img/B15916_06_03.jpg)

    图 6.3:显示带有缩进的响应 HTML 内容

    您导入`BeautifulSoup`然后打印输出，其中新行根据它们在 HTML 结构中的层次缩进。

9.  Take this a step further and actually display the HTML in Jupyter by using the IPython `display` module. Do this by running the following code:

    ```py
    from IPython.display import HTML
    HTML(page_html)
    ```

    下面是这段代码的输出截图:

    ![Figure 6.4: HTML rendering without fetching images or running JavaScript
    ](img/B15916_06_04.jpg)

    图 6.4:不提取图像或运行 JavaScript 的 HTML 呈现

    这里，假设没有运行 JavaScript 代码，也没有加载外部资源，您可以看到尽可能好的 HTML 呈现。例如，托管在`jupyter.org`服务器上的图像不会被渲染。相反，我们可以看到替代文本，即`circle of programming icons`、`jupyter logo`等等。

10.  Compare this to the live website, which can be opened in Jupyter using an IFrame, by running the following code:

    ```py
    from IPython.display import IFrame
    IFrame(src=url, height=800, width=800)
    ```

    下面是 IFrame 的截图:

    ![Figure 6.5: Loading a live web page in the Jupyter Notebook
    ](img/B15916_06_05.jpg)

    图 6.5:在 Jupyter 笔记本中加载动态网页

    在这里，您可以看到完整的站点呈现，包括 JavaScript 和外部资源。事实上，您甚至可以点击超链接并在 IFrame 中加载这些页面，就像常规的浏览会话一样。

    注意

    使用 IFrame 后关闭它是一个好习惯。这可以防止它耗尽内存和处理能力。选择单元格，点击 Jupyter 笔记本中的`Current Outputs` | `Clear from the Cell`菜单，即可关闭。

11.  At the start of this exercise, you made a request by preparing it and then used a session to send it. This is often done using a shorthand method instead, as seen here.

    通过运行以下代码向[http://www.python.org/](http://www.python.org/)发出请求:

    ```py
    url = 'http://www.python.org/'
    resp = requests.get(url)
    resp
    ```

    这将输出页面的字符串表示:

    ```py
    <Response [200]>
    ```

    它应该显示一个`200`状态代码，表示对您的请求的成功响应。

12.  Run the following command to print the URL of your page:

    ```py
    resp.url
    ```

    输出如下所示:

    ```py
    'https://www.python.org/'
    ```

13.  Run the following command to print the history attributes of the page.

    ```py
    resp.history
    ```

    这将返回`[<Response [301]>]`。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

    你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。

返回的 URL 不是我们输入的内容；注意到区别了吗？我们从输入 URL[http://www.python.org/](http://www.python.org/)被重定向到该页面的安全版本[https://www.python.org/](https://www.python.org/)。在协议中，不同之处由 URL 开头的附加`s`表示。任何重定向都存储在`history`属性中；在这种情况下，我们在这里找到一个状态代码为`301`(永久重定向)的页面，对应于被请求的原始 URL。

## 使用 Python 进行 API 调用

API 调用可以只是一个 HTTP 请求，就像我们在前面的练习中看到的那样。一个不同之处是，API 请求通常被期望以机器可读的格式返回数据，而不是常规的 web 请求，后者被期望返回浏览器可以呈现的 HTML。

另一个区别是 API 调用更可能需要某种认证，比如在请求 URL 中传递令牌参数或指定请求头。许多 API 有更复杂的认证方法，比如使用 OAuth 2.0 协议。这些类型的 API 请求超出了本书的范围，因为我们将只关注不需要认证的最简单的情况。

在下一个练习中，我们将展示如何使用免费的 API 从维基百科中提取文章信息。我们将使用这个 API 从 wiki 表中提取各个国家的利率数据。然后，在稍后的活动*活动 6.02* 、*分析国家人口和利率*中，我们将再次访问同一个表，并学习如何使用 HTML 网页抓取技术提取相同的数据。

## 练习 6.02:使用 Python 和 Jupyter 笔记本进行 API 调用

API 调用允许您按需访问结构良好的数据。知道如何使用它们是数据科学家必须具备的技能。在本练习中，您将使用 Wikipedia API 来学习 API 的一般用法。您将发出 API 请求并接收 JSON 响应数据。按照以下步骤完成本练习:

1.  Start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

2.  加载下列库。您将使用这些来配置笔记本的绘图设置:

    ```py
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os

    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    import requests
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")

    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

3.  Run the following code to define your API request URL:

    注意

    注意下面字符串中的斜线。请记住，反斜杠(`\`)用于将代码分成多行，而正斜杠(`/`)是 URL 的一部分。

    ```py
    url = ('https://en.wikipedia.org/w/api.php' \
           '?action=parse' \
           '&page=List_of_countries_by_central_bank_interest_rates' \
           '&section=1' \
           '&prop=wikitext' \
           '&format=json')
    url
    ```

    这将导致以下输出:

    ```py
    https://en.wikipedia.org/w/api.php?action=parse&page=List_of_countries_by_central_bank_interest_rates&section=1&prop=wikitext&format=json
    ```

    这里，您正在请求满足一组参数的资源，例如`action`、`page`、`section`等等。请注意，您已经通过在 URL 后面附加`&format=json`明确请求了一个`.json`格式的响应。这些参数特定于 Wikipedia API，但是许多 API 以类似的方式工作。

4.  Make the API request by running the following code:

    ```py
    resp = requests.get(url)
    resp
    ```

    这将打印以下输出:

    ```py
    <Response [200]>
    ```

5.  Run `resp.text[:100]` to print the first 100 lines of the response string. This should result in the following output:

    ```py
    '{"parse":{"title":"List of countries by central bank interest rates","pageid":20582369,"wikitext":{"'
    ```

    注意字符串是如何表示 JSON 数据的，这是我们在发出请求时所要求的。

6.  Convert the string into a Python dictionary by running the following code:

    ```py
    data = resp.json()
    type(data)
    ```

    这将输出数据对象类型，如下所示:

    ```py
    dict
    ```

7.  Run the `data` command to print the data object. Take note of some of the nested fields in the data, such as `parse`, `pageid`, and `wikitext`:![Figure 6.6: The response data in JSON format
    ](img/B15916_06_06.jpg)

    图 6.6:JSON 格式的响应数据

8.  Extract the page title from the API response data by running the following command:

    ```py
    data['parse']['title']
    ```

    这将输出以下内容:

    ```py
    'List of countries by central bank interest rates'
    ```

9.  Extract a row from the table contained in the API response data. This can be done by running the following code:

    ```py
    row_idx = 16
    wikitext = data['parse']['wikitext']['*']
    table_row = wikitext.split('|-')[row_idx]
    table_row
    ```

    这应该会输出类似如下的内容:

    ```py
    '                \n|align="left"| {{flag|Canada}} || 1.75 || {{dts|format=dmy|2018-10-24}}<ref name="CentralBankNews"/><ref name="GlobalRates">{{Cite web|url=http://www.global-rates.com/interest-rates/central-banks/central-banks.aspx|title=Central banks - summary of current interest rates|work=global-rates.com|accessdate=13 July 2017}}</ref>\n|1.40\n|0.35\n|1.25\n'
    ```

    理想情况下，从 Wikipedia 的免费 API 返回的表格数据将是一种更好的格式，便于我们以编程方式摄取。如你所见，事实并非如此。在前面的输出中，您从响应数据中提取了一个表作为一个`wikitext`字符串，然后通过在`|-`上拆分来分隔行。

10.  Use regular expressions to parse data from the row. Get the country for the extracted row by running the following code:

    ```py
    import re
    re.findall('flag\|([^}]+)}', table_row)
    ```

    这将输出列表中的国家名称，例如，如下所示:

    ```py
    ['Canada']
    ```

    同样，请注意，API 通常会使使用它的应用可以轻松地获得这些数据。在这种情况下，对于 Wikipedia，您仍然可以通过提取`flag|`和`}`之间的字段来相对容易地访问数据。在这种情况下，你从`{{flag|Canada}}`中提取了`Canada`。

11.  Instead of using regular expressions, some data is easier to extract using Python string methods, such as `split` and `strip`. Get the interest rate for your extracted row by running the following command:

    ```py
    table_row.split('||')[1].strip()
    ```

    这应该以字符串的形式输出利率；例如:

    ```py
    '1.75'
    ```

    通过迭代 API 响应数据中的所有行，您可以将这种提取应用于每一行，并提取所请求的表资源的所有数据。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

    你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。

我们关于 API 调用的部分到此结束。在接下来的练习中，我们将查看我们在这里使用的相同的 Wikipedia 数据，并展示如何通过解析 HTML 提取它，而不是通过使用 API 调用提取数据。这纯粹是出于演示的目的，与从 HTML 抓取获取数据相比，您应该总是更喜欢使用 API 作为数据源(如果可用的话)。

## 使用 Jupyter 笔记本解析 HTML

从 web 页面抓取数据包括对 HTML 资源发出 HTTP 请求，然后从响应内容中提取数据。一种简单的方法是将这个响应内容(HTML)输入到一个高级解析库中，比如 Python 的`BeautifulSoup`。这并不是说这是唯一的方法；原则上，可以使用正则表达式或 Python 字符串方法(如`split`)来挑选数据。然而，追求这些选项中的任何一个都是对时间的低效利用，并且很容易导致错误。因此，通常不赞成这样做，相反，建议使用值得信赖的解析工具。

为了理解如何从 HTML 中提取内容，了解 HTML 的基础知识很重要。首先， **HTML** 代表**超文本标记语言**。像 **Markdown** 或 **XML** ( **可扩展标记语言**)，它只是一种标记文本的语言。

在 HTML 中，显示文本包含在 HTML 元素的内容部分中；例如:

```py
<p>Here is the text to display!</p>
```

在这段 HTML 中，要显示的内容文本由`<p>`标签包装。一些常见的标签类型如下:

*   `<p>`(段落)
*   `<div>`(文本块)
*   `<table>`(数据表)
*   `<h1>`(标题)
*   `<img>`(图片)
*   `<a>`(超链接)

标签可以有指定重要元数据的属性。最常见的是，该元数据用于控制元素文本在页面上的显示方式和位置。这就是 CSS 文件发挥作用的地方。考虑下面的例子:

```py
<p id="my-paragraph">Here is the text to display!</p>
```

在这段 HTML 中，我们将一个`id`赋给了`<p>`标签。这个`id`可以在一个 CSS 文件中被引用，以便设置标签的样式属性。

属性可以存储其他有用的信息，比如在一个`<a>`标签中的`href`超链接，它指定一个 URL 链接，或者在一个`<img>`标签中的另一个`alt`标签，它指定当图像资源不能被加载时显示的文本。考虑下面的例子:

```py
<a href="/my_picture_full_resolution.png">
<img src="/my_picture.png" alt="A photo of me!"></img>
</a>
```

在这段 HTML 中，我们显示了一个来自于`my_picture.png`资源的图像。如果没有找到这个资源，那么将会看到`alt`属性文本。然后这个`<img>`元素被包装在一个`<a>`标签中，这里的`href`属性指向高质量图像的位置。这将允许用户通过点击图像导航到该页面。

现在我们已经掌握了一些 HTML 的基础知识，让我们把注意力转回到 Jupyter 笔记本上并解析一些数据。我们还将使用 Chrome 浏览器的开发者工具窗口，因为它对 HTML 解析非常有帮助。

## 练习 6.03:用 Python 和 Jupyter 笔记本解析 HTML

在本练习中，我们将重点关注从 HTML 文档中提取数据。您将学习如何实现 Python 解析技术，并了解为什么 Jupyter 笔记本如此适合这项任务。按照以下步骤完成本练习:

1.  Create a new notebook using either of the following commands:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

2.  运行下面的代码来加载一些库。您将使用这些库来配置笔记本的绘图设置:

    ```py
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os

    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    import requests

    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

3.  Scrape the central bank interest rates for each country, as reported by Wikipedia. Before diving into the code, open up the web page containing the data to be extracted.

    在网页浏览器中进入[https://en . Wikipedia . org/wiki/List _ of _ countries _ by _ central _ bank _ interest _ rates](https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates)。如果可能的话，使用 Chrome，在本练习的后面，我们将向您展示如何使用 Chrome 的开发工具查看和搜索 HTML。

    查看页面，除了一大串国家及其利率，你能看到的内容很少。这是你要刮的桌子。

4.  Return to the Jupyter Notebook and request the page by running the following code:

    ```py
    url = 'https://en.wikipedia.org/wiki/List_of_countries_by_'\
          'central_bank_interest_rates'
    resp = requests.get(url)
    print(resp.url, resp.status_code)
    ```

    这将输出 URL，后跟一个表示成功的`200`状态代码:

    ```py
    https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates 200
    ```

5.  Load the HTML as a `BeautifulSoup` object so that it can be parsed. Do this by running the following code:

    ```py
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(resp.content, 'html.parser')
    ```

    注意

    我们使用 Python 默认的`html.parser`作为解析器，但是也可以安装和使用其他解析库，比如`lxml`。由于每个 HTML 解析器有不同的解释文档的逻辑，结果产生的`BeautifulSoup`对象可能会有所不同，这取决于使用的是哪一个。

6.  Usually, when working with a new object in Jupyter, such as the `BeautifulSoup` object you created previously, it's a good idea to pull up the docstring.

    通过运行`soup?`来做到这一点，如下面的屏幕截图所示:

    ![Figure 6.7: The docstring for a BeautifulSoup object
    ](img/B15916_06_07.jpg)

    图 6.7:beautiful soup 对象的文档字符串

    可以看出，在这种情况下它并没有特别大的帮助，因为 docstring 提供的信息并不多。

    探索 Python 对象的另一个工具是内置的`dir`函数，它列出了对象的属性和方法。通过运行`dir(soup)`调用它，如下所示:

    ![Figure 6.8: The output of dir(soup)
    ](img/B15916_06_08.jpg)

    图 6.8:dir(soup)的输出

    滚动列表，你会看到我们稍后会用到的方法和属性，比如`find_all`、`attrs`和`text`。尽管如此，这不是特别有用的信息。

7.  There is yet another way of getting information on Python objects, which you will see here. Install the external library called `pdir2` with `pip`, by running the following in your Terminal:

    ```py
    pip install pdir2
    ```

    安装后，可以通过运行以下代码来使用它:

    ```py
    import pdir
    pdir(soup)
    ```

    注意，我们导入了`pdir`，尽管这个包在 **Python 打包索引** ( **PyPI** )中被列为`pdir2`:

    ![Figure 6.9: The output of pdir(soup)
    ](img/B15916_06_09.jpg)

    图 6.9:pdir(soup)的输出

    在这里，您可以看到可以在 soup 上调用的方法和属性的类似列表，但是现在它们被组织成分组，并且在适当的地方包含了描述。

    因为我们将使用`find_all`方法，所以让我们在列表中搜索该描述。其内容应如下:

    ![Figure 6.10: Description of the find_all method
    ](img/B15916_06_10.jpg)

    图 6.10:find _ all 方法的描述

8.  It's time to start parsing data from our HTML. To start with, get the `h1` heading for the page by running the following code:

    ```py
    h1 = soup.find_all('h1')
    h1
    ```

    这将输出一个包含页面标题元素的列表:

    ```py
    [<h1 class="firstHeading" id="firstHeading" lang="en">List of countries by central bank interest rates</h1>]
    ```

    通常，页面只有一个`h1`(顶层标题)元素，所以这里只有一个也就不足为奇了。

9.  At this point, you have identified the HTML element that contains your data, but the field still needs to be extracted as a string. To do this, run the next few cells one by one in the notebook:

    ```py
    h1 = h1[0]
    ```

    按如下方式打印 HTML 元素属性:

    ```py
    h1.attrs
    ```

    输出如下所示:

    ```py
    {'id': 'firstHeading', 'class': ['firstHeading'], 'lang': 'en'}
    ```

    按如下方式打印可见文本

    ```py
    h1.text
    ```

    输出如下所示:

    ```py
    'List of countries by central bank interest rates'
    ```

    以下屏幕截图显示了上述代码的输出:

    首先，用`h1 = h1[0]`将`h1`变量赋给第一个(也是唯一一个)列表元素。

    然后，用`h1.attrs`打印出 HTML 元素属性。在这里，你可以看到`id`和`class`元素，它们都可以在 CSS 样式表中被引用。

    最后，您通过运行`h1.text`获得纯文本形式的标题。

10.  Run the following to see the number of image tags you were able to extract:

    ```py
    imgs = soup.find_all('img')
    len(imgs)
    ```

    这应该会返回一个 100 左右的数字，表示页面上的图像数量。

11.  In this case, most of these images correspond to country flags in the table. This can be seen by printing the source of each image. Do this by running the following code:

    ```py
    for element in imgs:
        if 'src' in element.attrs.keys():
            print(element.attrs['src'])
    ```

    这将输出每个图像资源的路径，如以下示例所示:

    ```py
    //upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/21px-Flag_of_Albania.svg.png
    //upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/23px-Flag_of_Angola.svg.png
    //upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/23px-Flag_of_Argentina.svg.png
    ```

12.  Now, scrape the data from the table. Use Chrome's developer tools to hunt down the relevant HTML elements.

    如果你还没有这样做，在 Chrome 中打开维基百科页面。然后，在浏览器中，从带有三个垂直点的`More Tools`菜单按钮中选择`Developer Tools`:

    ![Figure 6.11: Opening the developer tools menu
    ](img/B15916_06_11.jpg)

    图 6.11:打开开发者工具菜单

    也可以在 Windows 或 Linux 上按*Ctrl*+*Shift*+*I*，或者在 Mac 上按*Ctrl*+*Option*+*I*打开`Developer Tools`。将会打开一个侧边栏，您可以在其中查看 HTML。您可以点击`Elements`选项卡来完成此操作。

13.  Select the arrow icon at the top left of the tools sidebar. This allows us to hover over the page and see where the HTML element is located. Do this by going to the `Elements` section of the sidebar:![Figure 6.12: Arrow icon for locating HTML elements 
    ](img/B15916_06_12.jpg)

    图 6.12:定位 HTML 元素的箭头图标

14.  Hover over the body to see how the table is contained within the `div` that has `id="bodyContent"`.![Figure 6.13: The parent div of the table we want to extract
    ](img/B15916_06_13.jpg)

    图 6.13:我们要提取的表的父 div

15.  Now, go back to your notebook and select that `div` by running the following command:

    ```py
    body_content = soup.find('div', {'id': 'bodyContent'})
    ```

    这里，您使用的是`find`方法，它与您之前使用的`find_all`方法相同，只是它只返回第一个匹配。在调用这个函数时，您传递了第二个参数`{'id': 'bodyContent'}`，其形式为`{attribute_name: attribute_value}`。

16.  Having narrowed down the content of interest, continue to seek out the table within this subset of the full HTML.

    通常，表格被组织成标题(`<th>`)、行(`<tr>`)和数据条目(`<td>`)。利用这些知识，通过运行以下代码尝试获取表格标题:

    ```py
    table_headers = body_content.find_all('th')
    table_headers
    ```

    这将输出一个标题元素列表，从下面显示的内容开始:

    ```py
    [<th>Country or<br/>currency union</th>,
     <th>Central bank <br/> interest rate (%)</th>,
     <th>Date of last <br/> change
     </th>,
     <th>Average inflation rate 2013-2017 (%)
    ...
    ```

17.  Our next step is parsing the headers as plain text, from this list of HTML elements. Do this by running the following code:

    ```py
    for i, t in enumerate(table_headers):
        print(i, t.text.strip())
        print('-'*10)
    ```

    这将输出列名，从下面显示的列名开始:

    ```py
    0 Country orcurrency union
    ----------
    1 Central bank  interest rate (%)
    ----------
    2 Date of last  change
    ----------
    3 Average inflation rate 2013-2017 (%)
    by WB and IMF[1][2] as in the List
    ----------
    ...
    ```

18.  现在，您将为表的前四列提取数据。使用前面的输出作为参考，通过运行下面的代码为我们的表手动设置标题名称:

    ```py
    table_headers = ['Country or currency union', \
                     'Central bank interest rate (%)', \
                     'Date of last change', \
                     'Average inflation rate (%)']
    ```

19.  Now, you are ready to extract the data., first figuring out how this should be done on a per-row basis.

    通过运行以下代码，为表中的一行选择 HTML 元素:

    ```py
    row_number = 8
    row_data = body_content.find_all('tr')[row_number]\
               .find_all('td')
    ```

    虽然您之前搜索了所有的 header 元素，但是在这里，您要查找所有的行，然后在索引 8 处选择它们(随机选择)。然后，在同一行代码中，搜索该行中的所有数据元素`td`。

20.  Run the `row_data` command in the notebook to see the resulting data elements that make up the row:![Figure 6.14: The data elements in a selected row
    ](img/B15916_06_14.jpg)

    图 6.14:选定行中的数据元素

21.  Recall when you iterated through the header elements and pulled the text for each. Here, you will do an analogous operation on the row data.

    运行以下代码:

    ```py
    for i, d in enumerate(row_data):
        print(i, d.text)
    ```

    这将输出行数据和索引，以便于理解，如下所示:

    ```py
    0  Bahrain
    1 2.50
    2 31 July 2019[3]
    3 2.40
    4 0.10
    5 1.04
    ```

    以该行为例，查看每个感兴趣的元素，并确定如何最好地解析数据条目。回想一下，您对以下几列感兴趣:`Country or currency union`、`Central bank interest rate (%)`、`Date of last change`和`Average inflation rate (%)`。

22.  The first entry you are interested in is the country. Assign the `d1` variable to the HTML element for that entry by running the following code:

    ```py
    d1 = row_data[0]
    d1
    ```

    这应该将未解析的元素作为字符串输出，如下所示:

    ```py
    <td align="left"><span class="flagicon"><img alt="" class="thumbborder" data-file-height="900" data-file-width="1500" decoding="async" height="14" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/23px-Flag_of_Bahrain.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/35px-Flag_of_Bahrain.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/46px-Flag_of_Bahrain.svg.png 2x" width="23"/> </span><a href="/wiki/Bahrain" title="Bahrain">Bahrain</a></td>
    ```

    通过查看前面的元素，您可以看到我们的国家名称嵌入在一个`<a>`标签中。因此，通过搜索那个`<a>`标签，然后获取文本内容，可以清楚地解析国家。

23.  首先，运行`d1.text`,注意输出中国家前面有一个字符，比如`\xa0`。这显然不是想要的。
24.  Next, run the following three commands, where you will find the `<a>` tag and then get its content, which should be the clean country name, as expected:

    ```py
    d1.txt
    d1.find('a')
    d1.find('a').text
    ```

    `d1.txt`命令返回`'\xa0Bahrain'`，`d1.find('a')`返回`<a href="/wiki/Bahrain" title="Bahrain">Bahrain</a>`，`d1.find('a').text`命令返回`'Bahrain'`。

25.  Moving on to the second data element, which will correspond to the interest rate, run the next couple of cells in the notebook to parse the value:

    ```py
    d2 = row_data[1]
    d2
    ```

    这将返回`<td>2.50</td>`。

    现在，请注意我们是如何将数字作为字符串的:

    ```py
    d2.text
    ```

    输出为`'2.50'`。

26.  For the next data element, you need to parse the date. Look at the element itself by running the following code:

    ```py
    d3 = row_data[2]
    d3
    ```

    这应该将未解析的元素作为字符串输出，如下所示:

    ```py
    <td><span data-sort-value="000000002019-05-24-0000" style="white-space:nowrap">24 May 2019</span><sup class="reference" id="cite_ref-CentralBankNews_3-1"><a href="#cite_note-CentralBankNews-3">[3]</a></sup>
    </td>
    ```

    类似于国家名称条目，您可以看到日期嵌入在另一个 HTML 元素中。特别是，它在一个`<span>`标签中。

    像之前一样，首先运行`d3.text`并注意输出中有不需要的字符，比如日期后面的“`3]\n`”。

    接下来，运行`d3.find_all('span')`和`d3.find_all('span')[0].text`命令，在这里找到`<span>`标记，然后获取它的内容，应该是干净日期，正如所料:

    ![Figure 6.15: Parsing the date from a span element
    ](img/B15916_06_15.jpg)

    图 6.15:解析来自 span 元素的日期

27.  The final entry we are interested in is the inflation rate. Look at this element by running the following code:

    ```py
    d4 = row_data[3]
    d4
    ```

    这应该将未解析的元素作为字符串输出，如下所示:

    ```py
    <td>2.40
    </td>
    ```

28.  Run the `d4.text` and `d4.text.strip()` commands to parse this data. Notice how, after running `d4.text`, we have an unwanted newline character. This can be removed by calling the `strip` string method.

    执行`d4.text`将返回`'2.40\n'`，执行`d4.text.strip()`将返回`'2.40'`。

29.  Having written the proper code for parsing a row, you are ready to perform the full scrape. This is done by iterating over the row elements, `<th>`, and attempting to extract the data for each, like you did previously. Do this by running the following code:

    ```py
    int_rates_data = []
    row_elements = body_content.find_all('tr')
    for i, row in enumerate(row_elements):
        row_data = row.find_all('td')
        if len(row_data) < 3:
            print('Ignoring row {} because length < 4'.format(i))
            continue
        d1, d2, d3, d4 = row_data[:4]
        errs = []
        try:
            d1 = d1.find('a').text
        except Exception as e:
            d1 = ''
            errs.append(str(e))
        try:
            d2 = d2.text
        except Exception as e:
            d2 = ''
            errs.append(str(e))
        try:
            d3 = d3.find_all('span')[0].text
        except Exception as e:
            d3 = ''
            errs.append(str(e))
        try:
            d4 = d4.text.strip()
        except Exception as e:
            d4 = ''
            errs.append(str(e))
        data = [d1, d2, d3, d4]
        print(data)
        int_rates_data.append(data)
        if errs:
            print('Errors in row {}: {}'.format(i, ', '.join(errs)))
    ```

    这个循环有几个有趣的部分:

    *   我们观察具有意外性质的行——在本例中，长度小于 4 的行——并忽略它们。
    *   我们使用`try...except`逻辑来处理有错误的行。
    *   我们迭代时打印出数据。
    *   Errors do not go unnoticed! We print them out as well as we iterate.

        输出应该类似于以下内容:

        ```py
        Ignoring row 0 because length < 4
        ['Albania', '1.00', '6 June 2016', '1.75']
        ['Angola', '15.50', '24 May 2019', '17.54']
        ['Argentina', '68.00', '15 October 2019', '31.17']
        ['Armenia', '5.75', '29 January 2019', '2.38']
        ['Australia', '0.75', '1 October 2019', '1.93']
        ['Azerbaijan', '8.25', '26 July 2019', '6.51']
        ['Bahamas', '4.00', '22 December 2016', '0.97']
        ...
        Ignoring row 99 because length < 4
        Ignoring row 100 because length < 4
        Ignoring row 101 because length < 4
        Ignoring row 102 because length < 4
        Ignoring row 103 because length < 4
        ```

        您会注意到一些行引发了错误，例如以下消息:

        ```py
        Errors in row 26: list index out of range
        ```

        虽然在撰写本文时这个错误影响了`row 26`，但将来可能会改变。从这条消息中您所知道的只是列表索引超出了范围，所以让我们通过更详细地查看来理解这一点。

30.  Pick out the bad row by running the following command:

    ```py
    bad_row_26 = body_content.find_all('tr')[26]
    ```

    如果需要，请相应地调整索引，以反映在解析特定表的数据时出现的错误。

31.  接下来，将行条目解包到您之前使用的相同变量中，以确定如何解析我们的数据。通过运行以下命令来做到这一点:

    ```py
    d1, d2, d3, d4 = bad_row_26.find_all('td')[:4]
    ```

32.  Now, you can attempt to parse the data from each of these items by using the same methods as before, until you identify where the error came from. Do this by running the following few cells:

    ```py
    d1.find('a').text
    float(d2.text)
    d3.find_all('span')[0].text
    ```

    运行这些单元时，您将看到以下输出:

    ![Figure 6.16: Troubleshooting for a row that could not be parsed
    ](img/B15916_06_16.jpg)

    图 6.16:对无法解析的行进行故障排除

    可以看到，第一个和第二个条目解析正确，但是第三个条目解析失败，并返回最初看到的`index out of range`错误。

33.  Here, you can see that `find_all('span')` seems to have returned no match, and therefore fails to look up the first (position "0") index. Confirm that this is the case by running the following command:

    ```py
    d3.find_all('span')
    ```

    这应该会像预期的那样输出一个空列表。

34.  Now that you have parsed your data, write it to disk.

    首先，使用`table_headers`命令打印我们之前设置的表格标题:

    ```py
    ['Country or currency union',
     'Central bank interest rate (%)',
     'Date of last change',
     'Average inflation rate (%)']
    ```

35.  Next, print the data that we parsed from the page as follows:

    ```py
    int_rates_data
    ```

    输出如下所示:

    ![Figure 6.17: The data that was extracted from Wikipedia
    ](img/B15916_06_17.jpg)

    图 6.17:从维基百科中提取的数据

36.  为了将这些数据写入磁盘，您将使用 pandas 库。通过运行以下代码将数据保存到 CSV 文件中:

    ```py
    f_path = '../data/countries/interest_rates_raw.csv'
    pd.DataFrame(int_rates_data, columns=table_headers)\
                 .to_csv(f_path, index=False)
    ```

37.  Check that the data has been written properly by opening the CSV file with a text reader or Excel. If your Jupyter environment supports bash, then you can check the `head` of the table by running the following code:

    ```py
    %%bash
    head ../data/countries/interest_rates_raw.csv
    ```

    显示的输出如下所示:

    ![Figure 6.18: The first 10 rows of the extracted Wikipedia data in a CSV file
    ](img/B15916_06_18.jpg)

图 6.18:CSV 文件中提取的维基百科数据的前 10 行

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。

第一次网页抓取练习到此结束。希望您喜欢将学到的关于 HTTP 请求和 HTML 的知识应用到实践中，按国家收集利率数据。在接下来的活动中，您将有机会自己尝试相同的过程，以便拉出一个列出每个国家人口的表格。

## 活动 6.01:用 Jupyter 笔记本刮 Web

在这个活动中，我们将得到每个国家的人口。然后，在下一个主题中，我们将把这些数据与我们从上一个练习中提取的数据结合起来，以创建一个可用于分析的数据集。

在这里，我们将查看另一个维基百科页面，该页面位于[https://en . Wikipedia . org/wiki/List _ of _ countries _ and _ dependencies _ by _ population](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population)。

我们的目标是将我们之前看到的概念应用到具有不同数据的新网页中。

为此，请按照下列步骤操作:

1.  创建一个新的 Jupyter 笔记本并加载必要的库。
2.  运行以下代码将`url`指定为变量:

    ```py
    url = 'https://en.wikipedia.org/wiki/List_of_countries_and'\
          '_dependencies_by_population'
    ```

3.  用网页的实时版本在笔记本中呈现 IFrame。然后，通过选择`Cell` | `Current Outputs` | `Clear`将其关闭。
4.  使用`requests`库请求页面。
5.  实例化一个`BeautifulSoup`对象，我们可以用它来解析页面 HTML。
6.  找到页面的`h1`。页面的。
7.  用`id:bodyContent`选择`<div>`标签。
8.  在这个`div`元素中搜索表头，并打印每个表头的元素文本及其索引。
9.  手动设置变量中的表格标题。我们对索引为 1-4 的列感兴趣，如下所示:`Country`(或属地)`Population`、`% of World Population`和`Date`。
10.  找到表格的所有`<tr>`元素(行)。然后，选择其中一个，并在该行中找到所有的`<td>`元素(数据条目)。
11.  确定行中数据条目的数量。
12.  打印出您找到的数据输入元素。
13.  迭代数据条目并打印它们的文本，以及索引号。
14.  选择我们对解析感兴趣的元素。回想一下，这将对应于索引 1-4。
15.  将`d1`、`d2`、`d3`和`d4`变量分配给每个索引，并运行我们在上一练习中遵循的相同过程，以便解析每个索引的干净数据条目。现在，将任何数字字段保留为字符串。
16.  一旦您知道如何解析来自每个条目的数据，就像我们在前面的练习中所做的那样，遍历行，并解析每个条目的相关数据。
17.  打印出你能从表格中提取的数据。这应该有四列。
18.  打印出相应的标题，所有这些标题都是在本练习的前面手工设置的。
19.  使用 pandas 将数据写入 CSV 文件。使用`../data/countries/populations_raw.csv`文件路径。
20.  Open up your CSV file and make sure it looks as expected.

    注意

    这项活动的解决方案可以在第 301 页找到。

总的来说，我们已经看到了 Jupyter 笔记本如何用于网页抓取。我们通过学习 HTTP 方法和状态代码开始了这一章。然后，我们使用`requests`库通过 Python 实际执行 HTTP 请求，并了解了如何使用`BeautifulSoup`库解析响应 HTML。

我们的 Jupyter 笔记本非常适合这类工作。我们能够探索 web 请求的结果，试验各种 HTML 解析技术，呈现 HTML，甚至在笔记本中加载 web 页面的实时版本！

在本章的下一节，我们将处理目前为止收集的关于Pandas的原始数据，以便为分析做准备。然后，我们将用一个活动来结束这一章，这个活动使用我们新的网络搜集数据将我们在前面章节中学到的概念联系在一起。

# Pandas的数据工作流程

正如我们在本书中一次又一次看到的，pandas 是用 Python 和 Jupyter 笔记本进行数据科学的不可或缺的一部分。DataFrames 为我们提供了一种组织和存储标记数据的方法，但更重要的是，pandas 为转换数据提供了节省时间的方法。我们在本书中看到的例子包括删除重复项、将字典映射到列、对列应用函数以及填充缺失值。

在下一个练习中，我们将重新加载从维基百科中提取的原始表格，清理它们，并将它们合并在一起。这将产生一个适合分析的数据集，我们将在最后的练习中使用它，在最后的练习中，您将有机会执行探索性分析并应用您在前面章节中学习的建模概念。

## 练习 6.04:处理数据以供Pandas分析

在本练习中，我们继续处理前面几节中从维基百科中提取的国家数据。回想一下，我们提取了每个国家的中央银行利率和人口，并将原始结果保存在 CSV 文件中。我们将从这些文件中加载数据，并对它们进行处理，以准备用于分析的数据集。这将涉及重命名列、删除丢失的数据，并确保每列的数据类型都是适当的。按照以下步骤完成本练习:

1.  Start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

2.  运行以下代码来加载本练习所需的库:

    ```py
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

3.  Load the *raw* data that you pulled in the previous sections by running the following code:

    ```py
    df_populations = pd.read_csv('../data/countries'\
                                 '/populations_raw.csv')
    df_int_rates = (pd.read_csv('../data/countries'\
                                '/interest_rates_raw.csv'))
    ```

    注意

    这个文件也可以在本书位于 https://packt.live/2MYlCvs 的 GitHub 资源库中找到。

4.  Run the following command to check the first five rows of `df_populations`:

    ```py
    df_populations.head()
    ```

    输出如下所示:

    ![Figure 6.19: First five rows of df_populations
    ](img/B15916_06_19.jpg)

    图 6.19:df _ populations 的前五行

5.  Run the following command to check the head `df_int_rates`:

    ```py
    df_int_rates.head()
    ```

    输出如下所示:

    ![Figure 6.20: The head of each table we extracted from Wikipedia
    ](img/B15916_06_20.jpg)

    图 6.20:我们从维基百科中提取的每个表格的标题

    现在，您将清理每个表，确保每个列都有适当的数据类型，并查找缺少条目的行。这将给你做探索性分析的机会。

6.  设置 pandas 数据框的显示限制，以便您可以从每个表中看到多达 10，000 行(尽管在这些数据集中要少得多)。通过运行以下命令来做到这一点:

    ```py
    pd.options.display.max_rows = 10000
    ```

7.  Print the entire `populations` table and scan down the rows in your notebook.![Figure 6.21: Displaying the full populations table in Jupyter
    ](img/B15916_06_21.jpg)

    图 6.21:在 Jupyter 中显示完整的人口表

8.  First, deal with the column names. Print these out by running the following command:

    ```py
    df_populations.columns
    ```

    这将输出以下内容:

    ```py
    Index(['Country(or dependent territory)', 'Population', 
           '% of WorldPopulation', 'Date'],
          dtype='object')
    ```

9.  When working with data in pandas, you want column names to be descriptive and easy to type. It is good practice to ensure lowercase words are separated by underscores, as per the Python naming convention.

    通过运行以下代码手动设置列名:

    ```py
    df_populations.columns = ['country', 'population', \
                              'population_pct', 'date',]
    ```

10.  Print the datatypes of each column by running `df_populations.dtypes`. This should output the following:

    ```py
    country           object
    population        object
    population_pct    object
    date              object
    dtype: object
    ```

    每一列似乎都是`object`型，在Pandas中可以解释为`string`型。

11.  You would expect populations to be numeric values. Print out a random sample to try and understand why this is not the case. Do this by running the following commands:

    ```py
    np.random.seed(0)
    df_populations['population'].sample(10)
    ```

    在这里，您已经设置了`random`种子，因此对于给定的数据集，采样将是可再现的。您应该会看到类似下面的内容:

    ```py
    110     6,533,500
    150     1,902,000
    37     38,379,000
    75     13,249,924
    109     6,825,442
    71     16,244,513
    122     5,009,466
    73     15,473,818
    154     1,454,789
    234         3,198
    Name: population, dtype: object
    ```

    在这里，您可以看到每个条目中逗号的存在导致它们被解释为字符串，而不是整数。

12.  Convert the `population` column into a `numeric` datatype by running the following code:

    ```py
    df_populations['population'] = df_populations['population']\
                                   .str.replace(',','')
    df_populations['population'] = \
    pd.to_numeric(df_populations['population'], \
                  errors='coerce',)
    ```

    首先，使用一个`string`方法删除所有逗号，并用空格替换它们。然后，再次传递数据，并使用`pd.to_numeric`函数将每个条目转换成`numeric`数据类型。

13.  Did you notice that we set `errors='coerce'` in the preceding code? In order to understand why this was done, pull up the docstring for that function by running `pd.to_numeric?`:![Figure 6.22: The docstring for pd.to_numeric
    ](img/B15916_06_22.jpg)

    图 6.22:PD . to _ numeric 的文档字符串

    可以看出，设置`errors='coerce'`将导致任何解析错误产生`NaNs`。这与`pd.to_numeric`的默认行为形成对比，后者会在解析任何数据条目失败时引发错误。

14.  Moving on, the next column to deal with is population percent, which should also be a numeric datatype. Print out a random sample of that column by running the following commands:

    ```py
    np.random.seed(0)
    df_populations['population_pct'].sample(10)
    ```

    您应该会看到类似下面的内容:

    ```py
    110       0.0838%
    150       0.0244%
    37         0.492%
    75         0.170%
    109       0.0875%
    71         0.208%
    122       0.0642%
    73         0.198%
    154       0.0187%
    234    0.0000410%
    Name: population_pct, dtype: object
    ```

    这里，每个条目中百分号(`%`)的存在导致它们被解释为字符串，而不是整数。

15.  通过运行下面的代码将人口百分比列转换为`numeric`数据类型:

    ```py
    df_populations['population_pct'] = df_populations['population_pct']\
                                       .str.replace('%','')
    df_populations['population_pct'] = \
    pd.to_numeric(df_populations['population_pct'], \
                  errors='coerce',)
    ```

16.  The final column to fix is the `date` column, which was also loaded as a string. It's very normal for pandas to load dates as strings unless instructed otherwise.

    与前面的命令类似，您可以使用`pd.to_datetime`将日期条目从`string`转换成`datetime`对象。通过运行以下代码来实现这一点:

    ```py
    df_populations['date'] = pd.to_datetime(df_populations['date'], \
                                            errors='coerce',)
    ```

17.  通过运行`df_populations.dtypes`打印表格的`datatypes`。这应该输出以下内容:

    ```py
    country                   object
    population                 int64
    population_pct           float64
    date              datetime64[ns]
    dtype: object
    ```

18.  Look for missing records by running the following command:

    ```py
    df_populations.isnull().sum()
    ```

    输出应该类似于以下内容:

    ```py
    country           4
    population        1
    population_pct    1
    date              0
    dtype: int64
    ```

19.  Now, deal with this missing data. Identify the rows that have missing data and print them out by running the following code:

    ```py
    missing_mask = df_populations.isnull().any(axis=1)
    df_populations[missing_mask]
    ```

    这将显示缺失的数据，如下所示:

    ![Figure 6.23: Output showing the missing data
    ](img/B15916_06_23.jpg)

    图 6.23:显示缺失数据的输出

20.  You are simply going to drop these missing rows. In order to do this, first select the indices to drop. Looking back at the row indices displayed in the preceding table, these appear to be rows `121`, `190`, and `218`. Select these dynamically by running the following code:

    ```py
    drop_indices = df_populations.index[missing_mask]
    drop_indices
    ```

    这将输出预期的索引列表:

    ```py
    Int64Index([0, 123, 219, 242], dtype='int64')
    ```

21.  Drop these rows by running the code:

    ```py
    df_populations = df_populations.drop(drop_indices)
    ```

    现在已经清理完毕，您可以通过运行以下命令将结果表写入文件:

    ```py
    f_name = '../data/countries/populations.csv'
    df_populations.to_csv(f_name, index=False)
    ```

22.  Having processed the populations table, now do the same for the interest rates table. Start out by making sure the max row limit on pandas is large enough to view the entire table. Then, print it out by running `df_int_rates`:![Figure 6.24: Displaying the full interest rates table in Jupyter
    ](img/B15916_06_24.jpg)

    图 6.24:在 Jupyter 中显示完整的利率表

    通过运行`df_int_rates.columns`查看列名。

    您应该会看到以下输出:

    ```py
    Index(['Country or currency union', 'Central bank interest rate (%)',
           'Date of last change', 'Average inflation rate (%)'],
          dtype='object')
    ```

23.  现在，通过运行下面的代码手动设置它们:

    ```py
    df_int_rates.columns = ['country', 'interest_rate_pct', \
                            'date_of_last_change', \
                            'average_inflation_rate_pct']
    ```

24.  Print the datatypes of each column by running `df_int_rates.dtypes`.

    您应该会看到以下输出:

    ```py
    country                        object
    interest_rate_pct              object
    date_of_last_change            object
    average_inflation_rate_pct    float64
    dtype: object
    ```

25.  The interest rate column should be a `numeric` datatype. Print a sample of the entries to see what might be the problem here. Do this by running the following code:

    ```py
    np.random.seed(0)
    df_int_rates['interest_rate_pct'].unique()
    ```

    以下是显示示例条目的输出:

    ```py
    array(['1.00', '15.50', '68.00', '5.75', '0.75', '8.25', '4.00', 
           '2.50', '6.00', '7.00', '9.50', '5.00', '4.25', '0.00', 
           '1.75', '5.50', '2.95', '4.20', '2.00', '9.00', '3.00', 
           '-0.75', '5.25', '6.50', '13.25', '0.50', '20.00', '0.90', 
           '3.25', '5.40', '18.00', '0.25', '-0.10', '3.75', '10.00', 
           '2.75', '13.50', '7.50', '8.00', '11.00', '2.25', '12.75', 
           '1.50', '0.19', '13.00', '1.25', '7.25', '-0.25', '1.375', 
           '9.25', '7.75', '11.25', '-', '16.00', '10.25'], 
          dtype=object)
    ```

26.  Here, you can see one particular bad value that's made its way into the column: the `-` sign. Otherwise, the data looks pretty good. Use `pd.to_numeric` to convert this column:

    ```py
    df_int_rates['interest_rate_pct'] = \
    pd.to_numeric(df_int_rates['interest_rate_pct'], \
                  errors='coerce',)
    ```

    您刚刚确定的错误值`-`，将被转换为`NaN`条目，因为我们已经设置了`errors=because`。

27.  通过运行下面的代码将日期条目转换成`datetime`对象:

    ```py
    df_int_rates['date_of_last_change'] = \
    pd.to_datetime(df_int_rates['date_of_last_change'])
    ```

28.  通过运行`df_int_rates.dtypes`打印我们清理过的表的数据类型。这将输出以下内容:

    ```py
    country                               object
    interest_rate_pct                    float64
    date_of_last_change           datetime64[ns]
    average_inflation_rate_pct           float64
    dtype: object
    ```

29.  Look for missing records and deal with them, just like you did for the populations table. Do this by running the following command:

    ```py
    df_int_rates.isnull().sum()
    ```

    您应该会看到类似如下的输出:

    ```py
    country                        1
    interest_rate_pct              1
    date_of_last_change            2
    average_inflation_rate_pct     8
    dtype: int64
    ```

30.  Again, like you did for the populations table, select any rows that have missing entries and look at them. Do this by running the following code:

    ```py
    missing_mask = df_int_rates.isnull().any(axis=1)
    df_int_rates[missing_mask]
    ```

    下面的屏幕截图显示了缺少数据的表:

    ![Figure 6.25: Displaying the rows with missing values
    ](img/B15916_06_25.jpg)

    图 6.25:显示缺少值的行

31.  Since many of these rows have good information intact, you would generally want to keep them around. You may also want to go back to earlier work and adjust how the data was scraped so that you are able to capture this missing data.

    然而，出于本书的目的，您将删除这些数据。这样，在最后的活动中，您将拥有一个非常干净的数据集。通过运行以下代码选择要删除的行的索引:

    ```py
    drop_indices = df_int_rates.index[missing_mask]
    drop_indices
    ```

    根据上表，这应该会输出预期的行号:

    ```py
    Int64Index([16, 20, 22, 25, 27, 35, 77, 88, 89, 91], dtype='int64')
    ```

32.  通过运行以下代码删除这些行:

    ```py
    df_int_rates = df_int_rates.drop(drop_indices)
    ```

33.  Now that it's been cleaned up, you can write your resulting table to file by running the cell containing the following code:

    ```py
    f_name = '../data/countries/ interest_rates.csv'
    df_populations.to_csv(f_name, index=False)
    ```

    注意

    具体章节的源代码请参见[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

    你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。

这就结束了对Pandas进行数据分析的练习。我们以从维基百科中提取的格式加载原始数据，然后处理表格，以便在分析之前清理数据字段。这包括更改列名、检查空值、转换数据类型和删除丢失的数据。

在下一个练习中，我们将合并我们在这里处理的两个表。

## 练习 6.05:用Pandas合并数据

我们正准备开始研究在之前的练习中清理的数据。我们需要做的最后一步是合并两个表。这可以通过连接国家/地区的每个表中的行来完成。我们将从以前保存的文件中加载经过处理的数据，并将它们合并到一个数据帧中，该数据帧将用作我们最终分析活动的数据源。按照以下步骤完成本练习:

1.  Start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

    然后，按照终端中的提示，通过复制并粘贴 URL，打开您在 web 浏览器中选择的平台。

2.  运行以下代码来加载一些库，您将使用这些库来配置笔记本的绘图设置:

    ```py
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    %load_ext watermark
    %watermark -d -v -m -p numpy,pandas,matplotlib,seaborn
    ```

3.  通过运行以下代码重新加载已处理的数据集:

    ```py
    df_int_rates = pd.read_csv('../data/countries/interest_rates.csv')
    df_populations = pd.read_csv('../data/countries/populations.csv')
    ```

4.  Run the following command to print the `columns` table of `df_int_rates`:

    ```py
    df_int_rates.columns
    ```

    输出如下所示:

    ```py
    Index(['country', 'interest_rate_pct', 'date_of_last_change',
           'average_inflation_rate_pct'],
          dtype='object')
    ```

    运行以下命令打印`df_populations`的`columns`表:

    ```py
    df_populations.columns
    ```

    输出如下所示:

    ```py
    Index(['country', 'population', 'population_pct', 
           'date'], dtype='object')
    ```

5.  Looking at the columns for each table, you can see that they should be joined on the country key. Perform an outer merge by running the following code:

    ```py
    df_merge = pd.merge(df_populations, df_int_rates, \
                        left_on='country', right_on='country', \
                        how='outer')
    df_merge
    ```

    这将输出合并表的显示:

    ![Figure 6.26: The merged populations and interest rates table
    ](img/B15916_06_26.jpg)

    图 6.26:合并的人口和利率表

6.  When doing this `merge`, you lost some important context on the `date` columns. Fix this by renaming those columns, running the following code:

    ```py
    column_map = {'date': 'date_population_update', \
                  'date_of_last_change': \
                  'date_interest_rate_last_change'}
    df_merge = df_merge.rename(columns=column_map)
    ```

    这里，您已经定义了应该如何映射列，然后通过在我们的数据帧上使用`rename`函数来执行逻辑。

7.  Run the `df_merge.head()` command to print the head of our DataFrame and confirm the column names have been altered, as expected:![Figure 6.27: The merged table head, showing the updated column names
    ](img/B15916_06_27.jpg)

    图 6.27:合并的表头，显示更新的列名

    由于您执行了外部合并，pandas 将使用两个表中的所有数据，并为两个表之间没有对齐的键插入 NaN 条目。

8.  Print the number of NaN entries in the merged table by running the following command:

    ```py
    df_merge.isnull().sum()
    ```

    这将输出以下内容:

    ```py
    country                             0
    population                          1
    population_pct                      1
    date_population_update              1
    interest_rate_pct                 157
    date_interest_rate_last_change    157
    average_inflation_rate_pct        157
    dtype: int64
    ```

9.  Now, drop these rows. First, run `len(df_merge)` to print the current length of the table. Then, print out a few sample records by running the following command:

    ```py
    df_merge[df_merge.isnull().any(axis=1)].sample(10)
    ```

    表格的当前长度为`240`。下面是显示合并后的表的输出:

    ![Figure 6.28: A sample of rows with missing values
    ](img/B15916_06_28.jpg)

    图 6.28:带有缺失值的行示例

10.  您可以看到许多包含人口数据的行没有利率数据。为了删除有任何缺失值的行，运行以下命令:

    ```py
    df_merge = df_merge.dropna()
    ```

11.  Then, confirm that all the missing entries have been dropped by running the following command:

    ```py
    df_merge.isnull().sum()
    ```

    这将输出以下内容:

    ```py
    country                           0
    population                        0
    population_pct                    0
    date_population_update            0
    interest_rate_pct                 0
    date_interest_rate_last_change    0
    average_inflation_rate_pct         0
    dtype: int64
    ```

12.  运行`len(df_merge)`打印新长度。请注意，它比以前短了很多。
13.  Lastly, having cleaned up our dataset in preparation for analysis, write the table to a CSV file by running the following code:

    ```py
    f_name = '../data/countries/country_data_merged.csv'
    df_merge.to_csv(f_name, index=False)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

    你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。

到目前为止，在本章的练习中，我们已经从维基百科收集了国家人口和利率数据集，并使用Pandas和 Jupyter 笔记本清理了它们。这一切都是在一个可再现的环境中完成的，该环境可以作为数据管道的主干。这样做的一个积极意义是，在忘记了细节之后，能够回到你的分析中，轻松理解它是如何工作的，并调试你在浏览笔记本时可能出现的新问题。此外，您可以与可以从头开始复制您的结果的同事分享这项工作。该笔记本还可以作为数据管道的基础，您可能希望定期运行该管道来提取和处理最新数据。

## 活动 6.02:分析国家人口和利率

这个活动的目的是给你一个机会来应用你在本书中学到的分析技术，比如数据加载、可视化和探索性分析。

您将通过加载之前收集和处理的已处理数据来开始此活动。在本活动结束时，您将对数据集中的一些关键信息有更深入的理解，并对数据有一系列不同的可视化。按照以下步骤完成本练习:

1.  创建一个新的 Jupyter 笔记本并加载必要的库。
2.  将我们在前一个练习中处理的表加载到一个 pandas 数据帧中，变量名为`df`。回想一下，这个数据被保存到了`country_data_merged.csv`文件中。打印表格的标题，检查它看起来是否与您期望的一样。
3.  打印每列的数据类型。你注意到什么问题了吗？
4.  将`date`列数据类型转换为`datetimes`。
5.  检查表格中是否有丢失的数据。
6.  绘制并比较`date`列的直方图，即`date_population_update`和`date_interest_rate_last_change`。
7.  自上次利率变化以来，数据集中哪个国家的日期最早？
8.  数据集中有负利率的国家吗？如果有，那么是哪些？
9.  按人口数量绘制排名靠前的国家的条形图。
10.  绘制`population_pct`与`interest_rate_pct`的散点图。
11.  你能看到人口轴上的异常值吗？如果是，那么它们是什么？
12.  Plot a scatter chart of `average_inflation_rate_pct` versus `interest_rate_pct`.

    注意

    这项活动的解决方案可以在第 309 页找到。该解决方案还包括一些额外的材料，展示了如何通过对数据拟合聚类模型来进一步进行分析。请随意探索奖励材料，并将其作为您自己进一步研究的起点。

# 总结

在这一章中，我们完成了使用网络抓取技术从 *Wikipedia* 中提取表格的过程，用 pandas 清理结果数据，并产生最终分析。

我们从查看`HTTP`请求如何工作开始，重点关注`GET`请求及其响应状态代码。然后，我们进入 Jupyter 笔记本，使用`requests`库用 Python 发出 HTTP 请求。我们看到了如何使用 Jupyter 在笔记本中呈现 HTML，以及可以交互的实际网页。为了学习 web 抓取，我们看到了如何使用`BeautifulSoup`解析 HTML 中的文本，并使用这个库从维基百科中抓取表格数据。

在提取了两个数据表后，我们对它们进行了处理，以便对Pandas进行分析。第一个表包含每个国家的中央银行利率，而第二个表包含人口。我们将这些合并到一个表中，然后用于最终的分析，这涉及到创建可视化来寻找模式。

现在，让我们具体地从这一章后退一步，回顾一下从这本书开始以来我们所学到的东西。

在第一章中，我们回顾了数据科学的概况，以便为本书的其余部分提供背景知识。我们了解了数据科学的起源，并介绍了该领域的关键概念，例如能够规划出业务问题的数据驱动解决方案，以及理解数据可视化、探索和建模的作用。我们还探讨了各种职业道路、参与社区的方式，并讨论了数据科学带来的一些挑战。

接下来，我们介绍了 Jupyter 笔记本，并了解了它们为什么对数据分析和建模有用。我们通过两个界面来运行它们:旧的 Jupyter 笔记本平台和新的 Jupyter 实验室。然后，我们浏览了一个示例笔记本，探索它们是如何工作的，并了解了 Jupyter 的一些特性，比如神奇的功能和制表符补全。

学习了 Jupyter 的基本功能后，我们开始通过笔记本学习`Boston Housing`数据集。这是我们第一次使用 Jupyter 来研究现实世界的风格问题，我们使用 pandas 库探索了数据集，并创建了一些可视化和简单的模型。

在我们完成了`Boston Housing`数据集之后，我们将注意力转向了数据科学的一个关键组成部分:预测建模。通过详细讨论规划建模策略的步骤，我们扩展了在第一章中提出的一些观点。我们还了解了数据建模的另一个关键步骤，即数据准备阶段，这是使用训练算法之前所必需的。在其他考虑因素中，这包括填充缺失数据、将分类特征转换为数字特征，以及将数据分成训练集和测试集。

此时，我们引入了针对*员工保留问题*的人力资源分析数据集。在确定了围绕该数据集的建模策略之后，我们应用数据处理技术来清理它，以便它可以用于训练机器学习模型。我们开始只关注数据集中的两个特征，并学习了训练简单模型，如**支持向量机** ( **支持向量机**)和**k-最近邻** ( **KNN** )分类器。特别是，我们密切关注由这种模型形成的决策边界和过度拟合的影响。

在完成建模的基础工作后，我们接着介绍了机器学习中一些更高级的概念，包括 k-fold 交叉验证和验证曲线。将这些应用于我们的问题，我们试图通过优化超参数来提高我们的模型精度，例如我们的随机森林模型的最大深度。

最后，我们学习了称为 PCA 的降维技术，并将其应用于我们的问题，以帮助提高我们模型的整体准确性。

最后，在本书的最后一部分，我们简单地从数据建模和分析转到了从 web 收集数据。特别是，我们学习了如何使用 Python 发出 web 请求，以便调用 Web APIs 或 HTML 资源，并从响应中解析数据。我们通过应用本书前面所学的知识来探索、可视化和建模我们收集的数据，从而完成了这一部分。

不管你是否有 Python 或 Jupyter 的经验，希望你已经学到了很多有用的和适用的技巧来处理和解决数据科学问题。

随着你在学习道路上的前进，我们鼓励你继续挖掘激发你对本书兴趣的主题。您可能希望找到文章*数据科学一瞥*中讨论的资源，并从头到尾完成自己的数据科学项目。

作为数据科学的学生或专业人士，您将不断遇到必须克服的难题。面对逆境坚持不懈不是可以教会的，而是必须来自内心。所以，如果你曾经想放弃，也许你会被这样一个想法所激励:有价值的事情很少是容易的。