<title>Chapter 11. Supervised Machine Learning</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 第十一章。监督机器学习

本章涵盖以下主题:

*   用`lm`拟合线性回归模型
*   总结线性模型拟合
*   使用线性回归预测未知值
*   测量回归模型的性能
*   执行多元回归分析
*   用逐步回归法选择最佳拟合回归模型
*   应用高斯模型进行广义线性回归
*   执行逻辑回归分析
*   用递归划分树建立分类模型
*   可视化递归划分树
*   用混淆矩阵测量模型性能
*   使用`ROCR`测量预测性能

# 简介

监督机器学习方法的目的是从训练数据建立学习模型，该训练数据包括含有已知标签或结果的输入数据。该模型可以预测看不见的实例的特征或模式。通常，训练模型的输入由一对输入向量和期望值构成。如果目标函数的输出变量是连续的(但也可以是二元的)，则学习方法被视为回归分析。或者，如果期望的输出是分类的，则学习过程被认为是一种分类方法。

回归分析通常用于建模和分析因变量(响应变量)和一个或多个自变量(预测变量)之间的关系。可以使用回归来构建预测模型，该模型首先找到输入数据平方误差最小的最佳拟合模型。然后，拟合的模型可以进一步应用于连续值预测的数据。例如，可以使用回归来根据历史租赁价格预测租赁物业的价格范围。

另一方面，可以基于从训练数据集构建的分类模型，使用分类方法来识别新观察(测试数据集)的类别，其中类别是已知的。与回归类似，分类被归类为监督学习方法，因为它采用训练数据集的已知答案(标签)来预测测试数据集的答案(标签)。例如，可以使用分类方法来预测客户是否可能购买产品。

在这一章中，我们将首先介绍回归分析。我们将探讨如何用`lm`拟合回归模型，并获得拟合的摘要。接下来，我们将使用拟合线来预测新实例的值。此外，我们可以测量回归拟合的预测性能。然后，我们将介绍广义线性模型(`glm`)，并演示如何将高斯和二项式因变量拟合到这些模型中。接下来，我们将讨论分类模型。首先，我们将建立一个具有递归划分树的分类模型。然后，我们将可视化构建的模型，并最终在混淆和 ROC 指标中评估拟合模型的性能。

<title>Fitting a linear regression model with lm</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 用 lm 拟合线性回归模型

回归中最简单的模型是线性回归，它最好用在只有一个预测变量，且响应变量和自变量之间的关系是线性的时候。在这个配方中，我们演示了如何使用`lm`函数来拟合模型和数据。

## 准备就绪

先从[https://raw . githubusercontent . com/yw chiu/rcookbook/master/chapter 11/house _ rental . CSV](https://raw.githubusercontent.com/ywchiu/rcookbook/master/chapter11/house_rental.csv)下载房屋租赁数据集，确保你的操作系统上已经安装了 R。

## 怎么做……

执行以下步骤，将数据拟合到简单的线性回归模型中:

1.  将房屋租赁数据读入 R 会话:

    ```

    > house <- read.csv('house_rental.csv', header=TRUE)

    ```

2.  将自变量`Sqft`和因变量`Price`拟合到`glm` :

    ```

    > lmfit <- lm(Price ~ Sqft, data=house)

    > lmfit

    Call:

    lm(formula = Price ~ Sqft, data = house)

    Coefficients:

    (Intercept)         Sqft

     3425.13        38.33

    ```

3.  用`plot`和`abline`功能可视化拟合线:

    ```

    > plot(Price ~ Sqft, data=house)

    > abline(lmfit, col="red")

    ```

    ![How to do it…](graphics/B04007_11_01.jpg)

    图 1:一个简单的线性回归模型

## 它是如何工作的……

回归模型的形式为*响应~项*，其中响应是响应向量，项是指定预测值的一系列项。我们可以在公式 *y = α + βx* 中说明一个简单的回归模型，其中 *α* 是截距，而 *β* 描述了当 *x* 变化时 *y* 的变化。利用最小二乘法，我们可以估计出![How it works…](graphics/B04007_11_11.jpg)和![How it works…](graphics/B04007_11_12.jpg)(其中![How it works…](graphics/B04007_11_13.jpg)表示 *y* 的平均值，![How it works…](graphics/B04007_11_14.jpg)表示 *x* 的平均值)。

在这个配方中，我们首先将自变量`Sqft`和因变量`Price`输入到`lm`函数中，并将构建的模型分配给拟合。从输出中，我们发现拟合模型的系数显示截距等于`3425.13`，系数等于`38.33`。

## 还有更多……

一些预测变量和响应变量之间的关系可以建模为一个 *n* 阶多项式。例如，我们可以在公式![There's more…](graphics/B04007_11_15.jpg)中说明二阶多项式回归模型，其中 *α* 是截距，而 *β* 说明回归系数。以下示例显示了如何将`anscombe`数据拟合到二阶多项式回归模型中。

执行以下步骤，用 `lm`拟合多项式回归模型:

1.  加载`anscombe`数据集:

    ```

    > data(anscombe)

    ```

2.  制作变量`x1`和`y2` :

    ```

    > plot(y2 ~ x1, data=anscombe)

    ```

    散点图
3.  通过在参数中指定`2`来应用 poly 函数:

    ```

    > lmfit2 <- lm(y2~poly(x1,2), data=anscombe)

    > lines(sort(anscombe$x1), lmfit2$fit[order(anscombe$x1)], col = "red")

    ```

    ![There's more…](graphics/B04007_11_02.jpg)

    图 2:二阶多项式回归模型

<title>Summarizing linear model fits</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 总结线性模型拟合

`summary`函数可用于获得拟合模型的格式化系数、标准误差、自由度和其他汇总信息。该配方介绍了如何通过使用`summary`功能获得整体模型信息。

## 准备就绪

您需要通过将房屋租赁数据拟合到回归模型中来完成之前的配方，并将拟合的模型分配给变量`lmfit`。

## 怎么做……

执行以下步骤来总结线性回归模型:

1.  计算拟合模型的详细概要，`lmfit` :

    ```

    > summary(lmfit)

    Call:

    lm(formula = Price ~ Sqft, data = house)

    Residuals:

     Min     1Q Median     3Q    Max

    -76819 -12388  -3093  10024 112227

    Coefficients:

     Estimate Std. Error t value Pr(>|t|)

    (Intercept) 3425.133   1766.646   1.939    0.053 .

    Sqft          38.334      1.034  37.090   <2e-16 ***

    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Residual standard error: 20130 on 643 degrees of freedom

    Multiple R-squared:  0.6815, Adjusted R-squared:  0.681

    F-statistic:  1376 on 1 and 643 DF,  p-value: < 2.2e-16

    ```

## 它是如何工作的……

`summary`函数是一个通用函数，用于生成汇总统计数据。在这种情况下，它计算并返回拟合的线性模型的汇总统计列表。这里，它将输出信息，如残差、系数标准误差 R 平方、f 统计量和自由度。在`Call`部分，显示生成拟合模型所调用的函数。在`Residuals`部分，它提供了分布的快速摘要(`Min`、`1Q median`、`3Q`和`Max`)。在`Coefficients`部分，每个系数都是一个高斯随机变量。在此部分中，`Estimate`代表变量的平均分布；`Std.Error`显示变量的标准误差；`t value`是估计值除以`Std.Error`；并且`p-value`表示获得大于 *t* 值的值的概率。在本例中，由于 *Sqft (2e-16)* 的`p-value`远小于 0.05，我们可以说`Sqft`有 95%的概率是正确的，它对拟合的模型有意义。

`Residual standard error`输出残差的标准偏差，而自由度表示训练样本中的观察值与模型中使用的数量之间的差异。`Multiple R-squared`除以平方和得出。人们可以用`R-squared`来衡量数据与回归线的拟合程度。通常，R 平方值越高，模型就越符合您的数据。然而，它不一定表明回归模型是否足够。这意味着你可能得到一个 R 平方低的好模型，也可能得到一个 R 平方高的坏模型。由于 multiple `R-squared`忽略了自由度，计算出的分数有偏差。为了计算公平，`Adjusted R-squared` ( `0.681`)采用无偏估计，会比`Multiple R-squared` ( `0.6815`)略小。`F-statistic`通过对模型执行 f 检验来检索。等于`2.2e-16` ( < 0.05)的 p 值拒绝零假设(变量之间没有线性相关性)，并表明观察到的 *F* 大于临界 *F* 值。换句话说，结果显示`Sqft`和`Price`之间存在显著的正线性相关性。

## 还有更多……

有关用于获取拟合模型摘要的参数的更多信息，您可以使用`help`功能或`?`查看帮助页面:

```

> ?summary.lm

```

或者，您可以使用以下函数来显示模型的属性:

```

>  coefficients(lmfit) # Extract model coefficients

>  confint(lmfit, level=0.95) # Computes confidence intervals for model parameters

>  fitted(lmfit) # Extract model fitted values

>  residuals(lmfit) # Extract model residuals

>  anova(lmfit) # Compute analysis of variance tables for fitted model object

>  vcov(lmfit) # Calculate variance-covariance matrix for a fitted model object

>  influence(lmfit) # Diagnose quality of regression fits

```

<title>Using linear regression to predict unknown values</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 用线性回归预测未知值

有了拟合的回归模型，我们可以应用该模型来预测未知值。对于回归模型，我们可以用预测区间和置信区间来表示预测的精度。在下面的食谱中，我们介绍如何在这两种测量下预测未知值。

## 准备就绪

需要通过将房屋租赁数据拟合到回归模型中来完成之前的配方，并将拟合的模型分配给变量`lmfit`。

## 怎么做……

执行以下步骤，使用线性回归来预测值:

1.  将需要预测的值赋给`newdata` :

    ```

    > newdata <- data.frame(Sqft=c(800, 900, 1000))

    ```

2.  计算给定数据的预测结果:

    ```

    > predict(lmfit ,newdata)

     1        2        3

    34092.60 37926.04 41759.47

    ```

3.  另一方面可以获得拟合模型的系数和截距:

    ```

    > lmfit$coefficients[1]

    (Intercept)

     3425.133

    > lmfit$coefficients[2]

     Sqft

    38.33434

    ```

4.  执行以下函数将产生相同的预测值，这是我们在步骤 2 中使用预测函数获得的:

    ```

    > c(800, 900, 1000) * lmfit$coefficients[2] + lmfit$coefficients[1]

    ```

5.  继续，我们可以使用`level`设置为`0.95` :

    ```

    > predict(lmfit, newdata, interval="confidence", level=0.95)

     fit             lwr          upr

    1 34092.60 31947.18 36238.02

    2 37926.04 35914.92 39937.15

    3 41759.47 39870.37 43648.57

    ```

    的置信区间来计算预测结果
6.  我们可以得到给定数据的预测区间:

    ```

    > predict(lmfit ,newdata, interval='prediction')

     fit             lwr           upr

    1 34092.60 -5489.041 73674.25

    2 37926.04 -1648.555 77500.63

    3 41759.47  2190.892 81328.05

    ```

7.  最后，我们可以将预测结果绘制成散点图:

    ```

    > plot(Price ~ Sqft, data=house)

    > pred.res <- predict(lmfit, data = house, interval="prediction")

    > pred.conf <- predict(lmfit, data = house, interval = 'confidence')

    > lines(pred.conf[, 'fit'] ~ house$Sqft, col="red",  lty = 1, lwd=3)

    > lines(pred.conf[, 'lwr'] ~ house$Sqft, col="blue", lty = 3, lwd=3)

    > lines(pred.conf[, 'upr'] ~ house$Sqft, col="blue", lty = 3, lwd=3)

    > lines(pred.res[, 'lwr'] ~ house$Sqft, col="green", lty = 2, lwd=3)

    > lines(pred.res[, 'upr'] ~ house$Sqft, col="green", lty = 2, lwd=3)

    ```

    ![How to do it…](graphics/B04007_11_03.jpg)

    图 3:带有拟合线、置信区间和预测区间的散点图

## 它是如何工作的……

我们首先用变量`Sqft`和`Price`建立一个线性拟合模型。接下来，我们将要预测的值分配给数据帧`newdata`。需要注意的是，生成的模型是`Price ~ Sqft`的形式。

接下来，我们通过将`Sqft`与`800`、`900`和`1000`拟合到模型中来计算预测结果。从输出中，我们得到预测价格为`34092.60`、`37926.04`和`41759.47`。另一方面，也可以用模型系数和截距计算预测结果。

接下来，我们通过在参数`interval`中指定`confidence`，使用置信区间来计算预测结果。从第 1 行的输出中，我们得到输入`Sqft=800`的拟合`Price`等于`34092.60`，并且`Sqft=800`的均值`Price`的 95%置信区间(在自变量`level`中设置`0.95`)在`31947.18`和`36238.02`之间。此外，第 2 行和第 3 行给出了带有输入`Sqft=900`和`Sqft=1000`的`Price`的预测结果。

接下来，我们通过在参数`interval`中指定`prediction`来使用预测间隔计算预测结果。从第 1 行的输出中，我们看到输入`Sqft=800`的拟合`y1`等于`34092.60`，并且`Sqft=800`的`Price`的 95%预测区间在`-5489.041`和`73674.25`之间。第 2 行和第 3 行用输入`Sqft=900`和`Sqft=1000`输出`Price`的预测结果。

最后，我们在样本图上绘制拟合线、置信区间和预测区间。如图 3 所示，圆点代表数据点，红线表示回归线。红线旁边的蓝线显示 95%置信区间的置信区间，而边框上的绿线显示预测区间的上限和下限。

## 还有更多……

对预测区间和置信区间的区别感兴趣的，可以参考维基百科词条*对比置信区间*([http://en . Wikipedia . org/wiki/Prediction _ interval # Contrast _ with _ confidence _ intervals](http://en.wikipedia.org/wiki/Prediction_interval#Contrast_with_confidence_intervals))。

<title>Measuring the performance of the regression model</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 测量回归模型的性能

为了度量回归模型的性能，我们可以计算预测输出和实际输出的距离，作为模型性能的量化指标。在这个计算中，我们经常用**均方根误差** ( **RMSE** ) 、**相对平方误差** ( **RSE** ) 作为常用的度量。在下面的食谱中，我们说明了如何从构建的回归模型中计算这些度量。

## 准备就绪

您需要通过将房屋租赁数据拟合到回归模型中来完成之前的配方，并将拟合的模型分配给变量`lmfit`。

## 怎么做……

执行以下步骤来衡量回归模型的性能:

1.  使用预测函数检索预测值:

    ```

    > predicted <- predict(lmfit, data=house)

    ```

2.  计算均方根误差:

    ```

    > actual <- house$Sqft 

    > rmse <- (mean((predicted - actual)^2))^0.5

    > rmse

    [1] 66894.34

    ```

3.  计算相对平方误差:

    ```

    > mu <- mean(actual)

    > rse <- mean((predicted - actual)^2) / mean((mu - actual)^2) 

    > rse

    [1] 7610.695

    ```

## 它是如何工作的……

回归模型性能的测量采用预测值和实际值之间的距离。我们常用以下三个度量，均方根误差、相对平方误差、`R-squared`作为回归模型性能的量化指标。在这个配方中，我们使用`predict`函数计算预测值，并开始计算 RMSE。RMSE 用于测量回归模型的误差率，其公式如下:

![How it works…](graphics/B04007_11_16.jpg)

这里 *a* 是实际目标， *p* 是预测值， *n* 是观测次数。在这个例子中，我们获得`66894.34`作为回归模型的均方根误差。

由于 RMSE 只能比较误差测量单位相同的模型，我们可以使用 RSE 来比较误差测量单位不同的模型。相对平方误差可由下式表示:

![How it works…](graphics/B04007_11_17.jpg)

在这个例子中，我们得到`7610.695`作为模型的相对平方误差。

## 还有更多……

除了独立计算 RMSE 和 RSE，我们还可以使用机器学习评估指标包`MLmetrics`来衡量回归模型的性能:

1.  首先安装并加载`MLmetrics`包:

    ```

    > install.packages("MLmetrics")

    > library(MLmetrics)

    ```

2.  检索拟合模型的 **均方根误差损失**(**RRSE**)**相对平方误差损失**(**RSE**):

    ```

    > y_rrse <- RRSE(y_pred = predicted, y_true = actual)

    > y_rse <- y_rrse^2

    > y_rmse <- RMSE(y_pred = predicted, y_true = actual)

    > y_rrse

    [1] 87.2393

    > y_rse

    [1] 7610.695

    > y_rmse

    [1] 66894.34

    ```

<title>Performing a multiple regression analysis</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 进行多元回归分析

多元回归分析可以看作是简单线性回归的延伸。人们可以使用多元回归分析来预测基于多个自变量的因变量的值。在下面的食谱中，我们演示了如何用多个变量预测房屋租赁价格。

## 准备就绪

需要通过将房屋租赁数据下载到变量`house`中来完成之前的配方。

## 怎么做……

执行以下步骤将房屋租赁数据集拟合到多元回归模型中:

1.  首先，你需要将`Sqft`、`Floor`、`TotalFloor`、`Bedroom`、`Living.Room`、`Bathroom`拟合成一个线性回归模型:、

    ```

    > fit <- lm(Price ~ Sqft + Floor + TotalFloor  + Bedroom + Living.Room + Bathroom, data=house)

    ```

    、
2.  获取拟合模型的汇总:

    ```

    > summary(fit)

    Call:

    lm(formula = Price ~ Sqft + Floor + TotalFloor + Bedroom + Living.Room + Bathroom, data = house)

    Residuals:

     Min     1Q Median     3Q    Max -71429 -11314  -2075   8480 104063 

    Coefficients:

     Estimate Std. Error t value Pr(>|t|) 

    (Intercept)  2669.268   3665.690   0.728    0.467 

    Sqft           37.884      1.615  23.456  < 2e-16 ***

    Floor        1025.887    241.543   4.247 2.49e-05 ***

    TotalFloor    135.451    197.033   0.687    0.492 

    Bedroom     -1775.956   1079.401  -1.645    0.100 

    Living.Room -3368.837   2078.469  -1.621    0.106 

    Bathroom     2777.701   1835.922   1.513    0.131 

    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Residual standard error: 19560 on 638 degrees of freedom

    Multiple R-squared:  0.7015, Adjusted R-squared:  0.6987

    F-statistic: 249.9 on 6 and 638 DF,  p-value: < 2.2e-16

    ```

3.  现在，你可以用拟合的模型预测可能的房屋租赁价格:

    ```

    > predict(fit, house[1:3,])

     1        2        3 

    45550.75 81335.21 44368.01

    ```

## 它是如何工作的……

多元回归是一个有两个或更多独立变量的回归模型。与用直线建模的简单线性回归不同，多元回归分析被建模为具有多个独立变量的函数。模型的形式为:*response ~ terms 1+terms 2+…termsn*，其中 *n* 是自变量的个数。

在这个例子中，我们首先用`Sqft`、`Floor`、`TotalFloor`、`Bedroom`、`Living.Room`和`Bathroom`作为自变量，用`Price`作为因变量来拟合回归模型。然后，我们可以使用`summary`函数获得拟合模型的摘要。从系数表中，我们发现`Sqft`的 p 值等于`2e-16`，而`Floor`的 p 值等于`2.49e-05`；因此，我们可以拒绝零假设，使得`Sqft`和`Floor`对拟合的模型没有影响。或者，换句话说，我们可以说有 95%的概率是正确的，即`Sqft`和`Floor`对拟合的模型有意义。`F-statistic`表示`249.9`，p 值等于`2.2e-16` ( < 0.05)拒绝变量间无线性相关的零假设(![How it works…](graphics/B04007_11_18.jpg))。

最后，可以使用 predict 函数通过拟合的模型`fit`来预测前三个出租屋的可能价格。

## 还有更多……

诊断是评估回归假设的方法，可用于确定拟合模型是否充分代表数据。在下面的例子中，我们介绍如何通过使用诊断图来诊断回归模型。

绘制回归模型的诊断图:

```

> par(mfrow=c(2,2))

> plot(fit)

```

![There's more…](graphics/B04007_11_04.jpg)

图 4:拟合模型的诊断图

`plot`函数生成一个回归模型的四个诊断图。左上图显示了残差与拟合值的关系。在图中，残差表示从点到回归线的垂直距离。如果所有点都正好落在回归线上，则所有残差都将正好落在灰色虚线上。图中的红线是关于残差的平滑曲线，如果所有点都正好落在回归线上，红线的位置应该正好与灰色虚线匹配。

右上图显示了残差的正态 Q-Q 图。该图验证了残差呈正态分布的假设。因此，如果残差呈正态分布，它们应该正好位于灰色虚线上。

左下角的**刻度位置**图测量标准化残差与拟合值的平方根。因此，如果所有的点都位于回归线上， **y** 的值应该接近于 **0** 。由于假设残差的方差基本不改变分布，如果假设正确，红线应该相对平坦。

右下角的图显示了标准化残差与杠杆率的关系。杠杆是衡量每个数据点如何影响回归的指标。它是从回归形心的距离和隔离级别的测量值(通过是否有邻居来测量)。还有，你可以找到库克距离的等高线，这个等高线受高杠杆和大残差的影响。人们可以用这个来衡量如果删除一个点，回归将如何变化。红线在标准化残差方面是平滑的。对于完美拟合回归，红线应该接近虚线，在库克距离中没有超过 **0.5** 的点。

<title>Selecting the best-fitted regression model with stepwise regression</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 用逐步回归法选择最佳拟合回归模型

为了找到最佳拟合的回归模型，可以执行逐步回归，逐步添加或删除拟合模型中的一项，并最终输出具有最小 AIC 的模型。在下面的配方中，我们演示了如何使用阶跃函数执行逐步回归。

## 准备就绪

您需要通过将房屋租赁数据拟合到多元回归模型`fit`中来完成前面的配方。

## 怎么做……

执行以下步骤，用`step`功能搜索最佳拟合回归模型:

1.  首先可以用`step`用向后淘汰法选择最优模型:

    ```

    > step(fit, direction="backward")

    Start:  AIC=12753.77

    Price ~ Sqft + Floor + TotalFloor + Bedroom + Living.Room + Bathroom

     Df  Sum of Sq        RSS   AIC

    - TotalFloor   1 1.8081e+08 2.4428e+11 12752

    <none>                      2.4410e+11 12754

    - Bathroom     1 8.7580e+08 2.4497e+11 12754

    - Living.Room  1 1.0051e+09 2.4510e+11 12754

    - Bedroom      1 1.0357e+09 2.4513e+11 12754

    - Floor        1 6.9016e+09 2.5100e+11 12770

    - Sqft         1 2.1050e+11 4.5459e+11 13153

    Step:  AIC=12752.25

    Price ~ Sqft + Floor + Bedroom + Living.Room + Bathroom

     Df  Sum of Sq        RSS   AIC

    <none>                      2.4428e+11 12752

    - Bathroom     1 8.1619e+08 2.4509e+11 12752

    - Living.Room  1 1.0233e+09 2.4530e+11 12753

    - Bedroom      1 1.1225e+09 2.4540e+11 12753

    - Floor        1 1.1700e+10 2.5598e+11 12780

    - Sqft         1 2.3530e+11 4.7958e+11 13185

    Call:

    lm(formula = Price ~ Sqft + Floor + Bedroom + Living.Room + Bathroom, 

     data = house)

    Coefficients:

    (Intercept)         Sqft        Floor      Bedroom 

     3522.17        38.22      1116.95     -1841.61 

    Living.Room     Bathroom 

     -3398.44      2672.11

    ```

2.  将拟合数据与产生最小 AIC 的变量一起使用:

    ```

    > fit2 <- lm(Price ~ Sqft + Floor + Bedroom + Living.Room + Bathroom, data=house)

    ```

## 它是如何工作的……

AIC 公式化为 *AIC=2k-2ln(L)* ，其中 *k* 为估计参数个数， *L* 为模型似然函数的最大个数。AIC 的目的是防止你选择一个过度合身的模型。衡量本身并没有太大的意义，你应该用这个标准来比较类似的车型，选择 AIC 最少的那个。在这里，我们执行逐步回归以找到具有最小 AIC 的模型。

在这个配方中，我们通过设置参数`direction="backward"`使用 step 函数执行反向消除。第一步，模型首先包括所有自变量。然后，模型一次逐步消除一个独立变量。如果模型不能拒绝变量具有零系数的零假设，则从模型中移除该变量。最后，我们发现没有自变量`TotalFloor` (AIC: 12752)的拟合模型比有`TotalFloor` (AIC: 12754)的模型拟合得更好。

## 还有更多……

在前面的步骤中，我们已经展示了没有变量`TotalFloor`的模型比有变量`TotalFloor`的模型拟合得更好。然后我们可能会对测试`TotalFloor`的系数是否等于`0`感兴趣。因此，我们可以用一个`anova`函数比较`fit1`和`fit2`之间的**方差和** ( **SSE** ) 进行部分 f 检验:

```

> anova(fit1, fit2)

Analysis of Variance Table

Model 1: Price ~ Sqft + Floor + TotalFloor + Bedroom + Living.Room + Bathroom

Model 2: Price ~ Sqft + Floor + Bedroom + Living.Room + Bathroom

 Res.Df        RSS Df  Sum of Sq      F Pr(>F)

1    638 2.4410e+11 

2    639 2.4428e+11 -1 -180811489 0.4726  0.492

```

输出显示`F`等于`0.4726` (p 值等于`0.492`)。因此，我们不能以 5%的显著性水平拒绝零假设(系数`TotalFloor`等于`0`)。因此，我们可以推断，与其余的独立变量相比，`TotalFloor`变量对`Price`没有贡献重要的信息。

<title>Applying the Gaussian model for generalized linear regression</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 应用高斯模型进行广义线性回归

一个**广义线性模型** ( **GLM** ) 是线性回归的一个推广，它可以包含一个链接函数来进行线性预测。作为默认设置，`glm`的族对象是高斯，这使得`glm`的表现与`lm`完全相同。在这个菜谱中，我们首先演示如何使用`glm`函数将模型拟合到数据，然后展示使用高斯模型的`glm`与`lm`的表现完全相同。

## 准备就绪

您需要通过将房屋租赁数据下载到变量`house`中来完成前面的配方。此外，您需要将房屋租赁数据拟合到多元回归模型中，`fit`。

## 怎么做……

执行以下步骤，用高斯模型拟合广义线性回归模型:

1.  拟合自变量`Sqft`、`Floor`、`TotalFloor`、`Bedroom`、`Bathroom`至`glm` :

    ```

    > glmfit <- glm(Price ~ Sqft + Floor + TotalFloor  + Bedroom + Living.Room + Bathroom, data=house, family=gaussian())

    > summary(fit)

    ```

2.  使用`anova`比较两个拟合的模型:

    ```

    > anova(fit, glmfit)

    Analysis of Variance Table

    Model 1: Price ~ Sqft + Floor + TotalFloor + Bedroom + Living.Room + Bathroom

    Model 2: Price ~ Sqft + Floor + TotalFloor + Bedroom + Living.Room + Bathroom

     Res.Df       RSS Df   Sum of Sq F Pr(>F)

    1    638 2.441e+11 

    2    638 2.441e+11  0 -9.1553e-05

    ```

## 它是如何工作的……

`glm`函数以类似于`lm`函数的方式使模型适合数据。唯一的区别是您可以在参数`family`中指定不同的链接函数(您可以在控制台中使用`family`来查找不同类型的`link`函数)。在这个配方中，我们首先将自变量`Sqft`、`Floor`、`TotalFloor`、`Bedroom`、`Living.Room`、`Bathroom`和因变量`Price`输入到`glm`函数中，并将建立的模型赋给`glmfit`。您可以使用构建的模型进行进一步预测。通过将汇总函数应用于两个不同的模型，揭示了两个输出汇总的残差和系数完全相同。

最后，我们用`anova`函数进一步比较两个拟合模型。`anova`结果显示两个模型相似，具有相同的剩余自由度(`Res.DF`)和剩余平方和(`RSS Df`)。

## 参见

*   关于广义线性模型与线性模型的比较，请参考*维纳布尔斯 W. N .和里普利 B. D.* 、*斯普林格* (2002)的*带 S* 的现代应用统计学。

<title>Performing a logistic regression analysis</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 进行逻辑回归分析

在前面的例子中，我们已经讨论了如何将数据拟合到连续变量的线性模型中。此外，我们可以使用广义线性模型中的`logit`模型来预测分类变量。在本食谱中，我们将演示如何执行二项式逻辑回归来创建一个分类模型，该模型可以根据给定的一组预测因子预测二元响应。

## 准备就绪

先从[https://github . com/yw chiu/rcookbook/blob/master/chapter 11/customer . CSV](https://github.com/ywchiu/rcookbook/blob/master/chapter11/customer.csv)下载房屋租赁数据集，确保你的操作系统上已经安装了 R。

## 怎么做……

执行以下步骤，用`logit`模型拟合广义线性回归模型:

1.  将`customer.csv`读成 R 会话:

    ```

    > customer = read.csv('customer.csv', header=TRUE)

    > str(customer)

    'data.frame':	100 obs. of  5 variables:

     $ CustomerID : int  1 2 3 4 5 6 7 8 9 10 ...

     $ gender     : Factor w/ 2 levels "F","M": 1 2 1 1 2 2 1 1 2 2 ...

     $ age        : int  36 26 21 49 42 49 47 50 26 40 ...

     $ visit.times: int  5 3 2 5 4 1 4 1 2 3 ...

     $ buy        : Factor w/ 2 levels "no","yes": 2 1 2 2 1 1 2 1 1 1 ...

    ```

2.  将客户数据放入 R 会话:

    ```

    > logitfit = glm(buy ~ visit.times + age + gender, data=customer, family=binomial(logit))

    ```

3.  使用`summary`函数获得拟合模型的汇总:

    ```

    > summary(logitfit)

    Call:

    glm(formula = buy ~ visit.times + age + gender, family = binomial(logit),

     data = customer)

    Deviance Residuals: 

     Min      1Q  Median      3Q     Max 

    -1.909   0.000   0.000   0.000   1.245 

    Coefficients:

     Estimate Std. Error z value Pr(>|z|)

    (Intercept)   26.5278    18.6925   1.419    0.156

    visit.times    9.7809     6.1264   1.597    0.110

    age           -1.1396     0.7592  -1.501    0.133

    genderM      -71.0222  4170.8348  -0.017    0.986

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 133.7496  on 99  degrees of freedom

    Residual deviance:   7.1936  on 96  degrees of freedom

    AIC: 15.194

    Number of Fisher Scoring iterations: 21

    ```

4.  使用`predict` 函数获得预测结果:

    ```

    >pr <- predict(fit, customer, type="response")

    ```

5.  最后，可以使用 table 函数来检索混淆矩阵:

    ```

    > table(customer$buy, ifelse(pr > 0.5, 'yes', 'no'))

     no yes

     no  60   1

     yes  1  38

    ```

## 它是如何工作的……

与线性回归分析类似，逻辑回归用于根据一组预测因子(自变量)预测响应(因变量)。线性回归用于预测连续变量，而逻辑回归分析最适合预测二元结果的变量。

逻辑模型是一种广义线性回归模型，其响应遵循二项式分布。为了让响应概率始终取值于`0`和`1`之间，逻辑回归模型使用对数比值(`logit`)来转换因变量。`logit`可以用下面的等式来表示:

![How it works…](graphics/B04007_11_19.jpg)

这里的 *p(x)* 与 *1-p(x)* 之比是指奇数比，![How it works…](graphics/B04007_11_20.jpg)和![How it works…](graphics/B04007_11_21.jpg)代表系数。根据`logit`公式，我们可以用下面的公式表示响应的概率:

![How it works…](graphics/B04007_11_22.jpg)

将逻辑函数绘制成线形图会产生下图，其中`logit`模型的响应总是取值于`0`和`1`之间:

![How it works…](graphics/B04007_11_05.jpg)

图 logit 和线性模型之间的比较图

在这个方法中，我们首先将`customer.csv`加载到一个 R 会话中。该数据集包含客户特征，如 ID、性别、年龄、访问次数以及客户是否购买了产品。作为一家公司的销售经理，我们希望建立一个分类模型，可以预测客户是否会购买该产品。

接下来，我们将基于公式 *buy ~ visit.times +年龄+性别*构建分类模型。由于购买行为的结果是二元的，我们可以执行逻辑回归来将数据拟合到模型中。要执行逻辑回归，我们只需要使用`binomial(logit)`作为`glm`函数的链接函数。

然后，我们使用汇总函数来获得拟合的模型。从输出中，我们发现函数调用，偏差残差类似于其他回归分析输出。在系数部分，输出显示系数、标准误差 z 统计数据和相关的 p 值。这里，我们发现在 5%的显著性水平上，没有变量在统计上显著不同于 0。另一方面，我们发现`visit.times`每改变一个单位，购买该产品的对数几率增加 9.78。我们还发现，年龄或男性顾客的增加会降低购买该产品的几率。在系数表中，我们发现零、偏差残差和 AIC；我们可以选择使用阶跃函数来寻找最优模型。

我们现在可以使用预测函数和拟合模型来预测客户是否会购买产品。这里，我们设置`type="response"`来获得客户购买产品的概率。最后，我们使用表格函数来获得我们预测结果的混淆矩阵。从混淆矩阵中，我们发现预测模型在一百次预测中只犯了两次错误。

## 参见

如果您想知道您可以使用多少个替代链接，请通过`help`功能查阅`family`文档:

```

 > ?family

```

<title>Building a classification model with recursive partitioning trees</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 用递归分割树建立分类模型

在之前的食谱中，我们介绍了如何使用逻辑回归来建立分类模型。我们现在介绍如何使用递归划分树来预测客户行为。分类树使用分裂条件基于一个或多个输入变量来预测类别标签。分类过程从树的根节点开始；在每个节点，该过程将检查输入值是否应该根据分裂条件递归地继续到右边或左边的子分支，并且当遇到决策树的任何叶(终端)节点时停止。在这个菜谱中，我们将介绍如何对购物车数据集应用递归分区树。

## 准备就绪

先从[https://github . com/yw chiu/rcookbook/blob/master/chapter 11/customer . CSV](https://github.com/ywchiu/rcookbook/blob/master/chapter11/customer.csv)下载房屋租赁数据集，确保已经在你的操作系统上安装了 R。

## 怎么做……

执行以下步骤，用`rpart`构建分类树:

1.  加载`rpart`包:

    ```

    > library(rpart)

    ```

2.  将客户数据拆分成训练数据集和测试数据集:

    ```

    > set.seed(33)

    > idx <- sample(c(1,2),nrow(customer),prob = c(0.8,0.2), replace=TRUE)

    > trainset <- customer[idx == 1, ]

    > testset  <- customer[idx == 2, ]

    ```

3.  使用`rpart`拟合学习模型:

    ```

    > fit <- rpart(buy ~ gender + age + visit.times, data = trainset)

    ```

4.  键入`fit`检索分类树

    ```

    > fit

    n= 80 

    node), split, n, loss, yval, (yprob)

     * denotes terminal node

    1) root 80 30 no (0.6250000 0.3750000) 

     2) gender=M 42  0 no (1.0000000 0.0000000) *

     3) gender=F 38  8 yes (0.2105263 0.7894737) 

     6) visit.times< 2.5 14  6 no (0.5714286 0.4285714) *

     7) visit.times>=2.5 24  0 yes (0.0000000 1.0000000) *

    ```

    的节点明细
5.  使用`printcp`函数检查复杂度参数:

    ```

    > printcp(fit)

    Classification tree:

    rpart(formula = buy ~ gender + age + visit.times, data = trainset)

    Variables actually used in tree construction:

    [1] gender      visit.times

    Root node error: 30/80 = 0.375

    n= 80 

     CP nsplit rel error  xerror     xstd

    1 0.733333      0   1.00000 1.00000 0.144338

    2 0.066667      1   0.26667 0.26667 0.089443

    3 0.010000      2   0.20000 0.20000 0.078528

    ```

6.  使用`plotcp`函数对剧情成本复杂度参数:

    ```

    > plotcp(fit)

    ```

    ![How to do it…](graphics/B04007_11_06.jpg)

    图 6:成本复杂性图

7.  使用`summary`功能检查已建模型:

    ```

    > summary(fit)

    ```

## 它是如何工作的……

在这个方法中，我们使用来自`rpart`包的递归分区树来构建基于树的分类模型。递归划分树包括两个过程:递归和划分。在决策归纳的过程中，我们必须考虑一个统计评估问题(或简单的是/否问题)来根据评估结果将数据划分到不同的分区。然后，由于我们已经确定了子节点，我们可以重复执行分割，直到满足停止标准。

例如，关于性别是否等于 m 的问题，根节点中的数据(如图 7 所示)可以被分成两组，如果是，则数据被分成左手侧；否则，它将被拆分到右侧。我们继续用`visit.times`是否低于`2.5`的问题来划分左侧数据:

![How it works…](graphics/B04007_11_07.jpg)

图 7:递归分区树

我们首先用`library`函数加载`rpart`包。接下来，我们构建一个分类模型，使用`buy`变量作为分类类别(`class`标签)，其余变量作为输入特征。

模型构建完成后，我们键入构建模型的变量名`fit`，以显示树节点的详细信息。在打印的节点细节中，`n`表示样本大小，损失表示误分类成本，`yval`表示分类成员资格(本例中为`no`或`yes`)，而`yprob`表示两类的概率(左边的值表示到达`no`标签的概率，右边的值表示到达`yes`标签的概率)。

然后我们使用`printcp`函数打印构建的树模型的复杂性参数。从`printcp`的输出中，我们找到 **复杂度参数** ( **CP** )的值，作为控制树大小的惩罚。简而言之，CP 值越大，分裂次数越少(`nsplit`)。输出值(`rel`错误)代表当前树的平均偏差除以空树的平均偏差。`xerror`代表通过 10 倍分类估计的相对误差。`xstd`代表相对误差的标准误差。

为了使 CP(复杂度参数)表更具可读性，我们使用`plotcp`来生成 CP 表的信息图。根据*图 6* ，下方的 *x* 轴表示 CP 值， *y* 轴表示相对误差，上方的 *x* 轴表示树的大小。虚线表示一个标准偏差的上限。从图中，我们可以确定当树的大小为`3`时，交叉验证误差最小。

我们还可以使用`summary`函数来显示函数`call`，拟合的树模型的复杂度参数表，变量重要性(这有助于识别树分类的最重要变量，总计 100)，以及每个节点的详细信息。

使用决策树的优点是它非常灵活并且易于解释。它对分类和回归问题都有效，而且是非参数的。因此，我们不必担心数据是否是线性可分的。决策树的主要缺点是容易有偏差和过度拟合。但是，您可以通过使用条件推理树来克服偏差问题，并通过采用随机森林方法或树修剪来解决过拟合问题。

## 参见

*   有关`rpart`、`printcp`和`summary`功能的更多信息，请使用`help`功能:

    ```

    > ?rpart 

    > ?printcp

    > ?summary.rpart

    ```

*   `C50`是另一个提供决策树和基于规则的模型的包。如果你对这个包感兴趣，你可以在[http://cran.r-project.org/web/packages/C50/C50.pdf](http://cran.r-project.org/web/packages/C50/C50.pdf)查阅这个文件。

<title>Visualizing a recursive partitioning tree</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 可视化递归划分树

从上一个菜谱中，我们学习了如何以文本格式打印分类树。为了使树更具可读性，我们可以使用`plot`函数来获得构建的分类树的图形显示。

## 准备就绪

您需要通过生成一个分类模型来完成前面的配方，并将该模型分配到变量 fit 中。

## 怎么做……

执行以下步骤来可视化分类树:

1.  使用`plot`和`text`函数绘制分类树:

    ```

    > plot(fit, margin= 0.1)

    > text(fit, all=TRUE, use.n = TRUE)

    ```

    ![How to do it…](graphics/B04007_11_08.jpg)

    图 8:客户数据集的分类树

2.  您也可以指定`uniform`、`branch`、`margin`参数来调整布局:

    ```

    > plot(fit, uniform=TRUE, branch=0.6, margin=0.1)

    > text(fit, all=TRUE, use.n = TRUE)

    ```

    ![How to do it…](graphics/B04007_11_09.jpg)

    图 9:不同布局中的递归分配树

## 它是如何工作的……

在这里，我们演示如何使用`plot`函数以图形方式显示分类树。`plot`函数可以简单地可视化分类树，然后我们可以使用 text 函数向图中添加文本。

在图中，我们将`margin=0.1`指定为一个参数，在边框周围添加额外的空白，以防止显示的文本被边距截断。该图显示分支的长度显示了偏差下降的相对幅度。然后我们使用`text`函数为节点和分支添加标签。默认情况下，`text`函数会在每次拆分时添加一个拆分条件，并在每个终端节点添加一个类别标签。为了向树形图添加额外的信息，我们将`all`参数设置为等于`TRUE`，以便向所有节点添加一个标签。此外，我们通过指定`use.n = TRUE`来添加一个参数，以添加额外的信息，这表明实际的观察数量属于两个不同的类别(`no`和`yes`)。

在图 9 中，我们将选项`branch`设置为`0.6`,为每个绘制的分支添加一个肩部。此外，为了显示等长的分支，而不是偏差下降的相对幅度，我们将选项`uniform`设置为`TRUE`。因此，该图显示了一个具有短肩和等长分支的分类树。

## 参见

*   人们可以使用`?plot.rpart`来阅读更多关于分类树的绘制。本文档还包括如何指定`uniform`、`branch`、`compress`、`nspace`、`margin`和`minbranch`参数来调整分类树布局的信息。

<title>Measuring model performance with a confusion matrix</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 用混淆矩阵衡量模型性能

为了测量一个分类模型的性能，我们可以首先基于我们的预测标签和实际标签生成一个分类表。然后，我们使用混淆矩阵来获得性能度量，如精确度、召回率、特异性和准确性。在这个菜谱中，我们将演示如何使用`caret`包检索混淆矩阵。

## 准备就绪

您需要通过生成一个分类模型来完成之前的配方，并将该模型分配给变量`fit`。

## 怎么做……

执行以下步骤来生成分类测量:

1.  使用拟合的模型预测标签，`fit` :

    ```

    > pred = predict(fit, testset[,! names(testset) %in% c("buy")], type="class")

    ```

2.  生成分类表:

    ```

    > table(pred, testset[,c("buy")])

    pred  no yes

     no  11   1

     yes  0   8

    ```

3.  最后，使用来自测试数据集的预测结果和实际标签生成混淆矩阵:

    ```

    > confusionMatrix(pred, testset[,c("buy")])

    Confusion Matrix and Statistics

     Reference

    Prediction no yes

     no  11   1

     yes  0   8

     Accuracy : 0.95 

     95% CI : (0.7513, 0.9987)

     No Information Rate : 0.55 

     P-Value [Acc > NIR] : 0.0001114 

     Kappa : 0.898 

     Mcnemar's Test P-Value : 1.0000000 

     Sensitivity : 1.0000 

     Specificity : 0.8889 

     Pos Pred Value : 0.9167 

     Neg Pred Value : 1.0000 

     Prevalence : 0.5500 

     Detection Rate : 0.5500 

     Detection Prevalence : 0.6000 

     Balanced Accuracy : 0.9444 

     'Positive' Class : no

    ```

## 它是如何工作的……

在这个菜谱中，我们展示了如何获得一个混淆矩阵来衡量一个分类模型的性能。首先，我们使用`predict`函数从使用测试数据集`testset`的分类模型中提取预测标签。然后，我们基于预测标签和实际标签，执行表格功能以获得分类表格。

混淆表可以描述如下:

|   | 

积极的

(参考)

 | 

否定的；消极的；负面的；负的

(参考)

 |
| --- | --- | --- |
| **阳性****(预测)** | 真实的积极的 | 错误的积极的 |
| **负****(预测)** | 错误的否定的；消极的；负面的；负的 | 真实的否定的；消极的；负面的；负的 |

从这个混淆矩阵中，我们可以使用`caret`包中的`confusionMatrix`函数来生成分类模型在所有测量中的性能。我们列出了每次测量的公式:

*   **精度** : ![How it works…](graphics/B04007_11_23.jpg)
*   **95%置信区间**:这表示 95%的置信区间
*   **无信息率(NIR)** :这代表观察类的最大比例
*   **p 值【ACC>NIR】**:在单边二项式检验下计算 p 值，以确定准确率是否大于无信息率(最大类别的比率)
*   **Kappa** :该统计估计模型准确性的 Kappa 统计
*   **麦克内马检验 p 值**:麦克内马检验 p 值是通过麦克内马检验计算出来的，用来评估(正、负)对和(负、正)对之间是否存在显著差异
*   ![How it works…](graphics/B04007_11_24.jpg)
*   ![How it works…](graphics/B04007_11_25.jpg)
*   ![How it works…](graphics/B04007_11_26.jpg)
*   ![How it works…](graphics/B04007_11_27.jpg)
*   ![How it works…](graphics/B04007_11_28.jpg)
*   ![How it works…](graphics/B04007_11_29.jpg)
*   ![How it works…](graphics/B04007_11_30.jpg)
*   ![How it works…](graphics/B04007_11_31.jpg)

<title>Measuring prediction performance using ROCR</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 使用 ROCR 测量预测性能

使用混淆矩阵来评估分类模型的一个障碍是，您必须任意选择一个阈值来确定矩阵的值。确定阈值的一种可能的方法是将不同阈值下的混淆矩阵可视化在一条 **接收机工作特性** ( **ROC** )曲线中。

ROC 曲线是说明二元分类器系统的性能的图，并且绘制了不同分界点的真阳性率对假阳性率。我们最常用这个图来计算曲线 ( **AUC** )下的 **面积，来衡量一个分类模型的性能。在这个配方中，我们演示了如何绘制 ROC 曲线并计算 AUC 来衡量分类模型的性能。**

## 准备就绪

您需要通过生成分类模型来完成前面的配方，并将模型分配到变量 fit 中。

## 怎么做……

执行以下步骤，以生成两个具有不同成本的不同分类实例:

1.  首先安装并加载`ROCR`包:

    ```

    > install.packages("ROCR")

    > library(ROCR)

    ```

2.  在`probability`设置为`TRUE` :

    ```

    > pred2 <- predict(fit,testset[, !names(testset) %in% c("buy")], probability=TRUE)

    ```

    的测试数据集上，根据训练好的模型进行预测
3.  用`"yes"` :

    ```

    > pred.to.roc = pred2[,c("yes")]

    ```

    获得标签的概率
4.  使用`prediction`函数生成预测结果:

    ```

    > pred.rocr = prediction(pred.to.roc, testset$churn)

    ```

5.  使用`performance`功能获得性能测量:

    ```

    > perf.rocr = performance(pred.rocr, measure = "auc", x.measure = "cutoff")

    > perf.tpr.rocr = performance(pred.rocr, "tpr","fpr")

    ```

6.  使用的`plot`功能:

    ```

    > plot(perf.tpr.rocr, colorize=T,main=paste("AUC:",(perf.rocr@y.values)))

    ```

    ![How to do it…](graphics/B04007_11_10.jpg)

    可视化 ROC 曲线图 10:模型的皇家对空观察队曲线

## 它是如何工作的……

在这个配方中，我们演示了如何生成 ROC 曲线来说明二元分类器的性能。首先，我们安装并加载`ROCR`库。然后，我们使用来自`rpart`包的`rpart`来训练分类模型，并使用该模型来预测测试数据集的标签。接下来，我们使用`prediction`函数(来自`ROCR`包)来生成预测结果。我们采用`performance`函数来获得相对于假阳性率的真阳性率的性能测量。最后，我们使用`plot`函数来可视化 ROC 图，并在标题上添加 AUC 的值。在这个例子中，AUC 值是`0.994`，这表明`rpart`分类器在分类`customer`数据集时表现良好。

## 参见…

对 ROC 的概念和术语感兴趣的可以参考[http://en . Wikipedia . org/wiki/Receiver _ operating _ character istic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)。