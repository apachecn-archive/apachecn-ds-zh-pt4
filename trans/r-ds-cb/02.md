

# 二、数据提取、转换和加载

本章涵盖以下主题:

*   下载打开的数据
*   读取和写入 CSV 文件
*   扫描文本文件
*   使用 Excel 文件
*   从数据库中读取数据
*   抓取 web 数据
*   访问脸书数据
*   使用 twitteR

# 简介

在使用数据回答关键的业务问题之前，最重要的是做好准备。数据通常存档在文件中，使用 Excel 或文本编辑器可以很容易地获得数据。但是，数据可能位于一系列不同的来源中，如数据库、网站和各种文件格式。能够从这些来源导入数据至关重要。

有四种主要类型的数据。以文本格式记录的数据是最简单的。由于一些用户要求以结构化格式存储数据，扩展名为`.tab`或`.csv`的文件可用于将数据排列在固定数量的列中。多年来，Excel 在数据处理领域一直处于领先地位，该软件使用了`.xls`和`.xlsx`格式。了解如何从数据库中读取和操作数据是另一项关键技能。此外，由于大多数数据并不存储在数据库中，所以人们必须知道如何使用网络搜集技术从互联网上获取数据。作为本章的一部分，我们将介绍如何使用`rvest`包从互联网上抓取数据。

许多有经验的开发人员已经创建了一些包来让初学者更容易地获得数据，我们将重点放在利用这些包来执行数据提取、转换和加载。在这一章中，我们首先学习如何利用 R 包从文本格式中读取数据并逐行扫描文件。然后我们转到从数据库和 Excel 中读取结构化数据的主题。最后，我们学习如何使用 R web scraper 来抓取互联网和社交网络数据。



# 下载开放数据

在进行任何数据分析之前，重要的一步是收集高质量、有意义的数据。一个重要的数据源是开放数据，它是经过选择、组织的，并且可以免费提供给公众。大多数开放数据以文本格式或 API 的形式在线发布。这里，我们介绍如何用功能下载一个打开的数据文件的文本格式。

## 准备就绪

在这个菜谱中，您需要准备安装了 R 的环境和一台可以访问互联网的计算机。

## 怎么做……

请执行以下步骤从互联网下载开放数据:

1.  首先访问[http://finance.yahoo.com/q/hp?s=%5EGSPC+Historical+Prices](http://finance.yahoo.com/q/hp?s=%5EGSPC+Historical+Prices)链接查看雅虎财经中 S & P 500 的历史价格:![How to do it…](graphics/B04007_02_01.jpg)

    图 1:标准普尔 500 的历史价格

2.  向下滚动到页面底部，右键单击并复制**下载到电子表格**中的链接(链接应该类似于【http://real-chart.finance.yahoo.com/table.csv?s=%5EGSPC】d = 6&e = 3&f = 2015&g = d&a = 0&b = 3&c = 1950&ignore =。csv ):![How to do it…](graphics/B04007_02_02.jpg)

    图 2:下载到电子表格

3.  用`download.file`函数下载这个文件:

    ```

    > download.file('http://rea

    l-chart.finance.yahoo.com/table.csv?s=%5EGSPC&d=6&e=3&f=2015&g=d&a=0&b=3&c=1950&ignore=.csv', 'snp500.csv')

    ```

4.  你现在可以使用`getwd`函数确定当前目录，然后使用`list.files`搜索下载的文件:

    ```

    > getwd()

    > list.files('./')

    ```

## 它是如何工作的……

在这个菜谱中，我们演示了如何使用 r 中的`download.file`下载文件。首先，我们使用 Yahoo Finance 查看 S & P 500 的历史价格。在页面底部，我们发现了一个带有`http://` URL 前缀的链接。`http://` URL 前缀代表 **超文本传输协议** ( **HTTP** )，用于在互联网上传输和接收信息。因此，我们可以通过使用`download.file`向远程服务器请求链接地址。最后，我们可以请求链接并将远程文件保存到我们的本地目录中。

## 还有更多……

除了使用`download.file`功能下载文件，您还可以使用`RCurl`下载带有 HTTP URL 前缀或 HTTPS URL 前缀的文件:

1.  先去[https://NYC opendata . so crata . com/Social-Services/NYC-Wi-Fi-Hotspot-Locations/a9we-mtpn？](https://nycopendata.socrata.com/Social-Services/NYC-Wi-Fi-Hotspot-Locations/a9we-mtpn?)链接探索纽约市 Wi-Fi 热点位置文件公开数据:![There's more…](graphics/B04007_02_03.jpg)

    图 3:纽约的无线保真热点位置

2.  接下来点击**导出**，找到 **CSV** 下载链接:![There's more…](graphics/B04007_02_04.jpg)

    图 4:下载无线热点位置的战斗支援车格式

3.  然后你可以安装并加载`RCurl`包:

    ```

    > install.packages("RCurl")

    > library(RCurl)

    ```

4.  最后，使用`getURL`函数下载 HTTPS 网址前缀文件:

    ```

    > rows <- getURL("https://nycopendata.socrata.com/api/views/jd4g-ks2z/rows.csv?accessType=DOWNLOAD")

    ```



# 读取和写入 CSV 文件

在前一个菜谱中，我们从雅虎财经下载了历史标准普尔 500 指数(T2)。我们现在可以将数据读入 R 会话，以便进一步检查和操作。在这个菜谱中，我们演示了如何用 R 函数读取文件。

## 准备就绪

在这个菜谱中，您需要按照上一个菜谱将标准普尔 500 市场指数文本文件下载到当前目录。

## 怎么做……

请执行以下步骤以使从 CSV 文件中读取文本数据。

1.  首先用`getwd`确定当前目录，用`list.files`查看文件在哪里，如下:

    ```

    > getwd()

    > list.files('./')

    ```

2.  然后，您可以使用`read.table`函数通过指定逗号作为分隔符来读取数据:

    ```

    > stock_data <- read.table('snp500.csv', sep=',' , header=TRUE)

    ```

3.  接下来，通过选择带有列`Date`、`Open`、`High`、`Low`、`Close` :、的前六行来过滤数据
4.  用`head`函数检查加载数据的前六行:

    ```

    > head(stock_data)

    ```

5.  由于要加载的文件是 CSV 格式，您也可以使用`read.csv`来读取文件:

    ```

    > stock_data2 <- read.csv('snp500.csv', header=TRUE)

    > head(stock_data2)

    ```

## 它是如何工作的……

按照前面的方法，你现在应该已经把雅虎财经数据下载到当前目录中了。由于下载的文件是以表格的形式组织的，您可以使用`read.table`功能将数据从文件读入 R 数据帧。

由于下载的数据是用逗号分隔的，并且包含列标题，所以可以设置`header`等于`TRUE`，`,`作为函数参数中的字段分隔符。将`snp500.csv`读入`stock_data`数据框后，您可以从数据中选择前六行，用`head`功能进一步检查。

类似于`read.table`函数，也可以使用`read.csv`读取文本文件。`read.csv`和`read.table`唯一的区别是`read.csv`使用逗号作为默认分隔符读取文件，而`read.table`使用空格作为默认分隔符。您也可以使用`head`功能来检查加载的数据帧。

## 还有更多……

在上一节中，我们演示了如何使用`RCurl`从 NYC open data 站点获取 Wi-Fi 热点数据。由于下载的数据是字符向量，我们可以使用`read.csv`将文本读入 R 会话，方法是在函数参数中设置文本等于字符向量`rows`:

```

> wifi_hotspot <- read.csv(text = rows)

```



# 扫描文本文件

在之前的菜谱中，我们介绍了如何使用`read.table`和`read.csv`将数据加载到 R 会话中。但是，`read.table`和`read.csv`只有在列数固定且数据量很小时才有效。为了更灵活地处理数据，我们将演示如何使用`scan`函数从文件中读取数据。

## 准备就绪

在此配方中，您需要完成之前的配方，并将`snp500.csv`下载到当前目录中。

## 怎么做……

请执行以下步骤从 CSV 文件中扫描数据:

1.  首先，您可以使用`scan`函数从`snp500.csv` :

    ```

    > stock_data3 <- scan('snp500.csv',sep=',', what=list(Date = '', Open = 0, High = 0, Low = 0,Close = 0, Volume = 0, Adj_Close = 0),  skip=1, fill=T)

    Read 16481 records

    ```

    中读取数据
2.  然后，您可以使用`mode`和`str` :

    ```

    > mode(stock_data3)

    [1] "list"

    > str(stock_data3)

    List of 7

     $ Date     : chr [1:16481] "2015-07-02" "2015-07-01" "2015-06-30" "2015-06-29" ...

     $ Open     : num [1:16481] 2078 2067 2061 2099 2103 ...

     $ High     : num [1:16481] 2085 2083 2074 2099 2109 ...

     $ Low      : num [1:16481] 2071 2067 2056 2057 2095 ...

     $ Close    : num [1:16481] 2077 2077 2063 2058 2102 ...

     $ Volume   : num [1:16481] 3.00e+09 3.73e+09 4.08e+09 3.68e+09 5.03e+09 ...

     $ Adj_Close: num [1:16481] 2077 2077 2063 2058 2102 ...

    ```

    检查加载的数据

## 它是如何工作的……

比较`read.csv`和`read.table`时，`scan`功能在数据读取上更加灵活高效。这里，我们在`what`参数中指定列表中每个字段的字段名和支持类型。在这种情况下，第一个字段是字符类型，其余字段是数字类型。因此，我们可以为`Date`列设置两个单引号(或双引号),为其余的字段设置`0`。然后，由于我们需要跳过标题行并自动将空字段添加到字段少于列数的任何行，我们将`skip`设置为`1`并将`fill`设置为`True`。

至此，我们现在可以用一些内置函数检查数据。这里，我们使用`mode`来获取对象的类型，使用`str`来显示数据的结构。

## 还有更多……

在某些情况下，数据由固定宽度而不是固定分隔符分隔。要指定每列的宽度，可以使用`read.fwf`函数:

1.  首先可以用`download.file`从作者的 GitHub 页面下载`weather.op`:

    ```

    > download.file("https://github.com/ywchiu/rcookbook/raw/master/chapter2/weather.op", "weather.op")

    ```

2.  然后你可以用文件编辑器检查数据:![There's more…](graphics/B04007_02_05.jpg)

    图 5:使用文件编辑器检查文件

3.  通过在`widths`中指定每列的宽度，在`col.names`中指定列名来读取数据，通过将`skip`设置为`1` :

    ```

    > weather <- read.fwf("weather.op", widths = c(6,6,10,11,9,8), col.names = c("STN","WBAN","YEARMODA","TEMP","MAX","MIN"), skip=1)

    ```

    来跳过第一行
4.  最后，您可以使用`head`和`names`函数来检查数据:

    ```

    > head(weather)

     STN  WBAN YEARMODA        TEMP       MAX      MIN

    1 8403 99999 20140101     85.8 24    102.7*    69.3*

    2 8403 99999 20140102     86.3 24    102.9*    71.1*

    3 8403 99999 20140103     85.9 24    101.1*    72.0*

    4 8403 99999 20140104     85.6 24    102.7*    70.5*

    5 8403 99999 20140105     84.8 23    102.0*    66.6*

    6 8403 99999 20140106     86.8 23    102.0*    70.9*

    > names(weather)

    [1] "STN"      "WBAN"     "YEARMODA" "TEMP"     "MAX" 

    [6] "MIN" 

    ```



# 使用 Excel 文件

Excel 是另一种流行的数据存储和分析工具。当然，使用 Excel 可以将 Excel 文件转换为 CSV 文件或其他文本格式。或者，为了简化过程，您可以使用`install`并加载`xlsx`包来读取和处理 r 中的 Excel 数据。

## 准备就绪

在这个菜谱中，您需要准备安装了 R 的环境和一台可以访问互联网的计算机。

## 怎么做……

请执行下面的步骤来读取 Excel 文档:

1.  首先安装并加载`xlsx`包:

    ```

    > install.packages("xlsx")

    > library(xlsx)

    ```

2.  进入[www.data.worldbank.org/topic/economy-and-growth](http://www.data.worldbank.org/topic/economy-and-growth)在 Excel 中查找世界经济指标数据:![How to do it…](graphics/B04007_02_06.jpg)

    图 6:世界经济指标

3.  使用`download.file` :

    ```

    > download.file("http://api.worldbank.org/v2/en/topic/3?downloadformat=excel", "worldbank.xls", mode="wb")

    ```

    从以下网址下载世界经济指标数据
4.  用 Excel 检查下载的文件(或打开 Office):![How to do it…](graphics/B04007_02_07.jpg)

    图 7:使用擅长检查下载的文件

5.  您可以使用`read.xlsx2`从下载的 Excel 文件中读取数据:

    ```

    > options(java.parameters = "-Xmx2000m")

    > wb <- read.xlsx2("worldbank.xls", sheetIndex = 1, startRow = 4)

    ```

6.  从读取的数据:

    ```

    > wb2 <- wb[c("Country.Name","Country.Code","Indicator.Name","Indicator.Code", "X2014")]

    ```

    中选择国家名称、国家代码、指标名称、指标代码、2014 年数据
7.  随后，您可以使用`dim`功能检查文件的尺寸:

    ```

    > dim(wb2)

    ```

8.  最后，您可以将过滤后的数据写回到一个名为`2014wbdata.xlsx` :

    ```

    > write.xlsx2(wb2, "2014wbdata.xlsx", sheetName = "Sheet1")

    ```

    的文件中

## 它是如何工作的……

在这个菜谱中，我们介绍了如何使用`xlsx`包读写包含世界发展指标的 Excel 文件。在第一步中，我们需要安装并加载`xlsx`包，它使用户能够通过使用 Java POI 包在 R 命令提示符下读写 Excel 文件。因此，为了利用 Java POI 包，安装过程还将同时安装`rJava`和`xlsxjars`。你可以在`<R Installed Path>\library\xlsx]jars\java`下找到 Java POI `.jar`文件。以笔者安装了 Windows 7 操作系统的电脑为例，`.jar`文件位于`C:\Program Files\R\R-3.2.1\library\xlsxjars\java`路径。

接下来，我们用`download.file`函数从链接([http://data.worldbank.org/topic/economy-and-growth](http://data.worldbank.org/topic/economy-and-growth))下载了世界经济指标数据。默认情况下，`download.file`以 ASCII 模式下载文件。要以二进制模式下载文件，我们需要将下载模式设置为`wb`。

Excel 文件下载完成后，我们可以用 Excel 进行检查。Excel 文件截图显示经济指标从**表 1** 的第 **4** 行开始。因此，我们可以使用`read.xlsx2`函数从这个位置读取世界经济指标。`xlsx`包提供了两个从 Excel 中读取数据的函数:`read.xlsx`和`read.xlsx2`。由于`read.xlsx2`函数主要在 Java 中处理数据，`read.xlsx2`的性能更好(特别是， `read.xlsx2`在超过 100，000 个单元格的表上处理数据的速度要快得多)。

在我们将工作表的内容读入 R 数据框后，我们可以从提取的 R 数据框中选择变量`Country.Name`、`Country.Code`、`Indicator.Name`、`Indicator.Code`和`X2014`。接下来，我们可以使用`dim`函数来检查数据帧的尺寸。最后，我们可以使用`write.xlsx2`将转换后的数据写入 Excel 文件`2014wbdata.xlsx`。



# 从数据库中读取数据

当 R 将数据读入内存时，它非常适合处理和分析小型数据集。然而，随着企业在日常生活中积累的数据比个人多得多，为了存储和分析更大的数据，数据库文档变得越来越常见。要用 R 访问数据库，可以使用`RJDBC`、`RODBC`或`RMySQL`作为通信桥梁。在本节中，我们将演示如何使用`RJDBC`来连接存储在数据库中的数据。

## 准备就绪

在这一节中，我们需要首先准备一个 MySQL 环境。如果您的机器(Windows)上安装了 MySQL 环境，您可以从 MySQL 通知程序检查服务器状态。如果本地服务器正在运行，服务器状态应该提示 **localhost (Online)** ，如以下截图所示:

![Getting ready](graphics/B04007_02_08.jpg)

图 8: MySQL 通知程序

一旦我们的数据库服务器联机，我们需要验证我们是否被授权使用给定的用户名和密码通过使用任何数据库连接客户机来访问数据库。例如，您可以使用 MySQL 命令行客户端连接到数据库。

## 怎么做……

请执行以下步骤，用`RJDBC`将 R 连接到 MySQL:

1.  首先，我们需要安装并加载`RJDBC`包:

    ```

    > install.packages("RJDBC")

    > library(RJDBC)

    ```

2.  然后我们可以从[https://dev . MySQL . com/get/Downloads/Connector-J/MySQL-Connector-Java-5 . 0 . 8 . zip](https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.0.8.zip)链接下载 MySQL 的 JDBC 驱动程序。
3.  解压下载的`mysql-connector-java-5.0.8.zip`和将解压后的`.jar`文件`mysql-connector-java-5.0.8-bin.jar`放到相关位置。例如，在作者的计算机上，提取的`.jar`文件被放在`C:\Program Files\MySQL\`路径中。
4.  接下来，我们可以为了连接 MySQL 而加载 MySQL 驱动:

    ```

    > drv <- JDBC("com.mysql.jdbc.Driver",

    +       "C:\\Program Files\\MySQL\\mysql-connector-java-5.0.8-bin.jar"

    +       )

    ```

5.  然后，我们可以使用注册的 MySQL 驱动程序:

    ```

    > conn <- dbConnect(drv,

    +                   "jdbc:mysql://localhost:3306/finance",

    +                   "root",

    +                   "test")

    ```

    建立到 MySQL 的连接
6.  使用一些基本操作从连接中检索表列表:

    ```

    > dbListTables(conn)

    [1] "majortrade"

    ```

7.  使用`SELECT`操作获取数据:

    ```

    > trade_data <- dbGetQuery(conn, "SELECT * FROM majortrade")

    ```

8.  最后，我们可以断开与 MySQL 的连接:

    ```

    > dbDisconnect(conn)

    [1] TRUE

    ```

## 它是如何工作的……

R 可以用来访问数据库的两个主要标准是 ODBC 和 JDBC。 **JDBC** (也称为 **Java 数据库连接**)是由一组 Java 实现的类和接口组成的，它实现了 Java 和数据库之间的通信。另一方面， **ODBC** (又称 **开放式数据库连接**)是微软开发的标准接口。

比较这两个标准，ODBC 在导入和导出数据方面表现更好；然而，它也依赖于平台。换句话说，在让你的程序工作之前，你必须为不同的操作系统配置连接。相比之下，JDBC 是平台独立的，这意味着你写的程序可以在任何操作系统上工作。

要用 RJDBC 将 R 连接到 MySQL，我们首先需要从 CRAN 安装并加载 RJDBC 包。RJDBC 提供了通过 JDBC 接口连接到数据库的能力。因为 JDBC 是用 Java 实现的，所以应该在 RJDBC 之前安装`rJava`。

为了继续，我们从 MySQL 官方下载站点下载了 MySQL Connector/J，这是 MySQL 的官方 JDBC 驱动程序。在提取了`.zip`文件(或者`.tar`文件)之后，我们将该文件放在适当的文件路径中(或者您可以将`.jar`文件添加到类路径中)。我们现在可以开始编写一个 R 程序来访问数据库。

在我们的 R 脚本中，在向数据库发出任何查询之前，我们首先需要注册并初始化 MySQL 驱动程序。这里我们需要指定驱动的类名，`com.mysql.jdbc.Driver`(不同的数据库有不同的类名)，以及`.jar`文件，`mysql-connector-java-5.0.8-bin.jar`，在这里我们可以找到这个类。接下来，我们可以用注册的驱动程序建立到数据库的连接。这里，我们必须提供连接字符串(`"jdbc:mysql://localhost:3306/finance"`)、用户名(`"root"`)和密码(`"test"`)来访问数据库。由于我们的 MySQL 服务器安装并运行在 localhost 上，我们可以将连接字符串制作成`"jdbc:mysql://localhost:3306/finance"`。`3306`是 MySQL 的默认端口，`finance`是我们的目标数据库。

建立连接后，我们现在可以向数据库发出一些 SQL 查询。我们首先用`dbListTables`命令列出`finance`数据库中的表。接下来，我们创建名为`majortrade`的表，并用`insert`语句将从`snp500.csv`加载的记录插入到`majortrade`表中。然后，我们使用`select`语句从数据库中获取数据。最后，为了释放连接，我们需要使用`dbDisconnect`命令断开连接。

## 还有更多……

在 R 中，还可以使用`RODBC`和`RMySQL`连接到数据库。在这一节中，我们将说明如何通过`RMySQL`访问数据库。按照接下来的几个步骤安装并加载`RMySQL`包，然后向`MySQL`数据库发出一些查询:

1.  首先，我们需要安装并加载`RMySQL`包:

    ```

    > install.packages("RMySQL")

    > library(RMySQL)

    ```

2.  接下来，我们可以使用有效的用户名和密码访问 MySQL:

    ```

    > mydb <-dbConnect(MySQL(), user='root', password='test', host='localhost')

    ```

3.  此时，我们可以向数据库发送查询，从金融数据库中选择交易数据:

    ```

    > dbSendQuery(mydb, "USE finance")

    > fetch(dbSendQuery(mydb, "SELECT * FROM majortrade;"))

    ```



# 抓取网页数据

在大多数情况下，大多数数据不会存在于你的数据库中，而是以不同的形式发布在互联网上。为了从这些数据源中挖掘出更多有价值的信息，我们需要知道如何从 Web 上访问和抓取数据。在这里，我们将说明如何使用`rvest`包从 http://www.bloomberg.com/的[获取财务数据。](http://www.bloomberg.com/)

## 准备就绪

在这个菜谱中，您需要准备安装了 R 的环境和一台可以访问互联网的计算机。

## 怎么做……

执行以下步骤从[http://www.bloomberg.com](http://www.bloomberg.com)/中抓取数据:

1.  先进入以下链接在彭博商业网站浏览标准普尔 500 指数[http://www.bloomberg.com/quote/SPX:IND](http://www.bloomberg.com/quote/SPX:IND):![How to do it…](graphics/B04007_02_09.jpg)

    图 9:标准普尔 500 指数

2.  一旦页面出现，如前面截图中的所示，我们就可以开始安装和加载`rvest`包:

    ```

    >  install.packages("rvest")

    >  library(rvest)

    ```

3.  接下来，您可以使用`rvest`包中的 HTML 函数在:

    ```

    > spx_quote <- html("http://www.bloomberg.com/quote/SPX:IND")

    ```

    抓取并解析链接到 S & P 500 指数的 HTML 页面
4.  使用浏览器的内置 web 检查器检查索引图表下方的详细报价(用红色矩形标记)的位置:![How to do it…](graphics/B04007_02_10.jpg)

    图 10:检查标准普尔 500 索引的数字正射影像图位置

5.  然后，您可以将鼠标移动到详细报价上，并单击您希望向下搜索的目标元素。如下图所示，`<div class="cell">`部分包含了我们需要的所有信息:![How to do it…](graphics/B04007_02_11.jpg)

    图 11:检查详细报价的数字正射影像图位置

6.  用提取元素用`html_nodes`函数提取`cell`的类:

    ```

    > cell  <- spx_quote %>% html_nodes(".cell")

    ```

7.  此外，我们可以从具有类`cell__label`的元素中解析详细报价的标签，从抓取的 HTML 中提取文本，并最终从提取的文本中清除空格和换行符:

    ```

    > label <- cell %>% 

    +     html_nodes(".cell__label")  %>% 

    +     html_text() %>% 

    +     lapply(function(e) gsub("\n|\\s+", "", e))

    ```

8.  此外，我们可以用类`cell__value`从元素中提取详细引用的值，从抓取的 HTML 中提取文本，以及干净的空格和换行符:

    ```

    > value <- cell %>% 

    +     html_nodes(".cell__value") %>% 

    +     html_text() %>% 

    +     lapply(function(e)gsub("\n|\\s+", "", e))

    ```

9.  最后，我们可以将提取的`label`作为名称设置为`value` :

    ```

    > names(value) <- title

    ```

10.  接下来，我们可以通过以下链接进入能源和石油市场指数页面([http://www.bloomberg.com/energy](http://www.bloomberg.com/energy)):![How to do it…](graphics/B04007_02_12.jpg)

    图 12:检查原油和天然气的数字正射影像图位置

11.  然后我们可以使用 web inspector 来检查表格元素的位置:![How to do it…](graphics/B04007_02_13.jpg)

    图 13:检查桌子元素的数字正射影像图位置

12.  最后，我们可以使用`html_table`提取带有`data-table` :

    ```

    > energy <- html("http://www.bloomberg.com/energy")

    > energy.table <- energy %>% 

     html_node(".data-table") %>% html_table()

    ```

    类的表格元素

## 它是如何工作的……

从网站抓取数据最困难的一步是网络数据以不同的格式发布和组织。在继续之前，您必须完全理解 HTML 标记中的数据结构。

由于 **HTML** ( **超文本标记语言** ) 是一种与 XML 语法相似的语言，我们可以使用 XML 包来读取和解析 HTML 页面。然而，XML 包只提供了`XPath`方法，它有两个主要缺点，如下所示:

*   不同浏览器中的不一致行为
*   很难阅读和维护

出于这些原因，我们建议在解析 HTML 时使用 CSS 选择器而不是 XPath。

Python 用户可能熟悉如何通过使用请求和`BeautifulSoup`包快速抓取数据。`rvest`包是 R 中的对应包，它提供了同样的简单有效地从 HTML 页面获取数据的能力。

在这个菜谱中，我们的目标是从 http://www.bloomberg.com/的[获取标准普尔 500 明细报价的财务数据。我们的第一步是确保我们可以通过](http://www.bloomberg.com/)互联网访问我们的目标网页，接下来是安装和加载`rvest`包。安装和加载完成后，我们可以使用 HTML 函数将页面的源代码读取到`spx_quote`中。

一旦我们确认可以阅读 HTML 页面，我们就可以开始解析来自抓取的 HTML 的详细报价。然而，我们首先需要检查详细报价的 CSS 路径。有许多方法可以检查特定元素的 CSS 路径。最流行的方法是使用每个浏览器内置的开发工具(按 *F12* 或 *FN* + *F12* )来检查 CSS 路径。以谷歌 Chrome 为例，按下 *F12* 就可以打开开发工具。一个 DevTools 窗口可能会出现在可视区域的某个地方(参考下面的文档页面:[https://developer . chrome . com/dev tools/docs/DOM-and-styles # inspecting-elements](https://developer.chrome.com/devtools/docs/dom-and-styles#inspecting-elements))。

然后，您可以将鼠标光标移动到 **DevTools** 窗口的左上方，并选择 Inspect Element 图标(类似于![How it works…](graphics/B04007_02_31.jpg)的放大镜图标)。接下来，点击目标元素， **DevTools** 窗口会高亮显示所选区域的源代码。然后，您可以将鼠标光标移动到突出显示的区域，并右键单击它。从弹出菜单中，点击**复制 CSS 路径**提取 CSS 路径。或者，您可以检查源代码，发现所选的元素是用 HTML 代码构造的，带有类`cell`。

`rvest`的一个亮点是它被设计成与`magrittr`一起工作，这样我们可以使用管道操作符`%>%`来链接每个阶段解析的输出。因此，我们可以首先通过调用`spx_quote`获得输出源，然后通过管道将输出传递给`html_nodes`。由于`html_nodes`函数使用 CSS 选择器来解析元素，因此该函数使用基本的选择器，包括类型(例如，`div`)、ID(例如，`#header`)和类(例如，`.cell`)。由于要提取的元素具有类别`cell`，您应该在`cell`前放置一个句点(`.`)。

最后，我们应该从之前解析的节点中提取标签和值。在这里，我们首先提取类`cell__label`的元素，然后使用`html_text`提取文本。然后我们可以使用`gsub`函数从解析的文本中清除空格和换行符。同样，我们应用相同的管道来提取类`class__value`的元素。因为我们已经从详细报价中提取了标签和值，所以我们可以将标签作为名称应用于提取的值。我们现在已经将来自网络的数据组织成结构化数据。

或者，我们也可以使用`rvest`来获取表格数据。与收集 S & P 500 指数的过程类似，我们可以首先访问能源和石油市场指数页面。然后，我们可以使用 web 元素检查器来查找表格数据的元素位置。由于我们已经找到了位于类`data-table`中的元素，我们可以使用`html_table`函数将表内容读入 R 数据帧。

## 还有更多……

你可以考虑使用**SelectorGadget**([http://selectorgadget.com/](http://selectorgadget.com/))来搜索 CSS 路径，而不是使用每个浏览器内置的网页检查器。SelectorGadget 是 Google Chrome 的一个非常强大且简单易用的扩展，用户只需点击几下就可以提取目标元素的 CSS 路径:

1.  要开始使用 SelectorGadget，请访问以下链接:[https://chrome . Google . com/web store/detail/SelectorGadget/mhjhnkfbdhnjickkkdbjoemdmbfginb](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb)。然后，点击绿色按钮(如下图红色矩形所示)在 Chrome 中安装插件:![There's more…](graphics/B04007_02_14.jpg)

    图 14:向铬添加 SelectorGadget

2.  接下来，单击右上方的图标打开 SelectorGadget，然后选择需要向下刮的区域。选中的区域将被涂成绿色，小工具将显示该区域的 CSS 路径以及与该路径匹配的元素数量:![There's more…](graphics/B04007_02_15.jpg)

    图 15:使用 SelecorGadget 检查桌子元素的数字正射影像图位置

3.  最后，您可以将提取的 CSS 路径粘贴到`html_nodes`作为输入参数来解析数据。

除了`rvest`之外，还可以通过`Rselenium`连接 R 和 Selenium 来抓取网页。Selenium 最初被设计为一个自动化的 web 应用程序，使用户能够通过简单的脚本命令 web 浏览器来自动化流程。但是，您也可以使用 Selenium 从互联网上收集数据。以下说明展示了如何使用`Rselenium`刮取[http://www.bloomberg.com/](http://www.bloomberg.com/)的示例演示:

1.  首先，访问以下链接下载 Selenium 独立服务器([http://www.seleniumhq.org/download/](http://www.seleniumhq.org/download/)):![There's more…](graphics/B04007_02_16.jpg)

    图 16:下载硒独立服务器驱动程序

2.  接下来，使用以下命令启动 Selenium 独立服务器:

    ```

    $ java -jar selenium-server-standalone-2.46.0.jar

    ```

3.  如果您可以成功启动独立服务器，您应该会看到以下消息，这意味着您可以连接到绑定到端口`4444` :![There's more…](graphics/B04007_02_17.jpg)

    的服务器图 17:启动硒独立服务器

4.  此时，您可以使用下面的命令开始安装和加载`RSelenium`:

    ```

    > install.packages("RSelenium")

    > library(RSelenium)

    ```

5.  安装`RSelenium`后，注册驱动，连接 Selenium 服务器:

    ```

    > remDr <- remoteDriver(remoteServerAddr = "localhost" 

    +                      , port = 4444

    +                      , browserName = "firefox"

    +)

    ```

6.  查看注册司机的状态:

    ```

    > remDr$getStatus()

    ```

7.  接下来，我们导航到[http://www.bloomberg.com/](http://www.bloomberg.com/):

    ```

    > remDr$open()

    > remDr$navigate("http://www.bloomberg.com/quote/SPX:IND ")

    ```

8.  最后，我们可以使用 CSS 选择器:

    ```

    > webElem <- remDr$findElements('css selector', ".cell") 

    > webData <- sapply(webElem, function(x){

    +   label <- x$findChildElement('css selector', '.cell__label')

    +   value <- x$findChildElement('css selector', '.cell__value')

    +   cbind(c("label" = label$getElementText(), "value" = value$getElementText()))

    + }

    + )

    ```

    来抓取数据



# 访问脸书数据

社交网络数据是对探索和分析社交互动感兴趣的用户的另一个重要来源。社交网络数据和 web 数据的主要区别在于，社交网络平台通常提供一种半结构化的数据格式(主要是 JSON)。因此，人们可以容易地访问数据，而不需要检查数据是如何构造的。在这个菜谱中，我们将说明如何使用`rvest`和`rson`来读取和解析来自脸书的数据。

## 准备就绪

在这个菜谱中，您需要准备安装了 R 的环境和一台可以访问互联网的计算机。

## 怎么做……

执行以下步骤从脸书访问数据:

1.  首先我们需要登录脸书，进入开发者页面(【https://developers.facebook.com/】T3)[:T5图 18:访问脸书开发者页面](https://developers.facebook.com/)
2.  点击**工具&支持**，选择**图形 API 浏览器** :![How to do it…](graphics/B04007_02_19.jpg)

    图 19:选择图形应用程序接口浏览器

3.  接下来，点击 **获取令牌**，选择**获取访问令牌** :![How to do it…](graphics/B04007_02_20.jpg)

    图 20:选择获取访问令牌

4.  在 **用户数据权限**面板上，选择**用户标记位置**，然后点击**获取访问令牌** :![How to do it…](graphics/B04007_02_21.jpg)

    图 21:选择权限

5.  将生成的访问令牌复制到剪贴板:![How to do it…](graphics/B04007_02_22.jpg)

    图 22:复制访问令牌

6.  尝试使用`rvest` :

    ```

    > access_token <- '<access_token>'

    > fb_data <- html(sprintf("https://graph.facebook.com/me/tagged_places?access_token=%s",access_token))

    ```

    访问脸书 API
7.  安装并加载`rjson`包:

    ```

    > install.packages("rjson")

    > library(rjson)

    ```

8.  从`fb_data`中提取文本然后用`fromJSON`读取 JSON 数据:

    ```

    > fb_json <-  fromJSON(fb_data %>% html_text())

    ```

9.  使用`sapply`从`fb_json` :

    ```

    > fb_place <- sapply(fb_json$data, function(e){e$place$name})

    > fb_id <- sapply(fb_json$data, function(e){e$place$id})

    ```

    中提取地名和 ID
10.  最后，使用`data.frame`包装数据:

    ```

    > data.frame(place = fb_place, id = fb_id)

    ```

## 它是如何工作的……

在这份食谱中，我们讲述了如何通过脸书的 Graph API 检索社交网络数据。与抓取网页不同，在请求洞察信息之前，您需要获得一个脸书访问令牌。检索访问令牌有两种方法:一种是使用脸书的 Graph API Explorer，另一种是创建一个脸书应用程序。在这个菜谱中，我们将说明如何使用 Graph API Explorer 来获取访问令牌。

脸书的图形应用编程接口资源管理器是你可以代表自己创建访问脸书数据的请求 URL 的地方。要访问浏览器页面，我们首先要访问 https://developers.facebook.com/的开发者页面。图形 API 浏览器页面位于**工具&支持**的下拉菜单下。在进入浏览器页面后，我们从下拉菜单中选择**获取访问令牌**以获取令牌。随后，将出现一个选项卡式窗口；您可以检查对应用程序不同级别的访问权限。例如，我们可以选中 **tagged_places** 来访问我们之前标记的位置。在我们选择了我们需要的权限之后，我们可以点击**获取访问令牌**来允许 Graph API Explorer 访问我们的 insight 数据。完成这些步骤后，您将看到一个访问令牌，这是一个临时的、短期的令牌，您可以使用它来访问脸书 API。

有了访问令牌，我们可以然后用 r 访问脸书 API。首先，我们需要一个 HTTP 请求包。类似于 web 抓取方法，我们可以使用`rvest`包来发出请求。我们创建了一个请求 URL，添加了`access_token`(从 Graph API Explorer 复制而来)到脸书 API。从响应中，我们应该收到 JSON 格式的数据。为了读取 JSON 格式数据的属性，我们安装并加载了`RJSON`包。然后，我们可以使用`fromJSON`函数读取从响应中提取的 JSON 格式字符串。

最后，我们通过使用`sapply`函数读取地点和 ID 信息，然后我们可以使用数据帧将提取的信息转换成数据帧。在配方的最后，我们应该看到数据框中格式化的数据。

## 还有更多……

要了解更多关于 Graph API 的信息，请阅读以下来自脸书的官方文档:[https://developers . Facebook . com/docs/reference/API/field _ expansion/](https://developers.facebook.com/docs/reference/api/field_expansion/)。

1.  首先，我们需要安装并加载`Rfacebook`包:

    ```

    > install.packages("Rfacebook")

    > library(Rfacebook)

    ```

2.  然后，我们可以使用内置函数从用户那里检索数据，或者通过提供一个访问令牌来访问类似的信息:

    ```

    > getUsers("me", "<access_token>")

    ```

如果您想在不每次登录脸书的情况下收集公共粉丝页面，您可以创建一个脸书应用程序来代表该应用程序访问 insight 信息:

1.  要创建授权应用令牌，请登录脸书开发者页面，点击**添加新页面** :![There's more…](graphics/B04007_02_23.jpg)

    图 23:创建新的应用程序

2.  您可以使用任何名称和有效的电子邮件 id 创建一个新的脸书应用程序，前提是它尚未注册:![There's more…](graphics/B04007_02_24.jpg)

    图 24:创建新的应用 ID

3.  接下来，您可以复制应用程序 ID 和应用程序密码，并为`<APP ID>|<APP SECRET>`创建访问令牌。你现在可以用这个令牌用 Graph API 刮公共粉丝页面信息:![There's more…](graphics/B04007_02_25.jpg)

    图 25:获取应用身份证明和密码

4.  类似于 Rfacebook，我们可以将`access_token`替换为`<APP ID>|<APP SECRET>` :

    ```

    > getUsers("me", "<access_token>")

    ```



# 使用 twitteR

除了获得社交网络互动数据，人们还可以从 Twitter 收集数百万条推文，用于进一步的文本挖掘任务。从 Twitter 上检索数据的方法与脸书非常相似。对于这两个社交平台，我们只需要一个访问令牌来访问 insight 数据。在我们检索到访问令牌后，我们就可以使用`twitteR`来访问数百万条推文。

## 准备就绪

在这个菜谱中，您需要准备安装了 R 的环境和一台可以访问互联网的计算机。

## 怎么做……

执行以下步骤，从 Twitter 读取数据:

1.  首先，你需要在[https://apps.twitter.com/](https://apps.twitter.com/)登录 Twitter 并访问 **Twitter 应用**的页面。点击**创建新 App** :![How to do it…](graphics/B04007_02_26.jpg)

    图 26:创建新的推特应用程序

2.  填写所有必需的申请详细信息以创建新的申请:![How to do it…](graphics/B04007_02_27.jpg)

    图 27:填写所需的细节

3.  接下来，您可以选择**键和访问令牌**，然后访问**应用设置** :![How to do it…](graphics/B04007_02_28.jpg)

    图 28:复制应用程序接口密钥和秘密

4.  点击**创建我的访问令牌**按钮，浏览器会生成一个授权的访问令牌和密码:![How to do it…](graphics/B04007_02_29.jpg)

    图 29:创建访问令牌和密码

5.  安装并加载`twitteR`包:

    ```

    > install.packages("twitteR")

    > library(twitteR)

    ```

6.  使用从**应用程序设置**复制的消费者密钥和消费者秘密，以及从**复制的访问令牌和访问秘密，设置 Twitter OAuth 您的访问令牌** :

    ```

    > consumer_key <- '<consumer_key>'

    > consumer_secret <- '<consumer_secret>'

    > access_token <- '<access_token>'

    > access_secret <- '<access_secret>'

    > setup_twitter_oauth(consumer_key,

    +                     consumer_secret,

    +                     access_token,

    +                     access_secret)

    [1] "Using direct authentication"

    Use a local file to cache OAuth access credentials between R sessions?

    1: Yes

    2: No

    Selection: 1

    Adding .httr-oauth to .gitignore

    ```

7.  此时，您可以使用`searchTwitter`函数提取出以`world cup`为搜索关键字的前 100 个搜索结果:

    ```

    > res <- searchTwitter("world cup", n=100)

    ```

## 它是如何工作的……

在这个菜谱中，我们使用`twitteR`从 Twitter 获取 tweets。与脸书类似，我们在访问任何 tweets 之前需要一个访问令牌。要应用访问令牌，必须首先用一个登录帐户创建一个应用程序，然后填写所有必需的信息来创建一个新的应用程序。

创建应用程序后，我们选择**密钥和访问令牌**选项卡，并在标签为**应用程序设置**的部分中找到消费者密钥和秘密。进一步向下滚动到名为**的按钮，创建我的访问令牌**。点击这个按钮，在标签为**您的访问令牌**的部分提供访问令牌和访问令牌秘密。

至此，我们现在可以将 Twitter 与`twitteR`联系起来。首先，安装并加载`twitteR`包。然后，您可以从**应用程序设置**中复制消费者密钥和消费者秘密，从**您的访问令牌**中复制访问令牌和访问秘密。这些复制的信息可以用来设置 Twitter OAuth。最后，我们可以使用`searchTwitter`功能来查找`world cup`关键字的前 100 个搜索结果。

## 还有更多……

与脸书类似，Twitter 也为其用户提供了一个 API 测试控制台。您可以从[https://dev.twitter.com/rest/tools/console](https://dev.twitter.com/rest/tools/console)访问 API 控制台:

![There's more…](graphics/B04007_02_30.jpg)

图 30:探索 Twitter API