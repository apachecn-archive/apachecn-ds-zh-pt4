

# 十、分类和聚类

在前一章中，我们集中讨论了如何将大量连续变量中的信息压缩到一个较小的数字集合中，但是当我们处理分类数据时，例如在分析调查时，这些统计方法有一定的局限性。

虽然有些方法试图将离散变量转换成数字变量，例如通过使用一些虚拟变量或指示变量，但在大多数情况下，最好考虑我们的研究设计目标，而不是试图在分析中强行使用以前学到的方法。

### 注意

我们可以通过为原始离散变量的每个标签创建一个新变量，用多个虚拟变量替换一个分类变量，然后将 *1* 分配给相关列，将 *0* 分配给所有其他列。这些值可以在统计分析中用作数值变量，尤其是在回归模型中。

当我们通过分类变量分析样本和目标人群时，通常我们对单个例例不感兴趣，而是对相似的元素和组感兴趣。相似的元素可以定义为数据集中的行，列中有相似的值。

在本章中，我们将讨论不同的*有监督的*和*无监督的*方式来识别数据集中的相似案例，例如:

*   分层聚类
*   k 均值聚类
*   一些机器学习算法
*   潜在类别模型
*   判别分析
*   逻辑回归

# 聚类分析

**聚类** 是一种无监督的数据分析方法，用于不同的领域，如模式识别、社会科学和药学。聚类分析的目的是将同质的子群称为聚类，其中同一聚类中的对象是相似的，而聚类彼此不同。

## 层次聚类

聚类分析是最著名和最流行的模式识别方法之一；因此，有许多分析数据集中的分布、密度、可能的中心点等的聚类模型和算法。在这一节中，我们将研究一些层次聚类方法。

**层次聚类** 可以是凝聚的，也可以是分裂的。在凝聚方法中，每个例例开始时都是一个单独的聚类，然后最接近的聚类以迭代的方式合并在一起，直到最后它们合并成一个单一的聚类，该聚类包括原始数据集的所有元素。这种方法的最大问题是每次迭代都必须重新计算聚类之间的距离，这使得它在处理大数据时非常慢。我不建议尝试在`hflights`数据集上运行以下命令。

另一方面，分裂方法采取自上而下的方法。它们从单个集群开始，然后迭代地分成更小的组，直到它们都是单例。

`stats`包包含用于分层聚类的`hclust`函数，该函数将距离矩阵作为输入。为了了解它是如何工作的，让我们使用我们已经在[第 3 章](ch03.html "Chapter 3. Filtering and Summarizing Data")、*过滤和汇总数据*和[第 9 章](ch09.html "Chapter 9. From Big to Small Data")、*从大数据到小数据*中分析过的`mtcars`数据集。`dist`功能在后一章中也很熟悉:

```

> d <- dist(mtcars)

> h <- hclust(d)

> h

Call:

hclust(d = d)

Cluster method   : complete 

Distance         : euclidean 

Number of objects: 32

```

嗯，这是一个过于简短的输出，只显示了我们的距离矩阵包括 32 个元素和聚类方法。结果的可视化表示对于这样一个小数据集来说会有用得多:

```

> plot(h)

```

![Hierarchical clustering](graphics/2028OS_10_01.jpg)

通过绘制这个`hclust`物体，我们获得了一个*树状图*，其中展示了星团是如何形成的。它对于确定聚类的数量很有用，尽管在具有大量案例的数据集中，它变得难以解释。可以在 *y* 轴上的任何给定高度画一条水平线，这样与这条线相交的 *n* 个点就提供了一个 n 簇解。

r 可以提供非常方便的方式来可视化*树状图*上的集群。在下图中，红色方框显示了上图顶部的三集群解决方案的集群成员关系:

```

> plot(h)

> rect.hclust(h, k=3, border = "red")

```

![Hierarchical clustering](graphics/2028OS_10_02.jpg)

虽然这个图看起来很好，并且将相似的元素组合在一起非常有用，但是对于更大的数据集，很难看透。相反，我们可能更感兴趣的是向量中表示的实际集群成员:

```

> (cn <- cutree(h, k = 3))

 Mazda RX4       Mazda RX4 Wag          Datsun 710 

 1                   1                   1 

 Hornet 4 Drive   Hornet Sportabout             Valiant 

 2                   3                   2 

 Duster 360           Merc 240D            Merc 230 

 3                   1                   1 

 Merc 280           Merc 280C          Merc 450SE 

 1                   1                   2 

 Merc 450SL         Merc 450SLC  Cadillac Fleetwood 

 2                   2                   3 

Lincoln Continental   Chrysler Imperial            Fiat 128 

 3                   3                   1 

 Honda Civic      Toyota Corolla       Toyota Corona 

 1                   1                   1 

 Dodge Challenger         AMC Javelin          Camaro Z28 

 2                   2                   3 

 Pontiac Firebird           Fiat X1-9       Porsche 914-2 

 3                   1                   1 

 Lotus Europa      Ford Pantera L        Ferrari Dino 

 1                   3                   1 

 Maserati Bora          Volvo 142E 

 3                   1

```

以及作为频率表的结果聚类中的元素数量:

```

> table(cn)

 1  2  3 

16  7  9

```

似乎前面图中的第三个集群*集群 1* 具有最多的元素。你能猜出这一组和其他两个组有什么不同吗？好吧，那些熟悉汽车名称的读者也许能猜出答案，但是让我们看看这些数字实际上显示了什么:

### 注意

请注意，我们在下面的示例中使用了`round`函数来将代码输出中的小数位数抑制为 1 或 4，以适应页面宽度。

```

> round(aggregate(mtcars, FUN = mean, by = list(cn)), 1)

 Group.1  mpg cyl  disp    hp drat  wt qsec  vs  am gear carb

1       1 24.5 4.6 122.3  96.9  4.0 2.5 18.5 0.8 0.7  4.1  2.4

2       2 17.0 7.4 276.1 150.7  3.0 3.6 18.1 0.3 0.0  3.0  2.1

3       3 14.6 8.0 388.2 232.1  3.3 4.2 16.4 0.0 0.2  3.4  4.0

```

集群之间的平均性能和气体消耗有着惊人的差异！组内的标准差呢？

```

> round(aggregate(mtcars, FUN = sd, by = list(cn)), 1)

 Group.1 mpg cyl disp   hp drat  wt qsec  vs  am gear carb

1       1 5.0   1 34.6 31.0  0.3 0.6  1.8 0.4 0.5  0.5  1.5

2       2 2.2   1 30.2 32.5  0.2 0.3  1.2 0.5 0.0  0.0  0.9

3       3 3.1   0 58.1 49.4  0.4 0.9  1.3 0.0 0.4  0.9  1.7

```

与原始数据集中的标准偏差相比，这些值非常低:

```

> round(sapply(mtcars, sd), 1)

 mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb 

 6.0   1.8 123.9  68.6   0.5   1.0   1.8   0.5   0.5   0.7   1.6

```

当比较组之间的标准偏差时，同样适用:

```

> round(apply(

+   aggregate(mtcars, FUN = mean, by = list(cn)),

+   2, sd), 1)

Group.1     mpg     cyl    disp      hp    drat      wt    qsec 

 1.0     5.1     1.8   133.5    68.1     0.5     0.8     1.1 

 vs      am    gear    carb 

 0.4     0.4     0.6     1.0

```

这意味着我们实现了最初的目标，即识别数据中相似的元素，并将它们组织到彼此不同的组中。但是为什么我们要将原始数据精确地分成三个人工定义的组呢？为什么不是两个，四个，甚至更多？

## 确定理想的聚类数

`NbClust`包为提供了一种非常方便的方式，在运行实际的集群分析之前，对我们的数据进行一些探索性的数据分析。该软件包的主要功能可以计算 30 个不同的指数，所有这些指数都是为了确定理想的组数而设计的。其中包括:

*   单段链路
*   平均的
*   完整链接
*   麦奎蒂
*   质心(聚类中心)
*   中位数
*   k 均值
*   病房

加载包后，让我们从一个可视化方法开始，该方法表示我们数据中可能的聚类数——在一个膝状图上，这可能在[第 9 章](ch09.html "Chapter 9. From Big to Small Data")、*从大到小的数据*中很熟悉，在这里您还可以找到关于以下肘状规则的更多信息:

```

> library(NbClust)

> NbClust(mtcars, method = 'complete', index = 'dindex')

```

![Determining the ideal number of clusters](graphics/2028OS_10_03.jpg)

在前面的图中，我们传统上寻找*肘*，但是右边的第二个差异图对大多数读者来说可能更简单。我们感兴趣的是在哪里可以找到最重要的峰值，这表明在对`mtcars`数据集进行聚类时选择三个组将是理想的。

不幸的是，在如此小的数据集上运行所有的`NbClust`方法都失败了。因此，出于演示的目的，我们现在只运行一些标准方法，并通过相关的列表元素过滤建议的集群数量的结果:

```

> NbClust(mtcars, method = 'complete', index = 'hartigan')$Best.nc

All 32 observations were used. 

Number_clusters     Value_Index 

 3.0000         34.1696 

> NbClust(mtcars, method = 'complete', index = 'kl')$Best.nc

All 32 observations were used. 

Number_clusters     Value_Index 

 3.0000          6.8235

```

Hartigan 和 Krzanowski-Lai 指数都建议坚持三个集群。让我们也来查看一下 `iris`数据集，它包含了更多的案例和更少的数字列，因此我们可以运行所有可用的方法:

```

> NbClust(iris[, -5], method = 'complete', index = 'all')$Best.nc[1,]

All 150 observations were used. 

******************************************************************* 

* Among all indices: 

* 2 proposed 2 as the best number of clusters 

* 13 proposed 3 as the best number of clusters 

* 5 proposed 4 as the best number of clusters 

* 1 proposed 6 as the best number of clusters 

* 2 proposed 15 as the best number of clusters 

 ***** Conclusion ***** 

* According to the majority rule, the best number of clusters is  3 

 ******************************************************************* 

 KL         CH   Hartigan        CCC      Scott    Marriot 

 4          4          3          3          3          3 

 TrCovW     TraceW   Friedman      Rubin     Cindex         DB 

 3          3          4          6          3          3 

Silhouette       Duda   PseudoT2      Beale  Ratkowsky       Ball 

 2          4          4          3          3          3 

PtBiserial       Frey    McClain       Dunn     Hubert    SDindex 

 3          1          2         15          0          3 

 Dindex       SDbw 

 0         15

```

输出总结出，基于返回该数字的 13 种方法，理想的分类数是 3，另外 5 种方法建议使用 4 个分类，其他一些分类数也是由数量少得多的方法计算的。

这些方法不仅适用于之前讨论的分层聚类，而且通常也适用于 k-means 聚类，其中在运行分析之前定义聚类的数量，这与分层方法不同，在分层方法中，我们在繁重的计算已经运行之后切割树状图。

## K-均值聚类

**K-means 聚类** 是由 MacQueen 于 1967 年首先描述的一种无层次方法。与分层集群相比，它的最大优势是其出色的性能。

### 注意

与层次聚类分析不同，k-means 聚类要求您在运行实际分析之前确定聚类的数量。

简单来说，该算法运行以下步骤:

1.  初始化空间中预先定义的( *k* )个随机选择的质心。
2.  将每个对象指定给质心最近的簇。
3.  重新计算质心。
4.  重复第二步和第三步，直到收敛。

我们将使用 `stats`包中的`kmeans`函数。由于 k-means 聚类需要预先决定聚类的数量，我们可以使用前面描述的`NbClust`函数，或者我们可以得出一个符合分析目标的任意数字。

根据上一节中先前定义的最佳聚类数，我们将坚持三个组，其中类内平方和停止显著下降:

```

> (k <- kmeans(mtcars, 3))

K-means clustering with 3 clusters of sizes 16, 7, 9

Cluster means:

 mpg      cyl     disp       hp     drat       wt     qsec

1 24.50000 4.625000 122.2937  96.8750 4.002500 2.518000 18.54312

2 17.01429 7.428571 276.0571 150.7143 2.994286 3.601429 18.11857

3 14.64444 8.000000 388.2222 232.1111 3.343333 4.161556 16.40444

 vs        am     gear     carb

1 0.7500000 0.6875000 4.125000 2.437500

2 0.2857143 0.0000000 3.000000 2.142857

3 0.0000000 0.2222222 3.444444 4.000000

Clustering vector:

 Mazda RX4       Mazda RX4 Wag          Datsun 710 

 1                   1                   1 

 Hornet 4 Drive   Hornet Sportabout             Valiant 

 2                   3                   2 

 Duster 360           Merc 240D            Merc 230 

 3                   1                   1 

 Merc 280           Merc 280C          Merc 450SE 

 1                   1                   2 

 Merc 450SL         Merc 450SLC  Cadillac Fleetwood 

 2                   2                   3 

Lincoln Continental   Chrysler Imperial            Fiat 128 

 3                   3                   1 

 Honda Civic      Toyota Corolla       Toyota Corona 

 1                   1                   1 

 Dodge Challenger         AMC Javelin          Camaro Z28 

 2                   2                   3 

 Pontiac Firebird           Fiat X1-9       Porsche 914-2 

 3                   1                   1 

 Lotus Europa      Ford Pantera L        Ferrari Dino 

 1                   3                   1 

 Maserati Bora          Volvo 142E 

 3                   1 

Within cluster sum of squares by cluster:

[1] 32838.00 11846.09 46659.32

 (between_SS / total_SS =  85.3 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss" 

[5] "tot.withinss" "betweenss"    "size"         "iter" 

[9] "ifault" 

```

cluster means 显示了每个集群的一些非常重要的特征，这些特征是我们在上一节中为分层集群手动生成的。我们可以看到，在第一组中，汽车具有高 mpg(低油耗)，平均四缸(与六缸或八缸形成对比)，相当低的性能等等。输出还会自动显示实际的集群编号。

让我们将这些与分层方法定义的集群进行比较:

```

> all(cn == k$cluster)

[1] TRUE

```

结果似乎相当稳定，对吗？

### Tip

聚类数没有意义，它们的顺序是任意的。换句话说，集群成员是一个名义变量。基于此，当集群编号以不同的顺序分配时，前面的 R 命令可能会返回`FALSE`而不是`TRUE`，但是比较实际的集群成员将会验证我们是否找到了完全相同的组。例如，参见`cbind(cn, k$cluster)`生成一个包含两个集群成员的表。

## 可视化集群

绘制这些集群也是理解分组的好方法。为此，我们将使用 `cluster`包中的`clusplot`函数。为了更容易理解，该函数将维数减少到两个，类似于我们进行 PCA 或 MDS 时的方式(在第 9 章、*从大数据到小数据*中描述):

```

> library(cluster) 

> clusplot(mtcars, k$cluster, color = TRUE, shade = TRUE, labels = 2)

```

![Visualizing clusters](graphics/2028OS_10_04.jpg)

正如您所看到的，在维度减少后，这两个分量解释了 84.17%的方差，因此这一微小的信息损失是一个很好的权衡，有利于更容易地理解集群。

使用`shade`参数可视化椭圆的相对密度还可以帮助我们认识到相同组的元素是多么相似。我们使用 labels 参数来显示点和集群标签。在可视化大量元素时，请务必坚持默认的 *0* (无标签)或 *4* (仅椭圆标签)。



# 潜在类模型

**潜在类别分析** ( **LCA** ) 是一种在多种结果变量中识别潜在变量的方法。它类似于因子分析，但可用于离散/分类数据。为此，LCA 多用于分析调查。

在本节中，我们将使用 `poLCA`包中的`poLCA`函数。它使用期望最大化和牛顿-拉夫森算法来寻找参数的最大似然性。

`poLCA`函数要求将数据编码为从 1 开始的整数或因子，否则会产生错误信息。为此，让我们将`mtcars`数据集中的一些变量转换为因子:

```

> factors <- c('cyl', 'vs', 'am', 'carb', 'gear')

> mtcars[, factors] <- lapply(mtcars[, factors], factor)

```

### Tip

前面的命令将覆盖当前 R 会话中的`mtcars`数据集。要恢复到其他示例的原始数据集，如果需要，请在`rm(mtcars)`之前从会话中删除此更新的数据集。

## 潜在阶层分析

既然数据已经有了合适的格式，我们就可以进行 LCA 了。相关函数带有许多重要的参数:

*   首先，我们必须定义一个描述模型的公式。根据公式的不同，我们可以定义 LCA(类似于聚类但带有离散变量)或 **潜在类回归** ( **LCR** )模型。
*   `nclass`参数指定了模型中假设的潜在类的数量，默认情况下是 2。基于本章前面的例子，我们将把它改写为 3。
*   我们可以使用`maxiter`、`tol`、`probs.start`和`nrep`参数来微调模型。
*   `graphs`参数可以显示或隐藏参数估计值。

让我们从所有可用的离散变量定义的三个潜在类别的基本 LCA 开始:

```

> library(poLCA)

> p <- poLCA(cbind(cyl, vs, am, carb, gear) ~ 1,

+   data = mtcars, graphs = TRUE, nclass = 3)

```

输出的第一部分(也可以通过之前保存的`poLCA`列表的`probs`元素访问)总结了每个潜在类别的结果变量的概率:

```

> p$probs

Conditional item response (column) probabilities,

 by outcome variable, for each class (row) 

$cyl

 4      6 8

class 1:  0.3333 0.6667 0

class 2:  0.6667 0.3333 0

class 3:  0.0000 0.0000 1

$vs

 0      1

class 1:  0.0000 1.0000

class 2:  0.2667 0.7333

class 3:  1.0000 0.0000

$am

 0      1

class 1:  1.0000 0.0000

class 2:  0.2667 0.7333

class 3:  0.8571 0.1429

$carb

 1      2      3      4      6      8

class 1:  1.0000 0.0000 0.0000 0.0000 0.0000 0.0000

class 2:  0.2667 0.4000 0.0000 0.2667 0.0667 0.0000

class 3:  0.0000 0.2857 0.2143 0.4286 0.0000 0.0714

$gear

 3   4      5

class 1:  1.0000 0.0 0.0000

class 2:  0.0000 0.8 0.2000

class 3:  0.8571 0.0 0.1429

```

从这些概率中，我们可以看到所有的 8 缸车都属于第三类，第一类只包括自动变速器、一个化油器、三个档位的车，以此类推。也可以通过在函数调用中将图形参数设置为`TRUE`或直接调用绘图函数来绘制完全相同的值:

![Latent Class Analysis](graphics/2028OS_10_05.jpg)

该图还有助于突出显示第一个潜在类别与其他类别相比仅包含少数元素(也称为“估计类别人口份额”):

```

> p$P

[1] 0.09375 0.46875 0.43750

```

`poLCA`对象还可以揭示一系列关于结果的其他重要信息。举几个例子，让我们看看对象的命名列表部分，可以通过标准`$`操作符提取:

*   `predclass`返回最可能的类成员
*   另一方面，后验元素是包含每种情况的类成员概率的矩阵
*   **Akaike 信息准则**(`aic`)**贝叶斯信息准则**(`bic`)**偏差** ( `Gsq`)和`Chisq`值代表不同的拟合优度度量

## LCR 模特

另一方面，LCR 模型是一种受监督的方法，其中我们主要对解释我们在探索性数据分析尺度上的观察的潜在变量不感兴趣，而是我们使用一个或多个协变量预测潜在类别成员的概率的训练数据。



# 判别分析

**判别函数分析** ( **DA** ) 是指确定哪些连续的独立(预测)变量对一个离散的因(响应)变量的类别进行判别的过程，可以认为是一个反向的 **多元方差分析** ( **MANOVA** )。

这表明 DA 非常类似于逻辑回归(参见[第 6 章](ch06.html "Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)")、*超越线性趋势线(作者 Renata Nemeth 和 Gergely Toth)* 和以下章节)，后者因其灵活性而被更普遍地使用。虽然逻辑回归可以处理分类数据和连续数据，但 DA 需要数字自变量，并且有一些逻辑回归没有的进一步要求:

*   假设正态分布
*   离群值应该被消除
*   任何两个变量都不应高度相关(多重共线性)
*   最小类别的样本大小应大于预测值的数量
*   自变量的数量不应超过样本量

有两种不同类型的 DA，我们将使用的`MASS`包中的`lda`用于线性判别函数，使用`qda`用于二次判别函数。

让我们从齿轮数量的因变量开始，我们将使用所有其他数值作为自变量。为了确保我们从标准的`mtcars`数据集开始，在前面的例子中没有被覆盖，让我们清除名称空间并更新 gear 列以包括类别而不是实际的数值:

```

> rm(mtcars)

> mtcars$gear <- factor(mtcars$gear)

```

由于观察值数量较少(我们已经在第 9 章、*中讨论过相关选项，从大数据到小数据*，我们现在可以搁置进行正态性和其他测试。让我们进行实际分析。

我们调用`lda`函数，设置 **交叉验证** ( **CV** )为`TRUE`，这样就可以测试预测的准确性。公式中的点表示除明确提到的档位之外的所有变量:

```

> library(MASS)

> d <- lda(gear ~ ., data = mtcars, CV =TRUE)

```

因此，现在我们可以通过混淆矩阵将预测值与原始值进行比较来检查预测值的准确性:

```

> (tab <- table(mtcars$gear, d$class)) 

 3  4  5

 3 14  1  0

 4  2 10  0

 5  1  1  3

```

为了表示相对百分比而不是原始数字，我们可以做一些快速转换:

```

> tab / rowSums(tab)

 3          4          5

 3 0.93333333 0.06666667 0.00000000

 4 0.16666667 0.83333333 0.00000000

 5 0.20000000 0.20000000 0.60000000

```

我们还可以计算预测失误的百分比:

```

> sum(diag(tab)) / sum(tab)

[1] 0.84375

```

毕竟，大约有 84%的案例被归类到最有可能的类别中，这些类别是由列表中的`posterior`元素提取的实际概率组成的:

```

> round(d$posterior, 4)

 3      4      5

Mazda RX4           0.0000 0.8220 0.1780

Mazda RX4 Wag       0.0000 0.9905 0.0095

Datsun 710          0.0018 0.6960 0.3022

Hornet 4 Drive      0.9999 0.0001 0.0000

Hornet Sportabout   1.0000 0.0000 0.0000

Valiant             0.9999 0.0001 0.0000

Duster 360          0.9993 0.0000 0.0007

Merc 240D           0.6954 0.2990 0.0056

Merc 230            1.0000 0.0000 0.0000

Merc 280            0.0000 1.0000 0.0000

Merc 280C           0.0000 1.0000 0.0000

Merc 450SE          1.0000 0.0000 0.0000

Merc 450SL          1.0000 0.0000 0.0000

Merc 450SLC         1.0000 0.0000 0.0000

Cadillac Fleetwood  1.0000 0.0000 0.0000

Lincoln Continental 1.0000 0.0000 0.0000

Chrysler Imperial   1.0000 0.0000 0.0000

Fiat 128            0.0000 0.9993 0.0007

Honda Civic         0.0000 1.0000 0.0000

Toyota Corolla      0.0000 0.9995 0.0005

Toyota Corona       0.0112 0.8302 0.1586

Dodge Challenger    1.0000 0.0000 0.0000

AMC Javelin         1.0000 0.0000 0.0000

Camaro Z28          0.9955 0.0000 0.0044

Pontiac Firebird    1.0000 0.0000 0.0000

Fiat X1-9           0.0000 0.9991 0.0009

Porsche 914-2       0.0000 1.0000 0.0000

Lotus Europa        0.0000 0.0234 0.9766

Ford Pantera L      0.9965 0.0035 0.0000

Ferrari Dino        0.0000 0.0670 0.9330

Maserati Bora       0.0000 0.0000 1.0000

Volvo 142E          0.0000 0.9898 0.0102

```

现在我们可以再次运行`lda`而无需交叉验证，以查看实际的判别式以及`gear`的不同类别是如何构成的:

```

> d <- lda(gear ~ ., data = mtcars)

> plot(d)

```

![Discriminant analysis](graphics/2028OS_10_06.jpg)

上图中的数字代表`mtcars`数据集中由实际档位数表示的汽车。真的很直白，两个判别式渲染出来的元素，在`gear`栏中突出了相同挡位数的车的相似性和数值不相等的车的不同。

这些判别式也可以通过调用`predict`从`d`对象中提取出来，或者直接渲染在直方图上，通过自变量的类别来查看这个连续变量的分布情况:

```

> plot(d, dimen = 1, type = "both" )

```

![Discriminant analysis](graphics/2028OS_10_07.jpg)

# 逻辑回归

尽管逻辑回归在第 6 章[中有所涉及，但在线性趋势线之外的*(作者 Renata Nemeth 和 Gergely Toth)* 中，由于它经常被用于解决分类问题，我们将通过一些相关的例子和注释再次讨论这个主题，例如，在前面的章节中没有介绍的逻辑回归的多项式版本。](ch06.html "Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)")

我们的数据往往不符合*判别分析*的要求。在这种情况下，使用 logistic、logit 或 probit 回归可能是一个合理的选择，因为这些方法对非正态分布和各组内的不等方差不敏感；另一方面，它们需要更大的样本量。对于小样本量，判别分析要可靠得多。

根据经验，每个独立变量至少应该有 50 个观察值，这意味着，如果我们想像前面一样为`mtcars`数据集建立一个逻辑回归模型，我们至少需要 500 个观察值——但我们只有 32 个。

为此，我们将将本节限制为一个或两个如何进行 logit 回归的快速示例，例如，根据汽车的性能和重量来估计汽车是自动还是手动变速器:

```

> lr <- glm(am ~ hp + wt, data = mtcars, family = binomial)

> summary(lr)

Call:

glm(formula = am ~ hp + wt, family = binomial, data = mtcars)

Deviance Residuals: 

 Min       1Q   Median       3Q      Max 

-2.2537  -0.1568  -0.0168   0.1543   1.3449 

Coefficients:

 Estimate Std. Error z value Pr(>|z|) 

(Intercept) 18.86630    7.44356   2.535  0.01126 * 

hp           0.03626    0.01773   2.044  0.04091 * 

wt          -8.08348    3.06868  -2.634  0.00843 **

---

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

 Null deviance: 43.230  on 31  degrees of freedom

Residual deviance: 10.059  on 29  degrees of freedom

AIC: 16.059

Number of Fisher Scoring iterations: 8

```

前面输出中最重要的表是系数表，它描述了模型和自变量是否对自变量的值有显著影响。我们可以得出结论:

*   马力增加 1 个单位会增加拥有手动变速器的对数几率(至少在 1974 年收集数据时是这样)
*   另一方面，体重增加 1 个单位(以磅为单位),相同的对数优势减少 8

看起来，尽管(或者更确切地说，由于)样本量低，该模型非常符合数据，汽车的马力和重量可以解释汽车是自动变速器还是手动变速器:

```

> table(mtcars$am, round(predict(lr, type = 'response')))

 0  1

 0 18  1

 1  1 12

```

但是运行前面的命令会失败，因为 logit 回归默认期望一个二分法变量。我们可以通过在数据上拟合多个模型来克服这一点，例如用虚拟变量验证一辆汽车是否有 3/4/5 档，或者通过拟合多项式逻辑回归。`nnet`包有一个非常方便的功能可以做到这一点:

```

> library(nnet) 

> (mlr <- multinom(factor(gear) ~ ., data = mtcars)) 

# weights:  36 (22 variable)

initial  value 35.155593 

iter  10 value 5.461542

iter  20 value 0.035178

iter  30 value 0.000631

final  value 0.000000 

converged

Call:

multinom(formula = factor(gear) ~ ., data = mtcars)

Coefficients:

 (Intercept)       mpg       cyl      disp         hp     drat

4  -12.282953 -1.332149 -10.29517 0.2115914 -1.7284924 15.30648

5    7.344934  4.934189 -38.21153 0.3972777 -0.3730133 45.33284

 wt        qsec        vs       am     carb

4 21.670472   0.1851711  26.46396 67.39928 45.79318

5 -4.126207 -11.3692290 -38.43033 32.15899 44.28841

Residual Deviance: 4.300374e-08 

AIC: 44

```

正如所料，它返回一个与我们的小数据集高度吻合的模型:

```

> table(mtcars$gear, predict(mlr))

 3  4  5

 3 15  0  0

 4  0 12  0

 5  0  0  5

```

但是由于样本量小，这个模型极其有限。在继续下一个示例之前，请从当前 R 会话中删除更新的`mtcars`数据集，以避免意外错误:

```

> rm(mtcars)

```



# 机器学习算法

**机器学习** ( **ML** ) 是一组数据驱动的算法，这些算法在没有为特定任务显式编程的情况下工作。与非最大似然算法不同，它们需要训练数据(并从中学习)。最大似然算法分为监督型和非监督型。

**监督学习** 是指训练数据由输入向量及其对应的输出值组成。这意味着任务是在称为训练集的历史数据库中建立输入和输出之间的关系，从而使预测未来输入值的输出成为可能。

例如，银行拥有大量关于以前贷款交易细节的数据库。输入向量由个人信息组成——如年龄、工资、婚姻状况等——而输出(目标)变量显示是否遵守了支付期限。在这种情况下，监督算法可以检测可能倾向于无法遵守截止日期的不同人群，这可以作为对申请人的筛选。

无监督学习有不同的目标。由于输出值在历史数据集中不可用，因此目标是识别输入之间的潜在相关性，并定义任意案例组。

## K-最近邻算法

**K-最近邻** ( **k-NN** )，不同于的分层或 k-means 聚类，是一种监督分类算法。虽然它经常与 k-means 聚类混淆，但 k-NN 分类是一种完全不同的方法。它主要用于模式识别和商业分析。k-NN 的一个大优势是它对异常值不敏感，并且用法非常简单——就像大多数机器学习算法一样。

k-NN 的主要思想是识别历史数据集中观察值的最近邻的数量 *k* ，然后定义观察值的类以匹配前面提到的大多数邻居。

作为一个示例分析，我们将使用和`class`包中的`knn`函数。`knn`函数采用 4 个参数，其中`train`和`test`分别是训练和测试数据集，`cl`是训练数据的类成员资格，`k`是在对测试数据集的元素进行分类时要考虑的邻居数量。

`k`的缺省值是`1`，它总是工作正常，没有任何问题——尽管通常精度相当低。当定义更大数量的邻域用于分析以提高精度时，明智的做法是选择一个不是类数量倍数的整数。

让我们将`mtcars`数据集分成两部分:训练和测试数据。为了简单起见，一半的汽车属于训练集，另一半属于测试集:

```

> set.seed(42)

> n     <- nrow(mtcars)

> train <- mtcars[sample(n, n/2), ]

```

### Tip

为了可再现性，我们使用`set.seed`将随机生成器的状态配置为一个(众所周知的)数字:这样，在所有机器上将生成完全相同的*随机*数字。

因此，我们从 1 到 32 之间抽取了 16 个整数，从`mtcars`数据集中选择了 50%的行。有些人可能会认为下面的`dplyr`(在[第 3 章](ch03.html "Chapter 3. Filtering and Summarizing Data")、*过滤和汇总数据*和[第 4 章](ch04.html "Chapter 4. Restructuring Data")、*重组数据*中讨论)代码片段更适合这个任务:

```

> library(dplyr)

> train <- sample_n(mtcars, n / 2)

```

然后，让我们选择新创建的`data.frame`与原始数据相比有差异的其余行:

```

> test <- mtcars[setdiff(row.names(mtcars), row.names(train)), ]

```

现在，我们必须定义训练数据中观察值的类成员，我们希望在测试数据集中以分类的方式预测什么。为此，我们可以使用我们在上一节中所学的知识，而不是汽车的已知特征，我们可以运行一种聚类方法来定义训练数据中每个元素的类成员资格，但这不是我们出于教学目的应该做的事情。您也可以对您的测试数据运行聚类算法，对吗？监督方法和非监督方法之间的主要区别在于，我们使用前一种方法的经验数据来输入分类模型。

因此，相反，让我们使用汽车中的齿轮数作为类成员，并基于在训练数据中找到的信息，让我们预测测试数据集中的齿轮数:

```

> library(class)

> (cm <- knn(

+     train = subset(train, select = -gear),

+     test  = subset(test, select = -gear),

+     cl    = train$gear,

+     k     = 5))

[1] 4 4 4 4 3 4 4 3 3 3 3 3 4 4 4 3

Levels: 3 4 5

```

测试用例刚刚被分类到前面的类中。我们可以检查分类的准确性，例如，通过计算实际和预测齿轮数之间的相关系数:

```

> cor(test$gear, as.numeric(as.character(cm)))

[1] 0.5459487

```

嗯，这可能会好得多，特别是如果训练数据大得多的话。机器学习算法通常使用历史数据库中的数百万行，而我们的数据集只有 16 个例例。但是，让我们通过计算混淆矩阵来看看该模型未能提供准确预测的地方:

```

> table(test$gear, as.numeric(as.character(cm)))

 3 4

 3 6 1

 4 0 6

 5 1 2

```

因此，看起来 k-NN 分类算法可以非常准确地预测所有三档或四档汽车的档位数量(13 个中的一个)，但它最终无法预测五档汽车。这可以用原始数据集中相关汽车的数量来解释:

```

> table(train$gear)

3 4 5 

8 6 2

```

好吧，训练数据只有两辆 5 档的车，这对于建立一个提供准确预测的模型来说确实是很紧张的。

## 分类树

监督分类的另一种 ML 方法是通过决策树使用递归分割。这种方法的最大优点是可视化决策规则可以显著提高对底层数据的理解，并且在大多数情况下运行算法非常容易。

让我们加载`rpart`包，构建一个分类树，响应变量再次成为`gear`函数:

```

> library(rpart)

> ct <- rpart(factor(gear) ~ ., data = train, minsplit = 3)

> summary(ct)

Call:

rpart(formula = factor(gear) ~ ., data = train, minsplit = 3)

 n= 16 

 CP nsplit rel error xerror      xstd

1 0.75      0      1.00  1.000 0.2500000

2 0.25      1      0.25  0.250 0.1653595

3 0.01      2      0.00  0.125 0.1210307

Variable importance

drat qsec  cyl disp   hp  mpg   am carb 

 18   16   12   12   12   12    9    9 

Node number 1: 16 observations,    complexity param=0.75

 predicted class=3  expected loss=0.5  P(node) =1

 class counts:     8     6     2

 probabilities: 0.500 0.375 0.125 

 left son=2 (10 obs) right son=3 (6 obs)

 Primary splits:

 drat < 3.825 to the left,  improve=6.300000, (0 missing)

 disp < 212.8 to the right, improve=4.500000, (0 missing)

 am   < 0.5   to the left,  improve=3.633333, (0 missing)

 hp   < 149   to the right, improve=3.500000, (0 missing)

 qsec < 18.25 to the left,  improve=3.500000, (0 missing)

 Surrogate splits:

 mpg  < 22.15 to the left,  agree=0.875, adj=0.667, (0 split)

 cyl  < 5     to the right, agree=0.875, adj=0.667, (0 split)

 disp < 142.9 to the right, agree=0.875, adj=0.667, (0 split)

 hp   < 96    to the right, agree=0.875, adj=0.667, (0 split)

 qsec < 18.25 to the left,  agree=0.875, adj=0.667, (0 split)

Node number 2: 10 observations,    complexity param=0.25

 predicted class=3  expected loss=0.2  P(node) =0.625

 class counts:     8     0     2

 probabilities: 0.800 0.000 0.200 

 left son=4 (8 obs) right son=5 (2 obs)

 Primary splits:

 am   < 0.5   to the left,  improve=3.200000, (0 missing)

 carb < 5     to the left,  improve=3.200000, (0 missing)

 qsec < 16.26 to the right, improve=1.866667, (0 missing)

 hp   < 290   to the left,  improve=1.422222, (0 missing)

 disp < 325.5 to the right, improve=1.200000, (0 missing)

 Surrogate splits:

 carb < 5     to the left,  agree=1.0, adj=1.0, (0 split)

 qsec < 16.26 to the right, agree=0.9, adj=0.5, (0 split)

Node number 3: 6 observations

 predicted class=4  expected loss=0  P(node) =0.375

 class counts:     0     6     0

 probabilities: 0.000 1.000 0.000 

Node number 4: 8 observations

 predicted class=3  expected loss=0  P(node) =0.5

 class counts:     8     0     0

 probabilities: 1.000 0.000 0.000 

Node number 5: 2 observations

 predicted class=5  expected loss=0  P(node) =0.125

 class counts:     0     0     2

 probabilities: 0.000 0.000 1.000 

```

得到的对象是一个相当简单的*决策树*——尽管我们已经指定了一个极低的`minsplit`参数，以便能够生成多个节点。在没有这个参数的情况下运行前面的调用甚至不会产生一个决策树，因为我们的 16 个训练数据案例可以放在一个节点中，因为每个节点的缺省最小值是 20 个元素。

但我们已经建立了一个决策树，其中确定档位数量的最重要规则是后轮轴比率以及汽车是自动还是手动变速器:

```

> plot(ct); text(ct)

```

![Classification trees](graphics/2028OS_10_08.jpg)

将此翻译成简单明了的英语:

*   后轮轴比率高的汽车有四个档位
*   所有其他装有自动变速器的汽车都有三个档位
*   手动档汽车有五个档位

由于案例数量较少，该规则确实非常基本，混淆矩阵也揭示了该模型的严重局限性，即它无法成功识别 5 个档位的汽车:

```

> table(test$gear, predict(ct, newdata = test, type = 'class'))

 3 4 5

 3 7 0 0

 4 1 5 0

 5 0 2 1

```

但 16 辆车中有 13 辆被完美分类，这相当令人印象深刻，比之前的 k-NN 例子好一点！

让我们改进前面的代码，通过调用前面对象的`rpart.plot`包中的`main`函数，或者加载的`party`包，为`party`对象提供一个非常简洁的绘图函数。一种选择可能是通过调用`partykit`包对之前计算的`ct`对象调用`as.party`；或者，我们可以用它的`ctree`函数重新创建分类树。基于之前的经验，我们只将之前突出显示的变量传递给模型:

```

> library(party)

> ct <- ctree(factor(gear) ~ drat, data = train,

+   controls = ctree_control(minsplit = 3)) 

> plot(ct, main = "Conditional Inference Tree")

```

![Classification trees](graphics/2028OS_10_09.jpg)

似乎这个模型仅仅根据后轮轴比率来决定齿轮的数量，精确度低得多:

```

> table(test$gear, predict(ct, newdata = test, type = 'node'))

 2 3

 3 7 0

 4 1 5

 5 0 3

```

现在让我们看看哪些额外的 ML 算法可以提供更准确和/或更可靠的模型！

## 随机森林

**随机森林** 背后的主要思想是，我们不是建立一个具有不断增长的节点数量的深度决策树，这可能会有过度拟合数据的风险，而是生成多棵树，以最小化方差，而不是最大化准确性。与训练有素的决策树相比，这种方式的结果预计会更嘈杂，但平均而言，这些结果更可靠。

这可以通过与前面 R 中的示例类似的方式实现，例如通过`randomForest`包，该包提供了对经典随机森林算法的非常用户友好的访问:

```

> library(randomForest)

> (rf <- randomForest(factor(gear) ~ ., data = train, ntree = 250))

Call:

 randomForest(formula = factor(gear) ~ ., data = train, ntree = 250) 

 Type of random forest: classification

 Number of trees: 250

No. of variables tried at each split: 3

 OOB estimate of  error rate: 25%

Confusion matrix:

 3 4 5 class.error

3 7 1 0   0.1250000

4 1 5 0   0.1666667

5 2 0 0   1.0000000

```

这个函数使用起来非常方便:它会自动返回混淆矩阵，还会计算估计的错误率——当然，我们也可以基于`mtcars`的另一个子集来生成自己的错误率:

```

> table(test$gear, predict(rf, test)) 

 3 4 5

 3 7 0 0

 4 1 5 0

 5 1 2 0

```

但是这一次，plotting 函数返回了一些新的东西:

```

> plot(rf)

> legend('topright',

+   legend = colnames(rf$err.rate),

+   col    = 1:4,

+   fill   = 1:4,

+   bty    = 'n')

```

![Random forest](graphics/2028OS_10_10.jpg)

当我们在训练数据的随机子样本上生成越来越多的决策树时，我们会看到模型的均方误差是如何随时间变化的，其中误差率在一段时间后似乎不会改变，并且生成超过给定数量的随机样本没有太大意义。

对于这样一个小例子来说，这真的很简单，因为可能的子样本组合是有限的。同样值得一提的是，五档(蓝线)汽车的错误率并没有随着时间的推移而改变，这再次凸显了我们训练数据集的主要局限性。

## 其他算法

尽管继续讨论各种各样的相关 ML 算法(例如， `gbm`或`xgboost`包中的 ID3 和梯度增强算法)以及如何从 R 控制台调用 Weka 以使用 C4.5 会很棒，但在本章中，我只能集中讨论最后一个实际例子，即如何通过`caret`包使用所有这些算法的通用接口:

```

> library(caret)

```

这个软件包捆绑了一些真正有用的函数和方法，可以用作预测模型的通用的、独立于算法的工具。这意味着所有以前的模型都可以在不实际调用`rpart`、`ctree`或`randomForest`函数的情况下运行，我们可以简单地依赖 caret 的`train`函数，它将算法定义作为一个参数。

举个简单的例子，让我们看看 C4.5 的改进版本和开源实现如何处理我们的训练数据:

```

> library(C50)

> C50 <- train(factor(gear) ~ ., data = train, method = 'C5.0')

> summary(C50)

C5.0 [Release 2.07 GPL Edition]    Fri Mar 20 23:22:10 2015

-------------------------------

Class specified by attribute `outcome'

Read 16 cases (11 attributes) from undefined.data

-----  Trial 0:  -----

Rules:

Rule 0/1: (8, lift 1.8)

 drat <= 3.73

 am <= 0

 ->  class 3  [0.900]

Rule 0/2: (6, lift 2.3)

 drat > 3.73

 ->  class 4  [0.875]

Rule 0/3: (2, lift 6.0)

 drat <= 3.73

 am > 0

 ->  class 5  [0.750]

Default class: 3

*** boosting reduced to 1 trial since last classifier is very accurate

*** boosting abandoned (too few classifiers)

Evaluation on training data (16 cases):

 Rules 

 ----------------

 No      Errors

 3    0( 0.0%)   <<

 (a)   (b)   (c)    <-classified as

 ----  ----  ----

 8                (a): class 3

 6          (b): class 4

 2    (c): class 5

 Attribute usage:

 100.00%  drat

 62.50%  am

```

这个输出看起来非常有说服力，因为错误率正好为零，这意味着我们刚刚创建了一个模型，它用三个简单的规则完美地拟合了训练数据:

*   后轮轴比率大的汽车有四个档位
*   其他的有三档(手动换档)或五档(自动换档)

好吧，第二次看结果显示我们还没有找到圣杯:

```

> table(test$gear, predict(C50, test))

 3 4 5

 3 7 0 0

 4 1 5 0

 5 0 3 0

```

因此，该算法在我们的测试数据集上的整体性能导致 16 辆汽车中有 12 辆命中，这是一个很好的例子，说明单个决策树可能会过度拟合训练数据。



# 总结

本章介绍了对数据进行聚类和分类的各种方法，讨论了哪些分析程序和模型非常重要，以及数据科学家工具箱中的常用元素。在下一章，我们将关注一个不太普遍但仍然重要的领域——如何分析图表和网络数据。