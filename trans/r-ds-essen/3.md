<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 第二章。探索性数据分析

探索性数据分析是数据分析领域中一个非常重要的课题。这是一种分析数据和总结数据集主要特征的方法。探索性数据分析的主要目的是检查各种假设，以便更好地了解数据集。

探索性数据分析包括许多统计技术以及直观和非直观的分析。当你的研究必须与同龄人以及其他没有数据科学背景的观众交流时，建议使用大量有助于更好交流的视觉技术。

探索性数据分析的一些期望是从数据中获得洞察力，提取数据集中的重要变量(取决于要解决的问题)，识别数据中的异常值，以及获得各种测试假设的结果。这些结果在如何解决业务问题方面起着非常重要的作用，如果是建模问题，则决定使用哪个模型以及如何将其应用于数据集以提高准确性。

在本章中，您将学习如何执行探索性数据分析，首先是获得数据的总体视图，一次分析一个变量，然后是双变量分析，最后是分析多个变量以更好地了解相互依赖关系。

本章将涉及的主题如下:

*   泰坦尼克号数据集
*   描述统计学
*   推论统计
*   单变量分析
*   双变量分析
*   多元分析(分段散点图、热图和制表)

# 泰坦尼克号数据集

在这一章中，让我们使用 Titanic 数据集来实现各种技术，该数据集可在互联网上获得，也托管在 **GitHub、**上。将数据集放在 R 中的当前工作目录中；在此之前，首先使用`setwd()`命令相应地设置工作目录。`setwd()`函数用于指定应被视为当前工作目录的位置。现在，使用`read.csv`函数读取数据，并将其存储在数据框中。在本书中，我们将数据帧命名为`tdata`。GitHub 上托管的数据集中的各种细节如下:

```
tdata<- read.csv("titanic.csv")
names(tdata)

```

上述命令的输出如下:

![The Titanic dataset](Images/B04750_02_01.jpg)

这些是数据集中捕获的各种列。给出了这些变量的解释。关于数据集的更多详细了解，请访问 https://www.kaggle.com/c/titanic/data。出于学习目的，我们使用了名为`train.csv`的文件。

变量描述如下:

| 

可变的

 | 

描述

 |
| --- | --- |
| `Survival` | 生存(0 =否；1 =是) |
| `Pclass` | 乘客等级(1 =第一；2 =第二；3 =第三) |
| `Name` | 名字 |
| `Sex` | 性 |
| `Age` | 年龄 |
| `Sibsp` | 船上的兄弟姐妹/配偶人数 |
| `Parch` | 船上父母/孩子的数量 |
| `Ticket` | 票号 |
| `Fare` | 乘客票价 |
| `Cabin` | 小木屋 |
| `Embarked` | 启航港口(C =瑟堡；Q =皇后镇；S =南安普敦 |

这里使用`head`函数显示了前面数据集的快照:

```
head(tdata)

```

![The Titanic dataset](Images/B04750_02_02.jpg)<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 描述性统计

描述性统计是一种定量总结数据集的方法。这些摘要可以是关于数据的简单定量陈述，或者是足以成为关于数据集的初始描述的一部分的视觉表示。

为了对数据集有一个基本的了解，我们可以使用内置函数`summary`。该函数快速扫描数据集，并提供关于数据集的以下信息。这将真正有助于获得对数据的初步理解。这对于数字和分类数据都很有用。

```
summary(tdata)

```

输出如下所示:

![Descriptive statistics](Images/B04750_02_03.jpg)

`summary`函数为我们提供了数据集中变量的高级细节。为了了解数据集的更多信息，比如缺失值、数值变量的分布和分类变量的不同值，我们需要使用一个名为`Hmisc`的附加包。(这里给出了这个的实现。)可以使用`install.packages`功能安装软件包，并使用`library`功能将其加载到 R 环境中。然后，使用这里描述的函数，我们得到期望的输出:

```
install.packages("Hmisc")
library(Hmisc)
describe(tdata)

```

上述命令的输出如下:

![Descriptive statistics](Images/B04750_02_04.jpg)

R 中还有几个函数对汇总分析有用，可以单独应用于变量。为了了解数据集的标准偏差，我们可以使用仅适用于数值变量的`sd`函数:

```
sd(tdata$Fare)

```

上述命令的输出如下:

```
[1] 49.69343

```

为了获得关于百分比的细节，我们可以使用`quantile`函数。让我们尝试对`Fare`变量使用`quantile`函数，并检查分布情况:

```
quantile(tdata$Fare)

```

上述命令的输出如下:

![Descriptive statistics](Images/B04750_02_05.jpg)

默认情况下，我们只能以 0.25 的间隔获得百分位数，但是我们可以使用`probs`参数来改变这个值。我们将尝试同样的事情，但间隔为 10 个百分点:

```
quantile(tdata$Fare, probs = seq(0, 1, 0.1))

```

下面的是前面命令的输出:

![Descriptive statistics](Images/B04750_02_06.jpg)

这些是一些基本的描述性统计技术。这些技术足以获得关于数据集的第一印象。

## 方框图

我们可以使用`boxplot`函数以图形格式表示`summary`呈现的数据。只能为数字列绘制方框图；因此，我们首先选择数字列，然后使用`boxplot`函数来绘制它。该箱线图显示了所有变量的中值、第一个四分位数和第三个四分位数。异常值也可以显示在数据集中。在下面的箱线图中，异常值已被禁用:

```
bplot<- tdata[c("Pclass", "Age", "SibSp", "Parch", "Fare", "Cabin")]
boxplot(bplot, outline = FALSE)

```

上述命令的输出如下:

![Box plot](Images/B04750_02_07.jpg)

## 运动

到目前为止，我们已经对 Titanic 数据集进行了描述性统计分析。在尝试了前面的示例之后，下载一个公共数据集，并尝试您所学的技术。

公共数据集可通过以下链接访问:

*   [http://www.google.com/publicdata/directory](http://www.google.com/publicdata/directory)
*   [http://www.scaleunlimited.com/datasets/public-datasets/](http://www.scaleunlimited.com/datasets/public-datasets/)
*   [http://www.kdnuggets.com/2011/02/free-public-datasets.html](http://www.kdnuggets.com/2011/02/free-public-datasets.html)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 推断统计学

我们已经看到了足够多的描述性统计技术。现在，让我们检查一些推断统计技术。推断统计用于推断数据集的属性。

首先，让我们从简单的平均值开始，如果平均值必须落在 95%的置信区间之下，检查范围应该是什么。为了得到均值的置信区间，我们需要加载`lsr`包；如果软件包还没有安装，您需要使用`install.packages`功能安装它，然后使用`ciMean`功能获得想要的结果:

```
library(lsr)
ciMean(tdata$Fare)

```

下面的是前面命令的输出:

![Inferential statistics](Images/B04750_02_08.jpg)

`ciMean`函数让我们对`Fare`变量的置信区间有一个总体的了解。然而，为了了解男性和女性之间的差异，我们可以使用`aggregate`功能:

```
aggregate( tdata$Fare ~ tdata$Sex, tdata, ciMean )

```

前面的输出如下所示:

![Inferential statistics](Images/B04750_02_09.jpg)

从前面的输出中，我们可以推断出`male`的平均票价比`female`乘客的平均票价低得多。此外，我们了解到与`male`乘客相比，`female`乘客的范围相当大。

现在，让我们执行`t-test`来了解两个不同独立数据样本总体均值差异的区间估计，假设数据集遵循正态分布。我们将使用 Titanic 数据集，并考虑`Sex`列将数据集分成两个不同的独立样本:

```
t.test( tdata$Fare ~ tdata$Sex, data = tdata )

```

以下是前面命令的输出:

![Inferential statistics](Images/B04750_02_10.jpg)

从前面的例子来看，女性乘客的平均票价为`44.47982`，男性乘客的平均票价为`25.52389`。在 95%的置信区间内，女性乘客与男性乘客的平均票价差异在 11.62117 至 26.29068 之间。

让我们来理解两个变量是否相关。我们会考虑年龄和票价，找到两者之间的相关性。我们可以使用`cor`函数找到相关性:

```
cor(tdata$Fare, tdata$Age, use="complete")
 [1] 0.09606669

```

我们得到了`Age`和`Fare`变量之间的相关性。为了获得置信区间设置为 95%的相关性范围，我们需要使用`corr`函数:

```
cor.test(tdata$Fare, tdata$Age, use="complete")

```

上述命令的输出如下:

![Inferential statistics](Images/B04750_02_11.jpg)

从前面的输出中，我们可以知道在 95%置信区间范围内的相关范围。相关性范围从 0.023 到 0.168。这些是一些基本的统计技术，通常用于从大量数据中推断细节。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 单变量分析

单变量分析是最简单的分析形式，我们一次只考虑一个变量，了解数据。一些指标已经包含在描述性的统计数据中，比如数据的平均值和中值。

我们将再进行一个单变量分析:数据的分布。我们将考虑乘坐泰坦尼克号旅行的人的年龄，我们将找出不同年龄组有多少人:

```
age <- na.omit(tdata$Age)

```

首先，我们通过排除年龄不存在的情况，将数据读取到`age`数据框。因为我们想要得到一个固定范围的分布，我们首先使用`seq`函数从可用的数据集中得到船上旅行的最年轻和最年长的人的年龄。我们设置起始值为 0，最后一个值为 80；我们还将时间间隔设置为 10:

```
range(age)
breaks = seq(0, 80, by=10)

```

我们创建了间隔，并将它们存储在变量 breaks 中。使用`cut`函数，我们可以根据`breaks`变量中的定义将人分为不同的年龄组:

```
ageBreak = cut(age, breaks, right=FALSE)

```

我们使用`table`函数来寻找`age`变量的频率分布。为了便于阅读，我们可以将数据格式转换为数据框:

```
ageDistribution = table(ageBreak)
ageDistribution<- data.frame(ageDistribution)
data.frame(ageDistribution)

```

上述命令的输出如下:

![Univariate analysis](Images/B04750_02_12.jpg)

这是数据的表格表示；我们还可以用图形格式表示相同的数据，以便于理解和掌握。我们需要使用`ggplot2`包在 R 中绘制有吸引力的图形，但是如前所述，要确保首先安装这个包。我们使用`colnames`函数为上表中的列指定合适的名称:

```
library(ggplot2)
colnames(ageDistribution) <- c("Range","No.Of.People")

```

然后我们使用`qplot`函数来绘制频率分布的直方图。除了数据点之外，我们还传递了几个参数，以确保图形看起来很好，并且呈现的数据易于阅读。我们使用`geom_bar`参数来传递图形的颜色，最后，我们使用`ggsave`函数保存绘图:

```
q <- qplot(x=Range, y=No.Of.People, 
data=ageDistribution, geom="bar", stat="identity",
position="dodge")
q + geom_bar(stat="identity", fill="#0000FF", colour="black")

```

以下是上述代码的输出:

![Univariate analysis](Images/B04750_02_13.jpg)

```
ggsave(file="Distribution.png", dpi=500)

```

前面的命令有助于将输出保存到本地系统。

我们现在将探究泰坦尼克号乘客的性别。我们可以通过`summary`函数轻松获得计数，但是我们将使用饼图来绘制计数:

```
pieChart<- ggplot(tdata, aes(x = factor(1), fill = factor(tdata$Sex))) + geom_bar(width = 1) 
pieChart + coord_polar(theta = "y") +
ggtitle("Male and female")

```

上述代码片段的输出如下:

![Univariate analysis](Images/B04750_02_14.jpg)

与前面的示例类似，我们在这里使用了一些参数和数据来获得更好的输出。我们将使用`ggtitle`参数来给出这个图表的标题。我们将使用`coord_polar`参数表示该图应该是一个饼图，最后，我们将使用`ggsave`函数保存文件:

```
ggsave(file="pie-chart-sex.png", dpi=500)

```

这些是可以在数据集上执行的一些单变量分析示例。这些图形输出帮助我们轻松掌握结果。这些技巧在做商业演示时会非常有用。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 双变量分析

在本节中，我们将介绍双变量分析，以了解两个变量的综合影响以及一个变量对另一个变量的影响。在任何现实生活的例子中，都会有多个变量相互依赖。因此，这种分析将有助于了解这些情况。

快速了解两个变量的最佳方法是散点图。这种直观的表示让我们清楚地了解一个变量对另一个变量的影响。我们可以使用相同的`ggplot`函数来绘制散点图。我们将绘制散点图，以获得`Age`和`Fare`变量之间的关系:

```
ggplot(tdata, aes(x=Fare, y=Age)) +
geom_point(shape=1) + 
geom_smooth(method=lm)
ggsave(file="scatter-plot.png", dpi=500)

```

在前面的案例中，我们使用`geom_smooth`参数绘制了这两个变量之间的关系以及散点图，该散点图绘制了一条显示这两个变量之间关系的额外线性线。执行代码时可能会出现警告，但可以忽略。输出将如下所示:

![Bivariate analysis](Images/B04750_02_15.jpg)

从上图可以很清楚地看出，变量之间存在正相关关系，即当乘客的**年龄**增加时，**票价**趋于增加。此外，我们可以看到，与高价值票价相比，低价值票价的集中度非常高，而年龄呈正态分布。

因此，我们得到了两个变量之间的关系。前面的方法为我们提供了一个定性的度量，但是我们仍然不清楚当乘客的**年龄**增加 *x* 个单位时，乘客的**费用**最有可能增加多少。我们可以使用相关函数获得定量测量，如下所示:

```
cor(tdata$Age, tdata$Fare, use= "pairwise.complete.obs")
 [1] 0.09606669

```

在前面的函数中，我们使用了参数`use`并将其设置为`pairwise.complete.obs`，以确保我们在计算相关性时考虑了两列的值都存在的行；否则，我们可能会得到一个错误的相关值。

尽管可视化表示是交流的最佳方式，交叉列表分析方法是深入挖掘数据集的最佳方式。通常，在任何分析中，交叉列表被广泛用于从数据中获得洞察力。

我们可以使用`xtabs`功能进行交叉列表分析。我们将考虑`Survived`和`Sex`的变量，并使用`ftable`函数获得表格格式的`data`:

```
tab<-xtabs(~Survived+Sex, data=tdata)
ftable(tab)

```

上述命令的输出如下:

![Bivariate analysis](Images/B04750_02_16.jpg)

在前面的例子中，我们得到了实际的数字，但是如果同样的数据以百分比的形式显示，就很容易理解其影响了，这可以使用下面的代码来实现:

```
result <- replace(tab, , sprintf("%.1f%%",prop.table(tab,2)*100))

```

上述命令的输出如下:

![Bivariate analysis](Images/B04750_02_17.jpg)

交叉制表分析非常强大。当广泛应用于数据集中的各种变量时，我们可以很好地理解数据。在大多数情况下，交叉列表被用来探索模式，一旦发现，它们被表示在一个合适的图表中，以便于交流。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 多元分析

在多变量分析的情况下,,我们在研究中考虑两个以上的变量。一般方法是先进行单变量分析，然后进行双变量分析，最后考虑多元分析中的重要变量。

## 交叉列表分析

让我们首先用三个变量进行交叉制表。它非常类似于双变量交叉列表分析，但是这里我们传递的不是两个变量，而是三个变量。使用`ftable`函数，我们可以看到表格数据:

```
tab<-xtabs(~Survived+Sex+SibSp, data=tdata)
ftable(tab)

```

上述命令的输出如下:

![Cross-tabulation analysis](Images/B04750_02_18.jpg)

为了获得百分比的详细信息，我们可以使用下面的代码。由于有三个变量，我们需要指定要计算百分比的列。在下面的代码中，我们指定了`prop.table(tab, 3)`，这意味着将根据第三列`SibSp`计算百分比:

```
result <- replace(tab, , sprintf("%.1f%%",prop.table(tab,3)*100))
data.frame(result)

```

以下是输出:

![Cross-tabulation analysis](Images/B04750_02_19.jpg)

我们还将探索多变量分析中的一些图形表示。

## 图形分析

我们可以查看多个变量的趋势。在下面的例子中，我们将一次比较三个变量。我们将绘制存活和非存活与年龄的关系图，并测量其影响。我们需要使用`sqldf`包，这样我们就可以通过编写 SQL 查询按照我们的要求格式化数据。在 R 环境中加载软件包之前，一定要确保安装了软件包及其依赖项:

```
library(sqldf)
trendData1<- sqldf("select Age, count(PassengerId) from tdata where Survived in ('1') group by Age")
colnames(trendData1) <- c("Age","Survived")
trendData2<- sqldf("select Age, count(PassengerId) from tdata where Survived in ('0') group by Age")
colnames(trendData2) <- c("Age","NotSurvived")
trendData<- sqldf("select a.Age, Survived, NotSurvived from trendData1 a inner join trendData2 b on a.Age = b.Age")
head(trendData)

```

前面命令的输出如下:

![Graphical analysis](Images/B04750_02_20.jpg)

我们编写的代码将数据格式化为前面的格式。我们已经按`Age`分组，以确保`Age`中没有重复，这样我们就可以绘制折线图。然后，使用以下代码重新格式化数据，以便我们可以对多系列折线图只使用一个绘图:

```
library(reshape)
combinedData<- melt(trendData, id = 'Age')
ggplot(combinedData, aes(x = Age, y = value, colour = variable)) + 
geom_line() + 
ylab(label="Survival and Death") + 
xlab("Age of the passenger") + 
scale_colour_manual(values=c("green", "red"))

```

上述命令的输出如下:

![Graphical analysis](Images/B04750_02_21.jpg)

在前面的图中，红线代表死亡人数，绿线代表幸存人数。我们可以推断，20 岁到 30 岁之间的人存活的概率很低。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 总结

在这一章中，我们讲述了如何使用 Titanic 数据集实现各种探索性的数据分析技术。此外，您还学习了描述性统计，使用`summary`函数和箱线图将数据集定量总结为简单的定量陈述或可视化表示。此外，我们还讨论了推断统计，以推断出感兴趣的数据集的属性。此外，您还学习了单变量分析和双变量分析，前者一次仅使用一个变量来理解数据，后者使用两个变量来理解数据。一个变量对另一个变量的影响也包括在内。最后，我们通过执行单变量分析，然后执行双变量分析，研究了考虑两个以上变量的多变量分析，最后，我们考虑了使用分段散点图、热图和表格进行多变量分析的重要变量。

在下一章中，将向您介绍从原始数据中提取模式以及导出隐藏在数据中的序列模式的不同技术。我们还将涉及评估指标，以及如何调整它们来对关联规则进行排序。我们还将讨论可以使用这些技术的商业案例。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 先验分析

为了让执行先验分析，我们需要加载`arules`包。如果软件包尚未安装，请使用`install.packages`功能。

然后，我们可以对事务数据应用 Apriori 算法。在上一节中，我们创建了两个不同的事务数据集。让我们对这个数据集应用 Apriori 算法:

```
rules1<- apriori(Adult,parameter = list(sup = 0.5, conf = 0.9,target="rules"));

```

输出如下所示:

![Apriori analysis](Images/B04750_03_05.jpg)

从前面的输出中，我们可以看到总共生成了 52 条规则。在前面的函数中，我们使用了几个额外的`sup`和`conf`参数，它们分别是`support`和`confidence`。我们将详细研究这些参数，但是现在，我们已经根据输入生成了规则。然后我们可以检查使用`inspect`函数生成的规则，如下所示:

```
inspect(rules1)

```

以下是输出:

![Apriori analysis](Images/B04750_03_06.jpg)

输出在这里继续:

![Apriori analysis](Images/B04750_03_07.jpg)

### 注意

请注意，这只是实际输出的随机快照。

这就是我们如何产生先验规则。从前面的输出中，我们得到了一组频繁的规则。例如，让我们考虑第 28 行。在这种情况下，每当用户的`workclass`是`Private`并且`native-country`是`United-States`时，那么`capital-loss`最有可能是`None`。前面的代码可以应用于第二个事务性数据集，我们可以生成规则。我们还有`support`、`confidence`和`lift`。我们将详细了解其中之一。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 支持、信心和鼓舞

支撑、信心和升力是需要设置的重要参数，由定义输出。基于业务案例，我们相应地设置它们。下面我们来详细看看。

## 支持

支持度是关联规则提取过程中的一个重要手段。它定义了规则在数据集中适用的频率。例如，让我们考虑一下上一节中的第 23 条规则，这里我们有`lhs`中的`{sex: "Male," native: "United States"}`和 rhs 中的`{capital-loss: None}`；这里有`"0.5661`的`support`，表示包含`{sex: "Male", native: "United States", capital-loss: None}`的交易数约占总交易数的 56.61%。

一般来说，支持度非常低的规则会被忽略，因为它们大多是偶然发生的，不重要，并且对业务没有意义，因为发生的机会非常少，不值得监控。然而，在极少数情况下，当事务和项目集的数量巨大时，有可能大多数事件不会重复，并且将导致非常低的支持。例如，与购买的其他物品相比，购买笔和笔记本的人数会更少，但这不是偶然发生的。

## 自信

置信度是关联规则分析中一个同样重要的度量。它决定了包含`{sex: Mal, native: United States}`的事务中`{capital-loss: None}`出现的频率。在我们的例子中，置信度为 94.62%，这意味着在所有有`{sex: Male, native: United States}`的交易中，94.62%的情况下，资本损失的价值为零。因此，置信度是一致性的度量。

## 抬起

Lift 是过滤规则的另一个最重要的参数。它表示每当交易中发生`{sex: Male and native: United States}`时，右边的变量`{capital-loss: None}`最有可能出现的程度。

`{sex: "Male" and native: "United States" → capital-loss: None}/support{capital-loss: None}`。

因此，当升力比小于 1 时，这意味着该规则不太可能出现，不需要关注。因此，规则的入围不仅基于支持度和置信度，还基于升力比，升力比应该大于 1。在我们的例子中，升力比小于 1，所以我们可以忽略产生的规则。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 生成过滤规则

这些规则是使用`arules`包的`apriori`函数生成的。这次让我们为另一个数据集生成规则:

```
rules2<- apriori(sampdata,parameter = list(sup = 0.45, conf = 0.9, target="rules"));

```

以下是前面命令的输出:

![Generating filtering rules](Images/B04750_03_08.jpg)

我们将`support`的阈值设置为`0.45`，将`confidence`的阈值设置为`0.9`。从前面的输出中，我们可以看到大约生成了`40`规则。我们可以使用以下代码按照`lift`比率的降序打印它们:

```
inspect(head(sort(rules2, by="lift"),10))

```

输出如下所示:

![Generating filtering rules](Images/B04750_03_09.jpg)

从输出可以清楚地看出，无论何时`i128`和`i141`项同时出现，最有可能出现的是`i139`项。另外，当`lift`值大于 2 时，它进一步重申该组合最有可能发生。我们还可以使用`quality`函数获得基于`support`、`confidence`和`lift`组合的顶级规则:

```
head(quality(rules2));

```

输出如下所示:

![Generating filtering rules](Images/B04750_03_10.jpg)<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 策划

让我们绘图，然后查看样本数据集`sampdata`。从输出中可以清楚地看到，事务的数量在 100 左右，项目的数量在 500 左右。以下代码还包括将输出保存到当前工作目录下的本地文件中:

## 数据集

```
image(sampdata)
dev.copy(png,filename="sampdata.png", width=600, height=875);
dev.off ();

```

前面的命令的输出如下:

![Dataset](Images/B04750_03_11.jpg)

在`Adult`数据集中，交易数量巨大；如果项目数量更少，它将显示为一条直线。

## 规则

为了绘制规则，我们需要将`arulesviz`包加载到 R 环境中。如果该软件包尚未安装，使用`install.packages`功能进行安装:

```
install.packages("arulesViz")
library(arulesViz)

```

我们可以使用`plot`函数来绘制规则。该图将分别在 *x* 轴和 *y* 轴上具有**支持度**和**置信度**，阴影用于表示升力。我们还可以使用以下代码中的一些参数来更改表示形式，其中我们提到了度量应该是 support 和 lift，也就是说，通过散点图来显示，轴上是度量，置信度通过阴影来度量:

```
plot(rules1)

```

上述命令的输出如下:

![Rules](Images/B04750_03_12.jpg)

默认情况下，`support`和`confidence`分别标绘在`x`轴和`y`轴上，让我们改变它，看看它如何影响标绘:

```
plot(rules1, measure=c("support","lift"), shading="confidence")

```

输出如下:

![Rules](Images/B04750_03_13.jpg)<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 顺序数据集

到目前为止，我们根据交易和事件以及各种事件的同时发生生成了各种规则。现在，我们将考虑数据以及事件发生的顺序。序列分析是通过历史数据中的模式来预测事件发生的一种非常流行的方法。

为了理解序列分析，我们将考虑一个序列数据集。`zaki`数据集包含随`arulesSequences`包一起出现的顺序数据。我们使用`summary`函数来获取序列数据集的细节。为了更好地理解，我们将数据集转换为数据框:

```
library(arulesSequences)
data(zaki)
summary(zaki)

```

前面命令的输出如下:

![Sequential dataset](Images/B04750_03_14.jpg)

让我们使用以下命令来查看数据:

```
as(zaki, "data.frame")

```

上述命令的输出如下:

![Sequential dataset](Images/B04750_03_15.jpg)

该数据框以非常友好的方式传达大量信息，便于理解。`transactionID.size`参数保存序列中的项目数，列项目保存实际项目。

另一方面，我们也可以创建一个保存事件序列的数据帧，然后我们可以使用`read_baskets`函数将其转换成事务数据集。但是，它要求数据包含三列，即`("sequenceID","eventID","SIZE")`，其中`sequenceID`和`eventID`的组合是唯一的，而`SIZE`包含连续事件的数量，后面是由逗号分隔的事件序列。

如以下代码中所述，名为`sample`的数据集保存所解释的数据，并被转换为事务数据类型，以便 Apriori 序列算法使用:

```
zaki <- read_baskets(con = "sample.txt", info = c("sequenceID","eventID","SIZE"))

```

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 先验序列分析

先验序列分析是一种在数据的序列中寻找统计显著模式的技术。它在购物用例中被广泛使用——我们可以在知道客户购买了一件商品的情况下预测下一次最有可能的购买是什么。

我们可以使用`cspade`函数在 R 中实现 Apriori 序列算法。在下面的代码中，我们使用`support`而不是`0.55`来过滤规则:

```
seq_rules<- cspade(zaki, parameter = list(support = 0.55), control   = list(verbose=TRUE))

```

输出如下所示:

![Apriori sequence analysis](Images/B04750_03_16.jpg)

前面的输出将基于指定的参数生成规则。要了解如何使用其他参数，请参考位于[https://cran . r-project . org/web/packages/arulesequences/arulesequences . pdf](https://cran.r-project.org/web/packages/arulesSequences/arulesSequences.pdf)的软件包文档。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 了解结果

我们可以使用下面的代码检查由`cspade`函数生成的规则的细节。`summary`函数为您提供关于数据集的突出显示，例如数据集中出现的序列的数量、最常出现的项目、序列长度的频率表以及支持度的分布:

```
summary(seq_rules)

```

上述命令的输出如下:

![Understanding the results](Images/B04750_03_17.jpg)

我们可以通过使用下面的代码将规则转换成一个数据帧来看一看这些规则。我们可以检查各种序列以及每个序列的支持分数。默认情况下，生成的规则根据支持得分排序。

```
as(seq_rules, "data.frame")

```

上述命令的输出如下:

![Understanding the results](Images/B04750_03_18.jpg)

## 参考

关于规则绘制的细节，请参考[http://statistical-research . com/association-rule-learning-and-the-apriori-algorithm/](http://statistical-research.com/association-rule-learning-and-the-apriori-algorithm/)的这篇精彩博客。

有关软件包的详细文档，请参考以下链接:

*   [https://cran . r-project . org/web/packages/arulesequences/arulesequences . pdf](https://cran.r-project.org/web/packages/arulesSequences/arulesSequences.pdf)
*   [https://cran.r-project.org/web/packages/arules/index.html](https://cran.r-project.org/web/packages/arules/index.html)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 商业案例

了解了相似性分析和序列分析之后，让我们看看这些技术在不同领域的商业案例:

*   在一个在线音乐播放网站中，我们可以了解用户在没有自动/随机播放的情况下收听音乐的顺序
*   在零售店中，我们可以了解顾客挑选商品的顺序，以及顾客倾向于一起购买的各种商品
*   我们甚至可以从 LinkedIn 等网站的职业数据中了解个人的职业道路

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 总结

在本章中，我们了解了事务性数据集的重要性和用途，以及可以在事务性数据集上实现的 Apriori 规则、算法和分析，并提供了示例。在先验分析中定义输出的支持度、置信度和提升参数已经讨论过了。我们已经看到了可以生成并应用于数据集的过滤规则。此外，我们以图表的形式绘制数据集，以提供图形表示，并更清楚地了解数据集的趋势。已经解释了可用于通过历史数据中的模式来预测事件发生的顺序数据集。已经解释了在数据序列中寻找统计显著模式的先验序列分析技术。我们已经阅读并理解了通过上述分析算法提取和分析的数据集，最后，给出了理解播放列表中播放的音乐序列、杂货店购物序列和个人职业道路的练习。

在下一章中，我们将演示如何以及何时执行聚类分析，如何确定数据集的理想聚类数，以及如何使用 r 实现聚类。本章还将重点介绍层次聚类(它与普通聚类有何不同)以及聚类的可视化。