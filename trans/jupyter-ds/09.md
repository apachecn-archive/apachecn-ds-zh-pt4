

# 九、Jupyter 和机器学习

在本章中，我们将使用 Jupyter 下的几种机器学习算法。我们用 R 和 Python 两种语言来描述 Jupyter 开发人员可用的选择范围。



# 朴素贝叶斯

朴素贝叶斯是一种算法，它根据贝叶斯定理使用概率对数据进行分类，以实现特征的强独立性。贝叶斯定理根据先验条件估计事件的概率。因此，总的来说，我们使用一组特征值来估计一个值，假设当这些特征具有相似的值时，相同的条件成立。



# 使用 R 的朴素贝叶斯

我们的第一个朴素贝叶斯实现使用了 R 编程语言。算法的 R 实现编码在`e1071`库中。`e1071`似乎是开发该软件包的学校的部门标识符。

我们首先安装软件包，并加载库:

```
#install.packages("e1071", repos="http://cran.r-project.org") 
library(e1071) 
library(caret) 
set.seed(7317) 
data(iris) 
```

关于这些步骤的一些注意事项:

*   调用`install.packages`被注释掉，因为我们不想每次运行脚本时都运行它。
*   `e1071`是朴素贝叶斯算法包。
*   `caret`包包含一个随机划分数据集的方法。
*   我们设置`seed`以便能够重现结果。
*   我们在这个例子中使用了`iris`数据集。具体来说，使用其他`iris`因子来预测物种。

该包的无效内容如下:

```
model <- naiveBayes(response ~ ., data=training) 
prediction <- predict(model, test, type="class") 
```

其中`naiveBayes`的参数为:

*   形式为 *y ~ x1 + x2 的公式....*-尝试根据 *x1，x2 预测 *y* ，...*
*   数据帧
*   可选拉普拉斯平滑
*   基于布尔过滤器的可选数据子集
*   处理`na`值(`na.action`)的可选功能—默认为通过

一旦我们建立了模型，我们就可以尝试使用带有以下参数的`predict()`函数进行预测:

*   型号(来自之前的呼叫)
*   数据帧
*   键入数据是类数据还是原始数据(条件)

因此，我们继续以`iris`为例:

```
trainingIndices <- createDataPartition(iris$Species, p=0.75, list=FALSE) 
training <- iris[trainingIndices,] 
testing <- iris[-trainingIndices,] 
nrow(training) 
nrow(testing) 
114 
36 
```

我们将数据分为 75%用于培训，25%用于测试，正如您在每个数据框中看到的行数。

接下来，我们构建模型-我们尝试从数据框的其他要素/列预测`Species`:

```
model <- naiveBayes(Species ~ ., data=training) 
model 
```

![](../images/00164.jpeg)

有趣的是，先验假设是各种可能性的平均分配。萼片长度、宽度和花瓣长度对物种有很大的影响。

我们根据模型和测试数据进行预测:

```
prediction <- predict(model, testing, type="class") 
```

现在，我们需要衡量模型的准确性。通常我们可以使用散点图，使用来自实际的`x`和来自预测的`y`，但是我们有分类数据。我们可以构建实际值与预测值的向量，并在新的结果数据框架中比较两者:

```
results <- data.frame(testing$Species, prediction) 
results["accurate"] <- results['testing.Species'] == results['prediction'] 
nrow(results) 
nrow(results[results$accurate == TRUE,]) 
36 
35 
```

我们最终得到了一个准确率为 97% ( `35` / `36`)的模型。这是一个非常好的表现水平，几乎在优秀(+/- 2%)的统计界限内。



# 使用 Python 的朴素贝叶斯

算法的 Python 实现在`sklearn`库中。整个过程就简单多了。首先，加载`iris`数据集:

```
from sklearn import datasets 
irisb = datasets.load_iris() 
iris = irisb['data'] 
iris.shape 
```

调用内置的高斯朴素贝叶斯估计器一步完成模型和预测:

```
from sklearn.naive_bayes import GaussianNB 
gnb = GaussianNB() 
y_pred = gnb.fit(irisb.data, irisb.target).predict(irisb.data) 
```

确定模型的准确性:

```
print("Number of errors out of a total %d points : %d"  
      % (irisb.data.shape[0],(irisb.target != y_pred).sum())) 
Number of errors out of a total 150 points : 6
```

我们最终得到了非常相似的估计精度结果。



# 最近邻估计量

使用最近邻，我们有一个未分类的对象和一组已分类的对象。然后，我们获取未分类对象的属性，与现有的已知分类进行比较，并选择与未知分类最接近的分类。比较距离解析为欧几里得几何，计算两点之间的距离(已知属性与未知属性相比的位置)。



# 使用 R 的最近邻

对于这个例子，我们使用来自`ics.edu`的住房数据。首先，我们加载数据并分配列名:

```
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data") 
colnames(housing) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PRATIO", "B", "LSTAT", "MDEV") 
summary(housing) 
```

我们对数据重新排序，因此关键字(房价`MDEV`)是升序的:

```
housing <- housing[order(housing$MDEV),] 
```

现在，我们可以将数据分为训练集和测试集:

```
#install.packages("caret") 
library(caret) 
set.seed(5557) 
indices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE) 
training <- housing[indices,] 
testing <- housing[-indices,] 
nrow(training) 
nrow(testing) 
381 
125 
```

我们使用这两个集合来构建最近邻模型:

```
library(class) 
knnModel <- knn(train=training, test=testing, cl=training$MDEV) 
knnModel 
10.5 9.7 7 6.3 13.1 16.3 16.1 13.3 13.3... 
```

让我们看看结果:

```
plot(knnModel) 
```

![](../images/00165.jpeg)

有一个轻微的泊松分布，高点靠近左侧。我认为这作为*自然*数据是有意义的。开始和结束的尾巴戏剧性地离开页面。

这个模型的准确性如何？我没有找到一种干净的方法将`knnModel`中的预测因子转换成数值，所以我将它们提取到一个平面文件中，然后分别加载它们:

```
predicted <- read.table("housing-knn-predicted.csv") 
colnames(predicted) <- c("predicted") 
predicted
```

| 预测 |
| Ten point five |
| Nine point seven |
| Seven |

然后我们可以建立一个`results`数据帧:

```
results <- data.frame(testing$MDEV, predicted) 
```

计算我们的准确度:

```
results["accuracy"] <- results['testing.MDEV'] / results['predicted'] 
head(results) 
mean(results$accuracy) 
1.01794816307793
```

| **测试。MDEV** | **预测的** | **精度** |
| `5.6` | `10.5` | `0.5333333` |
| `7.2` | `9.7` | `0.7422680` |
| `8.1` | `7.0` | `1.1571429` |
| `8.5` | `6.3` | `1.3492063` |
| `10.5` | `13.1` | `0.8015267` |
| `10.8` | `16.3` | `0.6625767` |

因此，我们在测试数据的 2% ( `1.01`)内进行估计。



# 使用 Python 的最近邻

在 Python 中，我们有非常相似的产生最近邻估计的步骤。
首先，我们导入要使用的包:

```
from sklearn.neighbors import NearestNeighbors 
import numpy as np 
import pandas as pd 
```

Numpy 和Pandas是标配。最近邻是`sklearn`功能之一。
现在，我们加载我们的住房数据:

```
housing = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data",
                       header=None, sep='\s+') 
housing.columns = ["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", \ 
"DIS", "RAD", "TAX", "PRATIO", \ 
"B", "LSTAT", "MDEV"] 
housing.head(5) 
```

![](../images/00166.jpeg)

我们之前在 R.
中看到的相同数据让我们看看它有多大:

```
len(housing) 
506 
```

并将数据分解成`training`和`testing`组:

```
mask = np.random.rand(len(housing)) < 0.8 
training = housing[mask] 
testing = housing[~mask] 
len(training) 
417 
len(testing) 
89 
```

查找最近的邻居:

```
nbrs = NearestNeighbors().fit(housing) 
```

显示它们的指数和距离。指数变化很大。距离似乎是成带状的:

```
distances, indices = nbrs.kneighbors(housing) 
indices 
array([[  0, 241,  62,  81,   6], 
       [  1,  47,  49,  87,   2], 
       [  2,  85,  87,  84,   5], 
       ...,  
       [503, 504, 219,  88, 217], 
       [504, 503, 219,  88, 217], 
       [505, 502, 504, 503,  91]], dtype=int32) 
distances 
array([[  0\.        ,  16.5628085 , 17.09498324,18.40127391, 
         19.10555821], 
       [  0\.        ,  16.18433277, 20.59837827, 22.95753545, 
         23.05885288] 
       [  0\.        ,  11.44014392, 15.34074743, 19.2322435 , 
         21.73264817], 
       ...,  
       [  0\.        ,   4.38093898,  9.44318468, 10.79865973, 
         11.95458848], 
       [  0\.        ,   4.38093898,  8.88725757, 10.88003717, 
         11.15236419], 
       [  0\.        ,   9.69512304, 13.73766871, 15.93946676, 
         15.94577477]]) 
```

从`training`集合构建最近邻模型:

```
from sklearn.neighbors import KNeighborsRegressor 
knn = KNeighborsRegressor(n_neighbors=5) 
x_columns = ["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PRATIO", "B", "LSTAT"] 
y_column = ["MDEV"] 
knn.fit(training[x_columns], training[y_column]) 
KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', 
          metric_params=None, n_jobs=1, n_neighbors=5, p=2, 
          weights='uniform') 
```

有趣的是，使用 Python 我们不必单独存储模型。方法是有状态的。

做出我们的预测:

```
predictions = knn.predict(testing[x_columns]) predictions array([[ 20.62],
 [ 21.18], [ 23.96], [ 17.14], [ 17.24], [ 18.68], [ 28.88], 
```

确定我们对房价的预测有多准确:

```
columns = ["testing","prediction","diff"] 
index = range(len(testing)) 
results = pd.DataFrame(index=index, columns=columns) 

results['prediction'] = predictions 

results = results.reset_index(drop=True) 
testing = testing.reset_index(drop=True) 
results['testing'] = testing["MDEV"] 

results['diff'] = results['testing'] - results['prediction'] 
results['pct'] = results['diff'] / results['testing'] 
results.mean() 
testing       22.159551 
prediction    22.931011 
diff          -0.771461 
pct           -0.099104 
```

我们的平均差值为，平均值为`22`。这应该意味着大约 3%的平均百分比差异，但是计算的平均百分比差异接近 10%。所以，我们在 Python 下估计不好。



# 决策树

在本节中，我们将使用决策树来预测值。决策树有一个逻辑流程，用户根据属性沿着树向下到根级别进行决策，然后在根级别提供分类。

对于这个例子，我们使用汽车的特性，比如车重，来决定汽车是否能行驶很长的里程。该信息摘自 https://alliance.seas.upenn.edu/~cis520/wiki/index.php?的页面 n =讲座。决策树。我将数据复制到 Excel 中，然后将其作为 CSV 格式写入本例中。



# R 中的决策树

我们加载库来使用`rpart`和`caret`。`rpart`有决策树建模包。`caret`具有数据分区功能:

```
library(rpart) 
library(caret) 
set.seed(3277) 
```

我们加载我们的`mpg`数据集，并将其分成训练和测试集:

```
carmpg <- read.csv("car-mpg.csv") 
indices <- createDataPartition(carmpg$mpg, p=0.75, list=FALSE) 
training <- carmpg[indices,] 
testing <- carmpg[-indices,] 
nrow(training) 
nrow(testing) 
33 
9 
```

我们开发了一个基于其他因素预测`mpg`可接受性的模型:

```
fit <- rpart(mpg ~ cylinders + displacement + horsepower + weight + acceleration +
             modelyear + maker, method="anova", data=training) 
fit 
n= 33  

node), split, n, deviance, yval 
      * denotes terminal node 

1) root 33 26.727270 1.909091   
2) weight>=3121.5 10  0.000000 1.000000 * 
3) weight< 3121.5 23 14.869570 2.304348   
6) modelyear>=78.5 9  4.888889 1.888889 * 
7) modelyear< 78.5 14  7.428571 2.571429 * 
```

该显示是决策树的文本显示。您可以以图形方式看到决策树，如下所示:

```
plot(fit) 
text(fit, use.n=TRUE, all=TRUE, cex=.5) 
```

![](../images/00167.jpeg)

这似乎是一个非常简单的模型。1980 年的里程数肯定有变化，因为这是决策树的主要驱动力。
最后，我们预测值，并将其与我们的`testing`集合进行比较:

```
predicted <- predict(fit, newdata=testing) 
predicted 
testing 
```

![](../images/00168.jpeg)

看起来这个包已经将`Bad`、`OK`和`Good`转换成了一个等价的数字，其中`1`是`Bad`，其他的是`OK`或`Good`。总的来说，我们不确定我们是否有一个好的模型。显然没有太多的数据可以利用。一个更大的测试集将清理模型。



# Python 中的决策树

我们可以在 Python 中执行相同的分析。加载一些要使用的导入:

```
import pandas as pd 
import numpy as np 
from os import system 
import graphviz #pip install graphviz  
from sklearn.cross_validation import train_test_split 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score 
from sklearn import tree 
```

读入`mpg`数据文件:

```
carmpg = pd.read_csv("car-mpg.csv") 
carmpg.head(5) 
```

![](../images/00169.jpeg)

将数据分解为因素和结果:

```
columns = carmpg.columns 
mask = np.ones(columns.shape, dtype=bool) 
i = 0 #The specified column that you don't want to show 
mask[i] = 0 
mask[7] = 0 #maker is a string 
X = carmpg[columns[mask]] 
Y = carmpg["mpg"] 
```

在训练集和测试集之间拆分数据:

```
X_train, X_test, y_train, y_test  
= train_test_split( X, Y, test_size = 0.3,  
random_state = 100) 
```

创建决策树模型:

```
clf_gini = tree.DecisionTreeClassifier(criterion = "gini",  
random_state = 100, max_depth=3, min_samples_leaf=5) 
```

计算模型拟合度:

```
clf_gini.fit(X_train, y_train) 
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3, 
            max_features=None, max_leaf_nodes=None, 
            min_impurity_split=1e-07, min_samples_leaf=5, 
            min_samples_split=2, min_weight_fraction_leaf=0.0, 
            presort=False, random_state=100, splitter='best') 
```

绘制树形图:

```
#I could not get this to work on a Windows machine 
#dot_data = tree.export_graphviz(clf_gini, out_file=None,  
#                         filled=True, rounded=True,   
#                         special_characters=True)   
#graph = graphviz.Source(dot_data)   
#graph 
```



# 神经网络

我们可以将住房数据建模为神经网络，其中不同的数据元素是系统的输入，网络的输出是房价。通过神经网络，我们最终得到一个图形模型，该模型提供了应用于每个输入的因素，以便得出我们的房价。



# R 中的神经网络

r 中有一个可用的神经网络包。我们将它加载到:

```
#install.packages('neuralnet', repos="http://cran.r-project.org") 
library("neuralnet") 
```

加载房屋数据:

```
filename = "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data" 
housing <- read.table(filename) 
colnames(housing) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX",  
                       "RM", "AGE", "DIS", "RAD", "TAX", "PRATIO", 
                       "B", "LSTAT", "MDEV") 
```

将住房数据分成训练集和测试集(我们在前面的例子中已经看到了这种编码):

```
housing <- housing[order(housing$MDEV),] 
#install.packages("caret") 
library(caret) 
set.seed(5557) 
indices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE) 
training <- housing[indices,] 
testing <- housing[-indices,] 
nrow(training) 
nrow(testing) 
testing$MDEV 
```

计算我们的`neuralnet`模型:

```
nnet <- neuralnet(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX  
                  + RM + AGE + DIS + RAD + TAX + PRATIO  
                  + B + LSTAT, 
                  training, hidden=10, threshold=0.01) 
nnet 
```

`neuralnet`型号的显示信息相当广泛。第一组显示如下。不清楚这些要点是否有用:

```
$call 
neuralnet(formula = MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM +  
    AGE + DIS + RAD + TAX + PRATIO + B + LSTAT, data = training,  
    hidden = 10, threshold = 0.01) 

$response 
    MDEV 
399  5.0 
406  5.0 
... 
$covariate 
           [,1]  [,2]  [,3] [,4]   [,5]  [,6]  [,7]    [,8] [,9] [,10] [,11] 
  [1,] 38.35180   0.0 18.10    0 0.6930 5.453 100.0  1.4896   24   666  20.2 
  [2,] 67.92080   0.0 18.10    0 0.6930 5.683 100.0  1.4254   24   666  20.2 
  [3,]  9.91655   0.0 18.10    0 0.6930 5.852  77.8  1.5004   24   666  20.2 
.... 
```

显示模型:

```
plot(nnet, rep="best") 
```

![](../images/00170.jpeg)

这只是图表的上半部分。正如你所看到的，每个因素都被调整到模型中以得出我们的房价。这是没有用的——不可能每个因素都那么重要。
确定我们使用该模型的准确性:

```
results <- compute(nnet, testing[,-14]) 
diff <- results$net.result - testing$MDEV 
sum( (diff - mean(diff) )^2 ) #sum of squares 
9275.74672 
```

鉴于这个模型看起来非常不准确，我不确定在 Python 中进行同样的步骤是否有益。



# 随机森林

随机森林算法尝试许多随机决策树，并提供在用于驱动模型的参数范围内工作最佳的树。



# R 中的随机森林

对于 R，我们包括了我们将要使用的包:

```
install.packages("randomForest", repos="http://cran.r-project.org") 
library(randomForest) 
```

加载数据:

```
filename = "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data" 
housing <- read.table(filename) 
colnames(housing) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX",  
                       "RM", "AGE", "DIS", "RAD", "TAX", "PRATIO", 
                       "B", "LSTAT", "MDEV") 
```

分割它:

```
housing <- housing[order(housing$MDEV),] 
#install.packages("caret") 
library(caret) 
set.seed(5557) 
indices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE) 
training <- housing[indices,] 
testing <- housing[-indices,] 
nrow(training) 
nrow(testing) 
```

计算我们的模型:

```
forestFit <- randomForest(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX  
                  + RM + AGE + DIS + RAD + TAX + PRATIO  
                  + B + LSTAT, data=training) 
forestFit 
Call: 
 randomForest(formula = MDEV ~ CRIM + ZN + INDUS + CHAS + NOX +      RM + AGE + DIS + RAD + TAX + PRATIO + B + LSTAT, data = training)  
               Type of random forest: regression 
                     Number of trees: 500 
No. of variables tried at each split: 4 

          Mean of squared residuals: 11.16163 
                    % Var explained: 87.28 
```

这是关于一个模型的更多信息显示之一——我们看到这个模型解释了 87%的变量。

做出我们的预测:

```
forestPredict <- predict(forestFit, newdata=testing) 
See how well the model worked: 
diff <- forestPredict - testing$MDEV 
sum( (diff - mean(diff) )^2 ) #sum of squares 
1391.95553131418 
```

这是我们在本章制作的模型中平方和最低的一个。



# 摘要

在本章中，我们使用了几种机器学习算法，其中一些是在 R 和 Python 中进行比较和对比的。我们使用朴素贝叶斯来确定如何使用这些数据。我们以几种不同的方式应用最近邻来查看我们的结果。我们用决策树提出了一个预测算法。我们尝试用神经网络来解释房价。最后，我们使用随机森林算法来做同样的事情——结果最好！在下一章中，我们将探讨如何优化 Jupyter 笔记本。