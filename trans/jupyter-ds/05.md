

# 和 Jupyter 一起休息

在本章中，我们将在 Jupyter 中使用 R 编码。我认为 R 是 Jupyter 中预期使用的主要语言之一。Jupyter 用户可以使用该语言的全部内容。



# 如何为 Jupyter 设置 R

在过去，必须安装 Jupyter、Python 等独立组件才能拥有一个工作系统。借助 Continuum Analytics，安装 Jupyter 并将 R engine 添加到 Jupyter 解决方案集的过程非常简单，并且可以在 Windows 和 Mac 上运行。

假设您已经安装了 conda，我们有一个命令来为 Jupyter 添加对 R 编程的支持:

```
conda install -c r r-essentials  
```

此时，当您启动 Jupyter 时，其中一个列出的内核现在将是 r。



# 2016 年美国大选人口统计数据分析

为了了解 R 开发者可以获得的资源，我们可以看看 2016 年的选举数据。在这种情况下，我从维基百科([https://en . Wikipedia . org/wiki/United _ States _ presidential _ election，_2016](https://en.wikipedia.org/wiki/United_States_presidential_election,_2016) )中提取，具体来说就是名为“2016 年按人口统计分组的总统投票”的表格。我们有下面的代码。

定义一个辅助函数，这样我们就可以很容易地打印出值。新的`printf`函数接受传递给`(...)`的任何参数，并将它们传递给`sprintf`:

```
printf <- function(...)print(sprintf(...))  
```

我已经将单独的人口统计数据存储到不同的**(**制表符分隔值)**文件中，可以使用以下编码读入这些文件。对于每个表，我们使用`read.csv`函数，并将字段分隔符指定为制表符，而不是默认的逗号。然后，我们使用`head`函数来显示关于加载的数据帧的信息:**

```
age <- read.csv("Documents/B05238_05_age.tsv", sep="\t")
head(age)

education <- read.csv("Documents/B05238_05_education.tsv", sep="\t")
head(education)

gender <- read.csv("Documents/B05238_05_gender.tsv", sep="\t")
head(gender)

ideology <- read.csv("Documents/B05238_05_ideology.tsv", sep="\t")
head(ideology)

income <- read.csv("Documents/B05238_05_income.tsv", sep="\t")
head(income)

orientation <- read.csv("Documents/B05238_05_orientation.tsv", sep="\t")
head(orientation)

party <- read.csv("Documents/B05238_05_party.tsv", sep="\t")
head(party)

race <- read.csv("Documents/B05238_05_race.tsv", sep="\t")
head(race)

region <- read.csv("Documents/B05238_05_region.tsv", sep="\t")
head(region)

religion <- read.csv("Documents/B05238_05_religion.tsv", sep="\t")
head(religion)  
```

顶部是列的显示。每一行都显示该行中那些列的值。这些`read`操作中的每一个都会产生如下显示:

![](../images/00080.gif)

现在，我们可以找到每个道岔的主要特征:

```
printf("Most Clinton voters from %s",age[which.max(age$Clinton),'age'])
printf("Most Clinton voters from %s",education[which.max(education$Clinton),'education'])
printf("Most Clinton voters from %s",gender[which.max(gender$Clinton),'gender'])
printf("Most Clinton voters from %s",ideology[which.max(ideology$Clinton),'ideology'])
printf("Most Clinton voters from %s",income[which.max(income$Clinton),'income'])
printf("Most Clinton voters from %s",orientation[which.max(orientation$Clinton),'orientation'])
printf("Most Clinton voters from %s",party[which.max(party$Clinton),'party'])
printf("Most Clinton voters from %s",race[which.max(race$Clinton),'race'])
printf("Most Clinton voters from %s",region[which.max(region$Clinton),'region'])
printf("Most Clinton voters from %s",religion[which.max(religion$Clinton),'religion'])

printf("Most Trump voters from %s",age[which.max(age$Trump),'age'])
printf("Most Trump voters from %s",education[which.max(education$Trump),'education'])
printf("Most Trump voters from %s",gender[which.max(gender$Trump),'gender'])
printf("Most Trump voters from %s",ideology[which.max(ideology$Trump),'ideology'])
printf("Most Trump voters from %s",income[which.max(income$Trump),'income'])
printf("Most Trump voters from %s",orientation[which.max(orientation$Trump),'orientation'])
printf("Most Trump voters from %s",party[which.max(party$Trump),'party'])
printf("Most Trump voters from %s",race[which.max(race$Trump),'race'])
printf("Most Trump voters from %s",region[which.max(region$Trump),'region'])
printf("Most Trump voters from %s",religion[which.max(religion$Trump),'religion'])  
```

克林顿的结果如下:

![](../images/00081.gif)

特朗普的结果如下:

![](../images/00082.gif)

有趣的是，支持两位候选人的多数群体之间没有重叠。我认为，各方故意针对不同的群体，以避免重叠。必须有一个花钱的指导方针，通过向已知的、感兴趣的群体做广告，而不是争夺不明确的人群。



# 分析 2016 年选民登记和投票

类似地，我们可以将选民登记与实际投票进行对比(使用来自[https://www . census . gov/data/tables/time-series/demo/voting-and-registration/P20-580 . html](https://www.census.gov/data/tables/time-series/demo/voting-and-registration/p20-580.html)的普查数据)。

首先，我们加载数据集并显示头部信息，以直观地检查加载是否准确:

```
df <- read.csv("Documents/B05238_05_registration.csv")
summary(df)  
```

![](../images/00083.jpeg)

所以，我们有一些各州的登记和投票信息。使用 R 通过`plot`命令自动绘制出 *x* 和 *y* 格式的所有数据:

```
plot(df)  
```

我们特别关注登记投票和实际投票之间的关系。我们可以在下图中看到，大多数数据是高度相关的(正如大多数关系的 45 度角所证明的那样):

![](../images/00084.jpeg)

我们可以使用 Python 产生类似的结果，但是图形显示却差得很远。

导入我们在示例中使用的所有包:

```
from numpy import corrcoef, sum, log, arange from numpy.random import rand from pylab import pcolor, show, colorbar, xticks, yticks import pandas as pd import matplotlib from matplotlib import pyplot as plt 
```

用 Python 读取 CSV 文件非常类似。我们呼吁熊猫在文件中读到:

```
df 
```

如果数据框中有字符串数据，pandas 会抛出错误，所以只需删除列(带有州名):

```
del df['state'] #pandas do not deal well with strings 
```

一个近似的 Python 函数是`corr()`函数，它打印出数据框中项目之间所有互相关的数值。您可以浏览数据，寻找接近`1.0`的相关值:

```
#print cross-correlation matrix print(df.corr()) 
```

类似地，我们有`corrcoef()`函数，它为数据帧中相似相关的项目提供颜色强度。我没有找到标记相关项目的方法:

```
#graph same fig = plt.gcf() fig.set_size_inches(10, 10)
# plotting the correlation matrix R = corrcoef(df) pcolor(R) colorbar() yticks(arange(0.5,10.5),range(0,10)) xticks(arange(0.5,10.5),range(0,10)) show()
```

![](../images/00085.jpeg)

我们希望看到注册和投票之间相关性的实际数值。我们可以通过调用`cor`函数来传入两个感兴趣的数据点，如下所示:

```
cor(df$voted,df$registered)
0.998946393424037
```

不同机器类别的实际相关值可能不同。这也会对后续价值产生涓滴效应。

有了 99%的相关性，我们几乎是完美的。

我们可以使用数据点通过`lm`函数得出一条回归线，这里我们声明 *lm(y ~(预测值)x)* :

```
fit <- lm(df$voted ~ df$registered)
fit
Call:
lm(formula = df$voted ~ df$registered)

Coefficients:
 (Intercept)  df$registered 
 -4.1690         0.8741  
```

从这个输出中，给定一个注册的数字，我们将它乘以 87%，然后减去 4，得到实际投票者的人数。同样，数据是相互关联的。

我们可以通过调用`plot`函数并传入`fit`对象来显示回归线的特征(`par`函数用于布局输出——在本例中是四个图形的 2x2 矩阵显示):

```
par(mfrow=c(2,2))
plot(fit)  
```

![](../images/00086.jpeg)

然后，在 2x2 显示的第二部分，我们有这两个图形:

![](../images/00087.jpeg)

从前面的图中，我们可以看到以下内容:

*   残差值接近于零，直到我们得到非常大的数字
*   在大部分范围内，理论分位数与标准化残差非常吻合
*   拟合值与标准化残差之间的差距没有我预期的那么大
*   标准化残差与杠杆比率在大多数值中显示了相同的紧密配置。

总的来说，我们仍然有一个非常好的数据模型。我们可以看到调用`residuals`函数的回归的残差，如下所示:

![](../images/00088.gif)

```
... (for all 50 states)  
```

对于如此高度相关的数据，残值比我预期的要大。我原以为会看到非常小的数字，而不是数百的数值。

像往常一样，让我们展示一下我们得出的模型的摘要:

```
summary(fit)
Call:
lm(formula = df$voted ~ df$registered)

Residuals:
 Min      1Q  Median      3Q     Max 
-617.33  -29.69    0.83   30.70  351.27 

Coefficients:
 Estimate Std. Error t value Pr(>|t|) 
(Intercept)   -4.169018  25.201730  -0.165    0.869 
df$registered  0.874062   0.005736 152.370   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 127.9 on 49 degrees of freedom
Multiple R-squared:  0.9979,  Adjusted R-squared:  0.9979 
F-statistic: 2.322e+04 on 1 and 49 DF,  p-value: < 2.2e-16  
```

根据运行脚本的机器类别，您可能会看到不同的结果。

此显示中有许多与模型相关的数据点:

*   同样，残差显示出相当大的范围
*   系数和我们之前看到的一样，但是截距的标准误差很高
*   预计 r 的平方接近 1
*   *p* 预计值最小

我们还可以使用 Python 来得出一个线性回归模型，使用:

```
import numpy as np
import statsmodels.formula.api as sm

model = sm.ols(formula='voted ~ registered', data=df)
fitted = model.fit()
print (fitted.summary())  
```

我们看到标准形式的回归结果如下:

![](../images/00089.jpeg)

这些警告表明数据存在一些问题:

*   我们直接使用协方差矩阵，所以不清楚如何指定它
*   我认为有很强的多重共线性，因为数据只有这两项

我们还可以使用脚本在 Python 中绘制实际值与拟合值:

```
plt.plot(df['voted'], df['registered'], 'ro')
plt.plot(df['voted'], fitted.fittedvalues, 'b')
plt.legend(['Data', 'Fitted model'])
plt.xlabel('Voted')
plt.ylabel('Registered')
plt.title('Voted vs Registered')
plt.show()  
```

![](../images/00090.jpeg)

我认为我更喜欢 Python 的这个版本的图表，而不是 r 的显示。左下方增加的强度圆圈令人困惑。



# 分析大学招生的变化

我们可以看看过去几年大学录取率的趋势。对于这个分析，我使用的是[https://www . ivy wise . com/ivy wise-knowledge base/admission-statistics](https://www.ivywise.com/ivywise-knowledgebase/admission-statistics)上的数据。

首先，我们读入我们的数据集并显示总结要点，从头至尾进行验证:

```
df <- read.csv("Documents/acceptance-rates.csv")
summary(df)
head(df)  
```

我们看到学校录取率的汇总数据如下:

![](../images/00091.jpeg)

有趣的是，录取率差异很大，从 5%的低到 2017 年的 41%。

让我们再次查看数据图，以验证数据点是正确的:

```
plot(df)  
```

![](../images/00092.jpeg)

从显示的相关图表来看，我们似乎不能使用 2007 年的数据点。图表显示，2007 年和其他年份之间有很大的差异，而其他三年有很好的相关性。

所以，我们有来自美国 25 所主要大学的连续 3 年的数据。我们可以通过几个步骤将数据转换成时间序列。

首先，我们创建了 2015-2017 年这些大学的平均录取率向量。我们使用均值函数来确定数据框中所有学院的平均值。我们的数据框中有一些`NA`值，所以我们需要告诉均值函数忽略这些值(`na.rm=TRUE`):

```
myvector <- c(mean(df[["X2015"]],na.rm=TRUE),
 mean(df[["X2016"]],na.rm=TRUE),
 mean(df[["X2017"]],na.rm=TRUE))  
```

接下来，我们将向量点转换成时间序列。在向量中传递一个时间序列，以使用开始点和结束点，以及数据点的频率。在我们的例子中，频率是每年一次，所以`frequency = 1`:

```
ts <- ts(myvector, start=c(2015), end=c(2017), frequency=1)  
```

然后绘制时间序列以获得良好的视觉效果:

```
plot(ts)  
```

![](../images/00093.jpeg)

因此，明显的趋势是全面降低录取率，因为我们看到最初的录取率在`.15`稳步下降到 2017 年的`.14`。

数据看起来非常好，非常吻合，因为数据点排成了整齐的直线。我们可以用这个时间序列来预测未来几年。Holt-Winters 算法有多种版本，可以基于级别数据、级别数据加趋势分量以及级别数据加趋势分量加季节性分量进行预测。我们有趋势，但没有季节性:

```
# double exponential - models level and trend
fit <- HoltWinters(ts, gamma=FALSE)
fit
Holt-Winters exponential smoothing with trend and without seasonal component.

Call:
HoltWinters(x = ts, gamma = FALSE)

Smoothing parameters:
 alpha: 0.3
 beta : 0.1
 gamma: FALSE

Coefficients:
 [,1]
a  0.14495402
b -0.00415977  
```

我们的指数平滑系数为七分之一，接近于零，这意味着我们不会大幅降低录取率，但它们正在下降。

现在我们已经有了现有数据的良好时间序列模型，我们可以对未来三年进行预测并绘制图表:

```
install.packages("forecast", repos="http://cran.us.r-project.org")
library(forecast)
forecast(fit, 3)
plot(forecast(fit, 3))  
```

![](../images/00094.jpeg)

趋势显然是负面的，但如前所述，这不是一个戏剧性的下降-大约每年 0.5%。我们还可以看看 Python 中类似的代码，如下所示。导入我们将使用的所有 Python 包:

```
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
%matplotlib inline
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 15, 6  
```

在大学录取通知书中读到一个数据框:

```
data = pd.read_csv('Documents/acceptance-rates.csv')
print (data.head())
 School  2017  2016  2015  2007
0      Amherst College   NaN  0.14  0.14  0.18
1       Boston College  0.32  0.32  0.28  0.27
2     Brown University  0.08  0.09  0.09  0.15
3  Columbia University  0.06  0.06  0.06  0.12
4   Cornell University  0.13  0.14  0.15  0.21  
```

删除`School`列，因为 Python 无法从字符串计算:

```
del data['School']
print (data.head())
 2017  2016  2015  2007
0   NaN  0.14  0.14  0.18
1  0.32  0.32  0.28  0.27
2  0.08  0.09  0.09  0.15
3  0.06  0.06  0.06  0.12
4  0.13  0.14  0.15  0.21  
```

按年份将数据转换为集合:

```
data = data.transpose()
print (data.head())
```

我们看到数据集转置为我们想要的形状，如下所示:

![](../images/00095.jpeg)

查看数据的样子:

```
plt.plot(data);  
```

![](../images/00096.jpeg)

我们看到接受度同样略有下降。

在 Python 中使用 Holt-Winters 预测是有问题的，因为它需要进一步转换数据。总的来说，在前面的 R 中，做同样的处理要复杂得多。



# 预测飞机到达时间

r 具有内置的功能，可以在训练集和测试集之间分割数据帧，基于训练集构建模型，使用模型和测试集预测结果，然后可视化模型的工作情况。

对于这个例子，我使用的是 2008 年从[http://stat-computing.org/dataexpo/2009/the-data.html](http://stat-computing.org/dataexpo/2009/the-data.html)出发的航班到达和离开时间与计划到达和离开时间的对比。数据集以一个`.bz2`文件的形式分发，该文件解包为一个 CSV 文件。我喜欢这个数据集，因为初始行数超过了 700 万，而且在 Jupyter 中运行良好。

我们首先读入飞机数据并显示一个摘要。数据集中还有我们没有使用的其他列:

```
df <- read.csv("Documents/2008-airplane.csv")
summary(df)
...
CRSElapsedTime      AirTime          ArrDelay          DepDelay 
 Min.   :-141.0   Min.   :   0     Min.   :-519.00   Min.   :-534.00 
 1st Qu.:  80.0   1st Qu.:  55     1st Qu.: -10.00   1st Qu.:  -4.00 
 Median : 110.0   Median :  86     Median :  -2.00   Median :  -1.00 
 Mean   : 128.9   Mean   : 104     Mean   :   8.17   Mean   :   9.97 
 3rd Qu.: 159.0   3rd Qu.: 132     3rd Qu.:  12.00   3rd Qu.:   8.00 
 Max.   :1435.0   Max.   :1350     Max.   :2461.00   Max.   :2467.00 
 NA's   :844      NA's   :154699   NA's   :154699    NA's   :136246 
 Origin             Dest            Distance          TaxiIn 
 ATL    : 414513   ATL    : 414521   Min.   :  11.0   Min.   :  0.00 
 ORD    : 350380   ORD    : 350452   1st Qu.: 325.0   1st Qu.:  4.00 
 DFW    : 281281   DFW    : 281401   Median : 581.0   Median :  6.00 
 DEN    : 241443   DEN    : 241470   Mean   : 726.4   Mean   :  6.86 
 LAX    : 215608   LAX    : 215685   3rd Qu.: 954.0   3rd Qu.:  8.00 
 PHX    : 199408   PHX    : 199416   Max.   :4962.0   Max.   :308.00 
 (Other):5307095   (Other):5306783                    NA's   :151649 
 TaxiOut         Cancelled       CancellationCode    Diverted 
 Min.   :  0.00   Min.   :0.00000    :6872294        Min.   :0.000000 
 1st Qu.: 10.00   1st Qu.:0.00000   A:  54330        1st Qu.:0.000000 
 Median : 14.00   Median :0.00000   B:  54904        Median :0.000000 
 Mean   : 16.45   Mean   :0.01961   C:  28188        Mean   :0.002463 
 3rd Qu.: 19.00   3rd Qu.:0.00000   D:     12        3rd Qu.:0.000000 
 Max.   :429.00   Max.   :1.00000                    Max.   :1.000000 
 NA's   :137058 
 CarrierDelay      WeatherDelay        NASDelay       SecurityDelay 
 Min.   :   0      Min.   :   0      Min.   :   0      Min.   :  0 
 1st Qu.:   0      1st Qu.:   0      1st Qu.:   0      1st Qu.:  0 
 Median :   0      Median :   0      Median :   6      Median :  0 
 Mean   :  16      Mean   :   3      Mean   :  17      Mean   :  0 
 3rd Qu.:  16      3rd Qu.:   0      3rd Qu.:  21      3rd Qu.:  0 
 Max.   :2436      Max.   :1352      Max.   :1357      Max.   :392 
 NA's   :5484993   NA's   :5484993   NA's   :5484993   NA's   :5484993 
 LateAircraftDelay
 Min.   :   0 
 1st Qu.:   0 
 Median :   0 
 Mean   :  21 
 3rd Qu.:  26 
 Max.   :1316 
 NA's   :5484993 
```

许多数据点都有`NA`值。为了建立准确的模型，我们需要移除这些:

```
# eliminate rows with NA values
df <- na.omit(df)
```

让我们创建分区:

```
# for partitioning to work data has to be ordered
times <- df[order(df$ArrTime),]
nrow(times)
1524735

# partition data - 75% training
library(caret)
set.seed(1337)
trainingIndices <- createDataPartition(df$ArrTime,p=0.75,list=FALSE)
trainingSet <- df[trainingIndices,]
testingSet <- df[-trainingIndices,]
nrow(trainingSet)
nrow(testingSet)
1143553
381182  
```

让我们根据以下字段建立我们的到达时间(`ArrTime`)模型:

*   `CRSArrTime`:预定到达时间
*   `ArrDelay`:到达延迟
*   `DepDelay`:出发延误
*   `Diverted`:飞机是否使用了改航路线
*   `CarrierDelay`:承运人系统的延迟
*   `WeatherDelay`:因天气原因延误
*   `NASDelay`:因 NAS 造成的延迟
*   `SecurityDelay`:因安全原因造成的延误
*   由于其他原因，飞机晚点了

![](../images/00097.jpeg)

不幸的是，其中两个数据项只是标志(0/1)。最大的预测因素似乎是预定到达时间。其他各种延迟因素的影响很小。我想这只是感觉好像要多花 20 分钟进行安全检查之类的；当你在旅行时，这是一件大事。

现在我们有了一个模型，让我们使用测试集来进行预测:

```
predicted <- predict(model, newdata=testingSet)
summary(predicted)
summary(testingSet$ArrTime)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -941    1360    1629    1590    1843    2217 
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 1    1249    1711    1590    2034    2400  
```

绘制预测数据与实际数据的对比图，以了解模型的准确性:

```
plot(predicted,testingSet$ArrTime)  
```

![](../images/00098.jpeg)

从视觉上看，预测值与实际值吻合得很好，如几乎 45 度线所示。图形右下部分的整套预测点很麻烦。似乎有许多预测远低于实际情况。肯定有额外的因素涉及，因为我本以为所有的数据都在一个区域而不是两个区域。



# 摘要

在这一章中，我们首先将 R 设置为笔记本可用的引擎之一。然后，我们使用一些基本的 R 来分析总统选举的选民统计数据。我们对比了选民登记和实际投票。接下来，我们分析了大学录取的趋势。最后，我们研究了使用预测模型来确定航班是否会延误。

在下一章，我们将研究 Jupyter 下不同方式的争论数据。**