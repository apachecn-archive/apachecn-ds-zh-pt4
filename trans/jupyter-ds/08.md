<title>Statistical Modeling</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 统计建模

在这一章中，我们采用原始数据，并试图通过建立统计模型来解释这些信息。一旦我们建立了一个模型，那么通常更容易看到共性。我们也可以决定趋势。

<title>Converting JSON to CSV</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 将 JSON 转换为 CSV

在本章中，我们将使用在[https://www.yelp.com/dataset/challenge](https://www.yelp.com/dataset/challenge)挑战赛中获得的 Yelp 数据。本部分使用来自挑战的第 9 轮的*数据集。作为背景，Yelp 是一个对不同产品和服务进行评级的网站，Yelp 在这里向用户发布评级。*

数据集文件是非常大(几千兆字节)数量的评级。下载中有几组评级信息——商业评级、评论、提示(因为这是一个值得参观的好地方),还有一组用户信息。我们对审查数据感兴趣。

当处理如此大的文件时，找到并使用一个大文件编辑器可能是有用的，这样你就可以深入数据文件。在 Windows 上，大多数标准编辑器都被限制在几兆字节。我使用大文本文件查看程序打开这些 JSON 文件。

所有的文件都是 JSON 格式的。JSON 是一种具有结构化元素的可读格式，例如，包含街道对象的城市对象。虽然阅读 JSON 很方便，但在处理大量元素时，这种格式显得笨拙。在`reviews`文件中有几百万行。因此，我们首先将 JSON 转换为平面 CSV 格式，以便于使用以下脚本进行处理:

```
import time import datetime import json, csv print( datetime.datetime.now().time()) headers = True #with open('c:/Users/Dan/reviews.json') as jsonf, open('c:/Users/Dan/reviews.csv', "wb") as csvf: filein = 'c:/Users/Dan/yelp_academic_dataset_review.json' fileout = 'c:/Users/Dan/yelp_academic_dataset_review.csv' with open(filein) as jsonf, open(fileout, "wb") as csvf:
 for line in jsonf: data = json.loads(line)        #remove the review text
 data.pop('text') if headers: w = csv.DictWriter(csvf, data.keys()) w.writeheader() headers = False w.writerow(data)print( datetime.datetime.now().time())
```

我打印出开始和结束时间，以了解这需要多长时间。我的机器花了 1.5 分钟来转换文件。在让前面的代码以令人满意的速度工作之前，我已经尝试了这个代码的几个版本。在开发这个脚本的时候，我选取了原始数据文件的一个小的子集(2000 行),并对该文件进行处理，直到事情有了足够的进展。

如您所见，我正在读取 Yelp 提供的原始 JSON 文件，并写出相应的 CSV 文件。

该脚本读取 JSON 的每一行(一行包含整个对象)并写出相应的 CSV。我删除了评论文本，因为我没有评估评论的文本，并且评论文本占用了很多空间。使用这种编码，审查文件大小从 3 千兆字节下降到 300 兆字节。除此之外，我们确保将标题作为第一条记录写入 CSV。然后，我使用一个单独的脚本/笔记本条目读入 CSV 并处理它。

<title>Evaluating Yelp reviews</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 评估 Yelp 评论

我们使用这个脚本读入处理过的 Yelp 评论，并打印出一些数据统计:

```
reviews <- read.csv("c:/Users/Dan/yelp_academic_dataset_review.csv") 
```

我通常会查看一些加载后的数据，以直观地检查事情是否按预期运行。我们可以通过一个`head()`函数调用来做到这一点:

```
head(reviews)
```

![](../images/00146.jpeg)<title>Summary data</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 总计数据

所有列似乎都正确加载。现在，我们可以查看数据的汇总统计信息:

```
summary(reviews)  
```

![](../images/00147.jpeg)

总结中有几点一文不值:

*   我假设的一些数据点只是`TRUE` / `FALSE`，`0` / `1`有范围；比如`funny`max 值超过 600；`useful`有最高 1100，`cool`有 500。
*   所有的 id(用户、企业)都被破坏了。我们可以使用用户文件和业务文件来提供准确的引用。
*   星级`1` - `5`，不出所料。然而，平均值和中间值大约是一个`4`，我认为许多人只是花时间写好评论。

<title>Review spread</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 查看跨页

我们可以使用一个简单的直方图来了解评论的分布情况:

```
hist(reviews$stars)  
```

其生成内联直方图如下:

![](../images/00148.jpeg)

同样，我们看到了只做正面评价的偏好(但是负面评价的数量相当大)。

<title>Finding the top rated firms</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 寻找评级最高的公司

我们可以使用以下脚本找到评分最高(5 分)的企业。该脚本使用类似 SQL 的方式访问数据框。SQL 有内置的机制，可以根据需要搜索、选择和排序数据组件。

在这个脚本中，我们构建了一个包含两列的计算数据框:`business_id`和`count`。数据框是有序的，评级最高的公司首先出现。创建后，我们显示数据框的头部，以获得数据集中评级最高的企业:

```
#businesses with most 5 star ratings
#install.packages("sqldf", repos='http://cran.us.r-project.org')
library(sqldf)
five_stars = sqldf("select business_id, count(*) from reviews where stars = 5 group by business_id order by 2 desc")
head(five_stars)  
```

![](../images/00149.jpeg)

值得注意的是，排名前五的企业有如此不平衡的评分(数字`1`几乎是数字`6`的两倍)。你想知道在评级过程中是否存在某种共谋导致了这种背离。再说一次，到目前为止，这些名字都是错的。

<title>Finding the most rated firms</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 寻找评级最高的公司

所以，这些公司拥有最多，最好的评级。哪个收视率最高？我们可以使用类似的脚本来确定评级最高的公司:

```
#which places have most ratings
library(sqldf)
most_ratings = sqldf("select business_id, count(*) from reviews group by business_id order by 2 desc")
head(most_ratings)  
```

在此脚本中，我们没有限定评级，以确定其成员资格，从而产生集合:

![](../images/00150.jpeg)

因此，类似地，我们看到少数公司的评级远远高于平均水平。此外，这些公司的名字被篡改了，但我们确实看到四家评级最高的公司也被列入了评级最高的公司名单。

<title>Finding all ratings for a top rated firm</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 查找最高评级公司的所有评级

如果我们观察一家评级最高的公司，看看评级差距在哪里呢？我们可以使用以下脚本:

```
# range of ratings for business with most ratings
library(sqldf)
most_rated = sqldf("select * from reviews where business_id = '4JNXUYY8wbaaDmk3BPzlWw' ")
hist(most_rated$stars)  
```

该脚本获取一个最高评级的 id，访问它们的所有评级，并显示相同的直方图:

![](../images/00151.jpeg)

因此，对于评级最高的公司之一，很少有评级低的。

<title>Determining the correlation between ratings and number of reviews</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 确定评分和评论数量之间的相关性

我们可以使用以下脚本来查看星级和评论数量之间的相关性:

```
# correlation number of reviews and number of stars
library(sqldf)
reviews_stars = sqldf("select stars,count(*) as reviews from reviews group by stars")
reviews_stars
cor(reviews_stars)  
```

![](../images/00152.jpeg)

所以，我们看到的`5`星评论是`1`星评论的三倍。我们也看到评论数量和明星数量之间有很高的相关性(`0.8632361`)。人们只会费心去评价好的公司。这使得使用 Yelp 来确定公司是否被审查变得很有趣。如果公司没有被评级(或评级不多)，这些不成文的评论是不好的。

我们可以使用以下脚本来可视化公司评级和评论数量之间的关系:

```
#correlation business and rating
library(sqldf)
business_rating = sqldf("select business_id, avg(stars) as rating from reviews group by business_id order by 2 desc")
head(business_rating)
hist(business_rating$rating)  
```

其中`business_rating`数据框是企业和平均星级的列表。生成的直方图如下:

![](../images/00153.jpeg)

这看起来像泊松分布。有趣的是，公司评级的分布采取了这样一种自然的分散。

<title>Building a model of reviews</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 构建评论模型

我们可以从数据集建立一个模型来估计一个评级可能需要多少颗星。然而，审查中可用的数据点只有:

*   `funny`
*   `useful`
*   `cool`

这些似乎不是评级数字的良好指标。我们可以使用一个模型，例如:

```
model <- lm(stars ~ funny + useful + cool, data=reviews)
summary(model)  
```

这将生成模型的统计数据:

![](../images/00154.jpeg)

不出所料，我们没有足够的信息来处理:

*   超过 400 万个自由度，每个评论只有一个自由度
*   P 值非常小——我们正确估计的概率是不存在的
*   `3.7`截距(接近范围的中点)
*   如此低的影响率(每个因素不到一倍)意味着我们离截距不远

<title>Using Python to compare ratings</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 使用 Python 比较评级

在前面的例子中，我们使用 R 来处理从 JSON 转换成 CSV 文件构建的数据帧。如果我们要使用 Yelp 企业评级文件，我们可以直接使用 Python，因为它要小得多，并且产生类似的结果。

在这个例子中，我们根据业务类别是否包括餐馆，从 Yelp 文件中收集美食。我们累积所有菜系的评分，然后得出每种菜系的平均值。

我们将 JSON 文件读入单独的行，并将每一行转换成一个 Python 对象:

我们用`errors=ignore`选项将每一行转换成 Unicode。这是由于数据文件中存在许多错误字符。

```
import json
#filein = 'c:/Users/Dan/business.json'
filein = 'c:/Users/Dan/yelp_academic_dataset_business.json'
lines = list(open(filein))  
```

我们用字典来评定一种菜肴的等级。这本字典的关键词是菜名。字典的价值是该菜系的评级列表:

```
ratings = {}
for line in lines:
 line = unicode(line, errors='ignore')
 obj = json.loads(line)
 if obj['categories'] == None:
 continue
 if 'Restaurants' in obj['categories']:
 rating = obj['stars']
 for category in obj['categories']:
 if category not in ratings:
 ratings[category] = []
 clist = ratings.get(category)
 clist.append(rating)
```

现在我们已经收集了所有的评分，我们可以制作一个新的平均评分的美食字典。我们还会累计总数，得出总体平均值，并跟踪评分最高的菜肴:

```
cuisines = {}
total = 0
cmax = ''
maxc = 0
for cuisine in ratings:
 clist = ratings[cuisine]
 if len(clist) < 10:
 continue
 avg = float(sum(clist))/len(clist)
 cuisines[cuisine] = avg
 total = total + avg
 if avg > maxc:
 maxc = avg
 cmax = cuisine

print ("Highest rated cuisine is ",cmax," at ",maxc)
print ("Average cuisine rating is ",total/len(ratings))

print (cuisines)  
```

![](../images/00155.jpeg)

有意思的是`Personal Chefs`是评分最高的。我只听说过名人有私人厨师，但数据显示这可能是值得的。1.6 的平均分数对所有菜系来说都是很糟糕的。我们之前看的时候，数据似乎没有高低评分的平衡。然而，在结果输出中，有许多不是美食的项目，尽管存在`Restaurants`键。我曾试图通过只计算评分在 10 分或以上的美食来消除不良数据，这消除了一些不良数据，但仍有许多错误记录在起作用。

<title>Visualizing average ratings by cuisine</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 可视化按菜系的平均评分

现在我们已经计算出了菜肴的平均值，我们可以用直方图显示它们，以了解它们的分布情况。我们首先将字典转换为数据框。然后将数据帧的`Rating`列绘制成直方图:

我们使用五个箱来对应五个可能的评级。

```
import pandas as pd
import numpy as np
df = pd.DataFrame(columns=['Cuisine', 'Rating'])
for cuisine in cuisines:
 df.loc[len(df)]=[cuisine, cuisines[cuisine]]
hist, bin_edges = np.histogram(df['Rating'], bins=range(5))

import matplotlib.pyplot as plt
plt.bar(bin_edges[:-1], hist, width = 1)
plt.xlim(min(bin_edges), max(bin_edges))
plt.show()     
```

![](../images/00156.jpeg)

同样，我们看到一个明显的高平均值标志。我曾试图在数据显示上获得更好的梯度，但无济于事。

<title>Arbitrary search of ratings</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 任意搜索收视率

由于我们有一个易于加载的数据格式，我们可以搜索任意条件，如`Personal Chefs`允许狗-也许他们会为你的狗定制烹饪。

我们可以使用如下脚本:对于行中的行:

```
 line = unicode(line, errors='ignore')
 obj = json.loads(line)
 if obj['categories'] == None:
 continue
 if 'Personal Chefs' in obj['categories']:
 if obj['attributes'] == None:
 continue
 for attr in obj['attributes']:
 print (attr)
```

我们对过滤掉的项目做一些有用的事情。这个脚本将只显示`Personal Chefs`的属性。如下图所示:

![](../images/00157.jpeg)

我们可以轻松地执行一些计算或其他操作来缩小范围，并轻松地关注数据的特定部分。

<title>Determining relationships between number of ratings and ratings</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 确定评级数量和评级之间的关系

鉴于上述结果，似乎大多数人只会以积极的方式投票。我们可以看看一家公司收到多少投票和他们的评级之间是否有关系。

首先，我们使用以下脚本积累数据集，提取每家公司的投票数和评级:

```
#determine relationship between number of reviews and star rating
import pandas as pd
from pandas import DataFrame as df 
import numpy as np 

dfr2 = pd.DataFrame(columns=['reviews', 'rating'])
mynparray = dfr2.values

for line in lines:
 line = unicode(line, errors='ignore')
 obj = json.loads(line)
 reviews = int(obj['review_count'])
 rating = float(obj['stars'])
 arow = [reviews,rating]
 mynparray = np.vstack((mynparray,arow)) 

dfr2 = df(mynparray)
print (len(dfr2))  
```

这段代码只是用我们的两个变量构建了数据框架。我们使用 NumPy，因为它更容易向数据框添加一行。完成所有记录后，我们将 NumPy 数据框转换回 pandas 数据框。

列名在翻译中丢失了，所以我们把它们放回去，并绘制一些汇总统计数据:

```
dfr2.columns = ['reviews', 'rating']
dfr2.describe()
```

在如下所示的输出中，我们看到了我们收集的评论和评级数据的布局。Yelp 没有限制其对该数据集的数据输入。评级应该有 5 个唯一的值:

![](../images/00158.jpeg)

接下来，我们使用以下公式绘制数据，以获得这种关系的直观线索:

```
#import matplotlib.pyplot as plt
dfr2.plot(kind='scatter', x='rating', y='reviews')
plt.show()  
```

![](../images/00159.jpeg)

因此，与之前的`business_rating`直方图相比，数据看起来具有清晰的泊松分布。

接下来，我们计算回归参数:

```
#compute regression
import statsmodels.formula.api as smf

# create a fitted model in one line
lm = smf.ols(formula='rating ~ reviews', data=dfr2).fit()

# print the coefficients
lm.params  
```

![](../images/00160.jpeg)

我们计算了所有额定值的截距。我期望一个单一的值。

现在，我们使用以下公式确定观察数据的范围:

```
#min, max observed values
X_new = pd.DataFrame({'reviews': [dfr2.reviews.min(), dfr2.reviews.max()]})
X_new.head()  
```

![](../images/00161.jpeg)

所以，正如我们之前猜测的，有些商家的评论数量非常大。

```
Now, we can make predictions based on the extent data points:
#make corresponding predictions
preds = lm.predict(X_new)
preds  
```

![](../images/00162.jpeg)

我们看到的预测值范围比预期的要大得多。绘制观察到的和预测的数据；

```
# first, plot the observed data
dfr2.plot(kind='scatter', x='reviews', y='rating')

# then, plot the least squares line
plt.plot(X_new, preds, c='red', linewidth=2)
plt.show()  
```

在下图中，一家公司的点评数量和点评分数之间似乎没有关系。这似乎是一个数字游戏——如果你让人们评价你的公司，平均来说他们会给你一个高分。

![](../images/00163.jpeg)

对于一个公司来说，评论的数量和评论分数之间似乎没有关系。

<title>Summary</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 摘要

在本章中，我们使用了数据并将 JSON 文件转换为 CSV 文件。我们评估了 Yelp 美食评论数据集，确定了排名最高和最多的公司。我们看到了收视率的分布。我们使用 Python 对 Yelp 商业评级进行了类似的评估，发现数据的分布非常相似。

在下一章中，我们将研究 Jupyter 指导下的机器学习。