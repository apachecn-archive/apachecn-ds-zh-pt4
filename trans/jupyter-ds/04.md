<title>Data Mining and SQL Queries</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 数据挖掘和 SQL 查询

PySpark 向 Python 公开了 Spark 编程模型。Spark 是用于大规模数据处理的快速通用引擎。我们可以在 Jupyter 下用 Python。因此，我们可以在 Jupyter 中使用 Spark。

安装 Spark 需要在您的机器上安装以下组件:

*   爪哇 JDK。
*   斯卡拉来自[http://www.scala-lang.org/download/](http://www.scala-lang.org/download/)。
*   Python 推荐用 Python 下载 Anaconda(来自 [http://continuum.io](http://continuum.io) )。
*   来自[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)的火花。
*   `winutils`:这是一个命令行实用程序，向 Windows 公开 Linux 命令。有 32 位和 64 位版本，可从以下网址获得:
    *   32 位`winutils.exe`在[https://code.google.com/p/rrd-hadoop-win32/source/checkout](https://code.google.com/p/rrd-hadoop-win32/source/checkout)
    *   64 位`winutils.exe`at[https://github . com/steveloughran/winutils/tree/master/Hadoop-2 . 6 . 0/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin)

然后设置显示前面组件位置的环境变量:

*   `JAVA_HOME`:你安装 JDK 的 bin 目录
*   `PYTHONPATH`:Python 安装的目录
*   `HADOOP_HOME`:`winutils`所在的目录
*   `SPARK_HOME`:安装火花的地方

这些组件很容易通过互联网在各种操作系统上获得。我已经在 Windows 环境和 Mac 环境中成功安装了这些先前的组件。

一旦你安装了这些，你应该能够从命令行窗口运行命令`pyspark`，并且可以使用一个带有 Python 的 Jupyter 笔记本(可以访问 Spark)。在我的安装中，我使用了以下命令:

```
pyspark  
```

因为我已经将 Spark 安装在根目录下的`\spark`目录中。是的，`pyspark`是 Spark 使用的内置工具。

<title>Special note for Windows installation</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# Windows 安装的特别注意事项

Spark(实际上是 Hadoop)需要一个临时存储位置来存放其工作数据集。在 Windows 下，这默认为`\tmp\hive`位置。如果 Spark/Hadoop 启动时该目录不存在，它将创建它。不幸的是，在 Windows 下，安装没有正确的内置工具来设置对目录的访问权限。

您应该能够在`winutils`下运行`chmod`来设置对`hive`目录的访问权限。但是，我发现`chmod`功能无法正常工作。

更好的办法是在管理模式下自己创建`tmp\hive`目录。然后再次在管理模式下向所有用户授予对配置单元目录的完全权限。

没有这个改变，Hadoop 马上就失败了。当您启动`pyspark`时，输出(包括任何错误)会显示在命令行窗口中。其中一个错误是对该目录的访问权限不足。

<title>Using Spark to analyze data</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 使用 Spark 分析数据

要访问 Spark，首先要做的是创建一个`SparkContext`。`SparkContext`初始化所有的 Spark 并设置 Hadoop 可能需要的任何访问，如果你也在使用的话。

最初的对象曾经是一个`SQLContext`，但是最近被弃用，取而代之的是更加开放的`SparkContext`。

我们可以使用一个简单的例子来通读文本文件，如下所示:

```
from pyspark import SparkContext
sc = SparkContext.getOrCreate()

lines = sc.textFile("B05238_04 Spark Total Line Lengths.ipynb")
lineLengths = lines.map(lambda s: len(s))
totalLength = lineLengths.reduce(lambda a, b: a + b)
print(totalLength)  
```

在本例中:

*   我们获得一个`SparkContext`
*   使用上下文，读入一个文件(本例中是 Jupyter 文件)
*   我们使用 Hadoop `map`函数将文本文件分成不同的行并收集长度
*   我们使用 Hadoop `reduce`函数来计算所有行的长度
*   我们展示我们的结果

在 Jupyter 下，如下所示:

![](../images/00063.jpeg)<title>Another MapReduce example</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 另一个 MapReduce 示例

我们可以在另一个例子中使用 MapReduce，从文件中获取字数。这是一个标准问题，但是我们使用 MapReduce 来完成大部分繁重的工作。我们可以使用这个例子的源代码。我们可以使用与此类似的脚本来统计文件中的单词出现次数:

```
import pyspark
if not 'sc' in globals():
 sc = pyspark.SparkContext()

text_file = sc.textFile("Spark File Words.ipynb")
counts = text_file.flatMap(lambda line: line.split(" ")) \
 .map(lambda word: (word, 1)) \
 .reduceByKey(lambda a, b: a + b)
for x in counts.collect():
 print x  
```

我们有相同的编码前序。

然后我们将文本文件加载到内存中。

`text_file` is a Spark **RDD** (**Resilient Distributed Dataset**), not a data frame.

假设它是巨大的，并且内容分布在许多处理器上。

一旦文件被加载，我们就把每一行拆分成单词，然后使用一个`lambda`函数来勾掉每个出现的单词。代码实际上为每个单词的出现创建了一个新的记录，比如 *at 出现 1* ， *at 出现 1* 。例如，如果单词*在*出现两次，则每次出现都会添加一条记录，如*在*出现 1 次。我们的想法是不要汇总结果，只记录我们看到的表象。这个想法是，这个过程可以被分割到多个处理器上，其中每个处理器生成这些低级信息比特。我们根本不关心优化这个过程。

一旦我们有了所有这些记录，我们就根据提到的单词出现次数来减少/总结记录集。

这个`counts`物体也是《星火》中的 RDD。最后一个`for`循环针对 RDD 运行一个`collect()`。如上所述，这个 RDD 可以分布在许多节点中。`collect()`功能将 RDD 的所有副本放入一个位置。然后我们遍历每条记录。

当我们在 Jupyter 中运行它时，我们会看到类似于这样的显示:

![](../images/00064.jpeg)

该列表被缩写为单词列表持续一段时间。

前面的例子不适用于 Python 3。在 Python 中直接编码时有一个变通方法，但不适用于 Jupyter。

<title>Using SparkSession and SQL</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 使用 SparkSession 和 SQL

Spark 公开了许多可以在数据帧上执行的类似 SQL 的操作。例如，我们可以在 CSV 文件中加载包含产品销售信息的数据框:

```
from pyspark.sql import SparkSession 
spark = SparkSession(sc) 

df = spark.read.format("csv") \
 .option("header", "true") \
 .load("productsales.csv");
df.show()  
```

示例:

*   启动`SparkSession`(大多数数据访问需要)
*   使用进程读取包含标题记录的 CSV 格式文件
*   显示初始行

![](../images/00065.gif)

销售数据中有几个有趣的栏目:

*   按部门划分的产品实际销售额
*   按部门划分的产品预测销售额

如果这是一个更大的文件，我们可以使用 SQL 来确定产品列表的范围。然后下面是 Spark SQL 来确定产品列表:

```
df.groupBy("PRODUCT").count().show()  
```

数据框`groupBy`函数的工作方式非常类似于 SQL `Group By`子句。`Group By`根据指定列中的值收集数据集中的项目。在本例中为`PRODUCT`列。`Group By`导致使用结果建立数据集。作为一个数据集，我们可以用`count()`函数查询每个数据集有多少行。

因此，`groupBy`的结果是对应于分组元素的项目数的计数。例如，我们通过`CHAIR`对项目进行分组，找到了其中的 288 项:

![](../images/00066.jpeg)

因此，我们显然没有真正的产品数据。任何公司都不可能在每条生产线上拥有完全相同数量的产品。

在本例中，我们可以使用`filter()`命令来查看数据集，以确定不同部门在实际销售和预测销售中的表现如何:

```
df.filter(df['ACTUAL'] > df['PREDICT']).show()  
```

我们向将对数据集中的每一行进行操作的`filter`命令传递一个逻辑测试。如果该行中的数据通过测试，则返回该行。否则，将从结果中删除该行。

我们的测试只对实际销售额超过预测值的销售额感兴趣。

在 Jupyter 下，这看起来像下面这样:

![](../images/00067.gif)

所以，我们得到了一个精简的结果集。同样，这是由`filter`函数作为数据帧产生的，然后可以作为任何其他数据帧被调用到`show`。请注意，前一个显示中的第三条记录没有出现，因为它的实际销售额低于预测。使用快速调查来确保你有正确的结果总是一个好主意。

如果我们想进一步研究这个问题，并确定公司内哪些领域表现最好，会怎么样？

如果这是一个数据库表，我们可以创建另一个列来存储实际销售额和预测销售额之间的差异，然后在该列上对我们的显示进行排序。我们可以在 Spark 中执行非常相似的步骤。

使用数据框时，我们可以使用如下编码:

```
#register dataframe as temporary SQL table
df.createOrReplaceTempView("sales")
# pull the values by the difference calculated
sqlDF = spark.sql("SELECT *, ACTUAL-PREDICT as DIFF FROM sales ORDER BY DIFF desc")
sqlDF.show()  
```

第一条语句是在上下文中创建一个视图/数据框架，以便进一步操作。这个视图是延迟评估的，除非采取特定的步骤，否则不会持久，最重要的是可以作为一个 hive 视图访问。该视图可直接从`SparkContext`获得。

然后，我们使用新创建的 sales 视图创建一个新的数据框，其中包含计算出的新列。根据 Jupyter，这看起来如下:

![](../images/00068.gif)

同样，我不认为我们有现实的价值，因为差异是非常远离预测值。

与数据库表不同，创建的数据框是不可变的。

<title>Combining datasets</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 组合数据集

因此，我们已经看到了将数据帧移入 Spark 进行分析。这似乎非常接近 SQL 表。在 SQL 下，标准做法是不在不同的表中复制项目。例如，一个产品表可能有价格，而一个订单表将只通过产品标识符引用产品表，以避免重复数据。因此，另一个 SQL 实践是连接或组合表，以获得所需的全部信息。根据订单类比，我们将所有涉及的表组合起来，因为每个表都有完成订单所需的数据。

使用 Spark 创建一组表并连接它们会有多难？我们将使用`Product`、`Order`和`ProductOrder`的示例表:

| **表** | **列** |
| 产品 | 产品 ID，描述，价格 |
| 命令 | 订单 ID，订单日期 |
| 产品订单 | 订单 ID，产品 ID，量 |

因此，`Order`有一个关联的`Product` / `Quantity`值列表。

我们可以填充数据框并将其移动到 Spark 中:

```
from pyspark import SparkContext
from pyspark.sql import SparkSession

sc = SparkContext.getOrCreate()
spark = SparkSession(sc) 

# load product set
productDF = spark.read.format("csv") \
 .option("header", "true") \
 .load("product.csv");
productDF.show()
productDF.createOrReplaceTempView("product")

# load order set
orderDF = spark.read.format("csv") \
 .option("header", "true") \
 .load("order.csv");
orderDF.show()
orderDF.createOrReplaceTempView("order")

# load order/product set
orderproductDF = spark.read.format("csv") \
 .option("header", "true") \
 .load("orderproduct.csv");
orderproductDF.show()
orderproductDF.createOrReplaceTempView("orderproduct")  
```

现在，我们可以尝试在它们之间执行类似 SQL 的`JOIN`操作:

```
# join the tables
joinedDF = spark.sql("SELECT * " \
 "FROM orderproduct " \
 "JOIN order ON order.orderid = orderproduct.orderid " \
 "ORDER BY order.orderid")
joinedDF.show()  
```

在 Jupyter 中执行所有这些操作会得到如下显示:

![](../images/00069.gif)

我们的标准导入获得一个`SparkContext`并初始化一个`SparkSession`。注意，`SparkContext`的`getOrCreate`。如果你在 Jupyter 之外运行这段代码，就不会有上下文，而会创建一个新的上下文。在 Jupyter 下，Jupyter 中 Spark 的启动为所有脚本初始化一个上下文。我们可以在任何 Spark 脚本中随意使用该上下文，而不必自己创建一个。

加载我们的`product`表:

![](../images/00070.gif)

加载`order`工作台:

![](../images/00071.gif)

加载`orderproduct`工作台。请注意，至少有一个订单有多种产品:

![](../images/00072.jpeg)

结果集中有来自`order`和`orderproduct`的`orderid`列。我们可以在查询中更有选择性，并指定我们希望返回的确切列:

![](../images/00073.jpeg)

我曾尝试使用 Spark `join()`命令，但运气不佳。

我在互联网上找到的文档和例子都是旧的、稀疏的、不正确的。

使用该命令还会出现任务没有及时返回结果的持久错误。从底层的 Hadoop 来看，我认为处理任务通常被分解成独立的任务。我假设 Spark 将函数分解成单独的线程来完成。不清楚为什么这样的小任务没有完成，因为我没有要求它执行任何特殊的任务。

<title>Loading JSON into Spark</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 将 JSON 加载到 Spark 中

Spark 还可以访问 JSON 数据进行操作。这里我们有一个例子:

*   将 JSON 文件加载到 Spark 数据框中
*   检查数据框的内容并显示外观方案
*   像前面的其他数据帧一样，将数据帧移动到 Spark 会话直接访问的上下文中
*   显示了在 Spark 上下文中访问数据帧的示例

清单如下所示:

我们的 Spark 标准包括:

```
from pyspark import SparkContext
from pyspark.sql import SparkSession 
sc = SparkContext.getOrCreate()
spark = SparkSession(sc)  
```

阅读 JSON 并展示我们的发现:

```
#using some data from file from https://gist.github.com/marktyers/678711152b8dd33f6346
df = spark.read.json("people.json")
df.show()  
```

![](../images/00074.jpeg)

我很难将标准 JSON 加载到 Spark 中。Spark 似乎期望 JSON 文件的每个列表有一条数据记录，而我见过的大多数 JSON 都用缩进之类的方式格式化记录布局。

请注意，在实例中没有指定属性的地方使用了空值。

显示数据的解释模式:

```
df.printSchema()  
```

![](../images/00075.gif)

所有列的默认值为`nullable`。您可以更改列的属性，但不能更改列的值，因为数据值是不可变的。

将数据框移动到上下文中，并从上下文中进行访问:

```
df.registerTempTable("people")
spark.sql("select name from people").show()  
```

![](../images/00076.jpeg)

此时，`people`表就像 Spark 中任何其他临时 SQL 表一样工作。

<title>Using Spark pivot</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 使用火花枢轴

`pivot()`函数允许您将行转换成列，同时对一些列执行聚合。如果你想一想，你实际上是在围绕一个支点调整桌子的轴。

我想到了一个简单的例子来说明这一切是如何工作的。我认为这是其中的一个特点，一旦你看到它在行动中，你就会意识到你可以应用它的领域的数量。

在我们的示例中，我们有一些股票的原始价格点，我们希望将该表转换为每支股票每年的平均价格。

我们示例中的代码是:

```
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import functions as func

sc = SparkContext.getOrCreate()
spark = SparkSession(sc)

# load product set
pivotDF = spark.read.format("csv") \
 .option("header", "true") \
 .load("pivot.csv");
pivotDF.show()
pivotDF.createOrReplaceTempView("pivot")

# pivot data per the year to get average prices per stock per year
pivotDF \
 .groupBy("stock") \
 .pivot("year",[2012,2013]) \
 .agg(func.avg("price")) \
 .show()  
```

这在 Jupyter 中看起来如下:

![](../images/00077.gif)

所有的标准包括我们需要 Spark 来初始化`SparkContext`和`SparkSession`:

![](../images/00078.gif)

我们从 CSV 文件中加载股票价格信息。重要的是，至少有一种股票在同一年有一个以上的价格:

![](../images/00079.jpeg)

我们正在按股票代码将信息分类。在我们的数据集中，透视位于具有两个值 2012 和 2013 的年份。我们正在计算每年的平均价格。

<title>Summary</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 摘要

在这一章中，我们熟悉了获取一个`SparkContext`。我们看到了使用 Hadoop MapReduce 的例子。我们对 Spark 数据使用了 SQL。我们合并数据帧并对结果集进行操作。我们导入 JSON 数据，用 Spark 操纵它。最后，我们看了如何使用 pivot 来收集关于数据框的信息。

在下一章，我们将看看在 Jupyter 下使用 R 编程。