<title>C15019_05_Final_ePub_SW</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 5。执行您的首次聚类分析

概述

本章结束时，你将能够加载和可视化数据和散点图集群；为聚类分析准备数据；用 k-means 执行质心聚类；解释聚类结果并确定给定数据集的最佳聚类数。

本章将向您介绍无监督学习任务，其中算法必须自动从数据中学习模式，因为事先没有定义目标变量。

# 简介

前面的章节向你介绍了非常流行和极其强大的机器学习算法。它们都有一个共同点，那就是它们都属于同一类算法:监督学习。这种算法试图根据指定的结果列(目标变量)来学习模式，例如销售额、员工流失率或客户类别。

但是，如果您的数据集中没有这样的变量，或者您不想指定目标变量，该怎么办呢？你还能在上面运行一些机器学习算法并发现有趣的模式吗？答案是肯定的，使用属于无监督学习范畴的聚类算法。

聚类算法在数据科学行业中非常流行，用于对相似的数据点进行分组并检测异常值。例如，银行可以使用聚类算法通过从数据中识别不寻常的聚类来进行欺诈检测。电子商务公司也可以使用它们来识别具有相似浏览行为的用户组，如下图所示:

![Figure 5.1: Example of data on customers with similar browsing behaviors without clustering analysis performed
](image/C15019_05_01.jpg)

图 5.1:没有执行聚类分析的具有相似浏览行为的客户的数据示例

对此数据执行的聚类分析将通过对相似的数据点进行分组来揭示自然模式，因此您可能会得到以下结果:

![Figure 5.2: Clustering analysis performed on the data on customers with similar browsing behaviors
](image/C15019_05_02.jpg)

图 5.2:对具有相似浏览行为的客户数据进行聚类分析

现在，这些数据根据他们的经常性访问和在网站上花费的时间分为三个客户群，然后可以为每个客户群使用不同的营销计划，以最大限度地提高销售额。

在这一章中，你将学习如何使用一个非常著名的聚类算法 k-means 来执行这样的分析。

# 使用 k-均值聚类

k-means 是数据科学家中最流行的聚类算法之一(如果不是最流行的)，因为它简单且性能高。它的起源可以追溯到 1956 年，当时一位名叫雨果·施泰因豪斯的著名数学家奠定了它的基础，但直到十年后，另一位名叫詹姆斯·麦克奎恩的研究人员才将这种方法命名为 k-means。

k-means 的目标是将相似的数据点(或观察值)组合在一起，形成一个聚类。可以把它想象成把彼此靠近的元素分组(我们将在本章后面定义如何度量接近度)。例如，如果您手动分析移动应用程序上的用户行为，您可能最终会将频繁登录的客户或在应用程序内购买量较大的用户分组在一起。这是聚类算法(如 k-means)将自动从数据中为您找到的一种分组。

在本章中，我们将使用澳大利亚税务局(ATO)公开共享的开源数据集。该数据集包含关于每个邮政编码(*也称为 zip code)的统计数据，这是一个用于在 2014-15 财年期间在澳大利亚按区域*分类邮件的识别代码。

注意

澳大利亚税务局(ATO)数据集可以在 Packt GitHub 存储库中找到，地址:[https://packt.live/340xO5t](https://packt.live/340xO5t)。

数据集的来源可以在这里找到:[https://packt.live/361i1p3](https://packt.live/361i1p3)。

我们将对这个数据集执行两个特定变量(或列)的聚类分析:`Average net tax`和`Average total deductions`。我们的目标是找到在收到的税款和扣除的款项方面具有相似模式的邮政编码组(或集群)。这是这两个变量的散点图:

![Figure 5.3: Scatter plot of the ATO dataset
](image/C15019_05_03.jpg)

图 5.3:ATO 数据集的散点图

作为数据科学家，您需要分析从该数据集获得的图表并得出结论。假设您必须从这个数据集中手动分析潜在的观察分组。一个可能的结果如下:

*   左下角的所有数据点可以组合在一起(平均净税额从 0 到 40，000)。
*   第二组可以是中心区域的所有数据点(平均净税额从 40，000 到 60，000，平均总扣除额低于 10，000)。
*   最后，所有剩余的数据点可以被分组在一起。

但是，与其让您手动猜测这些分组，不如让我们使用一种算法来为我们做这件事。这就是我们在接下来的练习中将要看到的，我们将对这个数据集执行聚类分析。

## 练习 5.01:对 ATO 数据集执行第一次聚类分析

在本练习中，我们将对 ATO 数据集使用 k-means 聚类，并观察数据集分成的不同聚类，之后我们将通过分析输出得出结论:

1.  打开新的 Colab 笔记本。
2.  Next, load the required Python packages: `pandas` and `KMeans` from `sklearn.cluster`.

    我们将使用 Python 中的`import`函数:

    注意

    您可以使用下面代码片段中提到的函数为脚本中经常调用的包创建短别名。

    ```
    import pandas as pd
    from sklearn.cluster import KMeans
    ```

    注意

    我们将在本章的后面查看`KMeans`(来自`sklearn.cluster`)，你在这里的代码中使用了它，以获得更详细的解释。

3.  Next, create a variable containing the link to the file. We will call this variable `file_url`:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter05/DataSet/taxstats2015.csv'
    ```

    在下一步中，我们将使用`pandas`包将我们的数据加载到 DataFrame 中(把它想象成一个表格，就像 Excel 电子表格一样，有一个行索引和列名)。

    我们的输入文件是`CSV`格式的，`pandas`有一个方法可以直接读取这个格式，这个方法就是`.read_csv()`。

4.  Use the `usecols` parameter to subset only the columns we need rather than loading the entire dataset. We just need to provide a list of the column names we are interested in, which are mentioned in the following code snippet:

    ```
    df = pd.read_csv(file_url, usecols=['Postcode', 'Average net tax', 'Average total deductions'])
    ```

    现在我们已经将数据加载到一个`pandas`数据帧中。

5.  Next, let's display the first 5 rows of this DataFrame , using the method `.head()`:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 5.4: The first five rows of the ATO DataFrame
    ](image/C15019_05_04.jpg)

    图 5.4:ATO 数据帧的前五行

6.  Now, to output the last 5 rows, we use `.tail()`:

    ```
    df.tail()
    ```

    您应该得到以下输出:

    ![Figure 5.5: The last five rows of the ATO DataFrame
    ](image/C15019_05_05.jpg)

    图 5.5:ATO 数据帧的最后五行

    现在我们有了数据，让我们直接跳到我们想要做的事情:寻找集群。

    正如您在前面章节中看到的，`sklearn`为训练不同的机器学习算法提供了完全相同的 API，例如:

    *   用指定的超参数实例化一个算法(这里是 KMeans(超参数))。
    *   用方法`.fit()`用训练数据拟合模型。
    *   Predict the result with the given input data with the method `.predict()`.

        注意

        这里，除了`random_state`之外，我们将使用 k 均值超参数的所有默认值。指定一个固定的随机状态(也称为**种子**)将帮助我们在每次必须重新运行代码时获得可重复的结果。

7.  用随机状态`42`实例化 k-means，并保存到一个名为`kmeans` :

    ```
    kmeans = KMeans(random_state=42)
    ```

    的变量中
8.  现在把我们的训练数据输入 k-means。为此，我们只需要获得用于拟合模型的变量(或列)。在我们的例子中，变量是`'Average net tax'`和`'Average total deductions'`，它们被保存在一个名为`X` :

    ```
    X = df[['Average net tax', 'Average total deductions']]
    ```

    的新变量中
9.  Now fit `kmeans` with this training data:

    ```
    kmeans.fit(X)
    ```

    您应该得到以下输出:

    ![Figure 5.6: Summary of the fitted kmeans and its hyperparameters
    ](image/C15019_05_06.jpg)

    图 5.6:拟合的均值及其超参数汇总

    我们刚刚用几行代码运行了我们的第一个聚类算法。

10.  See which cluster each data point belongs to by using the `.predict()` method:

    ```
    y_preds = kmeans.predict(X)
    y_preds
    ```

    您应该得到以下输出:

    ![Figure 5.7: Output of the k-means predictions 
    ](image/C15019_05_07.jpg)

    图 5.7:k 均值预测的输出

11.  Now, add these predictions into the original DataFrame and take a look at the first five postcodes:

    ```
    df['cluster'] = y_preds
    df.head()
    ```

    注意

    sklearn `predict()`方法的预测与输入数据的顺序完全相同。因此，第一个预测将对应于数据帧的第一行。

    您应该得到以下输出:

![Figure 5.8: Cluster number assigned to the first five postcodes
](image/C15019_05_08.jpg)

图 5.8:分配给前五个邮政编码的分类编号

我们的 k-means 模型已经将前两行分组到同一个聚类 1 中。我们可以看到这两个观察值的平均净税额都在 28，000 左右。最后三个数据点被分配到不同的聚类(分别为 2、3 和 7)，我们可以看到它们的平均总扣除额和平均净税额的值彼此非常不同。似乎较低的值被归入第 2 类，而较高的值被归入第 3 类。我们开始了解 k-means 是如何决定对这个数据集中的观察值进行分组的。

这是一个很好的开始。您已经学习了如何用几行代码来训练(或拟合)k-means 模型。现在我们可以开始深入研究 k-means 背后的魔力了。

# Interpre ting k-means 结果

在训练我们的 k-means 算法之后，我们可能会对更详细地分析它的结果感兴趣。记住，聚类分析的目的是将具有相似模式的观察结果分组在一起。但是我们如何才能看出算法找到的分组是否有意义呢？在本节中，我们将通过使用刚刚生成的数据集结果来了解这一点。

对此进行研究的一种方法是使用为每个观察值分配的聚类逐行分析数据集。这可能非常繁琐，尤其是当数据集非常大时，因此最好有一种分类结果的摘要。

如果您熟悉 Excel 电子表格，您可能会考虑使用数据透视表来获取每个聚类的变量平均值。在 SQL 中，您可能会使用一个`GROUP BY`语句。如果您对这两者都不熟悉，您可能会考虑将每个分类组合在一起，然后计算每个分类的平均值。好消息是，这可以通过 Python 中的`pandas`包轻松实现。让我们通过一个例子来看看如何做到这一点。

为了创建一个类似于 Excel 的数据透视表，我们将使用来自`pandas`的`pivot_table()`方法。我们需要为此方法指定以下参数:

*   `values`:该参数对应于您要计算汇总(或聚合)的数值列，如取平均值或计数。在 Excel 数据透视表中，它也被称为`values`。在我们的数据集中，我们将使用`Average net tax`和`Average total deductions`变量。
*   `index`:该参数用于指定您想要查看汇总的列。在我们的例子中，它将是`cluster`列。在 Excel 的数据透视表中，这对应于`Rows`字段。
*   `aggfunc`:您可以在这里指定汇总数据的聚合函数，例如获取平均值或计数。在 Excel 中，这是`values`字段中的`Summarize by`选项:

```
import numpy as np
df.pivot_table(values=['Average net tax', 'Average total deductions'], index='cluster', aggfunc=np.mean)
```

注意

我们将使用`mean()`的`numpy`实现，因为它更适合熊猫数据帧。

![Figure 5.9: Output of the pivot_table function
](image/C15019_05_09.jpg)

图 5.9:数据透视表函数的输出

在这个总结中，我们可以看到算法已经将数据分组为八个簇(簇 0 到 7)。在所有分类中，分类 0 的平均净税额和总扣除额最低，而分类 4 的值最高。有了这个数据透视表，我们就能够使用它们的汇总值来比较它们之间的聚类。

使用集群的聚合视图是查看它们之间差异的好方法，但这不是唯一的方法。另一种可能性是在图形中可视化集群。这正是我们现在要做的。

你可能听说过不同的可视化软件包，比如`matplotlib`、`seaborn`和`bokeh`，但是在这一章中，我们将使用`altair`软件包，因为它使用起来非常简单(它的 API 与`sklearn`非常相似)。我们先导入一下:

```
import altair as alt
```

然后，我们将使用 DataFrame 实例化一个`Chart()`对象，并将其保存到一个名为`chart`的变量中:

```
chart = alt.Chart(df)
```

现在我们将使用`.mark_circle()`方法指定我们想要的图表类型，即散点图，并将它保存到一个名为`scatter_plot`的新变量中:

```
scatter_plot = chart.mark_circle()
```

最后，我们需要通过指定列的名称来配置我们的散点图，这些列将是图表上的`x` -和`y`-轴。我们还用`color`选项告诉散点图根据聚类值给每个点着色:

```
scatter_plot.encode(x='Average net tax', y='Average total deductions', color='cluster:N')
```

注意

您可能已经注意到，我们在`cluster`列名的末尾添加了`:N`。这个额外的参数在`altair`中用来指定这个列的值类型。`:N`表示此栏中包含的信息是分类信息。`altair`根据列的类型自动定义要使用的颜色方案。

您应该得到以下输出:

![Figure 5.10: Scatter plot of the clusters
](image/C15019_05_10.jpg)

图 5.10:聚类的散点图

我们现在可以很容易地看到这个图中的集群是什么，以及它们彼此之间有什么不同。我们可以清楚地看到，k-means 主要基于 x 轴变量为每个聚类分配数据点，这个变量就是`Average net tax`。聚类的边界是垂直直线。例如，红色和紫色星团的分界线大约在 18，000 左右。低于此限值的观测值被分配给红色聚类(2)，高于此限值的观测值被分配给紫色聚类(6)。

如果你使用过可视化工具，如 **Tableau** 或 **PowerBI** ，你可能会感到有点沮丧，因为这个图表是静态的，你不能将鼠标悬停在每个数据点上以获得更多信息，并找到，例如，分隔橙色集群和粉红色集群的界限是什么。但这可以很容易地用 altair 来实现，这也是我们选择使用它的原因之一，我们可以用最少的代码变化在你的图表中添加一些交互。

假设我们想要添加一个工具提示来显示两个感兴趣的列的值:邮政编码和分配的集群。使用`altair`，我们只需要在`encode()`方法中添加一个名为`tooltip`的参数，并列出相应的列名，然后调用`interactive()`方法，如下面的代码片段所示:

```
scatter_plot.encode(x='Average net tax', y='Average total deductions',color='cluster:N', tooltip=['Postcode', 'cluster', 'Average net tax', 'Average total deductions']).interactive()
```

您应该得到以下输出:

![Figure 5.11: Interactive scatter plot of the clusters with tooltip
](image/C15019_05_11.jpg)

图 5.11:带有工具提示的聚类的交互式散点图

现在，我们可以很容易地查看聚类边界附近的数据点，并发现用于区分橙色聚类(1)和粉红色聚类(7)的阈值在`'Average Net Tax'`中接近 32，000。我们还可以看到，邮政编码`3146`就在这条边界附近，它的平均净税收正好是`31992`，它的`'average total deductions'`是`4276`。

## 练习 5。 02:根据营业收入和费用对澳大利亚邮政编码进行聚类

在本练习中，我们将学习如何使用 k-means 执行聚类分析，并根据按业务收入和支出排序的邮政编码值可视化其结果。以下步骤将帮助您完成本练习:

1.  打开新的 Colab 笔记本。
2.  现在`import`需要的包(`pandas`、`sklearn`、`altair`、`numpy` ):

    ```
    import pandas as pd
    from sklearn.cluster import KMeans
    import altair as alt
    import numpy as np
    ```

3.  将 ATO 数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter05/DataSet/taxstats2015.csv'
    ```

    的变量
4.  使用 pandas 包中的`read_csv`方法，只加载带有`use_cols`参数的以下列的数据集:`'Postcode'`、`'Average total business income'`和`'Average total business expenses'` :

    ```
    df = pd.read_csv(file_url, usecols=['Postcode', 'Average total business income', 'Average total business expenses'])
    ```

5.  Display the last 10 rows from the ATO dataset using the `.tail()` method from pandas:

    ```
    df.tail(10)
    ```

    您应该得到以下输出:

    ![Figure 5.12: The last 10 rows of the ATO dataset
    ](image/C15019_05_12.jpg)

    图 5.12:ATO 数据集的最后 10 行

6.  使用下面的 pandas 列子集语法提取`'Average total business income'`和`'Average total business expenses'`列:`dataframe_name[<list_of_columns>]`。然后，将它们保存到一个名为`X` :

    ```
    X = df[['Average total business income', 'Average total business expenses']]
    ```

    的新变量中
7.  Now fit `kmeans` with this new variable using a value of `8` for the `random_state` hyperparameter:

    ```
    kmeans = KMeans(random_state=8)
    kmeans.fit(X)
    ```

    您应该得到以下输出:

    ![Figure 5.13: Summary of the fitted kmeans and its hyperparameters
    ](image/C15019_05_13.jpg)

    图 5.13:拟合的均值及其超参数汇总

8.  Using the `predict` method from the `sklearn` package, predict the clustering assignment from the input variable, `(X)`, save the results into a new variable called `y_preds`, and display the last `10` predictions:

    ```
    y_preds = kmeans.predict(X)
    y_preds[-10:]
    ```

    您应该得到以下输出:

    ![Figure 5.14: Results of the clusters assigned to the last 10 observations
    ](image/C15019_05_14.jpg)

    图 5.14:分配给最后 10 个观察值的聚类结果

9.  Save the predicted clusters back to the DataFrame by creating a new column called `'cluster'` and print the last `10` rows of the DataFrame using the `.tail()` method from the `pandas` package:

    ```
    df['cluster'] = y_preds
    df.tail(10)
    ```

    您应该得到以下输出:

    ![Figure 5.15: The last 10 rows of the ATO dataset with the added cluster column
    ](image/C15019_05_15.jpg)

    图 5.15:添加了聚类列的 ATO 数据集的最后 10 行

10.  使用来自具有以下参数的`pandas`包的`pivot_table`方法，为每个聚类值生成一个包含两列平均值的数据透视表:
    *   为参数值提供要聚合的列的名称`'Average total business income'`和 `'Average total business expenses'`。
    *   为参数索引提供要分组的列的名称`'cluster'`。
    *   Use the `.mean` method from NumPy (`np`) as the aggregation function for the `aggfunc` parameter:

        ```
        df.pivot_table(values=['Average total business income', 'Average total business expenses'], index='cluster', aggfunc=np.mean)
        ```

        您应该得到以下输出:

![Figure 5.16: Output of the pivot_table function
](image/C15019_05_16.jpg)

图 5.16:数据透视表函数的输出

1.  现在让我们使用交互式散点图来绘制集群。首先，使用`altair`包中的`Chart()`和`mark_circle()`实例化散点图:

    ```
    scatter_plot = alt.Chart(df).mark_circle()
    ```

2.  使用`altair`的`encode`和`interactive`方法，通过以下参数指定散点图的显示及其交互选项:
    *   将`'Average total business income'`列的名称提供给`x`参数(x 轴)。
    *   将`'Average total business expenses'`列的名称提供给`y`参数(y 轴)。
    *   将`cluster:N`列的名称提供给`color`参数(为每个组提供不同的颜色)。
    *   Provide these column names – `'Postcode'`, `'cluster'`, `'Average total business income'`, and `'Average total business expenses'` – to the `'tooltip'` parameter (this being the information displayed by the tooltip):

        ```
        scatter_plot.encode(x='Average total business income', y='Average total business expenses', color='cluster:N', tooltip=['Postcode', 'cluster', 'Average total business income', 'Average total business expenses']).interactive()
        ```

        您应该得到以下输出:

![Figure 5.17: Interactive scatter plot of the clusters
](image/C15019_05_17.jpg)

图 5.17:聚类的交互式散点图

我们可以看到，k-means 根据两个变量(`'Average total business income'`和`'Average total business expense'`)的值将观察值分成了八个不同的组。例如，所有低值数据点都被分配到聚类 7，而具有极高值的数据点属于聚类 2。因此，k-means 对具有相似行为的数据点进行了分组。

您刚刚成功完成了一个聚类分析并可视化了其结果。您了解了如何加载真实世界的数据集、拟合 k 均值以及显示散点图。这是一个很好的开始，我们将在本章后面的章节中深入探讨如何提高模型性能的更多细节。

# 选择集群的数量

在前面的章节中，我们看到了在给定的数据集上使用 k-means 算法是多么容易。在我们的 ATO 数据集中，我们发现了 8 个不同的聚类，它们主要由`Average net tax`变量的值定义。

但你可能问过自己:“*为什么是 8 簇？为什么不是 3 个或 15 个集群？*“这些确实是很好的问题。简而言之，我们使用 k-means 的超参数`n_cluster`的默认值，将待发现的聚类数定义为 8。

正如您在*第 2 章*、*回归*和*第 4 章*、*多类分类(NLP)* 中所回忆的，超参数的值不是由算法学习的，而是由您在训练之前任意设置的。对于 k-means，`n_cluster`是您必须调整的最重要的超参数之一。选择一个较低的值将导致 k-means 将许多数据点分组在一起，即使它们彼此非常不同。另一方面，选择较高的值可能会强制算法将相近的观测值分割成多个，即使它们非常相似。

从 ATO 数据集的散点图来看，八个集群似乎很多。在图中，一些分类看起来彼此非常接近，并且具有相似的值。直观地说，只要看一下图，你就可以说有两到四个不同的集群。如您所见，这很有启发性，如果有一个函数可以帮助我们为数据集定义正确的聚类数，那就太好了。这样的方法确实存在，被称为**肘**法。

这种方法评估聚类的紧密性，目标是最小化一个称为**惯性**的值。更多的细节和解释将在本章后面提供。现在，把惯性想成一个值，它表示，对于一组数据点，它们彼此之间有多远或者有多近。

让我们将这种方法应用到我们的 ATO 数据集。首先，我们将定义我们想要评估的集群编号的范围(在 1 到 10 之间)，并将它们保存在名为`clusters`的数据帧中。我们还将创建一个名为`inertia`的空列表，在这里我们将存储我们计算的值:

```
clusters = pd.DataFrame()
clusters['cluster_range'] = range(1, 10)
inertia = []
```

接下来，我们将创建一个`for`循环，该循环将遍历整个范围，用指定数量的`clusters`拟合 k-means 模型，提取`inertia`值，并将其存储在我们的列表中，如下面的代码片段所示:

```
for k in clusters['cluster_range']:
    kmeans = KMeans(n_clusters=k, random_state=8).fit(X)
    inertia.append(kmeans.inertia_)
```

现在我们可以使用`clusters`数据帧中的`inertia`值列表:

```
clusters['inertia'] = inertia
clusters
```

您应该得到以下输出:

![Figure 5.18: Dataframe containing inertia values for our clusters
](image/C15019_05_18.jpg)

图 5.18:包含集群惯性值的数据框架

然后，我们需要使用`altair`和`mark_line()`方法绘制一个折线图。我们将把`'cluster_range'`列指定为 x 轴，把`'inertia'`列指定为 y 轴，如下面的代码片段所示:

```
alt.Chart(clusters).mark_line().encode(x='cluster_range', y='inertia')
```

您应该得到以下输出:

![Figure 5.19: Plotting the Elbow method
](image/C15019_05_19.jpg)

图 5.19:绘制弯头方法

注意

您不必将每个`altair`对象保存在单独的变量中；您可以用"`.".`一个接一个地添加这些方法

现在我们已经绘制了惯性值与聚类数的关系，我们需要找到最优的聚类数。我们需要做的是找到图形中的拐点，在这个点上惯性值开始更缓慢地减小(也就是线的斜率几乎达到 45 度角的地方)。找到正确的拐点可能有点棘手。如果你把这个线图想象成一只手臂，我们想要的是找到肘部的中心(现在你知道这个方法的名字是从哪里来的了)。因此，在我们的示例中，我们会说集群的最佳数量是三个。如果我们继续添加更多的集群，惯性将不会急剧下降，并增加任何价值。这就是我们为什么要找手肘中间为拐点的原因。

现在，让我们用这个超参数重新训练我们的`Kmeans`,并绘制聚类，如下面的代码片段所示:

```
kmeans = KMeans(random_state=42, n_clusters=3)
kmeans.fit(X)
df['cluster2'] = kmeans.predict(X)
scatter_plot.encode(x='Average net tax', y='Average total deductions',color='cluster2:N',
tooltip=['Postcode', 'cluster', 'Average net tax', 'Average total deductions']
).interactive()
```

您应该得到以下输出:

![Figure 5.20: Scatter plot of the three clusters
](image/C15019_05_20.jpg)

图 5.20:三个集群的散点图

这与我们最初的结果大相径庭。观察这三个集群，我们可以看到:

*   第一个分类(蓝色)表示平均净税额和总扣除额都较低的邮政编码。
*   第二组(橙色)代表中等平均净税额和低平均总扣除额。
*   The third cluster (red) is grouping all postcodes with average net tax values above 35,000\.

    注意

    值得注意的是，数据点更多地分布在第三个集群中；这可能表明该组中存在一些异常值。

这个例子向我们展示了，如果我们想从数据中获得有意义的分组，在训练 k-means 算法之前定义正确的分类数是多么重要。我们使用了一种称为肘法的方法来寻找这个最佳数字。

## 练习 5.03:寻找最佳聚类数

在本练习中，我们将对与*练习 5.02* 、*中相同的数据应用 Elbow 方法，根据业务收入和支出对澳大利亚邮政编码进行聚类*，以在拟合 k-means 模型之前找到最佳聚类数:

1.  打开新的 Colab 笔记本。
2.  Now `import` the required packages (`pandas`, `sklearn`, and `altair`):

    ```
    import pandas as pd
    from sklearn.cluster import KMeans
    import altair as alt
    ```

    接下来，我们将加载数据集，选择与*练习 5.02* 、*中相同的列，并打印前五行。*

3.  将 ATO 数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter05/DataSet/taxstats2015.csv'
    ```

    的变量
4.  使用 pandas 包中的`.read_csv()`方法，使用`use_cols`参数加载仅包含以下列的数据集:`'Postcode'`、`'Average total business income'`和`'Average total business expenses'` :

    ```
    df = pd.read_csv(file_url, usecols=['Postcode', 'Average total business income', 'Average total business expenses'])
    ```

5.  Display the first five rows of the DataFrame with the `.head()` method from the pandas package:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 5.21: The first five rows of the ATO DataFrame
    ](image/C15019_05_21.jpg)

    图 5.21:ATO 数据帧的前五行

6.  将`'Average total business income'`和`'Average total business expenses'`列分配给一个名为`X` :

    ```
    X = df[['Average total business income', 'Average total business expenses']]
    ```

    的新变量
7.  Create an empty pandas DataFrame called `clusters` and an empty list called `inertia`:

    ```
    clusters = pd.DataFrame()
    inertia = []
    ```

    现在，使用`range`函数生成一个包含从`1`到`15`的集群编号范围的列表，并将其分配给来自`'clusters'`数据帧的一个名为`'cluster_range'`的新列:

    ```
    clusters['cluster_range'] = range(1, 15)
    ```

8.  创建一个`for`循环来遍历每个聚类数，并相应地拟合 k-means 模型，然后使用`'inertia_'`参数将`inertia`值附加到`'inertia'`列表:

    ```
    for k in clusters['cluster_range']:
        kmeans = KMeans(n_clusters=k).fit(X)
        inertia.append(kmeans.inertia_)
    ```

9.  Assign the `inertia` list to a new column called `'inertia'` from the `clusters` DataFrame and display its content:

    ```
    clusters['inertia'] = inertia
    clusters
    ```

    您应该得到以下输出:

    ![Figure 5.22: Plotting the Elbow method
    ](image/C15019_05_22.jpg)

    图 5.22:绘制弯头方法

10.  Now use `mark_line()` and `encode()` from the `altair` package to plot the Elbow graph with `'cluster_range'` as the x-axis and `'inertia'` as the y-axis:

    ```
    alt.Chart(clusters).mark_line().encode(alt.X('cluster_range'), alt.Y('inertia'))
    ```

    您应该得到以下输出:

    ![Figure 5.23: Plotting the Elbow method
    ](image/C15019_05_23.jpg)

    图 5.23:绘制弯头方法

11.  查看肘形图，确定最佳聚类数，并将该值赋给一个名为`optim_cluster` :

    ```
    optim_cluster = 4
    ```

    的变量
12.  使用来自`sklearn` :

    ```
    kmeans = KMeans(random_state=42, n_clusters=optim_cluster)
    kmeans.fit(X)
    ```

    的`fit`方法，用这个数量的聚类和`42`的`random_state`值训练一个 k-means 模型
13.  现在，使用来自`sklearn`的`predict`方法，为包含在`X`变量中的每个数据点获得预测的分配聚类，并将结果保存到来自`df`数据帧:

    ```
    df['cluster2'] = kmeans.predict(X)
    ```

    的名为`'cluster2'`的新列中
14.  Display the first five rows of the `df` DataFrame using the `head` method from the `pandas` package:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 5.24: The first five rows with the cluster predictions
    ](image/C15019_05_24.jpg)

    图 5.24:带有聚类预测的前五行

15.  Now plot the scatter plot using the `mark_circle()` and `encode()` methods from the `altair` package. Also, to add interactiveness, use the `tooltip` parameter and the `interactive()` method from the `altair` package as shown in the following code snippet:

    ```
    alt.Chart(df).mark_circle().encode(x='Average total business income', y='Average total business expenses',color='cluster2:N', tooltip=['Postcode', 'cluster2', 'Average total business income', 'Average total business expenses']).interactive()
    ```

    您应该得到以下输出:

![Figure 5.25: Scatter plot of the four clusters
](image/C15019_05_25.jpg)

图 5.25:四个集群的散点图

您刚刚学习了如何在拟合 k 均值模型之前找到最佳聚类数。在我们这里的输出中，数据点被分成四个不同的组:

*   聚类 0(蓝色)用于平均总业务收入值低于 100，000 且平均总业务费用值低于 80，000 的所有观察。
*   聚类 3(青色)对平均总业务收入值低于 180，000 且平均总业务支出值低于 160，000 的数据点进行分组。
*   聚类 1(橙色)用于平均总业务收入值低于 370，000 且平均总业务支出值低于 330，000 的数据点。
*   聚类 2(红色)用于具有极值的数据点，即平均总业务收入值高于 370，000 且平均总业务支出值高于 330，000 的数据点。

*练习 5.02* ，*根据业务收入和支出对澳大利亚邮政编码进行聚类*的结果有八个不同的聚类，其中一些聚类彼此非常相似。在这里，您可以看到，拥有最佳数量的聚类可以更好地区分不同的组，这就是为什么它是 k-means 中最重要的超参数之一。在下一节中，我们将看看初始化 k-means 的另外两个重要的超参数。

# 初始化集群

从本章开始，我们每次使用聚类算法时都会参考 k-means。但是你可能已经注意到在每个模型摘要中有一个名为`init`的超参数，默认值为 k-means++。事实上，我们一直在使用 k-means++语言。

k-means 和 k-means++之间的区别在于它们在训练开始时如何初始化聚类。k-means 随机选择每个聚类的中心(称为**质心**)，然后将每个数据点分配给其最近的聚类。如果该集群初始化选择不正确，这可能导致训练过程结束时的非最佳分组。例如，在下图中，我们可以清楚地看到数据的三个自然分组，但算法未能正确识别它们:

![Figure 5.26: Example of non-optimal clusters being found
](image/C15019_05_26.jpg)

图 5.26:发现的非最佳集群示例

k-means++试图在初始化时找到更好的聚类。其背后的想法是随机选择第一个集群，然后使用剩余数据点的概率分布选择下一个，那些更远的。尽管 k-means++与原始的 k-means 相比能够获得更好的结果，但是在某些情况下，它仍然会导致非最优的聚类。

科学家可以用来降低错误聚类风险的另一个超参数数据是`n_init`。这对应于使用不同初始化运行 k-means 的次数，最终模型是最佳运行。因此，如果这个超参数的数值很高，找到最佳聚类的机会就更大，但缺点是训练时间会更长。因此，您必须谨慎选择该值，尤其是当您有一个大型数据集时。

让我们看看下面的例子，在我们的 ATO 数据集上尝试一下。

首先，让我们使用随机初始化只运行一次迭代:

```
kmeans = KMeans(random_state=14, n_clusters=3, init='random', n_init=1)
kmeans.fit(X)
```

像往常一样，我们希望用散点图来可视化我们的聚类，如下面的代码片段中所定义的:

```
df['cluster3'] = kmeans.predict(X)
alt.Chart(df).mark_circle().encode(x='Average net tax', y='Average total deductions',color='cluster3:N',
tooltip=['Postcode', 'cluster', 'Average net tax', 'Average total deductions']
).interactive()
```

您应该得到以下输出:

![Figure 5.27: Clustering results with n_init as 1 and init as random
](image/C15019_05_27.jpg)

图 5.27:n _ init 为 1，init 为 random 的聚类结果

总的来说，这个结果和我们之前的结果非常接近。值得注意的是，由于不同的初始化，橙色和红色集群交换了位置。此外，它们的边界略有不同:现在图上的值约为 31，000，而以前的阈值约为 34，000。

现在让我们尝试五次迭代(使用`n_init`超参数)和 k-means++初始化(使用`init`超参数):

```
kmeans = KMeans(random_state=14, n_clusters=3, init='k-means++', n_init=5)
kmeans.fit(X)
df['cluster4'] = kmeans.predict(X)
alt.Chart(df).mark_circle().encode(x='Average net tax', y='Average total deductions',color='cluster4:N',
tooltip=['Postcode', 'cluster', 'Average net tax', 'Average total deductions']
).interactive()
```

您应该得到以下输出:

![Figure 5.28: Clustering results with n_init as 5 and init as k-means++
](image/C15019_05_28.jpg)

图 5.28:n _ init 为 5，init 为 k-means++的聚类结果

这里，结果非常接近于 10 次迭代的原始运行。这意味着我们不必为 k-means 收敛而运行如此多的迭代，并且可以用更低的数值节省一些时间。

## 练习 5.04:使用不同的初始化参数获得合适的结果

在本练习中，我们将使用与*练习 5.02* 、*中相同的数据，按业务收入和支出对澳大利亚邮政编码进行聚类*，并尝试对`init`和`n_init`超参数使用不同的值，看看它们如何影响最终的聚类结果:

1.  打开新的 Colab 笔记本。
2.  导入需要的包，分别是`pandas`、`sklearn`、`altair` :

    ```
    import pandas as pd
    from sklearn.cluster import KMeans
    import altair as alt
    ```

3.  将 ATO 数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter05/DataSet/taxstats2015.csv'
    ```

    的变量
4.  加载数据集，并选择与*练习 5.02* 、*按业务收入和支出对澳大利亚邮政编码进行聚类*和*练习 5.03* 、*查找最佳聚类数*中相同的列，使用`pandas`包中的`read_csv()`方法:

    ```
    df = pd.read_csv(file_url, usecols=['Postcode', 'Average total business income', 'Average total business expenses'])
    ```

5.  将`'Average total business income'`和`'Average total business expenses'`列分配给一个名为`X` :

    ```
    X = df[['Average total business income', 'Average total business expenses']]
    ```

    的新变量
6.  拟合一个 k 均值模型，使`n_init`等于`1`和一个随机的`init` :

    ```
    kmeans = KMeans(random_state=1, n_clusters=4, init='random', n_init=1)
    kmeans.fit(X)
    ```

7.  使用`sklearn`包中的`predict`方法，根据输入变量`(X)`预测聚类分配，并将结果保存到 DataFrame:

    ```
    df['cluster3'] = kmeans.predict(X)
    ```

    中名为`'cluster3'`的新列中
8.  使用交互式散点图绘制聚类图。首先，使用`altair`包中的`Chart()`和`mark_circle()`实例化散点图，如下面的代码片段所示:

    ```
    scatter_plot = alt.Chart(df).mark_circle()
    ```

9.  使用`altair`的`encode`和`interactive`方法，通过以下参数指定散点图的显示及其交互选项:
    *   将`'Average total business income'`列的名称提供给`x`参数(x 轴)。
    *   将`'Average total business expenses'`列的名称提供给`y`参数(y 轴)。
    *   将`'cluster3:N'`列的名称提供给`color`参数(它为每个组定义了不同的颜色)。
    *   Provide these column names – `'Postcode'`, `'cluster3'`, `'Average total business income'`, and `'Average total business expenses'` – to the `tooltip` parameter:

        ```
        scatter_plot.encode(x='Average total business income', y='Average total business expenses',color='cluster3:N',
        tooltip=['Postcode', 'cluster3', 'Average total business income', 'Average total business expenses']
        ).interactive()
        ```

        您应该得到以下输出:

![Figure 5.29: Clustering results with n_init as 1 and init as random
](image/C15019_05_29.jpg)

图 5.29:n _ init 为 1，init 为 random 时的聚类结果

1.  Repeat *Steps 5* to *8* but with different k-means hyperparameters, `n_init=10` and random `init`, as shown in the following code snippet:

    ```
    kmeans = KMeans(random_state=1, n_clusters=4, init='random', n_init=10)
    kmeans.fit(X)
    df['cluster4'] = kmeans.predict(X)
    scatter_plot = alt.Chart(df).mark_circle()
    scatter_plot.encode(x='Average total business income', y='Average total business expenses',color='cluster4:N',
    tooltip=['Postcode', 'cluster4', 'Average total business income', 'Average total business expenses']
    ).interactive()
    ```

    您应该得到以下输出:

    ![Figure 5.30: Clustering results with n_init as 10 and init as random
    ](image/C15019_05_30.jpg)

    图 5.30:n _ init 为 10，init 为 random 时的聚类结果

2.  Again, repeat *Steps 5* to *8* but with different k-means hyperparameters – `n_init=100` and random `init`:

    ```
    kmeans = KMeans(random_state=1, n_clusters=4, init='random', n_init=100)
    kmeans.fit(X)
    df['cluster5'] = kmeans.predict(X)
    scatter_plot = alt.Chart(df).mark_circle()
    scatter_plot.encode(x='Average total business income', y='Average total business expenses',color='cluster5:N',
    tooltip=['Postcode', 'cluster5', 'Average total business income', 'Average total business expenses']
    ).interactive()
    ```

    您应该得到以下输出:

![Figure 5.31: Clustering results with n_init as 10 and init as random
](image/C15019_05_31.jpg)

图 5.31:n _ init 为 10，init 为 random 时的聚类结果

您刚刚学习了如何调优负责初始化 k 均值聚类的两个主要超参数。在本练习中，您已经看到使用`n_init`增加迭代次数对该数据集的聚类结果没有太大影响。

在这种情况下，最好对此超参数使用较低的值，因为这将加快训练时间。但是对于一个不同的数据集，您可能会面临这样一种情况:根据`n_init`值的不同，结果会有很大的不同。在这种情况下，您将不得不找到一个不太小但也不太大的值`n_init`。您希望找到一个最佳点，在这个点上，与使用不同值获得的最后结果相比，结果不会有太大变化。

# 计算到质心的距离

在前面的章节中，我们已经谈了很多关于数据点之间的相似性，但是我们还没有真正定义这意味着什么。你可能已经猜到了，这与观测值之间的距离有多近或多远有关。你正朝着正确的方向前进。它与两点间的某种距离度量有关。k-means 使用的一个称为**平方欧几里德距离**，其公式为:

![Figure 5.32: The squared Euclidean distance formula
](image/C15019_05_32.jpg)

图 5.32:平方欧几里德距离公式

如果你没有统计学背景，这个公式可能看起来很吓人，但其实很简单。它是数据坐标之间的平方差之和。这里， *x* 和 *y* 是两个数据点，索引 *i* 表示坐标的数量。如果数据是二维的， *i* 等于 2。同样，如果有三个维度，那么 *i* 就是 3。

让我们将此公式应用于 a to 数据集。

首先，我们将获取所需的值——即前两次观察的坐标——并打印出来:

注意

在 pandas 中，`iloc`方法用于通过索引对数据帧的行或列进行子集化。例如，如果我们想要获取第 888 行和第 6 列，我们将使用下面的语法:`dataframe.iloc[888, 6]`。

```
x = X.iloc[0,].values
y = X.iloc[1,].values
print(x)
print(y)
```

您应该得到以下输出:

![Figure 5.33: Extracting the first two observations from the ATO dataset
](image/C15019_05_33.jpg)

图 5.33:从 ATO 数据集中提取前两个观察值

`x`的坐标是`(27555, 2071)`，`y`的坐标是`(28142, 3804)`。这里，公式告诉我们计算两个数据点的每个轴之间的平方差，并将它们相加:

```
squared_euclidean = (x[0] - y[0])**2 + (x[1] - y[1])**2
print(squared_euclidean)
```

您应该得到以下输出:

```
3347858
```

k-means 使用该度量来计算每个数据点与其所分配的聚类中心(也称为质心)之间的距离。该算法背后的基本逻辑如下:

1.  随机选择簇的中心(质心)。
2.  使用平方欧几里得距离将每个数据点分配到最近的质心。
3.  将每个质心的坐标更新为分配给它的数据点的新计算中心。
4.  重复*步骤 2* 和 *3* 直到聚类收敛(即直到聚类分配不再改变)或直到达到最大迭代次数。

就是这样。k-means 算法就是这么简单。我们可以在用`cluster_centers_`拟合 k-means 模型后提取质心。

让我们看看如何在一个例子中绘制质心。

首先，我们拟合一个 k-means 模型，如下面的代码片段所示:

```
kmeans = KMeans(random_state=42, n_clusters=3, init='k-means++', n_init=5)
kmeans.fit(X)
df['cluster6'] = kmeans.predict(X)
```

现在将`centroids`提取到数据帧中并打印出来:

```
centroids = kmeans.cluster_centers_
centroids = pd.DataFrame(centroids, columns=['Average net tax', 'Average total deductions'])
print(centroids)
```

您应该得到以下输出:

![Figure 5.34: Coordinates of the three centroids
](image/C15019_05_34.jpg)

图 5.34:三个质心的坐标

我们将绘制通常的散点图，但会将其分配给一个名为`chart1`的变量:

```
chart1 = alt.Chart(df).mark_circle().encode(x='Average net tax', y='Average total deductions',color='cluster6:N',
tooltip=['Postcode', 'cluster6', 'Average net tax', 'Average total deductions']
).interactive()
chart1
```

您应该得到以下输出:

![Figure 5.35: Scatter plot of the clusters
](image/C15019_05_35.jpg)

图 5.35:聚类的散点图

现在，只为名为`chart2`的质心创建第二个散点图:

```
chart2 = alt.Chart(centroids).mark_circle(size=100).encode(x='Average net tax', y='Average total deductions', color=alt.value('black'),
tooltip=['Average net tax', 'Average total deductions']).interactive()
chart2
```

您应该得到以下输出:

![Figure 5.36: Scatter plot of the centroids
](image/C15019_05_36.jpg)

图 5.36:质心散点图

现在我们将两个图表结合起来，这对于`altair`来说非常容易:

```
chart1 + chart2
```

您应该得到以下输出:

![Figure 5.37: Scatter plot of the clusters and their centroids
](image/C15019_05_37.jpg)

图 5.37:星团及其质心的散点图

现在，我们可以很容易地看到观测值最接近的质心。

## 练习 5.05:在数据集中寻找最近的质心

在本练习中，我们将对 k-means 的第一次迭代进行编码，以便将数据点分配给它们最近的聚类质心。以下步骤将帮助您完成练习:

1.  打开新的 Colab 笔记本。
2.  现在`import`需要的包，分别是`pandas`、`sklearn`和`altair` :

    ```
    import pandas as pd
    from sklearn.cluster import KMeans
    import altair as alt
    ```

3.  加载数据集，并选择与*练习 5.02* 、*中相同的列，使用`pandas`包中的`read_csv()`方法将澳大利亚邮政编码按照业务收入和支出*进行聚类:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter05/DataSet/taxstats2015.csv'
    df = pd.read_csv(file_url, usecols=['Postcode', 'Average total business income', 'Average total business expenses'])
    ```

4.  将`'Average total business income'`和`'Average total business expenses'`列分配给一个名为`X` :

    ```
    X = df[['Average total business income', 'Average total business expenses']]
    ```

    的新变量
5.  现在，使用`'Average total business income'`和`'Average total business income'`变量的`min()`和`max()`值计算最小值和最大值，如下面的代码片段所示:

    ```
    business_income_min = df['Average total business income'].min()
    business_income_max = df['Average total business income'].max()
    business_expenses_min = df['Average total business expenses'].min()
    business_expenses_max = df['Average total business expenses'].max()
    ```

6.  Print the values of these four variables, which are the minimum and maximum values of the two variables:

    ```
    print(business_income_min)
    print(business_income_max)
    print(business_expenses_min)
    print(business_expenses_max)
    ```

    您应该得到以下输出:

    ```
    0
    876324
    0
    884659
    ```

7.  现在导入`random`包并使用`seed()`方法设置一个`42`的种子，如下面的代码片段所示:

    ```
    import random
    random.seed(42)
    ```

8.  创建一个空的 pandas 数据帧，并将其赋给一个名为`centroids` :

    ```
    centroids = pd.DataFrame()
    ```

    的变量
9.  使用`random`包中的`sample()`方法生成四个随机值，可能值在使用`range()`的`'Average total business expenses'`列的最小值和最大值之间，并将结果存储在来自`centroids`数据帧的一个名为`'Average total business income'`的新列中:

    ```
    centroids['Average total business income'] = random.sample(range(business_income_min, business_income_max), 4)
    ```

10.  重复相同的过程，为`'Average total business expenses'` :

    ```
    centroids['Average total business expenses'] = random.sample(range(business_expenses_min, business_expenses_max), 4)
    ```

    生成`4`个随机值
11.  Create a new column called `'cluster'` from the `centroids` DataFrame using the `.index` attributes from the pandas package and print this DataFrame:

    ```
    centroids['cluster'] = centroids.index
    centroids
    ```

    您应该得到以下输出:

    ![Figure 5.38: Coordinates of the four random centroids
    ](image/C15019_05_38.jpg)

    图 5.38:四个随机质心的坐标

12.  用`altair`包创建一个散点图，显示包含在`df`数据帧中的数据，并将其保存在一个名为`'chart1'` :

    ```
    chart1 = alt.Chart(df.head()).mark_circle().encode(x='Average total business income', y='Average total business expenses', 
    color=alt.value('orange'),
    tooltip=['Postcode', 'Average total business income', 'Average total business expenses']
    ).interactive()
    ```

    的变量中
13.  现在使用`altair`包创建第二个散点图来显示质心，并保存在一个名为`'chart2'` :

    ```
    chart2 = alt.Chart(centroids).mark_circle(size=100).encode(x='Average total business income', y='Average total business expenses', 
    color=alt.value('black'),
    tooltip=['cluster', 'Average total business income', 'Average total business expenses']
    ).interactive()
    ```

    的变量中
14.  Display the two charts together using the altair syntax: `<chart> + <chart>`:

    ```
    chart1 + chart2
    ```

    您应该得到以下输出:

    ![Figure 5.39: Scatter plot of the random centroids and the first five observations
    ](image/C15019_05_39.jpg)

    图 5.39:随机质心和前五次观察的散点图

15.  定义一个计算`squared_euclidean`距离并返回其值的函数。该函数将获取数据点的`x`和`y`坐标以及质心:

    ```
    def squared_euclidean(data_x, data_y, centroid_x, centroid_y, ):
      return (data_x - centroid_x)**2 + (data_y - centroid_y)**2
    ```

16.  使用 pandas 包中的`.at`方法，提取第一行的`x`和`y`坐标，并保存在两个名为`data_x`和`data_y` :

    ```
    data_x = df.at[0, 'Average total business income']
    data_y = df.at[0, 'Average total business expenses']
    ```

    的变量中
17.  Using a `for` loop or list comprehension, calculate the `squared_euclidean` distance of the first observation (using its `data_x` and `data_y` coordinates) against the `4` different centroids contained in `centroids`, save the result in a variable called `distance`, and display it:

    ```
    distances = [squared_euclidean(data_x, data_y, centroids.at[i, 'Average total business income'], centroids.at[i, 'Average total business expenses']) for i in range(4)]
    distances
    ```

    您应该得到以下输出:

    ```
    [215601466600, 10063365460, 34245932020, 326873037866]
    ```

18.  使用包含`squared_euclidean`距离的列表中的`index`方法来查找距离最短的集群，如下面的代码片段所示:

    ```
    cluster_index = distances.index(min(distances))
    ```

19.  使用 pandas 包

    ```
    df.at[0, 'cluster'] = cluster_index
    ```

    中的`.at`方法将第一次观察的`df`数据帧中的`cluster`索引保存在名为`'cluster'`的列中
20.  Display the first five rows of `df` using the `head()` method from the `pandas` package:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 5.40: The first five rows of the ATO DataFrame with the assigned cluster number for the first row
    ](image/C15019_05_40.jpg)

    图 5.40:ATO 数据帧的前五行，第一行分配有集群编号

21.  Repeat *Steps 15* to *19* for the next `4` rows to calculate their distances from the centroids and find the cluster with the smallest distance value:

    ```
    distances = [squared_euclidean(df.at[1, 'Average total business income'], df.at[1, 'Average total business expenses'], centroids.at[i, 'Average total business income'], centroids.at[i, 'Average total business expenses']) for i in range(4)]
    df.at[1, 'cluster'] = distances.index(min(distances))
    distances = [squared_euclidean(df.at[2, 'Average total business income'], df.at[2, 'Average total business expenses'], centroids.at[i, 'Average total business income'], centroids.at[i, 'Average total business expenses']) for i in range(4)]
    df.at[2, 'cluster'] = distances.index(min(distances))
    distances = [squared_euclidean(df.at[3, 'Average total business income'], df.at[3, 'Average total business expenses'], centroids.at[i, 'Average total business income'], centroids.at[i, 'Average total business expenses']) for i in range(4)]
    df.at[3, 'cluster'] = distances.index(min(distances))
    distances = [squared_euclidean(df.at[4, 'Average total business income'], df.at[4, 'Average total business expenses'], centroids.at[i, 'Average total business income'], centroids.at[i, 'Average total business expenses']) for i in range(4)]
    df.at[4, 'cluster'] = distances.index(min(distances))
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 5.41: The first five rows of the ATO DataFrame and their assigned clusters
    ](image/C15019_05_41.jpg)

    图 5.41:ATO 数据帧的前五行及其分配的簇

22.  Finally, plot the centroids and the first `5` rows of the dataset using the `altair` package as in *Steps 12* to *13*:

    ```
    chart1 = alt.Chart(df.head()).mark_circle().encode(x='Average total business income', y='Average total business expenses', 
    color='cluster:N',
    tooltip=['Postcode', 'cluster', 'Average total business income', 'Average total business expenses']
    ).interactive()
    chart2 = alt.Chart(centroids).mark_circle(size=100).encode(x='Average total business income', y='Average total business expenses', 
    color=alt.value('black'),
    tooltip=['cluster', 'Average total business income', 'Average total business expenses']
    ).interactive()
    chart1 + chart2
    ```

    您应该得到以下输出:

![Figure 5.42: Scatter plot of the random centroids and the first five observations
](image/C15019_05_42.jpg)

图 5.42:随机质心和前五次观察的散点图

在这个最终结果中，我们可以看到四个聚类在图中的位置，以及五个数据点被分配到哪个聚类:

*   左下角的两个数据点被分配给聚类 2，聚类 2 对应于质心坐标为 26，000(平均总业务收入)和 234，000(平均总业务支出)的聚类。这是这两个点最近的质心。
*   中间的两个观察值非常接近质心，坐标为 116，000(平均总业务收入)和 256，000(平均总业务支出)，对应于聚类 1。
*   顶部的观察值被分配给聚类 0，其质心坐标为 670，000(平均总业务收入)和 288，000(平均总业务支出)。

您只是从零开始重新实现了 k-means 算法的很大一部分。您了解了如何随机初始化质心(聚类中心)，计算一些数据点的平方欧几里得距离，找到它们最近的质心，并将它们分配给相应的聚类。这不容易，但你做到了。

# 标准化数据

你已经学习了很多关于 k-means 算法的知识，我们已经接近本章的结尾了。在这最后一节，我们将不讨论另一个超参数(您已经讨论过主要的超参数)，而是一个非常重要的主题:**数据处理**。

拟合 k-means 算法非常容易。最棘手的部分是确保得到的集群对您的项目有意义，我们已经看到如何调整一些超参数来确保这一点。但是处理输入数据与您到目前为止所学的所有步骤一样重要。如果你的数据集没有准备好，即使你找到了最好的超参数，你仍然会得到一些不好的结果。

让我们再来看看我们的 ATO 数据集。在前面的章节中，*计算到质心的距离*，我们发现了三个不同的集群，它们主要由`'Average net tax'`变量定义。就好像 k-means 根本没有考虑第二个变量`'Average total deductions'`。这实际上是由于这两个变量具有非常不同的值范围以及平方欧几里德距离的计算方式。

平方欧几里得距离更倾向于高值变量。让我们举一个例子，用两个数据点 A 和 B 来说明这一点，这两个数据点的 x 和 y 坐标分别为(1，50000)和(100，100000)。a 和 b 之间的平方欧几里得距离将是(100000 - 50000)^2 + (100 - 1)^2.我们可以清楚地看到，结果将主要由 100，000 和 50，000 之间的差异驱动:50,000^2.100 减 1 (99^2)的差值在最终结果中所占的比重很小。

但是如果你看 10 万到 5 万之间的比值，是 2 的倍数(10 万/ 5 万= 2)，而 100 和 1 之间的比值是 100 的倍数(100 / 1 = 100)。更高值的变量“支配”聚类结果有意义吗？真的要看你的项目，这种情况可能是有意的。但是，如果您希望不同轴之间的事情是公平的，那么在拟合 k-means 模型之前，最好将它们都纳入一个相似的值范围。这就是为什么在运行 k-means 算法之前，您应该始终考虑标准化您的数据。

标准化数据有多种方法，我们将看看最流行的两种方法:最小-最大缩放和 z 值。幸运的是,`sklearn`包对这两种方法都有实现。

最小-最大缩放的公式非常简单:在每个轴上，您需要删除每个数据点的最小值，并将结果除以最大值和最小值之间的差。缩放后的数据将具有范围在 0 和 1 之间的值:

![Figure 5.43: Min-max scaling formula
](image/C15019_05_43.jpg)

图 5.43:最小-最大缩放公式

让我们在下面的例子中用`sklearn`来看看最小-最大缩放。

首先，我们导入相关的类并实例化一个对象:

```
from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()
```

然后，我们将它与我们的数据集相匹配:

```
min_max_scaler.fit(X)
```

您应该得到以下输出:

![Figure 5.44: Min-max scaling summary
](image/C15019_05_44.jpg)

图 5.44:最小-最大缩放摘要

最后，调用`transform()`方法来标准化数据:

```
X_min_max = min_max_scaler.transform(X)
X_min_max
```

您应该得到以下输出:

![Figure 5.45: Min-max-scaled data
](image/C15019_05_45.jpg)

图 5.45:最小-最大比例数据

现在我们打印两个轴的最小-最大比例数据的最小值和最大值:

```
X_min_max[:,0].min(), X_min_max[:,0].max(), X_min_max[:,1].min(), X_min_max[:,1].max()
```

您应该得到以下输出:

![Figure 5.46: Minimum and maximum values of the min-max-scaled data
](image/C15019_05_46.jpg)

图 5.46:最小-最大比例数据的最小值和最大值

我们可以看到两个轴的值都在 0 和 1 之间。

z 得分的计算方法是从数据点中移除总平均值，然后将结果除以每个轴的标准偏差。标准化数据的分布平均值为 0，标准偏差为 1:

![Figure 5.47: Z-score formula
](image/C15019_05_47.jpg)

图 5.47: Z 分数公式

要用`sklearn`应用它，首先，我们必须导入相关的`StandardScaler`类并实例化一个对象:

```
from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()
```

这一次，我们没有调用`fit()`然后调用`transform()`，而是使用了`fit_transform()`方法:

```
X_scaled = standard_scaler.fit_transform(X)
X_scaled
```

您应该得到以下输出:

![Figure 5.48: Z-score-standardized data
](image/C15019_05_48.jpg)

图 5.48: Z 分数标准化数据

现在我们来看看每个轴的最小值和最大值:

```
X_scaled[:,0].min(), X_scaled[:,0].max(), X_scaled[:,1].min(), X_scaled[:,1].max()
```

您应该得到以下输出:

![Figure 5.49: Minimum and maximum values of the z-score-standardized data
](image/C15019_05_49.jpg)

图 5.49:z 分数标准化数据的最小值和最大值

两个轴的值范围现在低得多，我们可以看到它们的最大值约为 9 和 18，这表明数据中存在一些极端的异常值。

现在，使用以下代码片段拟合 k-means 模型并绘制 z 分数标准化数据的散点图:

```
kmeans = KMeans(random_state=42, n_clusters=3, init='k-means++', n_init=5)
kmeans.fit(X_scaled)
df['cluster7'] = kmeans.predict(X_scaled)
alt.Chart(df).mark_circle().encode(x='Average net tax', y='Average total deductions',color='cluster7:N',
tooltip=['Postcode', 'cluster7', 'Average net tax', 'Average total deductions']
).interactive()
```

您应该得到以下输出:

![Figure 5.50: Scatter plot of the standardized data
](image/C15019_05_50.jpg)

图 5.50:标准化数据的散点图

k-means 结果与标准化数据非常不同。现在我们可以看到有两个主要的集群(蓝色和橙色),它们的边界不再是直的垂直线，而是对角线。所以，k-means 现在实际上同时考虑了两个轴。与之前的迭代相比，红色聚类包含的数据点要少得多，并且它似乎将所有具有高值的极端异常值分组在一起。如果您的项目是关于检测异常的，那么您应该已经找到了一种方法来轻松地将异常值从“正常”的观察值中分离出来。

## 练习 5.06:标准化数据集的数据

在最后一个练习中，我们将使用最小-最大缩放和 z 得分来标准化数据，并为每种方法拟合 k 均值模型，然后查看它们对 k 均值的影响:

1.  打开新的 Colab 笔记本。
2.  现在导入所需的`pandas`、`sklearn`和`altair`包:

    ```
    import pandas as pd
    from sklearn.cluster import KMeans
    import altair as alt 
    ```

3.  加载数据集，并选择与*练习 5.02* 、*中相同的列，使用`pandas`包中的`read_csv()`方法将澳大利亚邮政编码按照业务收入和支出*进行聚类:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter05/DataSet/taxstats2015.csv'
    df = pd.read_csv(file_url, usecols=['Postcode', 'Average total business income', 'Average total business expenses'])
    ```

4.  将`'Average total business income'`和`'Average total business expenses'`列分配给一个名为`X` :

    ```
    X = df[['Average total business income', 'Average total business expenses']]
    ```

    的新变量
5.  从`sklearn` :

    ```
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler
    ```

    中导入`MinMaxScaler`和`StandardScaler`类
6.  Instantiate and fit `MinMaxScaler` with the data:

    ```
    min_max_scaler = MinMaxScaler()
    min_max_scaler.fit(X)
    ```

    您应该得到以下输出:

    ![Figure 5.51: Summary of the min-max scaler
    ](image/C15019_05_51.jpg)

    图 5.51:最小-最大缩放器总结

7.  Perform the min-max scaling transformation and save the data into a new variable called `X_min_max`:

    ```
    X_min_max = min_max_scaler.transform(X)
    X_min_max
    ```

    您应该得到以下输出:

    ![Figure 5.52: Min-max-scaled data
    ](image/C15019_05_52.jpg)

    图 5.52:最小-最大比例数据

8.  用以下超参数对缩放后的数据拟合 k-means 模型:`random_state=1`，`n_clusters=4, init='k-means++', n_init=5`，如以下代码片段所示:

    ```
    kmeans = KMeans(random_state=1, n_clusters=4, init='k-means++', n_init=5)
    kmeans.fit(X_min_max)
    ```

9.  在`df`数据帧:

    ```
    df['cluster8'] = kmeans.predict(X_min_max)
    ```

    中名为`'cluster8'`的新列中，分配`X`的每个值的 k 均值预测
10.  Plot the k-means results into a scatter plot using the `altair` package:

    ```
    scatter_plot = alt.Chart(df).mark_circle()
    scatter_plot.encode(x='Average total business income', y='Average total business expenses',color='cluster8:N',
    tooltip=['Postcode', 'cluster8', 'Average total business income', 'Average total business expenses']
    ).interactive()
    ```

    您应该得到以下输出:

    ![Figure 5.53: Scatter plot of k-means results using the min-max-scaled data
    ](image/C15019_05_53.jpg)

    图 5.53:使用最小-最大比例数据的 k 均值结果散点图

11.  重新训练 k-means 模型，但是基于具有相同超参数值的 z 分数标准化数据，`random_state=1, n_clusters=4, init='k-means++', n_init=5` :

    ```
    standard_scaler = StandardScaler()
    X_scaled = standard_scaler.fit_transform(X)
    kmeans = KMeans(random_state=1, n_clusters=4, init='k-means++', n_init=5)
    kmeans.fit(X_scaled)
    ```

12.  在`df`数据帧:

    ```
    df['cluster9'] = kmeans.predict(X_scaled)
    ```

    中名为`'cluster9'` 的新列中，分配`X_scaled`的每个值的 k 均值预测
13.  Plot the k-means results in a scatter plot using the `altair` package:

    ```
    scatter_plot = alt.Chart(df).mark_circle()
    scatter_plot.encode(x='Average total business income', y='Average total business expenses',color='cluster9:N',
        tooltip=['Postcode', 'cluster9', 'Average total business income', 'Average total business expenses']
    ).interactive()
    ```

    您应该得到以下输出:

![Figure 5.54: Scatter plot of k-means results using the z-score-standardized data
](image/C15019_05_54.jpg)

图 5.54:使用 z 分数标准化数据的 k 均值结果散点图

k-means 聚类结果在 min-max 和 z-score 标准化之间非常相似，它们是*步骤 10* 和 *13* 的输出。与*练习 5.04* 、*中使用不同的初始化参数来实现合适的结果*相比，我们可以看到，在标准化之后，集群 1 和集群 2 之间的边界略低；事实上，蓝色星团在输出中交换了位置。如果您查看*练习 5.04* 、*中的*图 5.31* 使用不同的初始化参数来实现合适的结果*，您会注意到与*练习 5.05* 、*中的*图 5.54* 相比，簇 0 中的切换(蓝色)在我们的数据集*中找到最近的质心。这些结果非常接近的原因是由于这两个变量(平均总营业收入和平均总营业费用)的取值范围几乎相同:在 0 到 900，000 之间。因此，k-means 并没有将更多的权重放在这些变量中的一个上。

你已经完成了本章的最后一个练习。您已经学习了如何在用两种非常流行的方法拟合 k-means 模型之前预处理数据:最小-最大缩放和 z 得分。

## 活动 5.01:在一家银行使用 k-means 进行客户细分分析

你在一家国际银行工作。信贷部门正在审查其产品，并希望更好地了解其当前的客户。你的任务是进行客户细分分析。您将使用 k-means 执行聚类分析，以识别相似的客户群。

以下步骤将帮助您完成此活动:

1.  下载数据集并将其加载到 Python 中。
2.  使用`read_csv()`方法读取 CSV 文件。
3.  通过实例化一个`StandardScaler`对象来执行数据标准化。
4.  分析并定义最佳集群数量。
5.  用默认超参数拟合 k 均值。
6.  画出簇和它们的质心。
7.  调整超参数并重新训练 k-means。
8.  Analyze and interpret the clusters found.

    注意

    这是来自 UCI 机器学习知识库的德国信用数据集。

    可从位于 https://packt.live/31L2c2b 的[Packt 仓库获得。](https://packt.live/31L2c2b)

该数据集为`.dat`文件格式。您仍然可以使用`read_csv()`加载文件，但是您需要指定以下参数:`header=None, sep= '\s\s+' and prefix='X'`。

尽管这个数据集中的所有列都是整数，但是它们中的大多数实际上都是分类变量。这些列中的数据不是连续的。只有两个变量是真正的数字。找到并使用它们进行聚类。

您应该得到以下输出:

![Figure 5.55: Scatter plot of the four clusters found
](image/C15019_05_55.jpg)

图 5.55:发现的四个集群的散点图

注意

这个活动的解决方案可以在以下地址找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

# 总结

现在，您可以使用 k-means 算法对自己的数据集执行聚类分析了。这种类型的分析在行业中非常流行，用于细分客户档案以及检测可疑交易或异常情况。

我们学习了很多不同的概念，比如质心和平方欧几里得距离。我们检查了主要的 k-means 超参数:`init`(初始化方法)`n_init`(初始化运行次数)`n_clusters`(聚类数)，以及`random_state`(指定种子)。我们还讨论了选择最佳聚类数、正确初始化质心和标准化数据的重要性。你已经学会了如何使用以下 Python 包:`pandas`、`altair`、`sklearn`和`KMeans`。

在本章中，我们只看了 k-means，但它不是唯一的聚类算法。有相当多的算法使用不同的方法，例如层次聚类、主成分分析和高斯混合模型等等。如果您对这个领域感兴趣，那么您现在已经拥有了自己探索这些其他算法所需的所有基础知识。

接下来，您将看到我们如何评估这些模型的性能，以及可以使用什么工具来使它们变得更好。