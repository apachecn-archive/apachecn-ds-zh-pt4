

# 十、分析数据集

概述

本章结束时，你将能够解释执行探索性数据分析所涉及的关键步骤；确定数据集中包含的数据类型；对数据集进行总结，并详细说明每个变量；可视化每列中的数据分布；查找变量之间的关系，并分析每个变量的缺失值和异常值

本章将向您介绍执行探索性数据分析和可视化数据的艺术，以便识别质量问题、潜在的数据转换和有趣的模式。

# 简介

在前一章中，我们一直在改进我们的机器学习模型，调整其超参数，并解释其结果和参数，以向业务部门提供有意义的见解。本章开启了本书的第三部分:增强数据集。在接下来的三章中，我们将后退一步，专注于任何机器学习模型的关键输入:数据集。我们将学习如何探索新的数据集，为建模阶段做准备，以及创建新的变量(也称为特征工程)。这些都是非常令人兴奋和重要的话题，所以让我们开始吧。

当我们提到数据科学时，大多数人都会想到构建花哨的机器学习算法来预测未来的结果。他们通常不会考虑数据科学项目中涉及的所有其他关键任务。实际上，建模步骤只涵盖了这样一个项目的一小部分。你可能已经听说过*经验法则*指出数据科学家只花 20%的时间拟合模型，其余 80%的时间用于理解和准备数据。这个其实还是挺接近现实的。

行业中运行数据科学项目的一种非常流行的方法是 CRISP-DM。

注意

我们不会对这种方法进行过多的讨论，因为这超出了本书的范围。但是如果你有兴趣了解更多，你可以在这里找到 CRISP-DM 的描述:[https://packt.live/2QMRepG](https://packt.live/2QMRepG)。

这种方法将数据科学项目分为六个不同的阶段:

1.  商业理解
2.  数据理解
3.  数据准备
4.  建模
5.  估价
6.  部署

正如您所看到的，建模只代表了六个阶段中的一个阶段，它发生在项目接近尾声的时候。在本章中，我们将主要关注 CRISP-DM 的第二步:数据理解阶段。

你可能想知道为什么理解数据如此重要，为什么我们不应该在建模上花更多的时间。一些研究人员实际上已经表明，在高质量数据上训练非常简单的模型比在坏数据上训练非常复杂的模型要好。

如果你的数据不正确，即使是最先进的模型也无法找到相关的模式并预测正确的结果。这是*垃圾入，垃圾出*，意思是错误的输入会导致错误的输出。因此，我们需要很好地掌握数据集的局限性和问题，并在将它放入模型之前解决它们。

理解输入数据如此重要的第二个原因是，它还将帮助我们定义正确的方法，并相应地列出相关算法。例如，如果您发现与数据集中的其他类相比，某个特定类的代表性较低，您可能希望使用能够处理不平衡数据的特定算法，或者预先使用一些重采样技术来使类分布更加均匀。

在这一章中，您将了解一些关键概念和技术，以便更深入、更好地理解您的数据。

# 探索您的数据

如果您按照 CRISP-DM 方法运行您的项目，第一步将是与涉众讨论项目，并清楚地定义他们的需求和期望。只有清楚了这一点，你才能开始查看数据，看看你是否能够实现这些目标。

收到数据集后，您可能希望确保数据集包含项目所需的信息。例如，如果您正在进行受监督的项目，您将检查该数据集是否包含您需要的目标变量，以及该字段是否有任何缺失或不正确的值。您还可以检查有多少观察值(行)和变量(列)。这些是您最初在使用新数据集时会遇到的问题。本节将向您介绍一些可以用来回答这些问题的技巧。

在本节的其余部分，我们将使用包含在线零售店交易的数据集。

注意

这个数据集在我们的 GitHub 存储库中:https://packt。直播/36s4XIN。

它来源于，由陈大庆、赛梁赛恩和郭昆提供，UCI 机器学习知识库，在线零售行业的数据挖掘。

我们的数据集是一个 Excel 电子表格。幸运的是，`pandas`包提供了一种方法，我们可以用它来加载这种类型的文件:`read_excel()`。

让我们使用`.read_excel()`方法读取数据，并将其存储在`pandas`数据帧中，如下面的代码片段所示:

```
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

将数据加载到 DataFrame 中后，我们希望知道这个数据集的大小，即它的行数和列数。为了获得这些信息，我们只需要从`pandas`中调用`.shape`属性:

```
df.shape
```

您应该得到以下输出:

```
(541909, 8)
```

该属性返回一个元组，其中第一个元素是行数，第二个元素是列数。加载的数据集包含`541909`行和`8`不同的列。

因为这个属性返回一个元组，所以我们可以通过提供相关的索引来独立地访问它的每个元素。让我们提取行数(索引`0`):

```
df.shape[0]
```

您应该得到以下输出:

```
541909
```

类似地，我们可以得到第二个索引的列数:

```
df.shape[1]
```

您应该得到以下输出:

```
8
```

通常，数据集的第一行是标题。它包含每列的名称。默认情况下，`read_excel()`方法假设文件的第一行是标题。如果`header`存储在不同的行中，您可以使用来自`read_excel()`的参数 header 为标题指定不同的索引，例如`pd.read_excel(header=1)`用于指定标题列是第二行。

一旦加载到一个`pandas`数据帧中，您可以通过直接调用它来打印出它的内容:

```
df
```

您应该得到以下输出:

![Figure 10.1: First few rows of the loaded online retail DataFrame
](image/C15019_10_01.jpg)

图 10.1:加载的在线零售数据框架的前几行

要访问这个数据帧的列名，我们可以调用`.columns`属性:

```
df.columns
```

您应该得到以下输出:

![Figure 10.2: List of the column names for the online retail DataFrame
](image/C15019_10_02.jpg)

图 10.2:在线零售数据框架的列名列表

这个数据集中的列是`InvoiceNo`、`StockCode`、`Description`、`Quantity`、`InvoiceDate`、`UnitPrice`、`CustomerID`和`Country`。我们可以推断，该数据集中的一行表示特定客户在特定日期以给定数量和价格销售的商品。

查看这些名称，我们可以猜测这些列中包含什么类型的信息，但是，为了确定，我们可以使用`dtypes`属性，如下面的代码片段所示:

```
df.dtypes
```

您应该得到以下输出:

![Figure 10.3: Description of the data type for each column of the DataFrame
](image/C15019_10_03.jpg)

图 10.3:数据帧中每一列的数据类型描述

从这个输出中，我们可以看到`InvoiceDate`列是一个日期类型(`datetime64[ns]`)，`Quantity`是一个整数(`int64`)，而`UnitPrice`和`CustmerID`是十进制数(`float64`)。剩余的列是文本(`object`)。

`pandas`包提供了一个方法，可以显示我们到目前为止看到的所有信息，也就是`info()`方法:

```
df.info()
```

您应该得到以下输出:

![Figure 10.4: Output of the info() method
](image/C15019_10_04.jpg)

图 10.4:info()方法的输出

在短短几行代码中，我们了解了关于这个数据集的一些高级信息，比如它的大小、列名及其类型。

在下一节中，我们将分析数据集的内容。

# 分析数据集

之前，我们了解了数据集的整体结构及其包含的信息种类。现在，是时候真正深入研究它并查看每一列的值了。

首先，我们需要导入`pandas`包:

```
import pandas as pd
```

然后，我们将数据加载到一个`pandas`数据帧中:

```
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

`pandas`包提供了几种方法，这样您就可以显示数据集的快照。最受欢迎的是`head()`、`tail()`和`sample()`。

`head()`方法将显示数据集的顶行。默认情况下，`pandas`将显示前五行:

```
df.head()
```

您应该得到以下输出:

![Figure 10.5: Displaying the first five rows using the head() method
](image/C15019_10_05.jpg)

图 10.5:使用 head()方法显示前五行

`head()`方法的输出显示出`InvoiceNo`、`StockCode`和`CustomerID`列是每个采购发票、售出商品和客户的唯一标识符字段。`Description`字段是描述售出商品的文本。`Quantity`和`UnitPrice`分别是售出的商品数量和单价。`Country`是一个文本字段，可用于指定客户或商品位于何处，或从哪个国家版本的在线商店下订单。在一个真实的项目中，您可能会联系提供该数据集的团队，并确认`Country`列的含义，或者您可能需要的任何其他列细节。

使用`pandas`，您可以通过提供一个整数作为参数来指定要用`head()`方法显示的顶部行数。让我们通过显示第一个`10`行来尝试一下:

```
df.head(10)
```

您应该得到以下输出:

![Figure 10.6: Displaying the first 10 rows using the head() method
](image/C15019_10_06.jpg)

图 10.6:使用 head()方法显示前 10 行

查看这个输出，我们可以假设数据按照`InvoiceDate`列排序，并按照`CustomerID`和`InvoiceNo`分组。我们只能在`Country`列中看到一个值:`United Kingdom`。让我们通过查看数据集的最后几行来检查这是否是真的。这可以通过调用`tail()`方法来实现。像`head()`一样，默认情况下，这个方法将只显示 5 行，但是您可以指定想要的行数作为参数。这里，我们将显示最后八行:

```
df.tail(8)
```

您应该得到以下输出:

![Figure 10.7: Displaying the last eight rows using the tail() method
](image/C15019_10_07.jpg)

图 10.7:使用 tail()方法显示最后八行

看来我们假设数据按照`InvoiceDate`列的升序排序是正确的。我们还可以确认在`Country`列中实际上有不止一个值。

我们还可以使用`sample()`方法通过`n`参数从数据集中随机选取给定数量的行。您还可以指定一个**种子**(我们在*第 5 章*、*执行您的第一次聚类分析*中讨论过)，以便在使用`random_state`参数再次运行相同的代码时获得可重复的结果:

```
df.sample(n=5, random_state=1)
```

您应该得到以下输出:

![Figure 10.8: Displaying five random sampled rows using the sample() method
](image/C15019_10_08.jpg)

图 10.8:使用 sample()方法显示五个随机抽样的行

在这个输出中，我们可以在`Country`列中看到一个附加值:`Germany`。我们还可以注意到几个有趣的点:

*   `InvoiceNo`也可以包含字母(第`94,801`行以`C`开头，可能有特殊含义)。
*   `Quantity`可以有负值:`-2`(第`94801`)。
*   `CustomerID`包含缺失值:`NaN`(第`210111`行)。

## 练习 10.01:用描述性统计研究埃姆斯住宅数据集

在本练习中，我们将探索`Ames Housing dataset`,通过分析它的结构和查看它的一些行来更好地理解它。

我们将在这个练习中使用的数据集是 Ames Housing 数据集，可以在我们的 GitHub 存储库中找到:[https://packt.live/35kRKAo](https://packt.live/35kRKAo)。

注意

这个数据集是由迪安·德·科克编辑的。

该数据集包含爱荷华州埃姆斯市 2016 年至 2010 年间的住宅销售列表。

关于每个变量的更多信息可以在[https://packt.live/2sT88L4](https://packt.live/2sT88L4)找到。

以下步骤将帮助您完成本练习:

1.  打开一本新的笔记本。
2.  导入`pandas`包:

    ```
    import pandas as pd
    ```

3.  Assign the link to the AMES dataset to a variable called `file_url`:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter10/dataset/ames_iowa_housing.csv'
    ```

    注意

    文件 URL 是原始数据集 URL，可以在 GitHub 上找到。

4.  使用`pandas` 包中的`.read_csv()`方法，将数据集加载到名为`df` :

    ```
    df = pd.read_csv(file_url)
    ```

    的新变量中
5.  Print the number of rows and columns of the DataFrame using the `shape` attribute from the `pandas` package:

    ```
    df.shape
    ```

    您应该得到以下输出:

    ```
    (1460, 81)
    ```

    我们可以看到这个数据集包含了`1460`行和`81`个不同的列。

6.  Print the names of the variables contained in this DataFrame using the `columns` attribute from the `pandas` package:

    ```
    df.columns
    ```

    您应该得到以下输出:

    ![Figure 10.9: List of columns in the housing dataset
    ](image/C15019_10_09.jpg)

    图 10.9:住房数据集中的列列表

    我们可以通过查看一些变量的名称来推断其中包含的信息类型，如`LotArea`(房产大小)`YearBuilt`(建造年份)`SalePrice`(房产销售价格)。

7.  Print out the type of each variable contained in this DataFrame using the `dtypes` attribute from the `pandas` package:

    ```
    df.dtypes
    ```

    您应该得到以下输出:

    ![Figure 10.10: List of columns and their type from the housing dataset
    ](image/C15019_10_10.jpg)

    图 10.10:房屋数据集中的列及其类型列表

    我们可以看到变量要么是数字类型，要么是文本类型。此数据集中没有日期列。

8.  Display the top rows of the DataFrame using the `head()` method from `pandas`:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 10.11: First five rows of the housing dataset
    ](image/C15019_10_11.jpg)

    图 10.11:住房数据集的前五行

9.  Display the last five rows of the DataFrame using the `tail()` method from `pandas`:

    ```
    df.tail()
    ```

    您应该得到以下输出:

    ![Figure 10.12: Last five rows of the housing dataset
    ](image/C15019_10_12.jpg)

    图 10.12:住房数据集的最后五行

    看来`Alley`列有很多缺失值，用`NaN`值表示(T1 代表`Not a Number`)。`Street`和`Utilities`列似乎只有一个值。

10.  Now, display `5` random sampled rows of the DataFrame using the `sample()` method from `pandas` and pass it a `'random_state'` of `8`:

    ```
    df.sample(n=5, random_state=8)
    ```

    您应该得到以下输出:

![Figure 10.13: Five randomly sampled rows of the housing dataset
](image/C15019_10_13.jpg)

图 10.13:住房数据集的五个随机抽样行

通过这些随机样本，我们可以看到`LotFrontage`列也有一些缺失值。我们还可以看到，这个数据集包含数字和文本数据(对象类型)。我们将在*练习 10.02* 、*分析来自埃姆斯住宅数据集的分类变量*和*练习 10.03* 、*分析来自埃姆斯住宅数据集的数值变量*中对它们进行更详细的分析。

我们仅用几行代码就了解了这个数据集的很多信息，比如行数和列数、每个变量的数据类型以及它们的信息。我们还发现了一些缺少值的问题。

# 分析分类变量的内容

现在我们已经对包含在`online retail dataset`中的信息有了很好的感觉，我们想更深入地挖掘它的每一列:

```
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

例如，我们想通过调用`nunique()`方法来知道每个变量中包含多少不同的值。这对于值数量有限的分类变量特别有用，例如`Country`:

```
df['Country'].nunique()
```

您应该得到以下输出:

```
38
```

我们可以看到这个数据集中有 38 个不同的国家。如果我们能得到这个列中所有值的列表，那就太好了。幸运的是，`pandas`包提供了一种方法来获得这些结果:`unique()`:

```
df['Country'].unique()
```

您应该得到以下输出:

![Figure 10.14: List of unique values for the ‘Country’ column
](image/C15019_10_14.jpg)

图 10.14:“国家”列的唯一值列表

我们可以看到，有来自不同大陆的多个国家，但大多数来自欧洲。我们还可以看到有一个值叫做`Unspecified`，还有一个值叫做`European Community`，可能是针对欧元区所有没有单独列出的国家。

来自`pandas` 的另一个非常有用的方法是`value_counts()`。该方法列出了给定列中的所有值以及它们的出现次数。通过提供`dropna=False`和`normalise=True`参数，该方法将在列表中包含缺失值，并分别计算出现次数的比率:

```
df['Country'].value_counts(dropna=False, normalize=True)
```

您应该得到以下输出:

![Figure 10.15: List of unique values and their occurrence percentage for the ‘Country’ column
](image/C15019_10_15.jpg)

图 10.15:“国家”列的唯一值及其出现百分比列表

从这个输出中，我们可以看到,`United Kingdom`值完全控制了这个列，因为它代表了超过 91%的行，而其他值，如`Austria`和`Denmark`非常罕见，因为它们代表了这个数据集的不到 1%。

## 练习 10.02:分析埃姆斯住宅数据集中的分类变量

在本练习中，我们将通过分析数据集的分类变量来继续我们的数据集探索。为此，我们将实现自己的`describe`函数。

我们将在这个练习中使用的数据集是 Ames Housing 数据集，可以在我们的 GitHub 存储库中找到:[https://packt.live/35kRKAo](https://packt.live/35kRKAo)。让我们开始吧:

1.  打开新可乐 b 笔记本。
2.  导入`pandas` 包:

    ```
    import pandas as pd
    ```

3.  Assign the following link to the AMES dataset to a variable called `file_url`:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter10/dataset/ames_iowa_housing.csv'
    ```

    注意

    文件 URL 是原始数据集 URL，可以在 GitHub 上找到。

4.  使用`pandas`包中的`.read_csv()`方法，将数据集加载到名为`df` :

    ```
    df = pd.read_csv(file_url)
    ```

    的新变量中
5.  使用`pandas`包中的`select_dtypes`方法创建一个名为`obj_df`的新数据框架，其中只有数值类型的列。然后，将`object`值传递给`include` 参数:

    ```
    obj_df = df.select_dtypes(include='object')
    ```

6.  Using the `columns` attribute from `pandas`, extract the list of columns of this DataFrame, `obj_df`, assign it to a new variable called `obj_cols`, and print its content:

    ```
    obj_cols = obj_df.columns
    obj_cols
    ```

    您应该得到以下输出:

    ![Figure 10.16: List of categorical variables
    ](image/C15019_10_16.jpg)

    图 10.16:分类变量列表

7.  创建一个名为`describe_object`的函数，它将一个`pandas` 数据帧和一个列名作为输入参数。然后，在函数内部，使用`nunique()`方法打印出给定列的名称、唯一值的数量，使用`value_counts()`方法打印出值的列表及其出现次数，如下面的代码片段所示:

    ```
    def describe_object(df, col_name):
      print(f"\nCOLUMN: {col_name}")
      print(f"{df[col_name].nunique()} different values")
      print(f"List of values:")
      print(df[col_name].value_counts(dropna=False, normalize=True))
    ```

8.  Test this function by providing the `df` DataFrame and the `'MSZoning'` column:

    ```
    describe_object(df, 'MSZoning')
    ```

    您应该得到以下输出:

    ![Figure 10.17: Display of the created function for the MSZoning column
    ](image/C15019_10_17.jpg)

    图 10.17:为 MSZoning 列创建的函数的显示

    对于`MSZoning`列，`RL`值几乎代表了值的`79%`，而`C` `(all)`仅出现在少于`1%`行中。

9.  Create a `for` loop that will call the created function for every element from the `obj_cols` list:

    ```
    for col_name in obj_cols:
      describe_object(df, col_name)
    ```

    您应该得到以下输出:

![Figure 10.18: Display of the created function for the first columns contained in obj_cols
](image/C15019_10_18.jpg)

图 10.18:显示为 obj_cols 中包含的第一列创建的函数

我们可以确认`Street`列几乎是恒定的，因为 99.6%的行包含相同的值:`Pave`。对于列，即`Alley`，几乎 94%的行都缺少值。

我们刚刚分析了这个数据集中的所有分类变量。我们看到了如何查看任何特性中包含的所有值的分布。我们还发现，其中一些是由一个单一的价值占主导地位，而另一些主要是缺失的价值。

# 汇总数字变量

现在，让我们看看一个数字列，并很好地理解它的内容。我们将使用一些统计方法来概括一个变量。所有这些措施被称为描述性统计。在这一章中，我们将向你介绍最受欢迎的。

有了`pandas`包，许多这样的措施已经作为方法实现了。例如，如果我们想知道`'Quantity'`列中包含的最大值是多少，我们可以使用`.max()`方法:

```
df['Quantity'].max()
```

您应该得到以下输出:

```
80995
```

我们可以看到，在这个数据集中，一件商品的最大销售量是`80995`，这对于一个零售企业来说似乎是非常高的。在实际项目中，这种意外值必须与数据所有者或关键利益相关者讨论和确认，以确定这是真实值还是错误值。现在，让我们用`.min()`方法来看看`'Quantity'`的最小值:

```
df['Quantity'].min()
```

您应该得到以下输出:

```
-80995
```

这个变量的最小值非常低。我们可以认为返回的项目可能有负值，但是这里的最小值(`-80995`)非常低。同样，这需要与您组织中的相关人员进行确认。

现在，我们要看看这个专栏的中心倾向。集中趋势是一个统计学术语，指的是数据聚集的中心点。最著名的中心趋势度量是平均值。平均值的计算方法是将一列中的所有值相加，然后除以值的个数。

如果我们将`Quantity` 列用其平均值绘制在图表上，我们将得到以下输出:

![Figure 10.19: Average value for the ‘Quantity’ column
](image/C15019_10_19.jpg)

图 10.19:“数量”列的平均值

我们可以看到`Quantity` 列的平均值非常接近于 0，大部分数据在`-50`和`+50`之间。

我们可以通过使用来自`pandas`的`mean()`方法获得一个特征的平均值:

```
df['Quantity'].mean()
```

您应该得到以下输出:

```
9.55224954743324
```

在这个数据集中，售出商品的平均数量在`9.55`左右。平均值对异常值非常敏感，正如我们之前看到的，`Quantity`列的最小值和最大值非常极端(`-80995 to +80995`)。

我们可以使用中间值作为集中趋势的另一种度量。中位数的计算方法是将列分成长度相等的两组，然后通过将这两组分开来获得中间点的值，如下例所示:

![Figure 10.20: Sample median example
](image/C15019_10_20.jpg)

图 10.20:样本中值示例

在`pandas`中，可以调用`median()`方法来获得这个值:

```
df['Quantity'].median()
```

您应该得到以下输出:

```
3.0
```

该列的中值是 3，这与我们之前找到的平均值(`9.55`)有很大不同。这告诉我们，在这个数据集中有一些异常值，我们必须在做了更多调查后决定如何处理它们(这将在第十一章*、*准备数据*中讨论)。*

我们还可以评估该列的分布(数据点与中心点的差异有多大)。一个常见的衡量标准是标准差。这个度量值越小，数据就越接近其平均值。另一方面，如果标准偏差很高，这意味着有些观察值远离平均值。我们将使用`pandas` 中的`std()`方法来计算该度量:

```
df['Quantity'].std()
```

您应该得到以下输出:

```
218.08115784986612
```

正如所料，该列的标准偏差很高，因此数据与平均值相差很大，在本例中为`9.55`。

在`pandas` 包中，有一个方法可以用一行代码显示大部分描述性统计数据:`describe()`:

```
df.describe()
```

您应该得到以下输出:

![Figure 10.21: Output of the describe() method
](image/C15019_10_21.jpg)

图 10.21:describe()方法的输出

对于`Quantity`列，我们得到了与前面看到的完全相同的值。该方法计算了三个数字列(`Quantity`、`UnitPrice`和`CustomerID`)的描述性统计量。

尽管`CustomerID`列只包含数字数据，但我们知道这些值用于识别每个客户，没有数学意义。例如，在表中添加客户 ID `12680 to 17850`或者计算这些标识符的平均值是没有意义的。这一栏实际上不是数字，而是分类。

`describe()`方法不知道这个信息，只是注意到有数字，所以它假设这是一个数字变量。这是一个完美的例子，说明了为什么您应该完全理解您的数据集，并在将数据提供给算法之前确定要修复的问题。在这种情况下，我们必须将该列的类型更改为 categorical。在*第 11 章*、*准备数据*中，我们将看到我们如何处理这类问题，但现在，我们将看看一些图形工具和技术，它们将帮助我们更好地理解数据。

## 练习 10.03:分析埃姆斯住宅数据集中的数值变量

在本练习中，我们将通过分析数据集的数值变量来继续我们的数据集探索。为此，我们将实现自己的`describe` 函数。

我们将在这个练习中使用的数据集是 Ames Housing 数据集，可以在我们的 GitHub 存储库中找到:[https://packt.live/35kRKAo](https://packt.live/35kRKAo)。让我们开始吧:

1.  打开一本新的笔记本。
2.  导入`pandas`包:

    ```
    import pandas as pd
    ```

3.  Assign the link to the AMES dataset to a variable called `file_url`:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter10/dataset/ames_iowa_housing.csv'
    ```

    注意

    文件 URL 是原始数据集 URL，可以在 GitHub 上找到。

4.  使用`pandas` 包中的`.read_csv()`方法，将数据集加载到名为`df` :

    ```
    df = pd.read_csv(file_url)
    ```

    的新变量中
5.  使用`pandas` 包中的`select_dtypes`方法创建一个名为`num_df`的新数据帧，其中只有数字列，并将`'number'`值传递给`include`参数:

    ```
    num_df = df.select_dtypes(include='number')
    ```

6.  Using the `columns` attribute from `pandas`, extract the list of columns of this DataFrame, `num_df`, assign it to a new variable called `num_cols`, and print its content:

    ```
    num_cols = num_df.columns
    num_cols
    ```

    您应该得到以下输出:

    ![Figure 10.22: List of numerical columns
 
    ](image/C15019_10_22.jpg)

    图 10.22:数字列列表

7.  Create a function called `describe_numeric` that takes a `pandas` DataFrame and a column name as input parameters. Then, inside the function, print out the name of the given column, its minimum value using `min()`, its maximum value using `max()`, its average value using `mean()`, its standard deviation using `std()`, and its `median` using `median()`:

    ```
    def describe_numeric(df, col_name):
      print(f"\nCOLUMN: {col_name}")
      print(f"Minimum: {df[col_name].min()}")
      print(f"Maximum: {df[col_name].max()}")
      print(f"Average: {df[col_name].mean()}")
      print(f"Standard Deviation: {df[col_name].std()}")
      print(f"Median: {df[col_name].median()}")
    ```

    您应该得到以下输出:

    ![Figure 10.23: Output showing the Min, Max, Avg, Std Deviation, and the Median
    ](image/C15019_10_23.jpg)

    图 10.23:显示最小值、最大值、平均值、标准差和中值的输出

8.  Now, test this function by providing the `df` DataFrame and the `SalePrice` column:

    ```
    describe_numeric(df, 'SalePrice')
    ```

    您应该得到以下输出:

    ![Figure 10.24: The display of the created function for the ‘SalePrice’ column
    ](image/C15019_10_24.jpg)

    图 10.24:为“销售价格”列创建的函数的显示

    售价从`34,900`到`755,000` 不等，平均为`180,921`。中位数略低于平均值，这告诉我们有一些高销售价格的异常值。

9.  Create a `for` loop that will call the created function for every element from the `num_cols` list:

    ```
    for col_name in num_cols:
      describe_numeric(df, col_name)
    ```

    您应该得到以下输出:

![Figure 10.25: Display of the created function for the first few columns contained in ‘num_cols’
](image/C15019_10_25.jpg)

图 10.25:显示为“num_cols”中包含的前几列创建的函数

`Id`列的范围从`1`到`1460`，这是该数据集中行数的精确值。这意味着这个列肯定是被出售的资产的唯一标识符。看起来来自`MSSubClass`的值都被四舍五入了。这可能表明该列中包含的信息已经被聚类成 10 个一组或分类变量。

我们看到了如何用几行代码来研究新收到的数据集。这有助于我们理解它的结构，每个变量中包含的信息类型，也有助于我们识别一些潜在的数据质量问题，例如缺失值或不正确的值。

# 可视化您的数据

在上一节中，我们看到了如何探索一个新的数据集并计算一些简单的描述性统计数据。这些措施有助于将数据集总结成可解释的指标，如平均值或最大值。现在是时候更深入地研究，并使用数据可视化获得每个列的更细粒度的视图了。

在数据科学项目中，数据可视化可用于数据分析或交流获得的见解。以利益相关者可以轻松理解和解释的可视化方式呈现结果，绝对是任何优秀数据科学家的必备技能。

然而，在这一章中，我们将把重点放在使用数据可视化来分析数据。与阅读书面信息相比，大多数人倾向于更容易理解图表上的信息。例如，在查看以下描述性统计数据和同一变量的散点图时，您认为哪一个更容易解释？让我们来看看:

![Figure 10.26: Sample visual data analysis 
](image/C15019_10_26.jpg)

图 10.26:样本可视化数据分析

尽管描述性统计显示的信息更加详细，但是通过查看图表，您已经看到数据被拉伸，并且主要集中在值 0 附近。你可能花了不到 1 或 2 秒的时间得出这个结论，也就是说，在 0 值附近有一簇点，随着远离 0 值，这些点会减少。如果你是在解释描述性统计数据，得出这个结论会花费你更多的时间。这就是为什么数据可视化是有效分析数据的一个非常强大的工具。

## 如何使用 Altair API

我们将使用一个名为`altair`的包(如果您还记得，我们已经在第 5 章、*执行您的第一次聚类分析*中简要使用过它)。市场上有很多用于数据可视化的 Python 包，如`matplotlib`、`seaborn`或`Bokeh`，与它们相比，`altair`相对较新，但由于其简单的 API 语法，其用户群体正在快速增长。

让我们看看如何在在线零售数据集上逐步显示条形图。

首先，导入`pandas`和`altair`包:

```
import pandas as pd
import altair as alt
```

然后，将数据加载到`pandas`数据帧中:

```
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

我们将使用`sample()`方法对此数据帧的 5000 行进行随机采样(`altair` 需要额外的步骤来显示更大的数据集):

```
sample_df = df.sample(n=5000, random_state=8)
```

现在从`altair`实例化一个`Chart`对象，用`pandas` 数据帧作为它的输入参数:

```
base = alt.Chart(sample_df)
```

接下来，我们调用`mark_circle()`方法来指定我们想要绘制的图形类型:散点图:

```
chart = base.mark_circle()
```

最后，我们使用`encode()`方法指定将在 *x* 和 *y* 轴上显示的列的名称:

```
chart.encode(x='Quantity', y='UnitPrice')
```

我们刚刚用 7 行代码绘制了一个散点图:

![Figure 10.27: Output of the scatter plot
](image/C15019_10_27.jpg)

图 10.27:散点图的输出

Altair 提供了将其所有方法组合成一行代码的选项，如下所示:

```
alt.Chart(sample_df).mark_circle().encode(x='Quantity', y='UnitPrice')
```

您应该得到以下输出:

![Figure 10.28: Output of the scatter plot with combined altair methods
](image/C15019_10_28.jpg)

图 10.28:结合牛郎星方法的散点图输出

我们可以看到，我们得到了与之前完全相同的输出。这张图表向我们展示了两个变量都有很多异常值(极端值):`UnitPrice`的大部分值都低于 100，但也有一些超过 300，`Quantity`的范围从-200 到 800，而大部分观察值都在-50 到 150 之间。我们还可以注意到一种模式，其中单价高的项目数量较低(单价超过 50 的项目数量接近 0)，反之亦然(数量超过 100 的项目单价接近 0)。

现在，假设我们想要在添加`Country`列的信息时可视化相同的图。一个简单的方法是使用来自`encode()`方法的`color`参数。这将根据`Country`列中的值给所有数据点着色:

```
alt.Chart(sample_df).mark_circle().encode(x='Quantity', y='UnitPrice', color='Country')
```

您应该得到以下输出:

![Figure 10.29: Scatter plot with colors based on the ‘Country’ column
](image/C15019_10_29.jpg)

图 10.29:基于“国家”列的颜色散点图

我们将来自`Country`列的信息添加到图表中，但正如我们所看到的，有太多的值，很难区分国家:有许多蓝色点，但很难区分它们代表哪些国家。

使用`altair`，我们可以很容易地在图上添加一些交互，以便显示每个观察的更多信息；我们只需要使用来自`encode()`方法的`tooltip`参数，并指定要显示的列的列表，然后调用`interactive()`方法使整个事情交互(如前面在*第 5 章*、*执行您的第一个聚类分析*中所见):

```
alt.Chart(sample_df).mark_circle().encode(x='Quantity', y='UnitPrice', color='Country', tooltip=['InvoiceNo','StockCode','Description','InvoiceDate','CustomerID']).interactive()
```

您应该得到以下输出:

![Figure 10.30: Interactive scatter plot with tooltip
](image/C15019_10_30.jpg)

图 10.30:带有工具提示的交互式散点图

现在，如果我们将鼠标悬停在具有最高`UnitPrice`值的观测值(接近 600 的那个)上，我们可以看到工具提示显示的信息:这个观测值对于`StockCode`没有任何值，它的`Description`是`Manual`。所以，这似乎不是一个正常的交易发生在网站上。可能是人工输入系统的特殊订单。这是你必须与你的股东讨论并确认的事情。

## 数值变量直方图 les

现在我们已经熟悉了`altair` API，让我们看看一些特定类型的图表，它们将帮助我们分析和理解每个变量。首先，让我们关注在线零售数据集中的数字变量，如`UnitPrice`或`Quantity`。

对于这种类型的变量，直方图通常用于显示给定变量的分布。直方图的 x 轴将显示该列中的可能值，y 轴将绘制每个值下的观察值数量。由于一个数值变量的可能值的数量可能非常多(潜在的潜在值的数量可能是无限的)，因此最好将这些值按块(也称为 bin)分组。例如，我们可以将价格分成 10 个级别(即每组 10 件商品)，如 0 到 10、11 到 20、21 到 30 等等。

让我们用一个真实的例子来看看这个。我们将使用具有以下参数的`mark_bar()`和`encode()`方法绘制`'UnitPrice'`的直方图:

*   `alt.X("UnitPrice:Q", bin=True)`:这是另一个`altair` API 语法，允许你调整 x 轴的一些参数。在这里，我们告诉牛郎星使用`'UnitPrice'`列作为轴。`':Q'`指定该列是定量数据(即数值)，而`bin=True`强制将可能的值分组到箱中。
*   `y='count()'`:用于计算观察值的数量，并绘制在 y 轴上，如下所示:

```
alt.Chart(sample_df).mark_bar().encode(
alt.X("UnitPrice:Q", bin=True), 
y='count()'
)
```

您应该得到以下输出:

![Figure 10.31: Histogram for UnitPrice with the default bin step size
](image/C15019_10_31.jpg)

图 10.31:默认 bin 步长下的单价直方图

默认情况下，`altair`按 100 步的区间对观察值进行分组:0 到 100，然后是 100 到 200，依此类推。选择的步长不是最佳的，因为几乎所有的观察值都落在第一个箱(0 到 100)中，我们看不到任何其他箱。使用`altair`，我们可以指定参数 bin 的值，我们将使用 5，即`alt.Bin(step=5)`来尝试:

```
alt.Chart(sample_df).mark_bar().encode(
alt.X("UnitPrice:Q", bin=alt.Bin(step=5)), 
y='count()'
)
```

您应该得到以下输出:

![Figure 10.32: Histogram for UnitPrice with a bin step size of 5
](image/C15019_10_32.jpg)

图 10.32:仓位步长为 5 的单价直方图

这样好多了。用这个步长，我们可以看到大部分观测值的单价都在 5 以下(差不多 4200 个观测值)。我们还可以看到，500 多个数据点的单价在 10 美元以下。随着单价的增加，记录的数量不断减少。

让我们绘制`Quantity`列的直方图，仓步长为 10:

```
alt.Chart(sample_df).mark_bar().encode(
alt.X("Quantity:Q", bin=alt.Bin(step=10)), 
y='count()'
)
```

您应该得到以下输出:

![Figure 10.33: Histogram for Quantity with a bin step size of 10
](image/C15019_10_33.jpg)

图 10.33:箱步长为 10 的数量直方图

在此直方图中，大多数记录的数量在 0 到 30 之间(前三个最高的条柱)。还有一个包含大约 50 个观察值的箱，其数量从-10 到 0 不等。正如我们前面提到的，这些可能是客户退回的物品。

## 分类变量条形图

现在，我们来看看分类变量。对于此类变量，没有必要将值分组到箱中，因为根据定义，它们具有有限数量的潜在值。我们仍然可以使用一个简单的条形图来绘制这些柱的分布。在`altair`中，这非常简单——类似于绘制直方图，但没有`bin`参数。让我们在`Country`列上试试，看看每个值的记录数:

```
alt.Chart(sample_df).mark_bar().encode(x='Country',y='count()')
```

您应该得到以下输出:

![Figure 10.34: Bar chart of the Country column’s occurrence
](image/C15019_10_34.jpg)

图 10.34:国家/地区列出现的条形图

我们可以确认`United Kingdom`是该数据集中最具代表性的国家(到目前为止)，其次是`Germany`、`France`和`EIRE`。我们显然有不平衡的数据，这可能会影响预测模型的性能。在*第 13 章*、*不平衡数据集*中，我们将看看如何处理这种情况。

现在，我们来分析 datetime 列，即`InvoiceDate`。`altair`包提供了一些功能，我们可以使用这些功能按时间段对日期时间信息进行分组，比如日、星期几、月等等。例如，如果我们想要一个变量分布的月度视图，我们可以使用`yearmonth`函数对日期时间进行分组。我们还需要通过在列名中添加`:O`来指定这个变量的类型是序数(值之间有一个顺序):

```
alt.Chart(sample_df).mark_bar().encode(
    alt.X('yearmonth(InvoiceDate):O'),
    y='count()'
)
```

您应该得到以下输出:

![Figure 10.35: Distribution of InvoiceDate by month
](image/C15019_10_35.jpg)

图 10.35:发票日期按月分布

这张图表告诉我们，在 2011 年 11 月有一个巨大的商品销售高峰。本月最高时卖出了 800 件商品，而平均水平在 300 件左右。当时是否有促销或广告活动可以解释这种增长？这些是你可能想问你的利益相关者的问题，以便他们可以确认这种销售的突然增加。

# 箱线图

现在，我们来看看另一种叫做箱线图的特殊类型的图表。这种图形用于显示基于四分位数的变量分布。四分位数是将数据集分成四份的值。每个季度正好包含 25%的观察值。例如，在以下样本数据中，四分位数如下:

![Figure 10.36: Example of quartiles for the given data
](image/C15019_10_36.jpg)

图 10.36:给定数据的四分位数示例

所以，第一个四分位数(通常指 Q1)是 4；第二个(Q2)，也是中位数，是 5；第三个四分位数(Q3)是 8。

箱线图将显示这些四分位数以及附加信息，如下所示:

*   四分位数间距(或 IQR)，对应于 Q3 - Q1
*   *最低*值，对应 Q1 - (1.5 * IQR)
*   *最高*值，对应 Q3 + (1.5 * IQR)
*   异常值，即最低点和最高点之外的任何点:

![Figure 10.37: Example of a boxplot
](image/C15019_10_37.jpg)

图 10.37:箱线图示例

有了箱线图，很容易看到中心点(中位数)，50%的数据落在(IQR)和离群值之下。

使用箱线图的另一个好处是绘制分类变量相对于数值变量的分布，并对它们进行比较。让我们使用`mark_boxplot()`方法对`Country`和`Quantity`列进行尝试:

```
alt.Chart(sample_df).mark_boxplot().encode(
    x='Country:O',
    y='Quantity:Q'
)
```

您应该会收到以下输出:

![Figure 10.38: Boxplot of the ‘Country’ and ‘Quantity’ columns
](image/C15019_10_38.jpg)

图 10.38:“国家”和“数量”列的箱线图

这个图表向我们展示了`Quantity`变量在这个数据集的不同国家中是如何分布的。我们可以看到`United Kingdom`有很多离群值，尤其是在负值上。`Eire`是另一个有负异常值的国家。除了`Japan`、`Netherlands`和`Sweden`销售了更多的商品外，大多数国家的价值数量都很低。

在本节中，我们看到了如何使用`altair`包来生成图表，帮助我们获得关于数据集的更多见解，并识别一些潜在的问题。

## 练习 10.04:用 Altair 可视化 Ames 住房数据集

在本练习中，我们将学习如何使用数据可视化功能(如直方图、散点图或箱线图)更好地理解数据集和变量之间的关系。

注意

您将使用与之前练习中使用的相同的 Ames housing 数据集。

1.  打开新的 Colab 笔记本。
2.  导入`pandas`和`altair`包:

    ```
    import pandas as pd
    import altair as alt
    ```

3.  将 AMES 数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter10/dataset/ames_iowa_housing.csv'
    ```

    的变量
4.  Using the `read_csv` method from the pandas package, load the dataset into a new variable called `'df'`:

    ```
    df = pd.read_csv(file_url)
    ```

    使用`altair`包中的`mark_bar()`和`encode()`方法绘制`SalePrice`变量的直方图。使用`alt.X`和`alt.Bin`API 指定 bin 步骤的数量，即`50000`:

    ```
    alt.Chart(df).mark_bar().encode(
        alt.X("SalePrice:Q", bin=alt.Bin(step=50000)), 
        y='count()'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.39: Histogram of SalePrice
    ](image/C15019_10_39.jpg)

    图 10.39:销售价格直方图

    这张图表显示，大多数房产的销售价格都以`100,000 – 150,000`为中心。也有少数异常值的销售价格高于`500,000`。

5.  Now, let's plot the histogram for `LotArea` but this time with a bin step size of `10000`:

    ```
    alt.Chart(df).mark_bar().encode(
        alt.X("LotArea:Q", bin=alt.Bin(step=10000)), 
        y='count()'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.40: Histogram of LotArea
    ](image/C15019_10_40.jpg)

    图 10.40:地块面积的直方图

    `LotArea`的分布与`SalePrice`完全不同。大部分观测都在`0`和`20,000`之间。其余的观察值代表了数据集的一小部分。我们还可以注意到`150,000`的一些极端异常值。

6.  Now, plot a scatter plot with `LotArea` as the *x* axis and `SalePrice` as the *y* axis to understand the interactions between these two variables:

    ```
    alt.Chart(df).mark_circle().encode(
        x='LotArea:Q',
        y='SalePrice:Q'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.41: Scatter plot of SalePrice and LotArea
    ](image/C15019_10_41.jpg)

    图 10.41:销售价格和地块面积散点图

    房产的大小和售价之间显然有关联。如果只看`LotArea`在 5 万以下的属性，可以看到一个线性关系:如果从(`0,0`)坐标到(`20000,800000`)坐标画一条直线，可以说`LotArea`每增加 1000，`SalePrice`就增加 4 万。这条直线(或回归线)的公式将为`SalePrice = 40000 * LotArea / 1000`。我们还可以看到，对于一些物业来说，虽然它们的规模相当高，但它们的价格并没有遵循这一模式。比如 16 万的房产，卖了不到 30 万。

7.  Now, let's plot the histogram for `OverallCond`, but this time with the default bin step size, that is, (`bin=True`):

    ```
    alt.Chart(df).mark_bar().encode(
        alt.X("OverallCond", bin=True), 
        y='count()'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.42: Histogram of OverallCond
    ](image/C15019_10_42.jpg)

    图 10.42:总代码直方图

    我们可以看到这个列中包含的值是离散的:它们只能取有限个值(在`1`和`9`之间的任何整数)。这个变量不是数字的，而是顺序的:顺序很重要，但是你不能对它执行一些数学运算，比如把值`2`加到值`8`上。这个栏目是一个任意的映射，用来评估房产的整体质量。在下一章，我们将看看如何改变这样一个列的类型。

8.  Build a boxplot with `OverallCond:O` (`':O'` is for specifying that this column is ordinal) on the *x* axis and `SalePrice` on the *y* axis using the `mark_boxplot()` method, as shown in the following code snippet:

    ```
    alt.Chart(df).mark_boxplot().encode(
        x='OverallCond:O',
        y='SalePrice:Q'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.43: Boxplot of OverallCond
    ](image/C15019_10_43.jpg)

    图 10.43:总成本的箱线图

    看起来`OverallCond`变量是按升序排列的:条件值越高，销售价格越高。然而，我们会注意到`SalePrice`对于值 5 来说是相当高的，这似乎代表了中等条件。可能还有其他因素影响这一类别的销售价格，例如买家之间对此类物业的竞争加剧。

9.  Now, let's plot a bar chart for `YrSold` as its *x* axis and `count()` as its *y* axis. Don't forget to specify that `YrSold` is an ordinal variable and not numerical using `':O'`:

    ```
    alt.Chart(df).mark_bar().encode(
        alt.X('YrSold:O'),
        y='count()'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.44: Bar chart of YrSold
    ](image/C15019_10_44.jpg)

    图 10.44:yr sold 的条形图

    我们可以看到，除了 2010 年，每年售出的房产数量大致相同。从 2006 年到 2009 年，平均每年售出 310 套房产，而 2010 年只有 170 套。

10.  Plot a boxplot similar to the one shown in *Step 8* but for `YrSold` as its *x* axis:

    ```
    alt.Chart(df).mark_boxplot().encode(
        x='YrSold:O',
        y='SalePrice:Q'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.45: Boxplot of YrSold and SalePrice
    ](image/C15019_10_45.jpg)

    图 10.45:yr sold 和 SalePrice 的箱线图

    总的来说，销售价格中位数在这些年里相当稳定，在 2010 年略有下降。

11.  Let's analyze the relationship between `SalePrice` and `Neighborhood` by plotting a bar chart, similar to the one shown in *Step 9*:

    ```
    alt.Chart(df).mark_bar().encode(
        x='Neighborhood',
        y='count()'
    )
    ```

    您应该得到以下输出:

    ![Figure 10.46: Bar chart of Neighborhood
    ](image/C15019_10_46.jpg)

    图 10.46:邻域条形图

    出售的房产数量因地理位置不同而有所不同。该社区售出的房产数量最多:超过 220 套。另一方面，像`'Blueste'`或`'NPkVill'`这样的社区只有少数房产售出。

12.  Let's analyze the relationship between `SalePrice` and `Neighborhood` by plotting a boxplot chart similar to the one in *Step 10*:

    ```
    alt.Chart(df).mark_boxplot().encode(
        x='Neighborhood:O',
        y='SalePrice:Q'
    )
    ```

    您应该得到以下输出:

![Figure 10.47: Boxplot of Neighborhood and SalePrice
](image/C15019_10_47.jpg)

图 10.47:邻域和销售价格的箱线图

出售房产的位置对销售价格有很大的影响。`noRidge`、`NridgHt`、`StoneBr`街区整体价格较高。同样值得注意的是，`NoRidge`有一些极端的异常值，一些房产的售价远高于附近的其他房产。

有了这个分析，我们就完成了这个练习。我们看到，通过使用数据可视化，我们可以获得关于数据集的一些有价值的见解。例如，使用散点图，我们确定了`SalePrice`和`LotArea`之间的线性关系，其中价格倾向于随着房产规模的变大而增加。直方图有助于我们理解数字变量的分布，条形图为我们提供了分类变量的类似视图。例如，我们看到在一些社区出售的房产比其他社区多。最后，我们能够通过使用箱线图来分析和比较一个变量的不同值对`SalePrice`的影响。我们看到，物业状况越好，售价就越高。数据可视化对于数据科学家来说是一个非常重要的工具，这样他们就可以探索和分析数据集。

## 活动 10.01:使用可视化数据分析技术分析流失数据

你在一家大型电信公司工作。营销部门已经注意到最近客户流失的高峰(*客户停止使用或取消了公司*的服务)。

你被要求分析客户档案并预测未来的客户流失。在建立预测模型之前，您的首要任务是分析营销部门与您共享的数据，并评估其整体质量。

注意

本活动中使用的数据集可以在我们的 GitHub 资源库中找到:[https://packt.live/2s1yquq](https://packt.live/2s1yquq)。

我们将要使用的数据集最初是由 Eduardo Arino De La Rubia 共享的。世界:[https://packt.live/2s1ynie](https://packt.live/2s1ynie)。

以下步骤将帮助您完成此活动:

1.  使用`.read_csv()`将数据集下载并加载到 Python 中。
2.  使用`.shape`、`.dtypes`、`.head()`、`.tail()`或`.sample()`浏览数据集的结构和内容。
3.  用`.describe()`计算和解释描述性统计。
4.  使用条形图、直方图或箱线图的数据可视化来分析每个变量。
5.  确定需要市场部门澄清的领域和潜在的数据质量问题。

**预期产出**

下面是预期的条形图输出:

![Figure 10.48: Expected bar chart output
](image/C15019_10_48.jpg)

图 10.48:预期的条形图输出

这里是预期的直方图输出:

![Figure 10.49: Expected histogram output
 
](image/C15019_10_49.jpg)

图 10.49:预期的直方图输出

以下是预期的箱线图输出:

![Figure 10.50: Expected boxplot output
](image/C15019_10_50.jpg)

图 10.50:预期的箱线图输出

注意

这个活动的解决方案可以在这里找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

您刚刚完成了本章的活动。您已经分析了与客户流失相关的数据集。通过使用描述性统计和数据可视化，您对该数据集了如指掌。在几行代码中，您理解了数据帧的结构(行数和列数)以及每个变量中包含的信息类型。通过绘制一些栏目的分布图，我们了解到白天、晚上或国际电话都有具体的收费。我们还看到客户流失变量是不平衡的:大约只有 10%的客户流失。最后，我们看到其中一个变量`numbervmailmessages`，对于搅拌过或没有搅拌过的客户有一个非常不同的分布。这可能是客户流失的一个强有力的预测因素。

# 总结

你刚刚学到了很多关于如何分析数据集的知识。这是任何数据科学项目中非常关键的一步。深入了解数据集将有助于您更好地评估实现业务需求的可行性。

以正确的格式和正确的质量水平获得正确的数据是任何机器学习算法获得良好预测性能的关键。这就是在进入下一阶段之前花时间分析数据如此重要的原因。这项任务在 CRISP-DM 方法中称为数据理解阶段，也可以称为探索性数据分析(EDA)。

您了解了如何使用描述性统计来汇总数据集的关键属性，例如数字列的平均值、其标准差分布或范围(最小值和最大值)、分类变量的唯一值及其最常见的值。您还了解了如何使用数据可视化来获得每个变量的有价值的见解。现在，您知道了如何使用散点图、条形图、直方图和箱线图来了解一个列的分布。

在分析数据时，我们遇到了一些额外的问题，在正常的项目中，这些问题需要业务部门来解决。我们还发现了一些潜在的数据质量问题，例如需要修复的缺失值、异常值或不正确的值。这是我们将在下一章讨论的主题:准备数据。敬请关注。