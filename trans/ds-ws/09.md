<title>C15019_09_Final_ePub_SP</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 9。解释机器学习模型

概观

本章将向您展示如何解释机器学习模型的结果，并更深入地了解它发现的模式。本章结束时，你将能够分析线性模型的权重和`RandomForest`的可变重要性。您将能够通过排列实现可变重要性来分析特性重要性。您将使用部分相关图来分析单个变量，并使用 lime 包进行局部解释。

# 简介

在前一章中，您看到了如何找到一些最流行的机器学习算法的最佳超参数，以便获得更好的预测性能(即更准确的预测)。

机器学习算法总是被称为黑箱，我们只能看到输入和输出，算法内部的实现非常不透明，所以人们不知道内部发生了什么。

随着时间的推移，我们可以感觉到对机器学习模型更加透明的需求越来越高。在过去的几年里，我们已经看到了一些算法被指控歧视人群的案例。例如，几年前，一家名为 ProPublica 的非营利新闻机构强调了由 Northpointe 公司开发的 COMPAS 算法中的偏见。该算法的目标是评估罪犯再次犯罪的可能性。结果显示，该算法根据人口统计数据而非其他特征预测特定人群的风险水平较高。这个例子强调了正确清晰地解释模型结果及其逻辑的重要性。

幸运的是，一些机器学习算法提供了理解他们为给定任务和数据集学习的参数的方法。还有一些函数是模型不可知的，可以帮助我们更好地理解所做的预测。因此，有不同的技术来解释模型，这些技术要么是模型特定的，要么是模型不可知的。

这些技术的范围也可能不同。在文献中，我们要么有一个全球或局部的解释。全局解释意味着我们从数据集中查看所有观测值的变量，我们希望了解哪些特征对目标变量具有最大的整体影响。例如，如果您预测一家电信公司的客户流失，您可能会发现您的模型最重要的特征是客户使用情况和每月平均支付金额。另一方面，局部解释只关注单个观察，并分析不同变量的影响。我们将研究一个具体的案例，看看是什么导致这个模型做出最终的预测。例如，您将关注一个预计会流失的特定客户，并会发现他们通常会在每年 9 月购买新款 iPhone。

在这一章中，我们将介绍一些如何解释你的模型或其结果的技巧。

# 线性模型系数

在*第 2 章回归*和*第 3 章二元分类*中，您看到线性回归模型以如下形式学习函数参数:

![Figure 9.1: Function parameters for linear regression models
](image/B15019_09_01.jpg)

图 9.1:线性回归模型的函数参数

目标是找到最佳参数(w1，w2 …，wn)，使预测值 ŷ̂非常接近实际目标值`y`。因此，一旦您训练了您的模型，并且在没有太多过度拟合的情况下获得了良好的预测性能，您就可以使用这些参数(或系数)来了解哪些变量对预测有很大影响。如果系数接近 0，这意味着相关特征对结果没有太大影响。另一方面，如果它非常高(正的或负的)，这意味着它的特征极大地影响了预测结果。

让我们以下面的函数为例:`100 + 0.2 * x` 1 `+ 200 * x` 2 `- 180 * x` 3。x1 的系数只有 **0.2** 。与其他的相比，它是相当低的。对最终结局没有太大影响。x2 的系数为正，因此它将对预测产生积极影响。它与 x3 系数相反，因为 x3 系数为负。

但是为了能够比较苹果和苹果，您需要重新调整特征的比例，以便它们具有相同的比例，这样您就可以比较它们的系数。如果没有，那么可能 x1 在 100 万到 500 万之间，而 x2 和 x3 在 **1** 和 **88** 之间。在这种情况下，即使 x1 系数很小，x1 值的微小变化也会对预测产生巨大影响。另一方面，如果所有 3 个系数都在-1 和 1 之间，那么我们可以说预测目标变量的关键驱动因素是特征 x2 和 x3。

在`sklearn`中，得到一个线性模型的系数是极其容易的；你只需要调用`coef_`属性。让我们用来自`sklearn`的糖尿病数据集在一个真实的例子上实现它:

```
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
data = load_diabetes()
# fit a linear regression model to the data
lr_model = LinearRegression()
lr_model.fit(data.data, data.target)
lr_model.coef_
```

输出如下所示:

![Figure 9.2: Coefficients of the linear regression parameters](image/B15019_09_02.jpg)

图 9.2:线性回归参数的系数

让我们用这些值和列名创建一个数据帧:

```
import pandas as pd
coeff_df = pd.DataFrame()
coeff_df['feature'] = data.feature_names
coeff_df['coefficient'] = lr_model.coef_
coeff_df.head()
```

输出如下所示:

![Figure 9.3: Coefficients of the linear regression model
](image/B15019_09_03.jpg)

图 9.3:线性回归模型的系数

特征系数的大正数或大负数意味着它对结果有很大的影响。另一方面，如果系数接近 0，这意味着变量对预测没有太大影响。

从这个表中，我们可以看到列`s1`的系数非常低(一个很大的负数),因此它会对最终预测产生负面影响。如果`s1`增加 1 个单位，预测值将减少`-792.184162`。另一方面，`bmi`在预测上有一个很大的正数(`519.839787`)，因此糖尿病的风险与这一特征高度相关:体重指数(身体质量指数)的增加意味着糖尿病风险的显著增加。

## Ex ercise 9.01:提取线性回归系数

在本练习中，我们将训练一个线性回归模型来预测客户退出率并提取其系数。

注意

我们将使用的数据集由多伦多大学的卡尔·拉斯姆森分享:[https://packt.live/37hInDr](https://packt.live/37hInDr)。

该数据集是通过模拟综合生成的，用于预测因排长队而离开银行的银行客户比例。

这个数据集的 CSV 版本可以在这里找到:[https://packt.live/37hGPcD](https://packt.live/37hGPcD)。

可以在以下位置找到表示该数据集中变量的数据字典:

[https://packt.live/3aBGhQD](https://packt.live/3aBGhQD)。

以下步骤将帮助您完成练习:

1.  Open a new Colab notebook.

    导入以下包:`pandas`、`sklearn.model_selection`的`train_test_split`、`sklearn.preprocessing`的`StandardScaler`、`sklearn.linear_model`的`LinearRegression`、`sklearn.metrics`的`mean_squared_error`、`altair`:

    ```
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared _error
    import altair as alt
    ```

2.  创建一个名为`file_url`的变量，包含数据集的 URL:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter09/Dataset/phpYYZ4Qc.csv'    
    ```

3.  使用`.read_csv()` :

    ```
    df = pd.read_csv(file_url)
    ```

    将数据集加载到名为`df`的数据帧中
4.  Print the first five rows of the DataFrame:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 9.4: First five rows of the loaded DataFrame
    ](image/B15019_09_04.jpg)

    图 9.4:加载的数据帧的前五行

    注意

    出于演示目的，输出已被截断。完整输出请参考[https://packt.live/2NOMfnC](https://packt.live/2NOMfnC)。

5.  使用`.pop()`提取`rej`列，并保存到一个名为`y` :

    ```
    y = df.pop('rej')
    ```

    的变量中
6.  Print the summary of the DataFrame using `.describe()`.

    ```
    df.describe()
    ```

    您应该得到以下输出:

    ![Figure 9.5: Statistical measures of the DataFrame
    ](image/B15019_09_05.jpg)

    图 9.5:数据框架的统计测量

    从这个输出中，我们可以看到数据没有标准化。变量有不同的标度。

7.  使用带有`test_size=0.3`和`random_state = 1` :

    ```
    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=1)
    ```

    的`train_test_split()`将数据帧分成训练集和测试集
8.  实例化`StandardScaler` :

    ```
    scaler = StandardScaler()
    ```

9.  在训练集上训练`StandardScaler`，并使用`.fit_transform()` :

    ```
    X_train = scaler.fit_transform(X_train)
    ```

    将其标准化
10.  使用`.transform()` :

    ```
    X_test = scaler.transform(X_test)
    ```

    标准化测试集
11.  实例化`LinearRegression`并保存到一个名为`lr_model` :

    ```
    lr_model = LinearRegression()
    ```

    的变量中
12.  Train the model on the training set using `.fit()`:

    ```
    lr_model.fit(X_train, y_train)
    ```

    您应该得到以下输出:

    ![Figure 9.6: Logs of LinearRegression
    ](image/B15019_09_06.jpg)

    图 9.6:线性回归的对数

13.  使用`.predict()` :

    ```
    preds_train = lr_model.predict(X_train)
    preds_test = lr_model.predict(X_test)
    ```

    预测训练集和测试集的结果
14.  Calculate the mean squared error on the training set and print its value:

    ```
    train_mse = mean_squared_error(y_train, preds_train)
    train_mse
    ```

    您应该得到以下输出:

    ![Figure 9.7: MSE score of the training set
    ](image/B15019_09_07.jpg)

    图 9.7:训练集的 MSE 分数

    我们在训练集上取得了相当低的 MSE 分数。

15.  Calculate the mean squared error on the testing set and print its value:

    ```
    test_mse = mean_squared_error(y_test, preds_test)
    test_mse
    ```

    您应该得到以下输出:

    ![Figure 9.8: MSE score of the testing set
    ](image/B15019_09_08.jpg)

    图 9.8:测试集的 MSE 分数

    我们在测试集上也有一个很低的 MSE 分数，与训练集非常相似。所以，我们的模型没有过度拟合。

16.  Print the coefficients of the linear regression model using `.coef_`:

    ```
    lr_model.coef_
    ```

    您应该得到以下输出:

    ![Figure 9.9: Coefficients of the linear regression model
    ](image/B15019_09_09.jpg)

    图 9.9:线性回归模型的系数

17.  创建一个名为`coef_df` :

    ```
    coef_df = pd.DataFrame()
    ```

    的空数据帧
18.  使用`.columns` :

    ```
    coef_df['feature'] = df.columns
    ```

    为这个数据帧创建一个名为`feature`的新列，列名为`df`
19.  使用`.coef_` :

    ```
    coef_df['coefficient'] = lr_model.coef_
    ```

    使用线性回归模型的系数为该数据框架创建一个名为`coefficient`的新列
20.  Print the first five rows of `coef_df`:

    ```
    coef_df.head()
    ```

    您应该得到以下输出:

    ![Figure 9.10: The first five rows of coef_df
    ](image/B15019_09_10.jpg)

    图 9.10:coef _ df 的前五行

    从这个输出中，我们可以看到变量`a1sx`和`a1sy`具有最低的值(最大的负值),因此它们比这里显示的其他三个变量对预测的贡献更大。

21.  Plot a bar chart with Altair using `coef_df` and `coefficient` as the `x` axis and `feature` as the `y` axis:

    ```
    alt.Chart(coef_df).mark_bar().encode(
      x='coefficient',
      y="feature"
    )
    ```

    您应该得到以下输出:

    ![Figure 9.11: Graph showing the coefficients of the linear regression model
    ](image/B15019_09_11.jpg)

图 9.11:显示线性回归模型系数的图表

从这个输出中，我们可以看到对预测影响最大的变量是:

*   `a2pop`，对应 2 区人口
*   `a1pop`，对应 1 区人口
*   `a3pop`，对应 3 区人口
*   `mxql`，队列的最大长度
*   `b1eff`，这是银行 1 的效率水平
*   `temp`，即温度

前三个变量对结果有积极影响(增加目标变量值)。这意味着，随着这三个地区中任何一个地区的人口增长，客户流失的机会也会增加。另一方面，最后三个特征对目标变量产生负面影响(降低目标变量值):如果最大长度、第一组效率水平或温度增加，则客户离开的风险降低。

在本练习中，您学习了如何提取线性回归模型的系数，并确定了哪些变量对预测的贡献最大。

# 随机变量重要性

*第四章*，*用 RandomForest* 进行多类分类，给大家介绍了一个非常强大的基于树的算法:`RandomForest`。它是业内最流行的算法之一，不仅因为它在预测方面取得了非常好的结果，还因为它提供了几个解释它的工具，如变量重要性。

还记得在*第四章*、*随机森林多类分类*中，`RandomForest`构建了多个独立的树，然后对它们的结果进行平均以做出最终预测。我们还了解到，它在每棵树中创建节点，以找到将观察结果清楚地分成两组的最佳分割。`RandomForest`使用不同的方法寻找最佳分割。在`sklearn`中，你可以使用基尼系数或熵值进行分类，使用 MSE 或 MAE 进行回归。在不深入研究每种方法的细节的情况下，这些方法可以计算出给定组分的杂质水平。这种杂质水平看的是一个节点内的观测值之间的差异。例如，如果一个组在一个特征中具有所有相同的值，那么它将具有较高的纯度。另一方面，如果一个团体有很多不同的价值观，它将有很高的杂质水平。

构建的每个子树都会降低这个杂质分数。因此，我们可以使用这个杂质分数来评估每个变量对于最终预测的重要性。这种技术不仅仅针对`RandomForest`；它可以应用于任何基于树的算法，如决策树或梯度提升树。

在训练完`RandomForest`之后，你可以用`feature_importances_`属性来评估它的变量重要性(或者特性重要性)。

让我们看看如何从`sklearn`的乳腺癌数据集中提取这些信息:

```
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
data = load_breast_cancer()
X, y = data.data, data.target
rf_model = RandomForestClassifier(random_state=168)
rf_model.fit(X, y)
rf_model.feature_importances_
```

输出将如下图所示:

![Figure 9.12: Feature importance of a Random Forest model
](image/B15019_09_12.jpg)

图 9。随机森林模型的特征重要性

从该输出中评估哪个重要性值对应于哪个变量可能有点困难。让我们创建一个数据帧，它将包含这些值以及列名:

```
import pandas as pd
varimp_df = pd.DataFrame()
varimp_df['feature'] = data.feature_names
varimp_df['importance'] = rf_model.feature_importances_
varimp_df.head()
```

输出如下所示:

![Figure 9.13: RandomForest variable importance for the first five features 
of the Breast Cancer dataset
](image/B15019_09_13.jpg)

图 9.13:乳腺癌数据集前五个特征的随机森林变量重要性

从这个输出中，我们可以看到`mean radius`和`mean perimeter`的得分最高，这意味着它们在预测目标变量时最重要。`mean smoothness`列的值非常低，因此它似乎不会对预测产量的模型产生太大影响。

注意

对于数据集，可变重要性的值的范围是不同的；这不是一个标准化的衡量标准。

让我们使用`altair`将这些可变的重要性值绘制到一个图表上:

```
import altair as alt
alt.Chart(varimp_df).mark_bar().encode(
    x='importance',
    y="feature"
)
```

输出如下所示:

![Figure 9.14: Graph showing RandomForest variable importance
](image/B15019_09_14.jpg)

图 9.14:显示随机森林变量重要性的图表

从这个图中，我们可以看到这个随机森林模型最重要的特征是`worst perimeter`、`worst area`和`worst concave points`。所以现在我们知道，对于这个随机森林模型，这些特征是预测肿瘤是良性还是恶性的最重要的特征。

## 练习 9.02 :提取随机森林特征重要性

在本练习中，我们将提取随机森林分类器模型的特征重要性，该模型用于预测客户退出率。

我们将使用与上一个练习相同的数据集。

以下步骤将帮助您完成练习:

1.  打开新的 Colab 笔记本。
2.  导入以下包:`pandas`、`sklearn.model_selection`的`train_test_split`、`sklearn.ensemble`的`RandomForestRegressor`:

    ```
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error
    import altair as alt
    ```

3.  创建一个名为`file_url`的变量，包含数据集的 URL:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter09/Dataset/phpYYZ4Qc.csv'
    ```

4.  使用`.read_csv()` :

    ```
    df = pd.read_csv(self.file_url)
    ```

    将数据集加载到名为`df`的数据帧中
5.  使用`.pop()`提取`rej`列，并保存到一个名为`y` :

    ```
    y = df.pop('rej')
    ```

    的变量中
6.  使用带有`test_size=0.3`和`random_state = 1` :

    ```
    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=1)
    ```

    的`train_test_split()`将数据帧分成训练集和测试集
7.  用`random_state=1`、`n_estimators=50`、`max_depth=6`、`min_samples_leaf=60` :

    ```
    rf_model = RandomForestRegressor(random_state=1, n_estimators=50, max_depth=6, min_samples_leaf=60)
    ```

    实例化`RandomForestRegressor`
8.  Train the model on the training set using `.fit()`:

    ```
    rf_model.fit(X_train, y_train)
    ```

    您应该得到以下输出:

    ![Figure 9.15: Logs of the Random Forest model
    ](image/B15019_09_15.jpg)

    图 9.15:随机森林模型的日志

9.  使用`.predict()` :

    ```
    preds_train = rf_model.predict(X_train)
    preds_test = rf_model.predict(X_test)
    ```

    预测训练集和测试集的结果
10.  Calculate the mean squared error on the training set and print its value:

    ```
    train_mse = mean_squared_error(y_train, preds_train)
    train_mse
    ```

    您应该得到以下输出:

    ![Figure 9.16: MSE score of the training set
    ](image/B15019_09_16.jpg)

    图 9.16:训练集的 MSE 分数

    我们在训练集上实现了相当低的 MSE 分数。

11.  Calculate the MSE on the testing set and print its value:

    ```
    test_mse = mean_squared_error(y_test, preds_test)
    test_mse
    ```

    您应该得到以下输出:

    ![Figure 9.17: MSE score of the testing set
    ](image/B15019_09_17.jpg)

    图 9.17:测试集的 MSE 分数

    我们在测试集上也有一个很低的 MSE 分数，与训练集非常相似。所以，我们的模型没有过度拟合。

12.  Print the variable importance using `.feature_importances_`:

    ```
    rf_model.feature_importances_
    ```

    您应该得到以下输出:

    ![Figure 9.18: MSE score of the testing set
    ](image/B15019_09_18.jpg)

    图 9.18:测试集的 MSE 分数

13.  创建一个名为`varimp_df` :

    ```
    varimp_df = pd.DataFrame()
    ```

    的空数据帧
14.  使用`.columns` :

    ```
    varimp_df['feature'] = df.columns
    ```

    为这个数据帧创建一个名为`feature`的新列，列名为`df`
15.  Print the first five rows of `varimp_df`:

    ```
    varimp_df.head()
    ```

    您应该得到以下输出:

    ![Figure 9.19: Variable importance of the first five variables
    ](image/B15019_09_19.jpg)

    图 9.19:前五个变量的变量重要性

    从这个输出中，我们可以看到变量`a1cy`和`a1sy`具有最高值，因此它们比这里显示的其他三个变量对预测目标变量更重要。

16.  Plot a bar chart with Altair using `coef_df` and `importance` as the `x` axis and `feature` as the `y` axis:

    ```
    alt.Chart(varimp_df).mark_bar().encode(
        x='importance',
        y="feature"
    )
    ```

    您应该得到以下输出:

    ![Figure 9.20: Graph showing the variable importance of the first five variables
    ](image/B15019_09_20.jpg)

图 9.20:显示前五个变量重要性的图表

从这个输出中，我们可以看到对这个随机森林模型的预测影响最大的变量是`a2pop`、`a1pop`、`a3pop`、`b1eff`和`temp`，按重要性降序排列。

在本练习中，您学习了如何提取由随机森林模型获得的要素重要性，并确定哪些变量对其预测最重要。

# 通过排列改变重要性

在上一节中，我们看到了如何提取 RandomForest 的特征重要性。实际上还有另一种同名的技术，但其底层逻辑不同，可以应用于任何算法，而不仅仅是基于树的算法。

这种技术可以称为通过排列的可变重要性。假设我们训练了一个模型，用五个类来预测一个目标变量，并达到了 0.95 的精度。评估其中一个特征的重要性的一种方法是移除并训练一个模型，然后查看新的准确度分数。如果准确度分数显著下降，那么我们可以推断该变量对预测有显著影响。另一方面，如果分数略有下降或保持不变，我们可以说这个变量不是很重要，不会对最终预测产生太大影响。所以，我们可以用模型表现之间的这种差异来评估一个变量的重要性。

这种方法的缺点是，您需要为每个变量重新训练一个新模型。如果你花了几个小时来训练原始模型，并且你有 100 个不同的特征，那么计算每个变量的重要性就要花相当长的时间。如果我们不必重新培训不同的模型，那就太好了。因此，另一种解决方案是为给定的列生成噪声或新值，并根据修改后的数据预测最终结果，然后比较准确性得分。例如，如果您有一个值在 0 到 100 之间的列，您可以获取原始数据并为该列随机生成新值(保持所有其他变量不变)并预测它们的类。

这个选项也有一个问题。随机生成的值可能与原始数据非常不同。回到我们之前看到的同一个示例，如果一个列的原始值范围在 0 到 100 之间，并且我们生成的值可以是负的或取一个很高的值，那么它就不能很好地代表原始数据的真实分布。因此，在生成新值之前，我们需要了解每个变量的分布。

我们可以简单地在不同行之间交换(或置换)一列的值，并使用这些修改后的情况进行预测，而不是生成随机值。然后，我们可以计算相关的准确度分数，并与原始分数进行比较，以评估该变量的重要性。例如，我们在原始数据集中有以下行:

![Figure 9.21: Example of the dataset
](image/B15019_09_21.jpg)

图 9.21:数据集示例

我们可以交换 X1 列的值，得到一个新的数据集:

![Figure 9.22: Example of a swapped column from the dataset
](image/B15019_09_22.jpg)

图 9.22:数据集中交换列的示例

`mlxtend`包提供了执行变量置换和计算变量重要性值的函数:`feature_importance_permutation`。让我们看看如何使用来自`sklearn`的乳腺癌数据集。

首先，让我们加载数据并训练一个随机森林模型:

```
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier

data = load_breast_cancer()
X, y = data.data, data.target
rf_model = RandomForestClassifier(random_state=168)
rf_model.fit(X, y)
```

然后，我们将从`mlxtend.evaluate`调用`feature_importance_permutation`函数。该函数采用以下参数:

*   `predict_method`:模型预测将调用的函数。这里，我们将提供来自我们训练过的`rf_model`模型的`predict`方法。
*   `X`:数据集中的特征。它必须是 NumPy 数组形式。
*   `y`:数据集中的目标变量。它需要以`Numpy`数组的形式。
*   `metric`:用于比较模型性能的指标。对于分类任务，我们将使用准确性。
*   `num_round`:回合数`mlxtend`将对数据进行排列，评估性能变化。
*   `seed`:获得可重复结果的种子集。

考虑下面的代码片段:

```
from mlxtend.evaluate import feature_importance_permutation
imp_vals, _ = feature_importance_permutation(predict_method=rf_model.predict, X=X, y=y, metric='r2', num_rounds=1, seed=2)
imp_vals
```

输出应该如下所示:

![Figure 9.23: Variable importance by permutation
](image/B15019_09_23.jpg)

图 9.23:排列的可变重要性

让我们创建一个包含这些值和特性名称的数据框架，并用`altair`将它们绘制在一个图表上:

```
import pandas as pd
varimp_df = pd.DataFrame()
varimp_df['feature'] = data.feature_names
varimp_df['importance'] = imp_vals
varimp_df.head()
import altair as alt
alt.Chart(varimp_df).mark_bar().encode(
    x='importance',
    y="feature"
)
```

输出应该如下所示:

![Figure 9.24: Graph showing variable importance by permutation
](image/B15019_09_24.jpg)

图 9.24:通过排列显示变量重要性的图表

这些结果不同于我们从上一节的`RandomForest`中得到的结果。这里，最差凹点最重要，其次是最差面积，最差周长的值高于平均半径。所以，我们得到了同样的最重要变量的列表，但是顺序不同。这证实了这三个特征确实是预测肿瘤是否恶性的最重要因素。来自`RandomForest`的变量重要性和排列具有不同的逻辑，因此，它们的结果可能不同。

## 练习 9.03:通过排列提取特征重要性

在本练习中，我们将通过置换一个随机森林分类器模型来计算和提取特征重要性，该模型被训练来预测客户退出率。

我们将使用与上一练习相同的数据集。

以下步骤将帮助您完成练习:

1.  打开新的 Colab 笔记本。
2.  导入以下包:`pandas`、`sklearn.model_selection`的`train_test_split`、`sklearn.ensemble`的`RandomForestRegressor`、`mlxtend.evaluate`的`feature_importance_permutation`、`altair` :

    ```
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from mlxtend.evaluate import feature_importance_permutation
    import altair as alt
    ```

3.  创建一个名为`file_url`的变量，它包含数据集的 URL:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter09/Dataset/phpYYZ4Qc.csv'    
    ```

4.  使用`.read_csv()` :

    ```
    df = pd.read_csv(self.file_url)
    ```

    将数据集加载到名为`df`的数据帧中
5.  使用`.pop()`提取`rej`列，并保存到一个名为`y` :

    ```
    y = df.pop('rej')
    ```

    的变量中
6.  使用带有`test_size=0.3`和`random_state = 1` :

    ```
    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=1)
    ```

    的`train_test_split()`将数据帧分成训练集和测试集
7.  用`random_state=1`、`n_estimators=50`、`max_depth=6`、`min_samples_leaf=60` :

    ```
    rf_model = RandomForestRegressor(random_state=1, n_estimators=50, max_depth=6, min_samples_leaf=60)
    ```

    实例化`RandomForestRegressor`
8.  Train the model on the training set using `.fit()`:

    ```
    rf_model.fit(X_train, y_train)
    ```

    您应该得到以下输出:

    ![Figure 9.25: Logs of RandomForest
    ](image/B15019_09_25.jpg)

    图 9.25:随机森林的日志

9.  Extract the feature importance via permutation using `feature_importance_permutation` from `mxltend` with the Random Forest model, the testing set, `r2` as the metric used, `num_rounds=1`, and `seed=2`. Save the results into a variable called `imp_vals` and print its values:

    ```
    imp_vals, _ = feature_importance_permutation(predict_method=rf_model.predict, X=X_test.values, y=y_test.values, metric='r2', num_rounds=1, seed=2)
    imp_vals
    ```

    您应该得到以下输出:

    ![Figure 9.26: Variable importance by permutation
    ](image/B15019_09_26.jpg)

    图 9.26:排列的可变重要性

    很难解释原始结果。让我们通过在图上置换模型来绘制变量重要性。

10.  创建一个名为`varimp_df`的 DataFrame，包含两列:`feature`包含`df`的列名，使用`.columns`和`'importance'`包含`imp_vals` :

    ```
    varimp_df = pd.DataFrame({'feature': df.columns, 'importance': imp_vals})
    ```

    的值
11.  Plot a bar chart with Altair using `coef_df` and `importance` as the `x` axis and `feature` as the `y` axis:

    ```
    alt.Chart(varimp_df).mark_bar().encode(
      x='importance',
      y="feature"
    )
    ```

    您应该得到以下输出:

    ![Figure 9.27: Graph showing the variable importance by permutation
    ](image/B15019_09_27.jpg)

图 9.27:通过排列显示变量重要性的图表

从这个输出中，我们可以看到对这个随机森林模型的预测影响最大的变量依次是:`a2pop`、`a1pop`、`a3pop`、`b1eff`和`temp`。这和*练习 9.02* 的结果非常相似。

您通过置换该模型成功提取了特征重要性，并确定了哪些变量对其预测最重要。

# 部分相关图

另一个与模型无关的工具是部分依赖图。它是一个可视化工具，用于分析特征对目标变量的影响。为了实现这一点，我们可以在`x`轴上绘制我们有兴趣分析的特性的值，在`y`轴上绘制目标变量的值，然后在该图上显示数据集的所有观察值。让我们在来自`sklearn`的乳腺癌数据集上尝试一下:

```
from sklearn.datasets import load_breast_cancer
import pandas as pd
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
```

现在我们已经加载了数据并将其转换为数据帧，让我们看看最差凹点列:

```
import altair as alt
alt.Chart(df).mark_circle(size=60).encode(
    x='worst concave points',
    y='target'
)
```

![Figure 9.28: Scatter plot of the worst concave points and target variables
](image/B15019_09_28.jpg)

图 9.28:最差凹点和目标变量的散点图

从这个情节中，我们可以看出:

*   目标变量为 1 的大多数情况下，最差凹点列的值小于 0.16。
*   目标值为 0 的情况下，最差凹点的值超过 0.08。

对于该图，我们不太确定对于最差凹点，值在 0.08 和 0.16 之间会得到哪个结果(0 或 1)。为什么在这个值范围内的观察结果是不确定的，有多种可能的原因，例如没有很多记录属于这种情况，或者其他变量可能影响这些情况的结果。这就是部分依赖图有所帮助的地方。

这种逻辑非常类似于通过排列来改变重要性，但是我们不是随机替换某一列中的值，而是针对所有观察结果测试该列中的每一个可能值，并查看它会导致什么样的预测。如果我们以图 9.21 中的例子为例，从我们最初的三个观察值，这个方法将通过保持列`X2`和`X3`不变并替换`X1`的值来创建六个新的观察值:

![Figure 9.29: Example of records generated from a partial dependence plot
](image/B15019_09_29.jpg)

图 9.29:从部分相关图生成的记录示例

有了这些新数据，我们可以看到，例如，值 12 是否真的对预测目标变量 1 有很大的影响。原始记录中，值为 42 和 1 的`X1`列导致结果 0，只有值 12 生成预测 1。但是，如果我们对`X1`进行同样的观察，等于 42 和 1，并用 12 替换那个值，我们可以看到新的预测是否会导致目标变量为 1。这正是部分相关图背后的逻辑，它将评估一列的所有可能排列，并绘制预测的平均值。

`sklearn`提供了一个名为`plot_partial_dependence()`的函数来显示给定特征的部分依赖图。让我们看看如何在乳腺癌数据集上使用它。首先，我们需要获得我们感兴趣的列的索引。我们将使用来自`pandas`的`.get_loc()`方法来获取`worst concave points`列的索引:

```
import altair as alt
from sklearn.inspection import plot_partial_dependence
feature_index = df.columns.get_loc("worst concave points")
```

现在我们可以调用`plot_partial_dependence()`函数了。我们需要提供以下参数:训练模型、训练集和要分析的特征的指数:

```
plot_partial_dependence(rf_model, df, features=[feature_index])
```

![Figure 9.30: Partial dependence plot for the worst concave points column
](image/B15019_09_30.jpg)

图 9.30:最差凹点列的部分相关性图

该部分相关性图向我们显示，平均而言，最差凹点列的值低于 0.17 的所有观察值最有可能导致目标的预测值为 1(概率超过 0.5)，而所有超过 0.17 的记录的预测值为 0(概率低于 0.5)。

## 练习 9.04:绘制部分相关性

在本练习中，我们将绘制两个变量`a1pop`和`temp`的部分相关图，这两个变量来自一个随机森林分类器模型，该模型用于预测客户退出率。

我们将使用与上一练习相同的数据集。

1.  打开新的 Colab 笔记本。
2.  导入以下包:`pandas`、`sklearn.model_selection`的`train_test_split`、`sklearn.ensemble`的`RandomForestRegressor`、`sklearn.inspection`的`plot_partial_dependence`、`altair` :

    ```
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.inspection import plot_partial_dependence
    import altair as alt
    ```

3.  创建一个名为`file_url`的变量，其中包含数据集的 URL:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter09/Dataset/phpYYZ4Qc.csv'    
    ```

4.  使用`.read_csv()` :

    ```
    df = pd.read_csv(self.file_url)
    ```

    将数据集加载到名为`df`的数据帧中
5.  使用`.pop()`提取`rej`列，并保存到一个名为`y` :

    ```
    y = df.pop('rej')
    ```

    的变量中
6.  使用带有`test_size=0.3`和`random_state = 1` :

    ```
    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=1)
    ```

    的`train_test_split()`将数据帧分成训练集和测试集
7.  用`random_state=1`、`n_estimators=50`、`max_depth=6`、`min_samples_leaf=60` :

    ```
    rf_model = RandomForestRegressor(random_state=1, n_estimators=50, max_depth=6, min_samples_leaf=60)
    ```

    实例化`RandomForestRegressor`
8.  Train the model on the training set using `.fit()`:

    ```
    rf_model.fit(X_train, y_train)
    ```

    您应该得到以下输出:

    ![Figure 9.31: Logs of RandomForest
    ](image/B15019_09_31.jpg)

    图 9.31:随机森林的日志

9.  Plot the partial dependence plot using `plot_partial_dependence()` from `sklearn` with the Random Forest model, the testing set, and the index of the `a1pop` column:

    ```
    plot_partial_dependence(rf_model, X_test, features=[df.columns.get_loc('a1pop')])
    ```

    您应该得到以下输出:

    ![Figure 9.32: Partial dependence plot for a1pop
    ](image/B15019_09_32.jpg)

    图 9.32:a1pop 的部分相关图

    该部分相关图显示，平均而言，当值低于 2 时，`a1pop`变量不会对目标变量产生太大影响，但是从那里开始，`a1pop`每增加一个单位，目标变量线性增加 0.04。这意味着如果区域 1 的人口规模低于值 2，流失的风险几乎为零。但是超过这个限度，1 号区域人口规模的每一次增加都会增加`4%`流失的机会。

10.  Plot the partial dependence plot using `plot_partial_dependence()` from `sklearn` with the Random Forest model, the testing set, and the index of the `temp` column:

    ```
    plot_partial_dependence(rf_model, X_test, features=[df.columns.get_loc('temp')])
    ```

    您应该得到以下输出:

    ![Figure 9.33: Partial dependence plot for temp
    ](image/B15019_09_33.jpg)

图 9.33:温度的部分依赖图

这个部分依赖图表明，平均而言，`temp`变量对目标变量有负的线性影响:当`temp`增加 1 时，目标变量会减少 0.12。这意味着，如果温度上升一度，离开队列的机会就会减少 12%。

您学习了如何绘制和分析`a1pop`和`temp`特性的部分依赖图。在这个练习中，我们看到`a1pop`对目标有正的线性影响，而`temp`有负的线性影响。

# 石灰的局部利息

在训练我们的模型后，我们通常用它来预测未知数据的结果。我们之前看到的全局解释，如模型系数、变量重要性和部分相关图，为我们提供了大量关于总体特征的信息。有时，我们希望了解是什么影响了特定案例的模型来预测特定的结果。例如，如果您的模型是评估向新客户提供信贷的风险，您可能希望了解为什么它拒绝了特定线索的案例。这就是局部解释的目的:分析单个观察结果并理解模型决策背后的基本原理。在这一节中，我们将向您介绍一种称为局部可解释模型不可知解释(LIME)的技术。

如果我们使用线性模型，就很容易理解每个变量对预测结果的贡献。我们只需要看看模型的系数。例如，模型将学习以下函数:`y = 100 + 0.2 * x` 1 `+ 200 * x` 2 `- 180 * x` 3。因此，如果我们收到 x1=0、x2=2 和 x3=1 的观测值，我们将知道:

*   x1 为 0.2 * 0 = 0
*   x2 是 200 * 2 = +400
*   x3 为-180 * 1 = -180

所以，最终的预测(100 +0 + 400 -180 = 320)主要由 x2 驱动。

但是对于一个非线性模型，很难得到如此清晰的视图。在这种情况下，石灰是获得更多可见性的一种方法。LIME 的基本逻辑是用线性模型来近似原始的非线性模型。然后，它使用该线性模型的系数来解释每个变量的贡献，就像我们在前面的例子中看到的那样。但是，LIME 不是试图对整个数据集逼近整个模型，而是试图围绕您感兴趣的观测值对其进行局部逼近。LIME 使用训练好的模型来预测观察值附近的新数据点，然后对预测的数据进行线性回归。

让我们看看如何在乳腺癌数据集上使用它。首先，我们将加载数据并训练一个随机森林模型:

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
rf_model = RandomForestClassifier(random_state=168)
rf_model.fit(X_train, y_train)
```

在 Google Colab 上不能直接访问`lime`包，所以我们需要用下面的命令手动安装它:

```
!pip install lime
```

输出如下所示:

![Figure 9.34: Installation logs for the lime package
](image/B15019_09_34.jpg)

图 9.34:lime 包的安装日志

安装完成后，我们将通过提供训练数据、特性名称、要预测的类的名称以及任务类型来实例化`LimeTabularExplainer`类(在本例中，它是`classification`):

```
from lime.lime_tabular import LimeTabularExplainer
lime_explainer = LimeTabularExplainer(X_train,
      feature_names=data.feature_names,
      class_names=data.target_names,
      mode='classification')
```

然后，我们将使用我们感兴趣的观察值(这里，它将是来自测试集的第二个观察值)和预测结果概率的函数(这里，它是`.predict_proba()`)调用`.explain_instance()`方法。最后，我们将调用`.show_in_notebook()`方法来显示来自`lime`的结果:

```
exp = lime_explainer.explain_instance(X_test[1], rf_model.predict_proba, num_features=10)
exp.show_in_notebook()
```

输出如下所示:

![Figure 9.35: Output of LIME
](image/B15019_09_35.jpg)

图 9.35:石灰的输出

注意

您的输出可能略有不同。这是由于石灰的随机取样过程。

在前面的输出中有很多信息。让我们一次看一点。左侧显示了两类目标变量的预测概率。对于这一观察，模型认为预测值为恶性的概率为 0.7:

![Figure 9.36: Prediction probabilities from LIME
](image/B15019_09_36.jpg)

图 9.36:石灰的预测概率

右侧显示了此次观察的每个特征的值。每个特征都用颜色编码，以突出其对目标变量的可能类别的贡献。该列表按照重要性递减的方式对功能进行排序。在本例中，平均周长、平均面积和面积误差对模型有所贡献，从而增加了类 1 的概率。所有其他特征影响模型预测结果 0:

![Figure 9.37: Value of the feature for the observation of interest
](image/B15019_09_37.jpg)

图 9.37:感兴趣的观察的特征值

最后，中心部分显示了每个变量对最终预测的影响。在这个例子中，`worst concave points`和`worst compactness`变量导致预测结果 0 的概率分别增加了 0.10 和 0.05。另一方面，`mean perimeter`和`mean area`都使得预测类别 1 的概率增加了 0.03:

![Figure 9.38: Contribution of each feature to the final prediction
](image/B15019_09_38.jpg)

图 9.38:每个特征对最终预测的贡献

就这么简单。使用 LIME，我们可以很容易地看到每个变量如何影响预测模型不同结果的概率。如您所见，LIME 包不仅计算局部近似值，还提供其结果的可视化表示。这比查看原始输出要容易理解得多。这对于展示您的发现和说明不同的特征如何影响单个观察值的预测也非常有用。

## 练习 9.05: L 用石灰进行局部解释

在本练习中，我们将分析随机森林分类器模型的一些预测，该模型使用 LIME 来预测客户退出率。

我们将使用与上一个练习相同的数据集。

1.  打开新的 Colab 笔记本。
2.  导入以下包:`pandas`、`sklearn.model_selection`的`train_test_split`、`sklearn.ensemble`的`RandomForestRegressor`:

    ```
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    ```

3.  创建一个名为`file_url`的变量，它包含数据集的 URL:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter09/Dataset/phpYYZ4Qc.csv'    
    ```

4.  使用`.read_csv()` :

    ```
    df = pd.read_csv(self.file_url)
    ```

    将数据集加载到名为`df`的数据帧中
5.  使用`.pop()`提取`rej`列，并保存到一个名为`y` :

    ```
    y = df.pop('rej')
    ```

    的变量中
6.  使用带有`test_size=0.3`和`random_state = 1` :

    ```
    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=1)
    ```

    的`train_test_split()`将数据帧分成训练集和测试集
7.  用`random_state=1`、`n_estimators=50`、`max_depth=6`、`min_samples_leaf=60` :

    ```
    rf_model = RandomForestRegressor(random_state=1, n_estimators=50, max_depth=6, min_samples_leaf=60)
    ```

    实例化`RandomForestRegressor`
8.  Train the model on the training set using `.fit()`:

    ```
    rf_model.fit(X_train, y_train)
    ```

    您应该得到以下输出:

    ![Figure 9.39: Logs of RandomForest
    ](image/B15019_09_39.jpg)

    图 9.39:随机森林的日志

9.  使用`!pip`安装命令:

    ```
    !pip install lime
    ```

    安装 lime 包
10.  从`lime.lime_tabular` :

    ```
    from lime.lime_tabular import LimeTabularExplainer
    ```

    导入`LimeTabularExplainer`
11.  用训练集和`mode='regression'` :

    ```
    lime_explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, mode='regression')
    ```

    实例化`LimeTabularExplainer`
12.  Display the LIME analysis on the first row of the testing set using `.explain_instance()` and `.show_in_notebook()`:

    ```
    exp = lime_explainer.explain_instance(X_test.values[0], rf_model.predict)
    exp.show_in_notebook()
    ```

    您应该得到以下输出:

    ![Figure 9.40: LIME output for the first observation of the testing set
    ](image/B15019_09_40.jpg)

    图 9.40:测试集第一次观察的石灰输出

    该输出显示，该观察的预测值是 0.02 的客户退出机会，并且主要受`a1pop`、`a3pop`、`a2pop`和`b2eff`特征的影响。例如，`a1pop`低于 0.87 的事实使得目标变量的值减少了 0.01。

13.  Display the LIME analysis on the third row of the testing set using `.explain_instance()` and `.show_in_notebook()`:

    ```
    exp = lime_explainer.explain_instance(X_test.values[2], rf_model.predict)
    exp.show_in_notebook()
    ```

    您应该得到以下输出:

    ![Figure 9.41: LIME output for the third observation of the testing set
    ](image/B15019_09_41.jpg)

图 9.41:测试集第三次观察的石灰输出

该输出显示，该观察的预测值是 0.09 的客户退出机会，并且主要受`a2pop`、`a3pop`、`a1pop`和`b1eff`特征的影响。例如，`b1eff`低于 0.87 的事实使目标变量的值增加了 0.01。`b1eff`特征代表银行 1 的效率水平，因此来自 LIME 的结果告诉我们，如果该效率水平低于 0.87，客户离开的机会就会增加。

你已经完成了本章的最后一个练习。您看到了如何使用 LIME 来解释单个观察值的预测。我们了解到，`a1pop`、`a2pop`和`a3pop`特性对训练集的第一次和第三次观察有很大的负面影响。

## 活动 9.01:训练并分析网络入侵检测模型

您在一家网络安全公司工作，您的任务是构建一个模型，该模型可以识别网络入侵，然后分析其特征重要性，绘制部分相关性，并使用 LIME 对单个观察结果执行局部解释。

所提供的数据集包含 7 周网络流量的数据。

注意

本练习中使用的数据集来自 1999 年 KDD 杯:

[https://packt.live/2tFKUIV](https://packt.live/2tFKUIV)。

此数据集的 CSV 版本可在此处找到:

[https://packing . live/2 ryvsbm](https://packt.live/2RyVsBm)。

以下步骤将帮助您完成此活动:

1.  使用`pandas`中的`.read_csv()`下载并加载数据集。
2.  使用`pandas`中的`.pop()`提取响应变量。
3.  使用来自`sklearn.model_selection`的`train_test_split()`将数据集分成训练集和测试集。
4.  创建一个函数，该函数将使用来自`sklearn.ensemble`的`.fit()`实例化并拟合`RandomForestClassifier`。
5.  使用`.predict()`创建一个函数来预测训练和测试集的结果。
6.  创建一个函数，使用来自`sklearn.metrics`的`accuracy_score()`打印训练和测试集的准确度分数。
7.  使用`feature_importance_permutation()`通过排列计算特征重要性，并使用`altair`将其显示在条形图上。
8.  使用`src_bytes`变量上的`plot_partial_dependence`绘制部分相关图。
9.  使用`!pip install`安装`lime`。
10.  Perform a LIME analysis on row `99893` with `explain_instance()`.

    输出应该如下所示:

    ![Figure 9.42: Output for LIME analysis
    ](image/B15019_09_42.jpg)

图 9.42:石灰分析的输出

您已经成功地训练了一个随机林模型来预测网络连接的类型。您还分析了哪些特性对这个随机森林模型最重要，并了解到它主要依赖于`src_bytes`特性。我们还分析了这个特性的部分依赖图，以便理解它对`normal`类的影响。最后，我们使用 LIME 来分析单个观察结果，并找出哪些变量导致了预测的结果。

# 总结

在本章之三，我们学习了一些解释机器学习模型的技术。我们看到了一些特定于所用模型的技术:线性模型的系数和基于树的模型的可变重要性。还有一些方法是模型不可知的，比如通过排列改变重要性。

所有这些技术都是全局解释器，它们查看整个数据集，并分析每个变量对预测的总体贡献。我们不仅可以利用这些信息来确定哪些变量对预测影响最大，还可以对它们进行筛选。我们可以只保留影响较大的要素，而不是保留数据集中所有可用的要素。这可以显著减少训练模型或计算预测的计算时间。

我们还使用 LIME 完成了一个本地解释器场景，它分析了一个观察结果。它帮助我们更好地理解模型在预测给定案例的最终结果时所做的决定。这是一个非常强大的工具，用于评估模型是否偏向于可能包含个人详细信息或人口统计数据等敏感信息的特定变量。我们还可以用它来比较两个不同的观察结果，并理解从模型中获得不同结果的基本原理。

在下一章中，我们将重点分析数据集，并学习探索性数据分析和数据可视化技术，以更好地理解其中包含的信息。