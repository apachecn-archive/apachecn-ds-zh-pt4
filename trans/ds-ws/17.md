

# 十七、自动化特征工程

概观

在这一章中，你将使用特征工具处理自动化特征工程，分析数据集和构建业务环境；从数据集创建实体并映射基本数据集和实体之间的关系；以及基于自动特征工程建立分类模型。本章结束时，你将能够使用这些自动化特征工程技术。

# 简介

在前一章中，我们了解了一个名为 ML pipeline 的实用函数，它可以在数据科学生命周期内自动执行各种流程，如缩放、降维和建模。

在本章中，我们将学习另一个有助于自动化特征工程的工具。我们已经在前面的章节中完成了不同的特征工程任务，例如在*第三章，二进制分类*和*第十二章，特征工程*中。在前面的练习中构建功能时，您会意识到手动完成这一步是多么乏味。

例如，在*第 3 章，二进制分类*中，在*练习 3.02* 中，您使用传统的手动方式实现了不同的聚合函数来创建一个新的特征，如下面的代码片段所示:

```
# Aggregation on age
ageTot = bankData.groupby('age')['y'].agg(ageTot='count').reset_index()
ageTot.head()
```

![Figure 17.1: Different aggregation functions being used
](image/B15019_17_01.jpg)

图 17.1:使用不同的聚合函数

您可能还记得，在这个操作中，您通过`age`变量对数据框进行了分组，然后进行了聚合，以找到每个年龄组中所有示例的总数。

现在，想象你必须对数百个变量进行这样的运算。这简直令人难以置信。然而，有了你将在本章中学习的工具，手动做任何事情都可以被消除，而有利于自动特征创建。

除了减轻手动创建新功能的痛苦之外，还有一项为新功能寻找机会的艰巨任务。

你会想到(持续时间-余额/持续时间-前一个)这样的特征吗，它是一个变量的持续时间和余额之间的差异与持续时间和前一个值之间的差异的比率？您可能希望有一根魔棒来减轻为这些新功能寻找机会的痛苦。在这一章中，你的愿望将会实现，因为我们将介绍为我们创建特征的特征工具。

让我们深入了解一下它的作用。

# 特色工程

正如我们在*第 3 章，二进制分类*中提到的，特征工程是从原始数据中创建或转换特征的过程。这些特征被输入到各种机器学习模型中，以产生我们想要的商业结果。特征工程是数据科学生命周期中最重要的步骤之一，甚至比模型本身更重要。模型的准确性取决于模型中包含的内容，即您为数据集构建的要素。

构建最佳功能集在很大程度上取决于对领域的理解以及在探索性数据分析阶段从数据中获得的直觉。在创建和转换特征的过程中涉及到大量的创造性，因此特征工程可以被认为是一门艺术和一门科学。然而，手动执行特征工程是一个相当费力和耗时的过程。这就是自动化特征工程在数据科学生命周期中发挥重要作用的地方。

## 使用特征工具实现特征工程自动化

自动化特征工程需要通过预定义的数据模式从原始数据创建特征。这些新特性可以进一步提炼，这样我们就可以为下游建模选择最合适的候选者。Python 中名为 Featuretools 的库支持自动生成要素。

让我们从业务用例的角度来看看这个实用程序的内部工作。

## 业务背景

我们将用来学习不同的自动化特征工程方法的数据集将是银行营销数据集。

注意

这个数据集来自 UCI 机器学习图书馆，可以在[https://packt.live/2MItXEl](https://packt.live/2MItXEl)s .莫罗、p .科尔特斯和 p .丽塔找到。预测银行电话营销成功的数据驱动方法。UCI 机器学习知识库。

该数据集与一家银行的营销活动有关。它包含各种客户的详细信息，如下所示:

*   人口统计信息:年龄、工作、婚姻状况、教育等等
*   财务详细信息:住房、贷款、银行余额等等

问题陈述是预测某个特定客户是否会订阅定期存款。

任何特征工程过程的开始都是通过业务环境的定义来完成的。这需要理解影响问题陈述的各种业务因素。让我们通过创建一个领域故事来定义影响问题陈述的各种业务因素。

## 问题陈述的领域故事

我们的问题陈述是确定哪个客户可能购买定期存款。让我们假设，通过市场研究和与领域专家的讨论，我们已经确定了指示定期存款倾向的四个关键因素:

*   客户的人口统计数据
*   客户的资产组合
*   客户的责任
*   客户的金融行为

这些商业因素将推动我们特征工程策略的形成。

识别关键业务因素后，下一个任务是识别与这些因素相关的数据点。

从我们的数据集中，可以映射到这些因素的数据点如下:

*   客户人口统计:与人口统计相关的各种数据点是变量，如年龄、教育和婚姻状况。
*   资产组合:数据集中资产组合的一个指标是客户是否拥有房子。
*   负债:客户贷款的存在是负债的一个指标。
*   金融行为:金融行为是一个客户是否有信用的指标。在我们的数据集中，有一个指示金融行为的指标是客户是否违约。

![Figure 17.2: Business factors – data point mapping
](image/B15019_17_02.jpg)

图 17.2:业务因素——数据点映射

上图显示了各种业务因素到数据中相关数据点的映射，并定义了领域故事。一旦领域故事被定义，下一个任务就是使用特性工具实现这个结构。接下来我们来讨论这个。

## 功能工具–创建实体和关系

Featuretools 是 Python 中的一个库，它允许从可用数据集中创建完整的要素列表。Featuretools 中的一个基本构件叫做**实体**。可以将实体视为帮助创建特征的单个数据表。使用 Featuretools 的第一个任务是创建这些实体。这就是我们先前创建的领域故事变得相关的地方。与我们之前定义的四个业务因素相关的数据点将是我们案例中的实体，它们是人口统计、资产、负债和财务行为。

一旦定义了实体，下一个任务就是定义一个关系，这样我们就可以连接实体了。在我们的环境中，我们有一个客户，这个客户将有一个资产组合，一个负债组合，以及某种金融行为。

因此，我们可以在实体中定义的关系类型是，客户人口统计充当父项，其他实体与实体人口统计具有子关系。父实体和它的子实体集合放在一起，称为一个**实体集**。这可以从下图中看出:

![Figure 17.3: Entity set with its relationship
](image/B15019_17_03.jpg)

图 17.3:实体集及其关系

在上图中，我们可以看到人口统计依赖于资产、负债和财务行为实体。

现在，让我们看看数据集和数据集的所有变量。然后，我们将变量映射到我们拥有的不同实体。

下面的屏幕截图显示了我们从数据集获得的变量列表以及它们随后到实体的映射:

![Figure 17.4: List of variables from the dataset
](image/B15019_17_04.jpg)

图 17.4:数据集中的变量列表

请看下表。变量与其映射的实体一起设置:

![Figure 17.5: Variables of the mapped entity
](image/B15019_17_05.jpg)

图 17.5:映射实体的变量

注意

变量`y`是我们的目标变量，不会用于特征创建。

定义实体间关系的一个先决条件是拥有一些 id 来连接关系层次结构中的不同实体。然而，我们的数据集没有任何 id。因此，我们必须为每个实体创建 id。id 的创建必须基于每个实体拥有的唯一的一组值。

人口统计实体包含客户信息。数据集中的每一行都是唯一的客户。因此，对于人口统计实体，我们将创建一个名为`CustID`的 ID，它将等于数据集中的行数。

基于`housing`变量定义资产实体。这个变量有两个值:`Yes`和`No`。我们将这些值映射到一个数字 ID。我们将把`Yes`映射到`1`，把`No`映射到`0`。这个 ID 的变量名是`AssetId`。

类似地，映射到`loan`变量的负债实体和映射到`default`变量的金融行为实体也具有值`Yes`和`No`。它们的值也分别映射到 1 和 0。各个 id 的变量名为`LoanId`和`FinbehId`。

最终的数据模式定义了实体及其关系，如下所示:

![Figure 17.6: Entities relationship schema 
](image/B15019_17_06.jpg)

图 17.6:实体关系模式

现在，我们将在下面的练习中使用 Featuretools 实现这些初始任务。

## 练习 17.01:定义实体和建立关系

在本练习中，我们将使用银行数据集。我们将定义实体，创建 id，然后建立实体之间的关系。这些是我们在后续练习中创建自动化特征之前必须完成的初始任务。

注意

本练习中使用的数据集文件可以在本书的 GitHub 资源库中找到，该资源库位于[https://packt.live/2ZSHGxg](https://packt.live/2ZSHGxg)。

以下步骤将帮助您完成本练习:

1.  打开 Colab 笔记本。
2.  定义 GitHub 库的路径:

    ```
     # Defining the path to the 
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter17/Datasets/bank-full.csv'
    ```

3.  Next, load the data using pandas:

    ```
    # Loading data using pandas
    import pandas as pd
    bankData = pd.read_csv(file_url,sep=";")
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 17.7: Data loaded using pandas
    ](image/B15019_17_07.jpg)

    图 17.7:使用 pandas 加载的数据

4.  Remove the target variable.

    由于创建特征不需要目标变量`y`，我们将使用`.pop()`函数删除它:

    ```
     # Removing the target variable
    Y = bankData.pop('y')
    ```

5.  Create an ID for the Demographic entity, as shown in the following code snippet:

    ```
    # Creating the Ids for Demographic Entity
    bankData['custID'] = bankData.index.values
    bankData['custID'] = 'cust' + bankData['custID'].astype(str)
    ```

    因为每一行都属于一个唯一的客户，所以数据集中有多少行就有多少个客户 id。我们通过获取每一行的索引值并将名为`cust`的字符串附加到索引值来创建客户 ID。

    行的索引值可以通过`.index.values()`方法获得。然后，这些值被附加到第二行的`cust`字符串中，整个 ID(字符串值)被存储在`custID`变量中。

6.  Create an ID for Assets, as shown in the following code snippet:

    ```
    # Creating AssetId
    bankData['AssetId'] = 0
    bankData.loc[bankData.housing == 'yes','AssetId']= 1
    ```

    在这里，我们为资产创建了 ID。正如我们前面提到的，资产被映射到`housing`变量，它有两个值:`Yes`和`No`。在前面代码的第一行，我们将变量`AssetId`的所有值初始化为`0`。在第二行中，只要外壳变量为`yes`，我们就将`AssetId`的值更改为`1`。

7.  Now, create an ID for `Loans`:

    ```
    # Creating LoanId
    bankData['LoanId'] = 0
    bankData.loc[bankData.loan == 'yes','LoanId']= 1
    ```

    类似于资产的步骤，我们基于变量`loan`的值创建了一个`LoanId`。

8.  Create an ID for Financial Behavior, as shown in the following code snippet:

    ```
    # Creating Financial behaviour ID
    bankData['FinbehId'] = 0
    bankData.loc[bankData.default == 'yes','FinbehId']= 1
    ```

    类似地，您基于`default`变量的值创建金融行为的 ID。

9.  Display the data frame after adding the IDs:

    ```
     # Displaying the new data frame after adding the ids
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 17.8: Dataframe after adding the IDs
    ](image/B15019_17_08.jpg)

    图 17.8:添加 id 后的数据帧

    从前面的输出中，您可以看到我们创建的四个 id。

10.  Import the necessary libraries for implementing Featuretools, as shown in the following code snippet:

    ```
    # Importing necessary libraries
    import featuretools as ft
    import numpy as np
    ```

    注意

    使用`featuretools`库实现特征工具。

11.  Initialize `Entityset`:

    ```
    # creating the entity set 'Bankentities'
    Bankentities = ft.EntitySet(id = 'Bank')
    ```

    在这里，您初始化实体集。这是使用`.EntitySet()`方法实现的。我们提供一个字符串作为方法中的参数来跟踪实体集。我们这里给出的字符串是`Bank`。

    注意

    实体集是不同实体的组合，例如人口统计、资产、负债和财务行为。

12.  Map the data frame to the entity set to create the parent entity:

    ```
    # Mapping a dataframe to the entityset to form the parent entity
    Bankentities.entity_from_dataframe(entity_id = 'Demographic Data'
    , dataframe = bankData, index = 'custID')
    ```

    初始化实体集后，我们必须将银行数据集映射到实体集，然后创建父实体，即人口统计数据。父实体由`custID`跟踪。使用`.entity_from_dataframe()`方法映射数据帧。

    您应该得到以下输出:

    ![Figure 17.9: Output showing the mapping of the entity set to create the parent set
    ](image/B15019_17_09.jpg)

    图 17.9:显示实体集映射以创建父集的输出

    这个方法中的各种参数是`entity_id`，它只是一个跟踪名称，然后是`bankData`数据集，最重要的是索引，它是我们创建的`custID`。

    从输出中，我们可以看到第一个实体，即父实体`Demographic Data`，已经被创建。到目前为止，还没有为这个实体创建任何关系，但是我们将在接下来的步骤中这样做。

13.  Define the Assets entity and set the relationship.

    一旦创建了父实体，就应该定义每个子实体，然后在父实体`Demographic Data`和子实体之间创建关系。这是使用`.normalize_entity()`功能完成的。这是通过以下方式实现的:

    ```
     # Mapping Assets and setting the relationship
    Bankentities.normalize_entity(base_entity_id='Demographic Data', new_entity_id='Assets', index = 'AssetId', 
    additional_variables = ['housing'])
    ```

    您应该得到以下输出:

    ![Figure 17.10: Output showing the Assets entity and setting its relationship
    ](image/B15019_17_10.jpg)

    图 17.10:显示资产实体并设置其关系的输出

    从实现中我们可以看到，这个方法的各种参数是`base_entity_id`，是父实体，也就是`Demographic Data`；新实体`Assets`及其 ID`AssetId`；还有资产的变量，也就是`housing`变量。

    从输出中，我们可以看到已经创建了两个实体，并且在父实体和子实体之间形成了一个关系。

14.  Now, map the loan and financial behavior entities.

    类似于资产实体的创建，我们来映射另外两个实体，即贷款和金融行为。这是通过以下方式实现的:

    ```
     # Mapping Loans and Financial behavior entities
    Bankentities.normalize_entity(base_entity_id='Demographic Data', new_entity_id='Liability', index = 'LoanId', 
    additional_variables = ['loan'])
    Bankentities.normalize_entity(base_entity_id='Demographic Data', new_entity_id='FinBehaviour', index = 'FinbehId', 
    additional_variables = ['default']) 
    ```

    您应该得到以下输出:

    ![Figure 17.11: Mapping the Loan and Financial Behavior entities
    ](image/B15019_17_11.jpg)

图 17.11:映射贷款和金融行为实体

从新的输出中，我们可以看到所有的实体都已经形成，并且它们的关系也已经被定义。

在本练习中，我们能够使用 Featuretools 为自动化特征工程做好准备。我们通过定义实体和映射不同实体与父实体的关系来实现领域故事。

既然已经创建了实体并且设置了关系，那么是时候开始特性工程过程了。

## 特征工程-基本操作

从原始变量创建新要素可以总结为两个基本操作:

*   聚合
*   转换

聚合操作需要根据一个变量的值来更改另一个变量的值。让我们对银行数据集执行聚合操作。首先，我们将首先可视化数据集的头部:

```
bankData.head()
```

您应该得到以下输出:

![Figure 17.12: Data being displayed
](image/B15019_17_12.jpg)

图 17.12:正在显示的数据

假设我们想根据房屋的价值求`balance`的平均值。换句话说，我们想找到当房价为`yes`和`no`时的平衡均值。这是一个聚合操作，我们基于另一个变量的值聚合一个`balance`变量的值，在本例中是`housing`。这可以通过以下方式实现:

```
# Aggregating based on housing data
agg = bankData.groupby('housing')['balance'].agg('mean')
print(agg)
```

您将获得以下输出:

![Figure 17.13: Aggregated output
](image/B15019_17_13.jpg)

图 17.13:汇总输出

第一步是基于`housing`变量聚合`balance`数据。在前面的代码片段中，`.groupby()`函数根据`housing`的值对数据进行分组，`.agg()`函数查找`balance`数据的`mean`。

从输出中我们可以看到，我们有两个平均值对应于`housing`的每个值。用`no`表示的没有房子的客户的平均值是`1596.5`，用`yes`表示的有房子的客户的平均值是`1175.1`。

下一步是将该聚合合并到主数据框中，以便我们将聚合值作为数据框的一部分。这是通过以下方式实现的:

```
# Merging with the original data frame
bankNew = bankData.merge(agg,left_on = 'housing',right_index=True,how = 'left')
bankNew.head(10)
```

您将获得以下输出:

![Figure 17.14: Merging the aggregation with the main dataset
](image/B15019_17_14.jpg)

图 17.14:将聚合与主数据集合并

合并由`.merge()`函数完成，其中`bankData`与`agg`数据合并。`left_on`参数用于确保合并基于`housing`的值，而`how = left`参数确保结果数据帧与主数据帧具有相同的形式。这在*第 12 章“特征工程*中实现。从输出中，我们可以看到创建了一个名为`balance_y`的新变量，它显示了与住房价值相对应的平均值。

当手动完成时，这些聚合操作并不复杂；然而，它们涉及多个步骤。想象一个场景，当我们必须对 10 个不同的变量甚至 100 个变量进行聚合时。手动完成这些任务会非常费力。

第二种操作是转换操作，它比聚合操作简单得多。在变换运算中，我们只是根据一些变换函数来改变一个变量。例如，我们可以引入一个新变量，它是另一个变量的自然对数或另一个变量的平方根。在这样的操作中，变化不依赖于任何其他变量的值。新变量只是原变量的一个变换函数。

为了演示这一概念，让我们通过取自然对数对平衡变量进行转换:

```
# Transformation operation
import numpy as np
bankData['Tranbalance'] = np.log(bankData['balance'])
bankData.head()
```

您应该得到以下输出:

![Figure 17.15: Transformation operation performed on the dataset
](image/B15019_17_15.jpg)

图 17.15:对数据集执行的转换操作

在转换实现中，我们使用`np.log`函数对`balance`变量取对数，然后将其保存到一个名为`Tranbalance`的新变量中。您可以在最后一列中看到转换后的值。正如我们所看到的，转换操作只依赖于一个被转换的变量。

如前面的例子所示，手工执行这些操作非常繁琐。当涉及许多变量时，所涉及的工作量和花费的时间会成倍增加。

这就是 Featuretools 增加价值的地方。在下一节中，我们将了解如何使用 Featuretools 来实现自动特征提取

## 特征工具–自动化特征工程

在上一节中，我们看了特征工程的两个基本操作。按照特征工具的说法，这些操作被称为**特征原语**。要素基元是在 Featuretools 中预定义的聚合和转换可能性的列表。

可以使用以下命令获得可用原语的列表:

```
import featuretools as ft
ft.primitives.list_primitives()
```

您应该得到以下输出:

![Figure 17.16: List of available primitives
](image/B15019_17_16.jpg)

图 17.16:可用原语列表

在前面的输出中，您可以看到一些聚合名称，例如`min`、`std`、`skew`、`mean`和`median`。例如，如果需要数据集的列数据的中位数，可以使用聚合类型的 median 选项。

Featuretools 实用程序通过一种称为**深度特征合成** ( **DFS** )的方法自动生成新特征。通过这种方法，诸如聚集和变换的特征基元步骤被应用于原始特征以创建新特征。在实现深度特征合成时，方法内部有一些默认的原语，如`sum`、`standard deviation (std)`、`max`、`min`、`skew`。

此外，我们还可以从可用的原语列表中指定我们想要实现的原语列表。除此之外，我们还可以通过定义自定义函数来创建新的原语，然后将这些自定义函数添加到这个列表中。

现在，我们将查看如何实现深度特征合成方法来创建新特征。

## 练习 17.02:使用深度特征合成创建新特征

本练习将建立在我们上一个练习的基础上。在本练习中，我们将创建实体和关系，然后应用深度特征合成(DFS)方法来创建新特征。

注意

本练习中使用的数据集文件可以在本书的 GitHub 资源库中找到，该资源库位于[https://packt.live/2ZSHGxg](https://packt.live/2ZSHGxg)。

以下步骤将帮助您完成本练习:

1.  打开新的 Colab 笔记本。
2.  实施*练习 17.01* 的所有步骤，直到创建实体和关系。
3.  Create automated features using DFS.

    一旦创建了实体并定义了关系，就可以使用`ft.dfs()`功能合成特征。这可以通过以下方式实现:

    ```
    # Creating feature sets using Deep Feature Synthesis
    feature_set, feature_names = ft.dfs(entityset=Bankentities, 
    target_entity = 'Demographic Data', 
    max_depth = 2, 
    verbose = 1, 
    n_jobs = 1)
    ```

    您应该得到以下输出:

    ![Figure 17.17: Output showing the automated features
    ](image/B15019_17_17.jpg)

    图 17.17:显示自动化特征的输出

    该函数的前两个参数是我们首先创建的`entityset`和父实体`target_entity`。另一个重要的参数是`max_depth`，它定义了创建的特征的堆叠级别。

    在本例中，我们将`max_depth`设置为`2`。根据 max_depth 的级别，特征的数量会有所不同。需要注意的是，使用深度`1`，将不会应用任何聚合原语。使用深度`1`，将只应用转换图元。一旦我们看到自动化特征工程的输出，这个概念将会更加清晰。

    verbose 参数用于指定我们是否需要进程的通知。值`1`意味着我们需要通知。

    `n_jobs`参数用于指定我们是否希望在系统的多个内核上并行化作业。这适用于我们拥有多核机器的情况。对于 Colab，我们必须保持这个参数为`1`。

    DFS 实现有两个输出:功能集和功能名称。第一个是创建所有附加要素后修改的数据框。从输出中，我们可以看到该实用程序总共创建了`196`个特征。第二个输出是所有这些特性的名称。

4.  Reset the index of the features.

    一旦创建了特性集，索引就会变得混乱。我们需要重新索引它们，以便索引与原始数据集相似。这是通过以下方式实现的:

    ```
    # Reindexing the feature_set
    feature_set = feature_set.reindex(index=bankData['custID'])
    feature_set = feature_set.reset_index()
    ```

    在第一行中，使用`.reindex()`函数重新索引。作为论据，我们给出了目标索引，基于该索引必须进行重新索引。在参数中，我们指定索引必须基于`custID`的顺序进行。一旦实现了这一行，数据帧的索引就变成了`cust01`、`cust02` …，以此类推。第二行，`.reset_index()`，用于将这个索引改为`0`、`1`、`2`，以此类推。

5.  Verify the shape of the new data frame and compare it with the old data frame.

    让我们打印新旧数据框的形状，以便比较形状:

    ```
    # Verifying the shape of the features and original bank data
    print(feature_set.shape)
    print(bankData.shape)
    ```

    您应该得到以下输出:

    ```
    (45211, 197)
    (45211, 20)
    ```

    从输出中，我们可以看到新的数据帧具有`197`特征。在 DFS 过程之后，生成了 196 个特征。剩下的特性是`CustID`，这使得特性总数成为`197`。

6.  Print the head of the new data frame:

    ```
    # Printing head of the feature set
    feature_set.head()
    ```

    您应该得到以下输出:

    ![Figure 17.18: New DataFrame as the output
    ](image/B15019_17_18.jpg)

    图 17.18:作为输出的新数据帧

    注意

    在前面的屏幕截图中，只显示了一些功能。

    从前面的截图中，我们可以看到创建的堆叠特征，如`Assets.Sum(Demographic Data.balance)`。

    让我们来分析这个特性，以真正确定它的机制。

    在这个特性中，我们聚合了人口统计数据，这是关于资产组合的`balance`变量。这里我们是在汇总有房客户和无房客户的余额。要手动执行此操作，请使用以下代码:

    ```
    # Verifying the features for Assets.SUM(Demographic Data.balance)
    bankData.groupby('AssetId')['balance'].agg('sum')
    ```

    您应该得到以下输出:

    ![Figure 17.19: Each feature being analyzed
    ](image/B15019_17_19.jpg)

    图 17.19:正在分析的每个特征

    在前面的代码中，我们使用`groupby()`函数对`AssetId`变量进行分组。1 的`AssetId`表示客户有房，0 表示无房。分组后，使用`.agg()`函数对`balance`方法进行求和。从输出中，我们可以看到`'0'`的值是`32059342`，而`'1'`是`29530340`。从自动输出中，我们可以看到这些值分别对应于`Asset.housing = no`和`Asset.housing = 'yes'`变量。

7.  Let's print out all the feature names:

    ```
    # Printing the list of all features
    feature_names
    ```

    您应该得到以下输出:

    ![Figure 17.20: Output showing all the feature names
    ](image/B15019_17_20.jpg)

    图 17.20:显示所有特征名称的输出

    注意

    前面的输出只是部分特性列表。

8.  As we described at the beginning of this exercise, we can configure the list of primitives that are used for creating new features. Now, let's define the set of aggregation and transformation primitives we want. This is implemented as a list, as follows:

    ```
    # Creating aggregation and transformation primitives
    aggPrimitives=[
            'std', 'min', 'max', 'mean', 
            'last', 'count'

    ]
    tranPrimitives=[
            'percentile', 
             'subtract', 'divide'] 
    ```

    正如我们所看到的，我们正在指定想要实现的原语列表。这些被定义为列表，然后提供给 DFS 功能。

9.  Create a new set of features with the custom primitive list.

    在定义了我们的原语列表之后，我们将把这些原语提供给 DFS，这样它将创建一组新的特性。新的原语集作为 DFS 函数的参数给出:

    ```
    # Defining the new set of features
    feature_set, feature_names = ft.dfs(entityset=Bankentities, 
    target_entity = 'Demographic Data',
    agg_primitives=aggPrimitives,
    trans_primitives=tranPrimitives, 
    max_depth = 2, 
    verbose = 1, 
    n_jobs = 1)
    ```

    您应该得到以下输出:

    ![Figure 17.21: Creating a new set of features
    ](image/B15019_17_21.jpg)

    图 17.21:创建一组新的特征

    从输出中，我们可以看到特性的数量增加到了`3420`左右。这种实质性的增加是因为，在默认模式下，没有转换原语。默认模式只包含聚合原语。因此，通过我们提供的额外转换原语，DFS 方法创建了一组新的特性，因此特性列表增加到了`3420`。

10.  Let's print the head of the new data frame:

    ```
    # Displaying the feature set 
    feature_set.head()
    ```

    您应该会得到类似如下的输出:

![Figure 17.22: The new DataFrame
](image/B15019_17_22.jpg)

图 17.22:新的数据框架

从输出中，我们可以看到由转换原语(百分点)生成的新特性。

在本练习中，我们实施了 DFS 方法来生成一组新的要素。

在业务环境中，我们有一组新功能，可以帮助我们预测客户是否会购买定期存款。

让我们总结一下我们在这个练习中学到的东西。首先，让我们从默认的原语开始。从这个练习中可以看出，当我们没有指定任何原语时，只应用默认的聚合原语来生成一组新的特性。只有当我们指定了我们的自定义列表时，转换原语才会被应用来生成一个更大的特性集。应该注意的是，当我们指定一个原语列表时，这个新列表将覆盖默认的原语集合。另一个应该注意的方面是生成的特征数量的增长。生成的特征列表随着定义的图元数量以及深度而增长。通过改变这两个参数，我们可以改变生成的特征的数量。

需要注意的最重要的因素是特性的相关性。该实用程序生成的功能列表并不是万能的。Featuretools 实用程序根据我们建立的关系、我们实现的数据类型和可用的功能创建一个功能列表。Featuretools 允许我们给出一个非常详尽的列表，有效地减少了我们的手动工作。

数据科学家有责任最终筛选特征列表，并选择最相关的特征。

到目前为止，在这一章中，我们已经学习了生成自动化特征的各种技术。我们首先将一个领域故事放在一起，定义各种实体，并设置实体之间的关系。然后，我们使用实体集通过深度特征合成生成一组新的特征。特征工程的布丁的证明是在使用所有这些特征的模型的性能中。

现在，我们将使用构建的要素构建逻辑回归模型，然后验证生成的新要素的性能。

## 练习 17.03:自动特征生成后的分类模型

在本练习中，我们将基于银行营销数据集构建一个逻辑回归模型来预测定期存款购买倾向。我们将从拟合原始特性的基准模型开始这个练习，然后记录基准度量。之后，我们将使用 Featuretools 生成新要素，然后在新数据集上构建另一个模型。最后，我们将分析结果，以观察我们建立的模型的性能。

注意

本练习中使用的数据集文件可以在本书的 GitHub 资源库中找到，该资源库位于[https://packt.live/2SSXPl3](https://packt.live/2SSXPl3)。

以下步骤将帮助您完成本练习:

1.  打开 Colab 笔记本。
2.  定义 GitHub 存储库的路径:

    ```
    # Defining the path to the Github repository
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter17/Datasets/bank-full.csv'
    ```

3.  Load the data using pandas:

    ```
    # Loading data using pandas
    import pandas as pd
    bankData = pd.read_csv(file_url,sep=";")
    bankData.head()
    ```

    您应该会得到与下面类似的输出:

    ![Figure 17.23: Loading data using pandas
    ](image/B15019_17_23.jpg)

    图 17.23:使用 pandas 加载数据

4.  We'll remove the target variable using the `.pop()` function:

    ```
    # Removing the target variable
    Y = bankData.pop('y')
    ```

    这里，`.pop()`函数从数据集中删除已定义的变量。

5.  现在，我们将数据集分成训练集和测试集:

    ```
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(bankData, Y, test_size=0.3, random_state=123)
    ```

6.  Then, we will use pipelines to transform the variables.

    在本练习中，我们将使用管道缩放数值变量，并从分类变量创建虚拟变量。这个实现类似于我们在*第 16 章机器学习管道*中完成的练习:

    ```
    #Using pipeline to transform categorical variable and numeric variables
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
    ```

    首先，我们定义了分类和数值转换器，它们分别是一次性编码和缩放。

7.  在定义了转换管道之后，我们将定义分类和数字数据类型。这一步类似于我们在第十六章*中所做的，机器学习管道* :

    ```
    # Defining data types for numeric and categorical features
    numeric_features = bankData.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = bankData.select_dtypes(include=['object']).columns
    ```

8.  在这一步中，我们使用 scikit-learn 中的`ColumnTransformer()`函数创建预处理器管道。请注意，这个步骤类似于我们在*第 16 章，机器学习管道* :

    ```
    # Defining preprocessor
    from sklearn.compose import ColumnTransformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)])
    ```

    中实现的步骤
9.  同样，类似于我们在*第 16 章，机器学习管道*中所做的，我们将创建包含预处理器和逻辑回归分类器的估计器:

    ```
    # Defining the estimator for processing and classification
    from sklearn.linear_model import LogisticRegression
    estimator = Pipeline(steps=[('preprocessor', preprocessor),                      
                               ('classifier',LogisticRegression(random_state=123))])
    ```

10.  Next, we'll fit the estimator on the training set and print the model's score:

    ```
    # Fit the estimator on the training set
    estimator.fit(X_train, y_train)  
    print("model score: %.2f" % estimator.score(X_test, y_test)) 
    ```

    您应该会得到与下面类似的输出:

    ```
    Model score: 0.90
    ```

11.  现在，我们将在测试集上预测:

    ```
    # Predict on the test set
    pred = estimator.predict(X_test)
    ```

12.  Next, we'll generate the classification report, as follows:

    ```
    # Generating classification report
    from sklearn.metrics import classification_report
    print(classification_report(pred,y_test))
    ```

    您应该会得到与下面类似的输出:

    ![Figure 17.24: Expected classification matrix
    ](image/B15019_17_24.jpg)

    图 17.24:预期分类矩阵

    一旦在没有特征工程的情况下创建了基准模型，我们将继续使用 Featuretools 创建一些新的特征，然后在新的特征集上拟合另一个模型。现在，我们将继续定义实体及其关系。

13.  Create the customer ID for tracking entities:

    ```
    # Creating the Ids for Demographic Entity
    bankData['custID'] = bankData.index.values
    bankData['custID'] = 'cust' + bankData['custID'].astype(str)
    ```

    因为每一行都属于一个唯一的客户，所以数据集中有多少行就有多少个客户 id。您通过获取每一行的索引值并将名为`cust`的字符串附加到索引值来创建客户 ID。行的索引值可以通过`.index.values()`方法获得。然后这些值被附加到第二行的`cust`字符串中，整个 ID(一个字符串值)被存储在`custID`变量中。

14.  接下来，我们为资产创建 ID。正如我们前面解释的，Assets 被映射到`housing`变量，有两个可能的值:`yes`或`no`。在代码的第一行，我们将变量`AssetId`的所有值初始化为`0`。在第二行中，只要外壳变量为`yes`，我们就将`AssetId`的值更改为`1`。这可以按如下方式实现:

    ```
    # Creating AssetId
    bankData['AssetId'] = 0
    bankData.loc[bankData.housing == 'yes','AssetId']= 1 
    ```

15.  类似于资产的步骤，我们基于变量`loan` :

    ```
    # Creating LoanId
    bankData['LoanId'] = 0
    bankData.loc[bankData.loan == 'yes','LoanId']= 1
    ```

    的值创建`LoanId`
16.  现在，我们将基于`default`变量的值为金融行为创建 ID:

    ```
    # Creating Financial behaviour ID
    bankData['FinbehId'] = 0
    bankData.loc[bankData.default == 'yes','FinbehId']= 1
    ```

17.  Import the necessary libraries for Featuretools extraction.

    Featuretools 可以使用`featuretools`库来实现:

    ```
    # Importing necessary libraries
    import featuretools as ft
    import numpy as np 
    ```

18.  第一步是初始化实体集。这是使用`.EntitySet()`方法实现的。我们提供一个字符串作为方法中的参数来跟踪实体集。我们这里给出的字符串是`Bank` :

    ```
    # creating the entity set 'Bankentities'
    Bankentities = ft.EntitySet(id = 'Bank')
    ```

19.  Map the parent entity to the data frame.

    初始化实体集后，我们必须将银行数据集映射到实体集，然后创建父实体，即人口统计数据。父实体由`custID`跟踪。使用`.entity_from_dataframe()`方法映射数据帧。这是通过以下方式实现的:

    ```
    # Mapping a dataframe to the entityset to form the parent entity
    Bankentities.entity_from_dataframe(entity_id = 'Demographic Data', dataframe = bankData, index = 'custID')
    ```

    您应该得到以下输出:

    ![Figure 17.25: Mapping the parent entity
    ](image/B15019_17_25.jpg)

    图 17.25:映射父实体

    在前面的输出中，我们可以看到已经创建了第一个实体，即父实体`Demographic Data`。到目前为止，尚未为此实体创建任何关系。

20.  Map all the entities and set their relationships.

    一旦创建了父实体，就应该定义每个子实体，然后在父实体`Demographic Data`和子实体之间创建关系。这是使用`.normalize_entity()`功能完成的。这可以通过以下方式实现:

    ```
    #  Mapping to parent entity and setting the relationship
    Bankentities.normalize_entity(base_entity_id='Demographic Data', new_entity_id='Assets', index = 'AssetId', 
    additional_variables = ['housing'])
    Bankentities.normalize_entity(base_entity_id='Demographic Data', new_entity_id='Liability', index = 'LoanId', 
    additional_variables = ['loan'])
    Bankentities.normalize_entity(base_entity_id='Demographic Data', new_entity_id='FinBehaviour', index = 'FinbehId', 
    additional_variables = ['default'])
    ```

    您应该得到以下输出:

    ![Figure 17.26: Mapping the entities to the relationships
    ](image/B15019_17_26.jpg)

    图 17.26:将实体映射到关系

    从前面的输出中，我们可以看到所有四个实体(人口统计、资产、负债和财务行为)都已创建，并且在父实体(人口统计)和子实体之间形成了一种关系。

21.  Create aggregation and transformation primitives.

    我们可以配置用于创建新特征的原语列表。让我们定义一组我们想要的聚合和转换原语。这是通过如下列表实现的:

    ```
    # Creating aggregation and transformation primitives
    aggPrimitives=[
            'std', 'min', 'max', 'mean', 
             'last', 'count'

    ]
    tranPrimitives=[
            'percentile', 
             'subtract', 'divide']
    ```

22.  In this step, we define the DFS with the created primitives. We set the depth to `2`:

    ```
    # Defining the new set of features
    feature_set, feature_names = ft.dfs(entityset=Bankentities, 
    target_entity = 'Demographic Data',
    agg_primitives=aggPrimitives,
    trans_primitives=tranPrimitives, 
    max_depth = 2, 
    verbose = 1, 
    n_jobs = 1)
    ```

    您应该会得到与下面类似的输出:

    ![Figure 17.27: Defining the new features
    ](image/B15019_17_27.jpg)

    图 17.27:定义新特性

    从前面的输出中，我们可以看到已经创建了`3420`个特征。

23.  Once the feature sets have been created, the indexing will be all jumbled up. We need to reindex them so that the index is similar to the original dataset. This is implemented as follows:

    ```
    # Reindexing the feature_set
    feature_set = feature_set.reindex(index=bankData['custID'])
    feature_set = feature_set.reset_index()
    ```

    在第一行中，使用`.reindex()`函数重新索引。作为一个参数，我们传递目标索引，基于该索引必须进行重新索引。在参数中，我们指定索引必须基于`custID`的顺序进行。一旦实现了这一行，数据框的索引就变成‘cust 01’，‘cust 02’…，依此类推。第二行，也就是`.reset_index()`，用来把这个指标改成`0`、`1`、`2`，以此类推。

24.  Now that the feature set has been created, we can print the shape of the feature set:

    ```
    # Displaying the feature set 
    feature_set.shape
    ```

    您应该得到以下输出:

    ```
    (45211, 3421)
    ```

    从前面的输出中，我们可以看到已经创建了`3421`个新特性。行数，即`45211`，保持与原始数据集相同。

25.  Now, drop all the IDs. In the feature set, there are some features that are related to the IDs that we created. When building models, these IDs will not add any value as they are only for tracking purposes. Therefore, we can remove them. We can do this as follows:

    ```
    # Dropping all Ids
    X = feature_set[feature_set.columns[~feature_set.columns.str.contains(
        'custID|AssetId|LoanId|FinbehId')]]
    ```

    在这个实现中，波浪号`~`表示否定。在前面的代码中，我们用那些不包含`custID`、`AssetId`、`LoanId`或`FinbehId`的特性来子集化特性集。在这一步中，我们删除了与我们创建的 id 相关的所有特性。

26.  Replace all infinity values with `nan` values:

    ```
    # Replacing all columns with infinity with nan
    X = X.replace([np.inf, -np.inf], np.nan)
    ```

    像 divide 这样的变换原语的一个副作用是创建具有无穷大值的特征。当存在包含`0`的特征时，会发生这种情况。众所周知，除以`0`会产生一个无穷大的值。这些无穷大值必须从数据框中移除，因为在建模阶段不需要它们。这是通过用 nan 值替换所有的无穷大值，然后删除 nan 值来实现的。第一步使用`.replace()`功能执行如下。

    注意

    `np`。`Inf`和`–np.inf`代表无穷大的值。

27.  Drop all the columns containing `nan`. Once the replacement of infinity values with `nan` has been done, these columns can be dropped from the dataset. This is implemented using the `.dropna()` function:

    ```
    # Dropping all columns with nan
    X = X.dropna(axis=1, how='any')
    X.shape
    ```

    您应该得到以下输出:

    ```
    (45211, 1046)
    ```

    在前面的实现中，`axis = 1`表示沿着列。`how = 'any'`表示丢弃任何与`nan`值接触的列。我们可以看到，在删除所有冗余列后，特征的数量从`3421`下降到`1046`。

28.  现在，让我们将新数据集分成训练集和测试集进行建模:

    ```
    # Splitting train and test sets
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)
    ```

29.  Like we did during the benchmark model creation step, let's create the processing pipeline. We will use pipelines to scale numerical variables and create dummy variables from categorical variables. This implementation is similar to the implementation we completed in *Chapter 16, Machine Learning Pipelines*:

    ```
    # Creating the preprocessing pipeline
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X.select_dtypes(include=['object']).columns
    from sklearn.compose import ColumnTransformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)])
    ```

    从前面的实现可以看出，我们通过`transformers`参数给出了必要的转换。第一个转换器是数字转换器，用字符串`numeric`表示。然后，我们应用转换器`numTransformer`，它是数字特征`numFeatures`上的缩放函数。类似地，我们在分类变量上定义适当的变换。

30.  Let's create the estimator function, which contains the processing step and the classifier layer. After doing this, we'll fit the estimator on the training set and print the scores. This implementation is similar to the pipeline estimator we created in *Chapter 16, Machine Learning Pipelines*:

    ```
    # Creating the estimator function and fitting the training set
    estimator = Pipeline(steps=[('preprocessor', preprocessor),                      
                               ('classifier',LogisticRegression(random_state=123))])
    estimator.fit(X_train, y_train)  
    print("model score: %.2f" % estimator.score(X_test, y_test)) 

    ```

    您应该得到以下输出:

    ```
    Model score: 0.90
    ```

    正如我们所看到的，估计器是使用`Pipeline()`函数定义的，我们提供了前面描述的步骤。然后，我们使用`.fit()`函数在估计器上拟合训练集。最后，打印模型分数。

    从输出可以看出，精度水平与基准模型相同。我们也来看看分类报告是什么样子的。

31.  拟合完成后，使用`.predict()`功能生成预测。对测试集进行预测:

    ```
    # Predicting on the test set
    pred = estimator.predict(X_test)
    ```

32.  Once the predictions have been generated, the classification report is generated:

    ```
    # Generating the classification report
    from sklearn.metrics import classification_report
    print(classification_report(pred,y_test))
    ```

    您应该得到以下输出:

![Figure 17.28: Expected classification matrix
](image/B15019_17_28.jpg)

图 17.28:预期分类矩阵

从前面的输出中，我们可以看到准确性分数保持不变。然而，少数类的精确度、召回率和 f1 分数有所提高(是)。所有这些值分别从`34%`、`64%`和`43%`开始增加。我们应该记住，这是一个极度不平衡的数据集。然而，通过我们生成的新特性，我们能够显示出少数类的性能有微小的提高。

话虽如此，重要的是要注意自动化特征工程并不是总能保证性能改进的灵丹妙药。这个实用程序增加的价值在于给了我们一个详尽的功能列表供我们选择。这种方法的真正价值在于使特征工程过程变得非常高效，并为数据科学家提供了一个可供选择的详尽列表。一旦有了详尽的特性列表，数据科学家就有责任应用领域理解、经验和直觉，以便选择他们认为会有所不同的特性。

从商业角度来看，这一结果表明，在可能购买定期存款的总共 848 名客户中，只有 65%的人(回忆“是”)被正确识别为有购买定期存款的倾向。

# 新数据集上的功能工具

在本章中，我们学习了 Featuretools 以及如何使用它构建自动化功能。在下一个活动中，我们将把我们学到的知识应用到一个新的数据集。该数据集是来自加州尔湾 UCI 机器学习库的成人数据集的修改版本:加州大学信息与计算机科学学院，可在 https://packt.live/2Qr3ih6 的`adult.data`文件中找到。该数据集具有工作成人的各种属性，如年龄、职业、教育程度和籍贯。任务是预测一个特定的成年人的年薪是否会超过`50,000`。

关于各种属性的详细信息可在`adult.names`文件的前一个链接中找到。该数据集混合了分类数据和数值数据，是检验您对 Featuretools 了解程度的良好数据集。

## 活动 17.01:使用 Featuretools 生成的特征构建分类模型

在本活动中，您将在成人数据集上构建一个逻辑回归模型，以预测成人的年收入是否会超过 50，000 英镑。您将通过在原始特征上拟合基准模型来开始此活动，然后记录基准度量。之后，您将使用 Featuretools 生成新要素，然后在新数据集上构建另一个模型。您应该分析结果，以观察您构建的模型的性能。

注意

本活动中使用的数据集文件可以在本书的 GitHub 资源库中找到，该资源库位于[https://packt.live/39LrZfM](https://packt.live/39LrZfM)。

按照以下步骤完成本活动:

1.  使用`pandas`从下面的 GitHub 存储库中读取数据:[https://packt.live/2T7OAO7](https://packt.live/2T7OAO7)。
2.  使用`.dropna()`功能删除所有`na`值。
3.  使用`label`变量上的`.pop()`函数创建`Y`变量。
4.  将数据集分为训练集和测试集。
5.  创建处理器管道，将分类变量转换为一次性编码，将数值变量转换为比例变量。
6.  使用数据处理器和逻辑回归分类器定义估计函数。
7.  在训练集上拟合估计量，然后在测试集上打印分数。
8.  使用估计函数生成对测试集的预测。
9.  打印分类报告。
10.  创建一个名为`parentID`的父实体 ID，类似于我们在*练习 17.01* 中创建的`custID`。
11.  不同的子实体可以是变量，如教育、工作类别和职业。对于教育，education-num 变量可以用作 ID。对于其他两个变量，根据该变量中唯一值的数量创建一些唯一的 id。
12.  For the `workclass` variable, the unique values are as follows:

    `' Federal-gov', ' Local-gov', ' Private',' Self-emp-inc', ' Self-emp-not-inc', ' State-gov', ' Without-pay'`

13.  For the `Occupation` variable, the unique values are as follows:

    `' Adm-clerical', ' Armed-Forces',' Craft-repair', ' Exec-managerial', ' Farming-fishing',' Handlers-cleaners', ' Machine-op-inspct', ' Other-service', ' Priv-house-serv', ' Prof-specialty',' Protective-serv',' Sales', ' Tech-support', ' Transport-moving'`

14.  创建父实体，并使用各自的 id 设置与教育、工作类别和职业的关系。
15.  创建聚合和转换原语。
16.  用定义的原语创建 DFS。
17.  为创建的数据框重新编制索引。
18.  删除所有与您创建的 id 相关的变量。
19.  使用`dropna()`函数，用`na`替换所有的无穷大值，用`na`替换下拉列。
20.  将数据集分为训练集和测试集。
21.  创建处理管道。
22.  创建估计函数，并在估计函数上拟合训练集。然后，生成分数。
23.  对测试集生成预测并打印分类报告。

您应该会得到与此处所示类似的输出。

以下是基准模型的分类报告:

![Figure 17.29: Expected classification matrix for the benchmark model
](image/B15019_17_29.jpg)

图 17.29:基准模型的预期分类矩阵

以下是特征工程模型的分类报告:

![Figure 17.30: Expected classification matrix for the feature engineered model
](image/B15019_17_30.jpg)

图 17.30:特征工程模型的预期分类矩阵

注意

活动的解决方案可以在这里找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

# 总结

在本章中，我们学习了如何使用功能工具。作为数据科学家之旅的一部分，您将逐渐意识到构建要素是一个非常繁琐的过程，需要花费大量的精力和时间。然而，我们已经看到，通过使用 Featuretools，新特征的生成非常高效，并且我们获得了可以在我们的模型上尝试的一组非常详尽的潜在特征。

在这一章中，我们从领域/业务的角度来处理特性工程步骤。我们确定了一些关键的业务因素，并通过连接这些业务因素构建了一个领域故事。

我们还了解了自动化特征工程的一些关键构件，例如实体和实体集。我们使用我们制定的领域故事来创建实体，并设置父实体和子实体之间的关系。

一旦定义了实体集，我们就接触到了特性工具的基本操作，比如聚集和转换，这被称为原语。我们还使用特征工具中的深度特征合成方法创建了一组新特征。使用新创建的特征集，我们建立了一个逻辑回归模型来预测定期存款购买的倾向。我们发现新的特性集提高了少数类的性能。然后，我们将我们的学习应用到一个新的数据集，我们预测一个成年人的年收入是否会超过 5 万英镑。我们观察到我们创建的特征集提高了我们构建的逻辑回归模型的性能。

本章的目的是向您介绍一个非常有效的工具，它将有助于生成一个详尽的特性列表。但是，应该记住，构建一个详尽的功能集并不是目的，它只是达到目的的一种手段。数据科学家有责任仔细验证生成的特性列表，并根据领域理解、直觉和经验选择最重要的特性。

现在我们已经了解了一个非常重要的工具集，我们将进入下一章，在那里我们将被介绍如何将我们的模型构建为服务。

下一章是本书的最后一章，也是本课程数位互动版的一部分。按照本书开头的说明赎回您的交互式版本。