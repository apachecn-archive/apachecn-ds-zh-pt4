<title>C15019_11_Final_ePub_SW</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 11。数据准备

概述

本章结束时，您将能够使用特定条件过滤数据帧；删除重复或不相关的记录或列；将变量转换成不同的数据类型；替换列中的值，并处理缺失值和异常值。

本章将向您介绍可用于处理数据问题的主要技术，以便在建模之前获得高质量的数据集。

# 简介

在前一章中，您看到了很好地理解您的数据是多么重要，并了解了实现这一目标的不同技术和工具。在给定数据集上执行**探索性数据分析** ( **EDA** )时，您可能会发现一些需要在建模阶段之前解决的潜在问题。这正是本章将要讨论的主题。您将了解如何处理一些最常见的数据质量问题，并正确准备数据集。

本章将向您介绍在您的数据科学家职业生涯中经常遇到的问题(如重复的行、不正确的数据类型、不正确的值和缺失值)，您将了解可用于轻松解决这些问题的技术。但是要小心——你遇到的一些问题不一定需要解决。从商业角度来看，您发现的一些可疑或意外的价值可能是真实的。这包括那些很少出现但完全真实的价值观。因此，在修改数据集之前，获得利益相关者或数据工程团队的确认是非常重要的。在准备数据集时，您有责任确保为企业做出正确的决策。

例如，在*第 10 章*、*分析数据集*中，您查看了*在线零售数据集*，它在`Quantity`列中有一些负值。在这里，我们只期望正值。但是在直接解决这个问题之前(通过丢弃记录或者将它们转换成正值)，最好先与您的利益相关者取得联系，并确认这些值对业务并不重要。他们可能会告诉您，这些值非常重要，因为它们代表退回的物品，会让公司花费大量资金，因此他们希望分析这些案例，以减少这些数字。如果您直接进入数据清理阶段，您将会错过这条关键信息，并可能得出不正确的结果。

# 处理行重复

大多数情况下，您将接收或有权访问的数据集不会被 100%清理。他们通常有一些问题需要解决。其中一个问题可能是重复行。行重复意味着几个观察值在数据集中包含完全相同的信息。有了`pandas`包，查找这些案例变得极其容易。

让我们用我们在第 10 章*中看到的例子来分析数据集*。

首先将数据集导入数据框架:

```
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

来自`pandas`的`duplicated()`方法检查是否有任何行是重复的，并为每一行返回一个布尔值，如果该行是重复的，则返回`True`,否则返回`False`:

```
df.duplicated()
```

您应该得到以下输出:

![Figure 11.1: Output of the duplicated() method
](image/C15019_11_01.jpg)

图 11.1:复制的()方法的输出

注意

前面的输出已被截断。

在 Python 中，`True`和`False`二进制值分别对应数值 1 和 0。要找出有多少行被识别为重复行，可以对`duplicated()`的输出使用`sum()`方法。这将把所有的 1(即`True`值)相加，并给出重复项的数量:

```
df.duplicated().sum()
```

您应该得到以下输出:

```
5268
```

由于`duplicated()`方法的输出是每行二进制值的`pandas`序列，所以您也可以使用它来对数据帧的行进行子集化。`pandas`包为数据帧的子集化提供了不同的 API，如下所示:

*   df[ <rows>或</rows>
*   df.loc[ <rows>， <columns>]</columns></rows>
*   df.iloc[ <rows>， <columns>]</columns></rows>

第一个 API 按行或列划分数据帧的子集。要筛选特定的列，您可以提供一个包含其名称的列表。例如，如果您只想保留变量，即`InvoiceNo`、`StockCode`、`InvoiceDate`和`CustomerID`，您需要使用下面的代码:

```
df[['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID']]
```

您应该得到以下输出:

![Figure 11.2: Subsetting columns
](image/C15019_11_02.jpg)

图 11.2:设置列的子集

如果您只想过滤被认为是重复的行，那么您可以对`duplicated()`方法的输出使用相同的 API 调用。它将只保留值为 True 的行:

```
df[df.duplicated()]
```

您应该得到以下输出:

![Figure 11.3: Subsetting the duplicated rows
](image/C15019_11_03.jpg)

图 11.3:设置重复行的子集

如果您想同时对行和列进行子集化，您必须使用另外两个可用的 API 之一:`.loc`或`.iloc`。这些 API 做完全相同的事情，但是`.loc`使用标签或名称，而`.iloc`只接受索引作为输入。您将使用`.loc` API 对复制的行进行子集划分，并只保留选定的四列，如前面的示例所示:

```
df.loc[df.duplicated(), ['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID']]
```

您应该得到以下输出:

![Figure 11.4: Subsetting the duplicated rows and selected columns using .loc
](image/C15019_11_04.jpg)

图 11.4:使用。通信线路（LinesofCommunication）

前面的输出显示，前几个重复项是行号`517`、`527`、`537`，等等。默认情况下，`pandas`不会将第一次出现的重复标记为重复:同样，除了第一次出现之外，重复的值将为`True`。您可以通过指定`keep`参数来改变这种行为。如果想保留最后一个副本，需要指定`keep='last'`:

```
df.loc[df.duplicated(keep='last'), ['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID']]
```

您应该得到以下输出:

![Figure 11.5: Subsetting the last duplicated rows
](image/C15019_11_05.jpg)

图 11.5:设置最后一个重复行的子集

从前面的输出可以看出，行`485`的值与行`539`的值相同。正如所料，行`539`不再被标记为重复。如果您想将所有重复记录标记为重复记录，您必须使用`keep=False`:

```
df.loc[df.duplicated(keep=False), ['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID']]
```

您应该得到以下输出:

![Figure 11.6: Subsetting all the duplicated rows
](image/C15019_11_06.jpg)

图 11.6:设置所有重复行的子集

注意

前面的输出已被截断。

这一次，行`485`和`539`被列为重复行。既然您已经知道了如何识别重复观测值，那么您可以决定是否希望将它们从数据集中移除。正如我们前面提到的，在更改数据时必须小心。您可能需要与企业确认他们是否愿意让您这样做。您必须解释为什么要删除这些行。在在线零售数据集中，如果以行`485`和`539`为例，这两个观察结果是相同的。从商业角度来看，这意味着特定客户(`CustomerID 17908`)在完全相同的日期和时间(`InvoiceDate 2010-12-01 11:45:00`)在同一张发票(`InvoiceNo 536409`)上购买了相同的商品(`StockCode 22111`)。这非常可疑。当您与业务人员交谈时，他们可能会告诉您，当时发布了新软件，并且存在一个错误，即将所有购买的项目拆分为单行项目。

在这种情况下，您知道不应该删除这些行。另一方面，他们可能会告诉你，重复不应该发生，这可能是由于输入数据时或数据提取步骤中的人为错误。让我们假设情况是这样的；现在，您可以安全地删除这些行了。

为此，您可以使用来自`pandas`的`drop_duplicates()`方法。它有与`duplicated()`相同的`keep`参数，该参数指定您想要保留哪个重复的记录，或者您是否想要删除所有重复的记录。在这种情况下，我们希望至少保留一个重复的行。这里，我们希望保留第一个事件:

```
df.drop_duplicates(keep='first')
```

您应该得到以下输出:

![Figure 11.7: Dropping duplicate rows with keep=’first’
](image/C15019_11_07.jpg)

图 11.7:删除 keep='first '的重复行

此方法的输出是一个新的数据帧，其中包含唯一的记录，只保留第一次出现的重复项。如果你想替换现有的数据帧而不是获得一个新的数据帧，你需要使用`inplace=True`参数。

`drop_duplicates()`和`duplicated()`方法还有另一个非常有用的参数:`subset`。此参数允许您指定查找重复项时要考虑的列列表。默认情况下，DataFrame 的所有列都用于查找重复行。当只查看`InvoiceNo`、`StockCode`、`invoiceDate`和`CustomerID`列时，让我们看看有多少重复行:

```
df.duplicated(subset=['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID'], keep='first').sum()
```

您应该得到以下输出:

```
10677
```

通过只查看这四列而不是全部，我们可以看到重复行的数量从`5268`增加到了`10677`。这意味着有些行与这四列具有完全相同的值，但在其他列中具有不同的值，这意味着它们可能是不同的记录。在这种情况下，最好使用所有列来标识重复记录。

## 练习 e 11.01:处理乳腺癌数据集中的重复数据

在本练习中，您将学习如何识别重复记录，以及如何处理此类问题，以便数据集只包含唯一记录。让我们开始吧:

注意

我们在这个练习中使用的数据集是乳腺癌检测数据集，它由威斯康星大学医院的 William H. Wolberg 博士共享，由 UCI 机器学习知识库托管。这个数据集的属性信息可以在这里找到:[https://packt.live/39LaIDx](https://packt.live/39LaIDx)。

这个数据集也可以在本书的 GitHub 知识库中找到:[https://packt.live/2QqbHBC](https://packt.live/2QqbHBC)。

1.  打开一个新的 Colab 笔记本。
2.  导入`pandas`包:

    ```
    import pandas as pd
    ```

3.  将指向`Breast Cancer`数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter11/dataset/breast-cancer-wisconsin.data'
    ```

    的变量
4.  使用`pandas`包中的`read_csv()`方法，用`header=None`参数将数据集加载到一个名为`df`的新变量中。我们这样做是因为这个文件不包含列名:

    ```
    df = pd.read_csv(file_url, header=None)
    ```

5.  Create a variable called `col_names` that contains the names of the columns: `Sample code number, Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses`, and `Class`:

    注意

    关于这个文件中指定的名字的信息可以在这里找到:[https://packt.live/39J7hgT](https://packt.live/39J7hgT)。

    ```
    col_names = ['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size',
    'Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class'] 
    ```

6.  使用`columns`属性:

    ```
    df.columns = col_names
    ```

    指定数据帧的列名
7.  Display the shape of the DataFrame using the `.shape` attribute:

    ```
    df.shape
    ```

    您应该得到以下输出:

    ```
    (699, 11)
    ```

    该数据帧包含`699`行和`11`列。

8.  Display the first five rows of the DataFrame using the `head()` method:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 11.8: The first five rows of the Breast Cancer dataset
    ](image/C15019_11_08.jpg)

    图 11.8:乳腺癌数据集的前五行

    所有的变量都是数字。样本代码编号栏是测量样本的标识符。

9.  Find the number of duplicate rows using the `duplicated()` and `sum()` methods:

    ```
    df.duplicated().sum()
    ```

    您应该得到以下输出:

    ```
    8
    ```

    查看这个数据集中的 11 列，我们可以看到有`8`个重复的行。

10.  Display the duplicate rows using the `loc()` and `duplicated()` methods:

    ```
    df.loc[df.duplicated()]
    ```

    您应该得到以下输出:

    ![Figure 11.9: Duplicate records
 
    ](image/C15019_11_09.jpg)

    图 11.9:重复记录

    以下各行重复:`208`、`253`、`254`、`258`、`272`、`338`、`561`和`684`。

11.  Display the duplicate rows just like we did in *Step 9*, but with the `keep='last'` parameter instead:

    ```
    df.loc[df.duplicated(keep='last')]
    ```

    您应该得到以下输出:

    ![Figure 11.10: Duplicate records with keep=’last’
    ](image/C15019_11_10.jpg)

    图 11.10:keep = ' last '的重复记录

    通过使用`keep='last'`参数，以下行被视为重复:`42`、`62`、`168`、`207`、`267`、`314`、`560`和`683`。通过将该输出与上一步的输出进行比较，我们可以看到第 253 行和第 42 行是相同的。

12.  使用`drop_duplicates()`方法和`keep='first'`参数删除重复的行，并将其保存到一个名为`df_unique` :

    ```
    df_unique = df.drop_duplicates(keep='first')
    ```

    的新数据帧中
13.  Display the shape of `df_unique` with the `.shape` attribute:

    ```
    df_unique.shape
    ```

    您应该得到以下输出:

    ```
    (691, 11)
    ```

    现在我们已经删除了八个重复的记录，只剩下`691`行。现在，数据集只包含唯一的观察值。

在本练习中，您学习了如何识别和删除真实数据集中的重复记录。

# 转换数据类型

您在项目中可能面临的另一个问题是为某些列推断出不正确的数据类型。正如我们在*第 10 章*、*分析数据集*中看到的，`pandas`包为我们提供了一种非常简单的方法，使用`.dtypes`属性显示每一列的数据类型。你可能想知道，`pandas`是什么时候确定每个栏目的类型的？当您使用`read_csv()`、`read_excel()`等方法将数据集加载到`pandas`数据帧中时，会检测到这些类型。

完成后，`pandas`会尽力根据每一列中包含的值自动找到最佳类型。让我们看看这在`Online Retail`数据集上是如何工作的。

首先，您必须导入`pandas`:

```
import pandas as pd
```

然后，您需要将数据集的 URL 赋给一个新变量:

```
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
```

让我们使用`read_excel()`将数据集加载到`pandas`数据帧中:

```
df = pd.read_excel(file_url)
```

最后，让我们打印每一列的数据类型:

```
df.dtypes
```

您应该得到以下输出:

![Figure 11.11: The data type of each column of the Online Retail dataset
](image/C15019_11_11.jpg)

图 11.11:在线零售数据集每一列的数据类型

前面的输出显示了分配给每一列的数据类型。`Quantity`、`UnitPrice`和`CustomerID`已经被标识为数字变量(`int64`、`float64`)，`InvoiceDate`是一个`datetime`变量，所有其他列都被认为是文本(`object`)。这还不算太糟。`pandas`在识别非文本列方面做得很好。

但是如果你想改变一些列的类型呢？你有两种方法来实现这一点。

第一种方法是重新加载数据集，但是这一次，您将需要使用`dtype`参数指定感兴趣的列的数据类型。此参数接受一个字典作为输入，该字典以列名作为键，以正确的数据类型作为值，例如{'col1': np.float64，' col2': np.int32}。让我们在`CustomerID`上试试这个。我们知道这不是一个数字变量，因为它包含一个唯一的标识符(代码)。这里，我们将把它的类型改为 object:

```
df = pd.read_excel(file_url, dtype={'CustomerID': 'category'})
df.dtypes
```

您应该得到以下输出:

![Figure 11.13: The data types of each column after converting InvoiceNo
](image/C15019_11_12.jpg)

图 11.12:转换 CustomerID 后每一列的数据类型

如您所见，`CustomerID`的数据类型实际上已经变成了`category`类型。

现在，让我们看看将单个列转换为不同类型的第二种方法。在`pandas`中，您可以使用`astype()`方法并指定它将转换成的新数据类型作为它的参数。它将返回一个新列(更准确地说，是一个新的`pandas`系列)，所以您需要将它重新分配给 DataFrame 的同一个列。例如，如果您想将`InvoiceNo`列更改为分类变量，您应该执行以下操作:

```
df['InvoiceNo'] = df['InvoiceNo'].astype('category')
df.dtypes
```

您应该得到以下输出:

![Figure 11.13: The data types of each column after converting InvoiceNo
](image/C15019_11_13.jpg)

图 11.13:转换发票号后各列的数据类型

如您所见，`InvoiceNo`的数据类型已经变成了分类变量。`object`和`category`的区别在于，后者有有限个可能值(也叫离散变量)。一旦这些被改变成分类变量，`pandas`将自动列出所有的值。可以使用`.cat.categories`属性访问它们:

```
df['InvoiceNo'].cat.categories
```

您应该得到以下输出:

![Figure 11.14: List of categories (possible values) for the InvoiceNo categorical variable
](image/C15019_11_14.jpg)

图 11.14:invoice no 分类变量的类别列表(可能值)

`pandas`已确定该列中有 25，900 个不同的值，并已将它们全部列出。根据分配给变量的数据类型，`pandas`提供不同的属性和方法，这些属性和方法对于数据转换或特征工程来说非常方便(这将在*第 12 章*、*特征工程*中讨论)。

最后一点，您可能想知道何时使用第一种方法来更改某些列的类型(在加载数据集时)。要找出每个变量的当前类型，必须首先加载数据，那么为什么需要用新的数据类型重新加载数据呢？第一次加载后，用`astype()`方法更改类型会更容易。你使用它有几个原因。一个原因可能是您已经在不同的工具(如 Excel)上浏览过数据集，并且已经知道正确的数据类型。

第二个原因可能是您的数据集很大，您无法完整地加载它。您可能已经注意到，默认情况下，`pandas`对数值变量使用 64 位编码。这需要很大的内存，可能会大材小用。

例如，`Quantity`列具有 int64 数据类型，这意味着可能值的范围是-9，223，372，036，854，775，808 到 9，223，372，036，854，775，807。然而，在第 10 章、*分析数据集*中，在分析该列的分布时，您了解到该列的值的范围仅为-80，995 到 80，995。你不需要用这么多空间。通过将此变量的数据类型减少到 int32(范围从-2，147，483，648 到 2，147，483，647)，您可能能够重新加载整个数据集。

## 练习 11.02:转换 Ames 住房数据集的数据类型

在本练习中，您将通过将数据集的变量转换为正确的数据类型来准备数据集。

您将使用 Ames Housing 数据集来完成这项工作，我们在第 10 章、*分析数据集*中也使用了该数据集。有关此数据集的更多信息，请参考以下注释。让我们开始吧:

注意

在这个练习中使用的数据集是艾姆斯住宅数据集，它是由迪安·德·科克:[https://packt.live/2QTbTbq](https://packt.live/2QTbTbq)编辑的。

为了您的方便，这个数据集已经上传到本书的 GitHub 知识库:[https://packt.live/2ZUk4bz](https://packt.live/2ZUk4bz)。

1.  打开新的 Col ab 笔记本。
2.  导入`pandas`包:

    ```
    import pandas as pd
    ```

3.  将 Ames 数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter10/dataset/ames_iowa_housing.csv'
    ```

    的变量
4.  使用`pandas`包中的`read_csv`方法，将数据集加载到名为`df` :

    ```
    df = pd.read_csv(file_url)
    ```

    的新变量中
5.  Print the data type of each column using the `dtypes` attribute:

    ```
    df.dtypes
    ```

    您应该得到以下输出:

    ![Figure 11.15: List of columns and their assigned data types
    ](image/C15019_11_15.jpg)

    图 11.15:列及其分配的数据类型的列表

    注意

    前面的输出已被截断。

    从*第 10 章*、*分析一个数据集*你知道`Id`、`MSSubClass`、`OverallQual`和`OverallCond`列被错误地归类为数字变量。它们有有限数量的唯一值，您不能对它们执行任何数学运算。例如，将`Id`列中的两个不同值相加、相除、相乘或相除是没有意义的。因此，您需要将它们转换成分类变量。

6.  使用`astype()`方法，将`'Id'`列转换成分类变量，如下面的代码片段所示:

    ```
    df['Id'] = df['Id'].astype('category')
    ```

7.  将`'MSSubClass'`、`'OverallQual'`和`'OverallCond'`列转换成分类变量，就像我们在上一步中所做的:

    ```
    df['MSSubClass'] = df['MSSubClass'].astype('category')
    df['OverallQual'] = df['OverallQual'].astype('category')
    df['OverallCond'] = df['OverallCond'].astype('category')
    ```

8.  Create a for loop that will iterate through the four categorical columns `('Id', 'MSSubClass', 'OverallQual',` and `'OverallCond'`) and print their names and categories using the `.cat.categories` attribute:

    ```
    for col_name in ['Id', 'MSSubClass', 'OverallQual', 'OverallCond']:
      print(col_name)
      print(df[col_name].cat.categories)
    ```

    您应该得到以下输出:

    ![Figure 11.16: List of categories for the four newly converted variables
    ](image/C15019_11_16.jpg)

    图 11.16:四个新转换变量的类别列表

    现在，这四列已经被转换成分类变量。从*步骤 5* 的输出中，我们可以看到有很多`object`类型的变量。让我们看看它们，看看它们是否也需要转换。

9.  使用`select_dtypes`方法和`include='object'`参数:

    ```
    obj_df = df.select_dtypes(include='object')
    ```

    创建一个名为`obj_df`的新数据框架，它将只包含`object`类型的变量
10.  Create a new variable called `obj_cols` that contains a list of column names from the `obj_df` DataFrame using the `.columns` attribute and display its content:

    ```
    obj_cols = obj_df.columns
    obj_cols
    ```

    您应该得到以下输出:

    ![Figure 11.17: List of variables of the ‘object’ type
    ](image/C15019_11_17.jpg)

    图 11.17:对象类型的变量列表

11.  Like we did in *Step 8*, create a `for` loop that will iterate through the column names contained in `obj_cols` and print their names and unique values using the `unique()` method:

    ```
    for col_name in obj_cols:
      print(col_name)
      print(df[col_name].unique())
    ```

    您应该得到以下输出:

    ![Figure 11.18: List of unique values for each variable of the ‘object’ type
    ](image/C15019_11_18.jpg)

    图 11.18:“对象”类型的每个变量的唯一值列表

    如您所见，所有这些列都有有限数量的由文本组成的唯一值，这表明它们是分类变量。

12.  现在，创建一个`for`循环，该循环将遍历`obj_cols`中包含的列名，并使用`astype()`方法将每个列名转换为分类变量:

    ```
    for col_name in obj_cols:
      df[col_name] = df[col_name].astype('category')
    ```

13.  Print the data type of each column using the `dtypes` attribute:

    ```
    df.dtypes
    ```

    您应该得到以下输出:

![Figure 11.19: List of variables and their new data types
](image/C15019_11_19.jpg)

图 11.19:变量及其新数据类型的列表

注意

前面的输出已被截断。

您已成功地将具有不正确数据类型(数值或对象)的列转换为分类变量。现在，您的数据集离建模又近了一步。

在下一节中，我们将研究如何处理不正确的值。

# 处理不正确的值

新数据集可能面临的另一个问题是数据集中某些观察值不正确。有时，这是由于语法错误；例如，一个国家的名称可以全部用小写字母，全部用大写字母，作为标题(只有第一个字母大写)，甚至可以缩写。法国可能取不同的值，如' FRANCE '，' France '，' France '，' FR '等等。如果您将“法国”定义为标准格式，则所有其他变量都被视为数据集中的错误值，需要进行修正。

如果这种问题没有在建模阶段之前得到处理，可能会导致不正确的结果。该模型将认为这些不同的变量是完全不同的值，并且可能不太注意这些值，因为它们具有分离的频率。例如，假设“法国”代表 2%的值，“法国”代表 2%，而“法国”代表 1%。你知道这些值对应的是同一个国家，应该代表 5%的值，但是模型会认为是不同的国家。

让我们通过使用`Online Retail`数据集来学习如何检测现实生活中的此类问题。

首先，您需要将数据加载到一个`pandas`数据帧中:

```
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

在这个数据集中，有两个变量似乎是相互关联的:`StockCode`和`Description`。第一个包含售出商品的识别码，另一个包含商品的描述。然而，如果你看一些例子，比如`StockCode 23131`，`Description`列有不同的值:

```
df.loc[df['StockCode'] == 23131, 'Description'].unique()
```

您应该得到以下输出

![Figure 11.20: List of unique values for the Description column and StockCode 23131
](image/C15019_11_20.jpg)

图 11.20:描述列和股票代码 23131 的唯一值列表

在前面的输出中有多个问题。一个问题是单词`Mistletoe`被拼错了，读作`Miseltoe`。其他错误是意外值和缺失值，这将在下一节中讨论。看来`Description`栏目已经被用来记录`had been put aside`等评论了。

让我们关注拼写错误的问题。这里我们需要做的是修改不正确的拼写并用正确的值替换它。首先，让我们创建一个名为`StockCodeDescription`的新列，它是`Description`列的精确副本:

```
df['StockCodeDescription'] = df['Description']
```

您将使用这个新专栏来解决拼写错误的问题。要做到这一点，请使用本章前面所学的子集设置技术。你需要使用`.loc`并过滤你想要的行和列，即所有带有`StockCode == 21131`和`Description == MISELTOE HEART WREATH CREAM`的行和`Description`列:

```
df.loc[(df['StockCode'] == 23131) & (df['StockCodeDescription'] == 'MISELTOE HEART WREATH CREAM'), 'StockCodeDescription'] = 'MISTLETOE HEART WREATH CREAM'
```

如果您重新打印这一期的值，您将看到拼写错误的值已被修复，不再存在:

```
df.loc[df['StockCode'] == 23131, 'StockCodeDescription'].unique()
```

您应该得到以下输出:

![Figure 11.21: List of unique values for the Description column and StockCode 23131 after fixing the first misspelling issue
](image/C15019_11_21.jpg)

图 11.21:修复第一个拼写错误问题后，描述列和股票代码 23131 的唯一值列表

正如你所看到的，这个产品仍然有五个不同的值，但是对于其中的一个，也就是`MISTLETOE`，已经拼错了:`MISELTOE`。

这一次，我们不看完全匹配(一个单词必须与另一个单词相同)，而是看执行部分匹配(一个单词的一部分将出现在另一个单词中)。在我们的例子中，不看`MISELTOE`的拼写，我们只看`MISEL`。`pandas`包提供了一个叫做`.str.contains()`的方法，我们可以用它来寻找与给定表达式部分匹配的观察值。

让我们用这个来看看在整个数据集中是否有相同的拼写错误(`MISEL`)。您将需要添加一个额外的参数，因为这个方法不处理丢失的值。您还必须对那些没有缺失值的行进行子集划分，以用于`Description`列。这可以通过向`.str.contains()`方法提供`na=False`参数来实现:

```
df.loc[df['StockCodeDescription'].str.contains('MISEL', na=False),]
```

您应该得到以下输出:

![Figure 11.22: Displaying all the rows containing the misspelling ‘MISELTOE’
](image/C15019_11_22.jpg)

图 11.22:显示所有包含拼写错误“MISELTOE”的行

这个拼错的问题(`MISELTOE`)不仅和`StockCode 23131`有关，还和其他项目有关。您将需要使用`str.replace()`方法来修复所有这些问题。此方法将待替换的字符串和替换字符串作为参数:

```
df['StockCodeDescription'] = df['StockCodeDescription'].str.replace('MISELTOE', 'MISTLETOE')
```

现在，如果您打印包含拼错的`MISEL`的所有行，您将看到这样的行不再存在:

```
df.loc[df['StockCodeDescription'].str.contains('MISEL', na=False),]
```

您应该得到以下输出

![Figure 11.23: Displaying all the rows containing the misspelling MISELTOE after cleaning up
](image/C15019_11_23.jpg)

图 11.23:清除后显示所有包含拼写错误 MISELTOE 的行

您刚刚看到了使用`pandas`包提供的`.str.contains`和`.str.replace()`方法来清除具有不正确值的观察值是多么容易。这些方法只能用于包含字符串的变量，但同样的逻辑可以应用于数值变量，也可以用于处理极值或异常值。您可以使用==、>、<、> =或< =操作符对您想要的行进行子集化，然后用正确的值替换观察值。

## 练习 11.03:修复州列中的错误值

在本练习中，您将通过列出美国的所有财务人员来清除数据集修改版本中的`State`变量。我们这样做是因为数据集包含一些不正确的值。让我们开始吧:

注意

原始数据集由福里斯特·格雷格和德里克·埃德尔共享，可以在 https://packt.live/2rTJVns 找到。

我们在这里使用的修改过的数据集可以在这本书的 GitHub 知识库中找到:[https://packt.live/2MZJsrk](https://packt.live/2MZJsrk)。

1.  打开一个新的记事本 ok。
2.  导入`pandas`包:

    ```
    import pandas as pd
    ```

3.  将数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter11/dataset/officers.csv'
    ```

    的变量
4.  使用`pandas`包中的`read_csv()`方法，将数据集加载到名为`df` :

    ```
    df = pd.read_csv(file_url)
    ```

    的新变量中
5.  Print the first five rows of the DataFrame using the `.head()` method:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 11.24: The first five rows of the finance officers dataset
    ](image/C15019_11_24.jpg)

    图 11.24:财务官员数据集的前五行

6.  Print out all the unique values of the `State` variable:

    ```
    df['State'].unique()
    ```

    您应该得到以下输出:

    ![Figure 11.25: List of unique values in the State column
    ](image/C15019_11_25.jpg)

    图 11.25:州列中唯一值的列表

    所有的状态都被编码成两个大写的字符格式。如您所见，有一些非大写字符的不正确值，如`il`和`iL`(它们看起来像伊利诺伊州的拼写错误)，以及意外值，如`8I`、`I`和`60`。在接下来的几个步骤中，您将修复这些问题。

7.  Print out the rows that have the `il` value in the `State` column using the `pandas` `.str.contains()` method and the subsetting API, that is, DataFrame [condition]. You will also have to set the `na` parameter to `False` in `str.contains()` in order to exclude observations with missing values:

    ```
    df[df['State'].str.contains('il', na=False)]
    ```

    您应该得到以下输出:

    ![Figure 11.26: Observations with a value of il
    ](image/C15019_11_26.jpg)

    图 11.26:il 值的观察值

    如您所见，所有带有`il`值的城市都来自伊利诺伊州。所以，正确的`State`值应该是`IL`。您可能认为下面的值也是指伊利诺伊州:`Il`、`iL`和`Il`。我们接下来会看看它们。

8.  Now, create a `for` loop that will iterate through the following values in the `State` column: `Il`, `iL`, `Il`. Then, print out the values of the City and State variables using the `pandas` method for subsetting, that is, `.loc()`: DataFrame.loc[row_condition, column condition]. Do this for each observation:

    ```
    for state in ['Il', 'iL', 'Il']:
      print(df.loc[df['State'] == state, ['City', 'State']])
    ```

    您应该得到以下输出:

    ![Figure 11.27: Observations with the il value
    ](image/C15019_11_27.jpg)

    图 11.27:il 值的观察值

    注意

    前面的输出已被截断。

    如你所见，所有这些城市都属于伊利诺伊州。让我们用正确的值替换它们。

9.  通过使用`isin()`方法和这些值的列表作为参数，创建一个条件掩码(`il_mask`)来对包含四个错误值(`il`、`Il`、`iL`和`Il`)的所有行进行子集划分。然后，将结果保存到一个名为`il_mask`:T7 的变量中
10.  Print the number of rows that match the condition we set in `il_mask` using the `.sum()` method. This will sum all the rows that have a value of `True` (they match the condition):

    ```
    il_mask.sum()
    ```

    您应该得到以下输出:

    ```
    672
    ```

11.  使用`pandas` `.loc()`方法，用`il_mask`条件掩码对行进行子集化，并用`IL` :

    ```
    df.loc[il_mask, 'State'] = 'IL'
    ```

    替换`State`列的值
12.  Print out all the unique values of the `State` variable once more:

    ```
    df['State'].unique()
    ```

    您应该得到以下输出:

    ![Figure 11.28: List of unique values for the ‘State’ column
    ](image/C15019_11_28.jpg)

    图 11.28:“州”列的唯一值列表

    如您所见，这四个不正确的值不再存在了。让我们看看剩下的其他错误值:`II`、`I`、`8I`和`60`。我们将在下一步考虑如何处理`II`。

    使用`pandas`子集化 API，即 DataFrame.loc[row_condition，column_condition]，将值为`II`的行打印到`State`列中:

    ```
    df.loc[df['State'] == 'II',]
    ```

    您应该得到以下输出:

    ![Figure 11.29: Subsetting the rows with a value of IL in the State column
    ](image/C15019_11_29.jpg)

    图 11.29:在 State 列中设置 IL 值的行子集

    只有两种情况下，`II`值被用于`State`列，并且都将`Bloomington`作为城市，位于伊利诺斯州。这里，正确的`State`值应该是`IL`。

13.  Now, create a `for` loop that iterates through the three incorrect values (`I`, `8I`, and `60`) and print out the subsetted rows using the same logic that we used in *Step 12*. Only display the `City` and `State` columns:

    ```
    for val in ['I', '8I', '60']:
      print(df.loc[df['State'] == val, ['City', 'State']])
    ```

    您应该得到以下输出:

    ![Figure 11.30: Observations with incorrect values (I, 8I, and 60)
    ](image/C15019_11_30.jpg)

    图 11.30:带有错误值的观察值(I、8I 和 60)

    所有值不正确的观察值都是伊利诺斯州的城市。让我们现在修理他们。

14.  创建一个`for`循环，遍历四个不正确的值(`II`、`I`、`8I`和`60`，并重用*步骤 12* 中的子集逻辑，用`IL` :

    ```
    for val in ['II', 'I', '8I', '60']:
      df.loc[df['State'] == val, 'State'] = 'IL'
    ```

    替换`State`中的值
15.  Print out all the unique values of the `State` variable:

    ```
    df['State'].unique()
    ```

    您应该得到以下输出:

    ![Figure 11.31: List of unique values for the State column
    ](image/C15019_11_31.jpg)

    图 11.31:State 列的唯一值列表

    你解决了伊利诺伊州的问题。但是，这一列还有两个不正确的值:`In`和`ng`。

16.  Repeat *Step 13*, but iterate through the `In` and `ng` values instead:

    ```
    for val in ['In', 'ng']:
      print(df.loc[df['State'] == val, ['City', 'State']])
    ```

    您应该得到以下输出:

    ![Figure 11.32: Observations with incorrect values (In, ng)
    ](image/C15019_11_32.jpg)

    图 11.32:带有错误值的观察值(英寸，毫微克)

    在`State`中具有`ng`值的行是缺失值。我们将在下一节讨论这个主题。将`In`作为其`State`的观测值是印第安纳州的一个城市，因此正确的值应该是`IN`。让我们解决这个问题。

17.  Subset the rows containing the `In` value in `State` using the `.loc()` and `.str.contains()` methods and replace the state value with `IN`. Don't forget to specify the `na=False` parameter as `.str.contains()`:

    ```
    df.loc[df['State'].str.contains('In', na=False), 'State'] = 'IN'
    ```

    打印出`State`变量的所有唯一值:

    ```
    df['State'].unique()
    ```

    您应该得到以下输出:

![Figure 11.33: List of unique values for the State column
](image/C15019_11_33.jpg)

图 11.33:State 列的唯一值列表

您刚刚使用`pandas`包提供的方法修复了`State`变量的所有错误值。在下一节中，我们将研究如何处理缺失值。

# 处理缺失值

到目前为止，您已经了解了数据集的各种问题。现在是时候讨论另一个经常出现的问题了:缺少值。您可能已经猜到，这种类型的问题意味着某些变量缺少某些值。

`pandas`包提供了一种方法，我们可以用它来识别数据帧中缺失的值:`.isna()`。让我们看看它在`Online Retail`数据集上的运行情况。首先，您需要导入`pandas`并将数据加载到 DataFrame 中:

```
import pandas as pd
file_url = 'https://github.com/PacktWorkshops/The-Data-Science-Workshop/blob/master/Chapter10/dataset/Online%20Retail.xlsx?raw=true'
df = pd.read_excel(file_url)
```

`.isna()`方法为 DataFrame 的每个单元格返回一个带有二进制值的`pandas`序列，并声明它是否缺少一个值(`True`)或(`False`):

```
df.isna()
```

您应该得到以下输出:

![Figure 11.34: Output of the .isna() method
](image/C15019_11_34.jpg)

图 11.34:的输出。isna()方法

正如我们之前看到的，我们可以向`.sum()`方法提供一个二进制变量的输出，该方法将所有的`True`值加在一起(有缺失值的单元格),并为每一列提供一个摘要:

```
df.isna().sum()
```

您应该得到以下输出:

![Figure 11.35: Summary of missing values for each variable
](image/C15019_11_35.jpg)

图 11.35:每个变量缺失值的汇总

如您所见，在`Description`列中有`1454`个缺失值，在`CustomerID`列中有`135080`个缺失值。让我们来看看`Description`的缺失值观测值。您可以使用`.isna()`方法的输出对缺少值的行进行子集划分:

```
df[df['Description'].isna()]
```

您应该得到以下输出:

![Figure 11.36: Subsetting the rows with missing values for Description
](image/C15019_11_36.jpg)

图 11.36:为缺少描述值的行设置子集

从前面的输出中，您可以看到所有缺少值的行都将`0.0`作为单价，并且缺少`CustomerID`列。在一个真实的项目中，你将不得不与企业讨论这些案例，并检查这些交易是否真实。如果企业确认这些观察是不相关的，那么您将需要从数据集中删除它们。

`pandas`包提供了一种方法，我们可以用它来轻松地删除丢失的值:`.dropna()`。该方法返回一个新的 DataFrame，但不包含所有缺失值的行。默认情况下，它会查看所有列。您可以用`subset`参数指定它要查找的列列表:

```
df.dropna(subset=['Description'])
```

此方法返回一个新的 DataFrame，其中指定的列没有缺失值。如果想直接替换原始数据集，可以使用`inplace=True`参数:

```
df.dropna(subset=['Description'], inplace=True)
```

现在，查看每个变量缺失值的汇总:

```
df.isna().sum()
```

您应该得到以下输出:

![Figure 11.37: Summary of missing values for each variable
](image/C15019_11_37.jpg)

图 11.37:每个变量缺失值的汇总

如您所见，在`Description`列中不再有丢失的值。让我们看看`CustomerID`栏:

```
df[df['CustomerID'].isna()]
```

您应该得到以下输出:

![Figure 11.38: Rows with missing values in CustomerID
](image/C15019_11_38.jpg)

图 11.38:CustomerID 中缺少值的行

这一次，所有的事务看起来都很正常，除了它们缺少`CustomerID`列的值；所有其他变量都被填入了看似真实的值。没有其他方法可以推断出`CustomerID`列缺少的值。这些行代表了几乎 25%的数据集，所以我们不能删除它们。

但是，大多数算法要求每个观察值都有一个值，因此您需要为这些情况提供一个值。我们将使用来自`pandas`的`.fillna()`方法来做这件事。提供要估算的值为`Missing`，并使用`inplace=True`作为参数:

```
df['CustomerID'].fillna('Missing', inplace=True)
df[1443:1448]
```

您应该得到以下输出:

![Figure 11.39: Examples of rows where missing values for CustomerID have been replaced with Missing
](image/C15019_11_39.jpg)

图 11.39:CustomerID 的缺失值被替换为缺失值的行的示例

让我们看看数据集中是否有任何缺失值:

```
df.isna().sum()
```

您应该得到以下输出:

![Figure 11.40: Summary of missing values for each variable
](image/C15019_11_40.jpg)

图 11.40:每个变量的缺失值汇总

您已成功修复此数据集中所有缺失的值。当我们想要处理缺失的数值变量时，这些方法也是有效的。我们将在下面的练习中研究这一点。当你想用`.fillna()`估算一个值时，你需要做的就是提供一个数值。

## 练习 11.04:修复马疝气数据集的缺失值

在本练习中，您将清除`Horse Colic`数据集中所有数值变量的所有缺失值。

绞痛是马可能遭受的一种痛苦的状况，该数据集包含与这种状况的特定情况相关的各种信息。如果想了解数据集属性的更多信息，可以使用注释部分提供的链接。让我们开始吧:

注意

这个数据集来自 UCI 机器学习知识库。属性信息可以在[https://packt.live/2MZwSrW](https://packt.live/2MZwSrW)找到。

为了方便起见，我们将在本练习中使用的数据集文件已经上传到本书的 GitHub 资源库:[https://packt.live/35qESZq](https://packt.live/35qESZq)。

1.  打开新的 Colab 笔记本。
2.  导入`pandas`包:

    ```
    import pandas as pd
    ```

3.  将数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'http://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter11/dataset/horse-colic.data'
    ```

    的变量
4.  使用`pandas`包中的`.read_csv()`方法，将数据集加载到一个名为`df`的新变量中，并指定`header=None`、 `sep='\s+'`和 `prefix='X'`参数:

    ```
    df = pd.read_csv(file_url, header=None, sep='\s+', prefix='X')
    ```

5.  Print the first five rows of the DataFrame using the `.head()` method:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 11.41: The first five rows of the Horse Colic dataset
    ](image/C15019_11_41.jpg)

    图 11.41:马疝气数据集的前五行

    如您所见，作者使用了`?`字符来表示缺失值，但是`pandas`包认为这是一个正常的值。您需要将它们转换成缺失值。

6.  使用`.read_csv()`方法将数据集重新加载到一个`pandas`数据帧中，但是这一次，添加`na_values='?'`参数，以指定该值需要被视为缺失值:

    ```
    df = pd.read_csv(file_url, header=None, sep='\s+', prefix='X', na_values='?')
    ```

7.  Print the first five rows of the DataFrame using the `.head()` method:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 11.42: The first five rows of the Horse Colic dataset
    ](image/C15019_11_42.jpg)

    图 11.42:马疝气数据集的前五行

    现在，您可以看到`pandas`已经将所有的`?`值转换为缺失值。

8.  Print the data type of each column using the `dtypes` attribute:

    ```
    df.dtypes
    ```

    您应该得到以下输出:

    ![Figure 11.43: Data type of each column
    ](image/C15019_11_43.jpg)

    图 11.43:每一列的数据类型

9.  Print the number of missing values for each column by combining the `.isna()` and `.sum()` methods:

    ```
    df.isna().sum()
    ```

    您应该得到以下输出:

    ![Figure 11.44: Number of missing values for each column
    ](image/C15019_11_44.jpg)

    图 11.44:每列缺失值的数量

10.  创建一个名为`x0_mask`的条件掩码，这样就可以使用`.isna()`方法:

    ```
    x0_mask = df['X0'].isna()
    ```

    找到`X0`列中缺少的值
11.  Display the number of missing values for this column by using the `.sum()` method on `x0_mask`:

    ```
    x0_mask.sum()
    ```

    您应该得到以下输出:

    ```
    1
    ```

    在这里，你得到的`X0`的缺失值的数量与你在*步骤 9* 中得到的数量完全相同。

12.  Extract the mean of `X0` using the `.median()` method and store it in a new variable called `x0_median`. Print its value:

    ```
    x0_median = df['X0'].median()
    print(x0_median)
    ```

    您应该得到以下输出:

    ```
    1.0
    ```

    该列的中间值是`1`。您将用这个值替换`X0`列中所有缺少的值。

13.  使用`.fillna()`方法，连同`inplace=True`参数:

    ```
    df['X0'].fillna(x0_median, inplace=True)
    ```

    ，用它们的中值替换`X0`变量中所有缺失的值
14.  Print the number of missing values for `X0` by combining the `.isna()` and `.sum()` methods:

    ```
    df['X0'].isna().sum()
    ```

    您应该得到以下输出:

    ```
    0
    ```

    变量中不再有丢失的值。

15.  Create a `for` loop that will iterate through all the columns of the DataFrame. In the for loop, calculate the median for each and save them into a variable called `col_median`. Then, impute missing values with this median value using the `.fillna()` method, along with the `inplace=True` parameter, and print the name of the column and its median value:

    ```
    for col_name in df.columns:
      col_median = df[col_name].median()
      df[col_name].fillna(col_median, inplace=True)
      print(col_name)
      print(col_median)
    ```

    您应该得到以下输出:

    ![Figure 11.45: Median values for each column
    ](image/C15019_11_45.jpg)

    图 11.45:每列的中值

16.  Print the number of missing values for each column by combining the `.isna()` and `.sum()` methods:

    ```
    df.isna().sum()
    ```

    您应该得到以下输出:

![Figure 11.46: Number of missing values for each column
](image/C15019_11_46.jpg)

图 11.46:每列缺失值的数量

您已经使用`pandas`包:`.isna()`和`.fillna()`提供的方法成功地修复了所有数字变量的缺失值。

## 活动 11.01:准备速配数据集

作为一名企业家，你正计划向市场推出一款新的约会应用。将你的应用与其他竞争对手区分开来的关键特性将是你的高性能用户匹配算法。在建立这个模型之前，你已经与一家速配公司合作，从真实事件中收集数据。您刚刚收到来自合作公司的数据集，但意识到它不如您预期的那样干净；有缺失和不正确的值。您的任务是修复该数据集中的主要数据质量问题。

以下步骤将帮助您完成此活动:

1.  使用`.read_csv()`将数据集下载并加载到 Python 中。
2.  使用`.shape`打印出数据框的尺寸。
3.  通过在所有列上使用`.duplicated()`和`.sum()`来检查重复行。
4.  通过对标识符列使用`.duplicated()` 和`.sum()`(`iid`、`id`、`partner`和`pid`)来检查重复行。
5.  检查以下数值变量的意外值:`'imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping',`和`'yoga'`。
6.  更换识别出的错误值。
7.  使用`.dtypes`检查不同列的数据类型。
8.  对于不包含数值的列，使用`.astype()`将数据类型更改为分类。
9.  使用`.isna()`和`.sum()`检查每个数值变量是否有缺失值。
10.  Replace the missing values for each numerical variable with their corresponding mean or median values using `.fillna()`, `.mean()`, and `.median()`.

    注意

    这个活动的数据集可以在本书的 GitHub 知识库中找到:[https://packt.live/36u0jtR](https://packt.live/36u0jtR)。

    哥伦比亚商学院的雷·菲斯曼和希娜·艾扬格分享了原始数据集:[https://packt.live/2Fp5rUg](https://packt.live/2Fp5rUg)。

    作者提供了一个非常有用的文档，描述了数据集及其特性:[https://packt.live/2Qrp7gD](https://packt.live/2Qrp7gD)。

您应该得到以下输出:

下图显示了`imprace`具有意外值的行数和意外值列表:

![Figure 11.47: Number of rows with unexpected values for ‘imprace’ and a list of unexpected values
](image/C15019_11_47.jpg)

图 11.47:包含“imprace”意外值的行数和意外值列表

下图说明了具有意外值的行数以及每列的意外值列表:

![Figure 11.48: Number of rows with unexpected values and a list of unexpected values for each column
](image/C15019_11_48.jpg)

图 11.48:具有意外值的行数和每列的意外值列表

下图显示了游戏的独特价值列表:

![Figure 11.49: List of unique values for gaming
](image/C15019_11_49.jpg)

图 11.49:游戏的唯一值列表

下图显示了每列的数据类型:

![Figure 11.50: Data types of each column
](image/C15019_11_50.jpg)

图 11.50:每一列的数据类型

下图显示了每列的更新数据类型:

![Figure 11.51: Data types of each column
](image/C15019_11_51.jpg)

图 11.51:每一列的数据类型

下图显示了数值变量缺失值的数量:

![Figure 11.52: Number of missing values for numerical variables
](image/C15019_11_52.jpg)

图 11.52:数字变量缺失值的数量

下图显示了`int_corr`的唯一值列表:

![Figure 11.53: List of unique values for ‘int_corr’
](image/C15019_11_53.jpg)

图 11.53:int _ corr 的唯一值列表

下图显示了数值变量的唯一值列表:

![Figure 11.54: List of unique values for numerical variables
](image/C15019_11_54.jpg)

图 11.54:数值变量的唯一值列表

下图显示了数值变量缺失值的数量:

![Figure 11.55: Number of missing values for numerical variables
](image/C15019_11_55.jpg)

图 11.55:数字变量缺失值的数量

注意

这个活动的解决方案可以在以下地址找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

# 总结

在本章中，您学习了准备任何给定的数据集并解决其主要质量问题的重要性。这一点至关重要，因为数据集越干净，任何机器学习模型就越容易了解相关模式。最重要的是，大多数算法不能处理诸如丢失值之类的问题，所以它们必须在建模阶段之前得到处理。在本章中，您讨论了数据科学项目中面临的最常见问题:重复行、不正确的数据类型、意外值和缺失值。

本章的目标是向您介绍一些概念，这些概念将帮助您发现其中的一些问题并轻松地解决它们，以便您拥有能够处理其他情况的基本工具包。最后一点，在本章中，我们强调了与我们合作的业务或数据工程团队讨论我们发现的问题的重要性。例如，如果您在数据集中检测到意外值，您可能希望在删除或替换它们之前，从业务角度确认它们没有任何特殊意义。

在修复问题时，您还需要非常小心:您不希望过多地修改数据集，这样会产生额外的意外模式。这就是为什么建议您用数值变量的平均值或中值替换任何缺失值的原因。否则，你将彻底改变它的分布。例如，如果一个变量的值在 0 到 10 之间，用-999 替换所有缺失的值将极大地改变它们的平均值和标准差。

在下一章，我们将讨论特征工程这个有趣的话题。