

# 8。超参数调谐

概述

在本章中，在演示任何高级 scikit-learn 实现之前，将首先将每个超参数调整策略分解为其关键步骤。这是为了确保您在跳到更自动化的方法之前完全理解每种策略背后的概念。

在本章结束时，你将能够通过对具有不同超参数的估计量的系统评估，找到预测性能的进一步改进。您将成功部署手动、网格和随机搜索策略来找到最佳超参数。您将能够参数化 k-最近邻(k-NN)、支持向量机(SVM)、岭回归和随机森林分类器来优化模型性能。

# 简介

在前几章中，我们讨论了几种方法来得到一个性能良好的模型。这些包括通过预处理、特征工程和缩放来转换数据，或者简单地从 scikit-learn 用户可用的大量可能的估计器中选择适当的估计器(算法)类型。

根据您最终选择的估计器，可能有一些设置可以调整，以提高整体预测性能。这些设置称为超参数，导出最佳超参数称为调整或优化。适当地调整超参数可以将性能提高到两位数的百分比，因此在任何建模练习中都值得一试。

本章将讨论超参数调整的概念，并给出一些简单的策略，你可以用它们来帮助你的估值器找到最佳的超参数。

在前面的章节中，我们已经看到了一些使用一系列估计量的练习，但是我们没有进行任何超参数调整。读完这一章后，我们建议你重温这些练习，应用所教授的技巧，看看你是否能提高结果。

# 什么是超参数？

超参数可以被认为是每个估计量的一组拨号盘和开关，用于改变估计量如何解释数据中的关系。

看看*图 8.1* :

![Figure 8.1: How hyperparameters work
](image/B15019_08_01.jpg)

图 8.1:超参数如何工作

如果您在上图中从左向右阅读，您可以看到在调整过程中，我们改变了超参数的值，这导致了估计量的改变。这进而导致模型性能的变化。我们的目标是找到导致最佳模型性能的超参数化。这将是最佳的超参数化。

估计量可以有不同数量和类型的超参数，这意味着有时您可能面临大量可能的超参数来选择估计量。

例如，scikit-learn 实现的 SVM 分类器(`sklearn.svm.SVC`)是一个具有多个可能的超参数化的估计器，我们将在本章的后面介绍它。我们将只测试其中的一小部分，即使用 2、3 或 4 次线性核或多项式核。

这些超参数中的一些在本质上是连续的，而另一些是离散的，并且连续超参数的存在意味着可能的超参数化的数量在理论上是无限的。当然，当涉及到生成具有良好预测性能的模型时，一些超参数化要比其他超参数化好得多，而作为数据科学家，您的工作就是找到它们。

在下一节中，我们将更详细地查看这些超参数的设置。但首先，一些术语的澄清。

## 超参数和统计模型参数之间的差异

在阅读数据科学，尤其是统计学领域的内容时，您会遇到诸如“模型参数”、“参数估计”和(非参数模型)等术语。这些术语与模型数学公式中的参数有关。最简单的例子是没有截距项的单变量线性模型，其形式如下:

![Figure 8.2: Equation for a single variable linear model
](image/B15019_08_02.jpg)

图 8.2:单变量线性模型的方程

在这里，𝛽是统计模型参数，如果选择这个公式，那么数据科学家的工作就是使用数据来估计它的值。这可以使用普通的最小二乘(OLS)回归模型来实现，也可以通过一种称为中值回归的方法来实现。

超参数是不同的，因为它们在数学形式之外。在这种情况下，超参数的一个例子是估计𝛽的方法(OLS 或中位数回归)。在某些情况下，超参数可以完全改变算法(即生成完全不同的数学形式)。在这一章中，你会看到这样的例子。

在下一节中，您将了解如何设置超参数。

## 设置 g 超参数

在*第 7 章*、*机器学习模型的一般化*中，您了解了用于分类的 k-NN 模型，并且您看到了改变 k(最近邻的数量)如何导致与类别标签预测相关的模型性能的变化。这里，k 是一个超参数，手动尝试 k 的不同值的行为是超参数调整的一种简单形式。

每次初始化 scikit-learn 估计器时，它都会根据您为其参数设置的值进行超参数化。如果没有指定值，那么估计器将采用默认的超参数化。如果您想了解如何为您的估计器设置超参数，以及您可以调整哪些超参数，只需打印`estimator.get_params()`方法的输出。

例如，假设我们在没有指定任何参数(空括号)的情况下初始化一个 k-NN 估计器。要查看默认的超参数化，我们可以运行:

```
from sklearn import neighbors
# initialize with default hyperparameters
knn = neighbors.KNeighborsClassifier()
# examine the defaults
print(knn.get_params())
```

您应该得到以下输出:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
```

所有超参数的字典现在被打印到屏幕上，显示它们的默认设置。注意`k`，我们最近邻居的数量，被设置为`5`。

要获得有关这些参数的含义、如何更改它们以及它们可能产生的影响的更多信息，您可以运行以下命令并查看有问题的估计器的帮助文件。

对于我们的 k-NN 估计量:

```
?knn
```

输出如下所示:

![Figure 8.3: Help file for the k-NN estimator
](image/B15019_08_03.jpg)

图 8.3:k-NN 估计器的帮助文件

如果您仔细查看帮助文件，您会在`String form`标题下看到估计器的默认超参数化，以及在`Parameters`标题下对每个超参数含义的解释。

回到我们的例子，如果我们想将超参数化从`k = 5`更改为`k = 15`，只需重新初始化估计器并将`n_neighbors`参数设置为`15`，这将覆盖默认值:

```
# initialize with k = 15 and all other hyperparameters as default 
knn = neighbors.KNeighborsClassifier(n_neighbors=15)
# examine
print(knn.get_params())
```

您应该得到以下输出:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 15, 'p': 2, 'weights': 'uniform'}
```

您可能已经注意到，k 不是 k-NN 分类器唯一可用的超参数。设置多个超参数就像指定相关参数一样简单。例如，让我们将邻居的数量从`5`增加到`15`，并强制算法在训练时考虑邻域中点的距离，而不是简单的多数投票。有关更多信息，请参见帮助文件(`?knn`)中对`weights`参数的描述:

```
# initialize with k = 15, weights = distance and all other hyperparameters as default 
knn = neighbors.KNeighborsClassifier (n_neighbors=15, weights='distance')
# examine
print(knn.get_params())
```

输出如下所示:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}
```

在输出中，你可以看到`n_neighbors` ( `k`)现在被设置为`15`，`weights`现在被设置为`distance`，而不是`uniform`。

注意

这部分的代码可以在[https://packt.live/2tN5CH1](https://packt.live/2tN5CH1)找到。

## 关于违约的说明

通常，机器学习库的开发者已经努力为估计器设置合理的默认超参数。也就是说，对于某些数据集，通过调优可以显著提高性能。

# 寻找最佳超参数化

最佳的超参数化首先取决于你建立机器学习模型的总体目标。在大多数情况下，这是为了找到对看不见的数据具有最高预测性能的模型，通过其正确标记数据点(分类)或预测数字(回归)的能力来衡量。

可以使用保留测试集或交叉验证来模拟对未知数据的预测，前者是本章中使用的方法。在每种情况下，对性能进行不同的评估，例如，回归的均方误差(MSE)和分类的准确性。我们寻求降低 MSE 或者增加我们预测的准确性。

让我们在下面的练习中实现手动超参数化。

## 练习e 8.01:k-NN 分类器的手动超参数调整

在本练习中，我们将手动调整一个 k-NN 分类器，这在*第 7 章“机器学习模型的推广”*中有所介绍，我们的目标是根据来自受影响乳房样本的细胞测量结果预测恶性或良性乳腺癌的发病率。

注意

本练习中使用的数据集可以在我们位于 https://packt.live/36dsxIF 的 GitHub 存储库中找到。

这些是数据集的重要属性:

*   识别号
*   诊断(M =恶性，B =良性)
*   3-32)

如下计算每个细胞核的 10 个实值特征:

*   半径(从中心到周边各点的平均距离)
*   纹理(灰度值的标准偏差)
*   周长
*   面积
*   平滑度(半径长度的局部变化)
*   密实度(perimeter^2 /面积- 1.0)
*   凹度(轮廓凹陷部分的严重程度)
*   凹点(轮廓凹陷部分的数量)
*   对称
*   Fractal dimension (refers to the complexity of the tissue architecture; "coastline approximation" - 1)

    注意

    关于数据集属性的详细信息可以在[https://packt.live/30HzGQ6](https://packt.live/30HzGQ6)找到。

以下步骤将帮助您完成本练习:

1.  Create a new notebook in Google Colab.

    接下来，从 scikit-learn 导入`neighbours`、`datasets`和`model_selection`:

    ```
    from sklearn import neighbors, datasets, model_selection
    ```

2.  加载数据。我们将这个对象称为`cancer`，并分离出目标`y`，以及特征`X` :

    ```
    # dataset
    cancer = datasets.load_breast_cancer()
    # target
    y = cancer.target
    # features
    X = cancer.data
    ```

3.  用默认的超参数化初始化 k-NN 分类器:

    ```
    # no arguments specified
    knn = neighbors.KNeighborsClassifier()
    ```

4.  将该分类器输入 10 重交叉验证(`cv`)，计算每一重的精度分数。假设最大化精度(所有阳性分类中真阳性的比例)是本练习的主要目标:

    ```
    # 10 folds, scored on precision
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')
    ```

5.  Printing `cv` shows the precision score calculated for each fold:

    ```
    # precision scores
    print(cv)
    ```

    您将看到以下输出:

    ```
    [0.91891892 0.85365854 0.91666667 0.94736842 0.94594595 0.94444444
     0.97222222 0.91891892 0.96875    0.97142857]
    ```

6.  计算并打印所有折叠的平均精度分数。这将使我们对模型的整体性能有所了解，如下面的代码片段所示:

    ```
    # average over all folds
    print(round(cv.mean(), 2))
    ```

7.  You should get the following output:

    ```
    0.94
    ```

    你应该看到平均分数接近 94%。这还能改进吗？

8.  Run everything again, this time setting hyperparameter `k` to `15`. You can see that the result is actually marginally worse (1% lower):

    ```
    # k = 15
    knn = neighbors.KNeighborsClassifier(n_neighbors=15)
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')
    print(round(cv.mean(), 2))
    ```

    输出如下所示:

    ```
    0.93
    ```

9.  Try again with `k` = `7`, `3`, and `1`. In this case, it seems reasonable that the default value of 5 is the best option. To avoid repetition, you may like to define and call a Python function as follows:

    ```
    def evaluate_knn(k):
      knn = neighbors.KNeighborsClassifier(n_neighbors=k)
      cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')
      print(round(cv.mean(), 2))
    evaluate_knn(k=7)
    evaluate_knn(k=3)
    evaluate_knn(k=1)
    ```

    输出如下所示:

    ```
    0.93
    0.93
    0.92
    ```

    没有什么能打败 94%。

10.  Let's alter a second hyperparameter. Setting `k = 5`, what happens if we change the k-NN weighing system to depend on distance rather than having uniform weights? Run all code again, this time with the following hyperparameterization:

    ```
    # k =5, weights evaluated using distance
    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')print(round(cv.mean(), 2)) 
    ```

    性能提高了吗？

11.  您应该看不到默认超参数化的进一步改进，因为输出是:

    ```
    0.93
    ```

因此，我们得出结论，在这种情况下，默认的超参数化是最优的。

## 手动搜索的优点和缺点

在所有的超参数调整策略中，手动过程给你最多的控制。当你经历这个过程时，你可以感觉到你的估计器在不同的超参数化下的表现，这意味着你可以根据你的期望调整它们，而不必尝试大量不必要的可能性。然而，只有当你想尝试的可能性很小的时候，这个策略才是可行的。当可能性的数量超过大约五个时，这种策略就变得太耗费人力而不实用了。

在接下来的部分中，我们将介绍两种策略来更好地处理这种情况。

# 使用网格搜索调整

在机器学习的背景下，网格搜索指的是一种从预定义的一组可能性中为您选择的估计器系统地测试每个超参数化的策略。您决定用于评估性能的标准，一旦搜索完成，您可以手动检查结果并选择最佳的超参数化，或者让您的计算机自动为您选择。

总体目标是尝试并找到一个最佳的超参数化，从而在预测未知数据时提高性能。

在我们开始在 scikit-learn 中实现网格搜索之前，让我们首先使用简单的 Python `for`循环来演示这个策略。

## 网格搜索策略的简单演示

在下面的网格搜索策略演示中，我们将使用在*练习 8.01* 中看到的乳腺癌预测数据集，其中我们手动调整了 k-NN 分类器的超参数，以优化癌症预测的精度。

这一次，我们不是用不同的`k`值来手动拟合模型，而是定义我们想要尝试的`k`值，也就是 Python 字典中的`k = 1, 3, 5, 7`。这个字典将是我们将搜索的网格，以找到最佳的超参数化。

注意

这部分的代码可以在[https://packt.live/2U1Y0Li](https://packt.live/2U1Y0Li)找到。

代码如下所示:

```
from sklearn import neighbors, datasets, model_selection
# load data
cancer = datasets.load_breast_cancer()
# target
y = cancer.target
# features
X = cancer.data
# hyperparameter grid
grid = {
    'k': [1, 3, 5, 7]
}
```

在代码片段中，我们使用了字典`{}`并在 Python 字典中设置了`k`值。

在代码片段的下一部分中，为了进行搜索，我们遍历网格，为每个值`k`拟合一个模型，每次通过 10 重交叉验证来评估模型。

在每次迭代结束时，我们通过`print`方法提取、格式化并报告交叉验证后的平均精度分数:

```
# for every value of k in the grid
for k in grid['k']:

    # initialize the knn estimator
    knn = neighbors.KNeighborsClassifier(n_neighbors=k)

    # conduct a 10-fold cross-validation
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')

    # calculate the average precision value over all folds
    cv_mean = round(cv.mean(), 3)

    # report the result
    print('With k = {}, mean precision = {}'.format(k, cv_mean))
```

输出如下所示:

![Figure 8.4: Average precisions for all folds
](image/B15019_08_04.jpg)

图 8.4:所有褶皱的平均精度

我们可以从输出中看到,`k = 5`是找到的最好的超参数化，平均精度分数大约为 94%。将`k`增加到`7`并没有显著提高性能。重要的是要注意，我们在这里改变的唯一参数是 k，并且每次初始化 k-NN 估计器时，其余的超参数都设置为默认值。

为了明确这一点，我们可以运行相同的循环，这一次只是打印将要尝试的超参数化:

```
# for every value of k in the grid 
for k in grid['k']:

    # initialize the knn estimator
    knn = neighbors.KNeighborsClassifier(n_neighbors=k)

    # print the hyperparameterization
    print(knn.get_params())
```

输出如下所示:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}
```

您可以从输出中看到，我们更改的唯一参数是 k；在每次迭代中，其他一切都保持不变。

简单的单循环结构对于单个超参数的网格搜索来说很好，但是如果我们想尝试第二种呢？记住，对于 k-NN，我们也有取值为`uniform`或`distance`的权重，它们的选择会影响 k-NN 学习如何分类点。

接下来，我们需要做的就是创建一个包含 k 值和权重函数的字典，我们希望将它们作为单独的键/值对:

```
# hyperparameter grid
grid = {
    'k': [1, 3, 5, 7],
    'weight_function': ['uniform', 'distance']
}
# for every value of k in the grid
for k in grid['k']:

    # and every possible weight_function in the grid 
    for weight_function in grid['weight_function']:

        # initialize the knn estimator
        knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights=weight_function)

        # conduct a 10-fold cross-validation
        cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')

        # calculate the average precision value over all folds
        cv_mean = round(cv.mean(), 3)

        # report the result
        print('With k = {} and weight function = {}, mean precision = {}'.format(k, weight_function, cv_mean))
```

输出如下所示:

![Figure 8.5: Average precision values for all folds for different values of k
](image/B15019_08_05.jpg)

图 8.5:不同 k 值下所有折叠的平均精度值

您可以看到，当`k = 5`时，权重函数不基于距离，所有其他超参数保持默认值，平均精度最高。正如我们之前所讨论的，如果您想看到 k-NN 的完整的超参数化集，只需在估计器初始化后在`for`循环中添加`print(knn.get_params())`:

```
# for every value of k in the grid
for k in grid['k']:

  # and every possible weight_function in the grid 
    for weight_function in grid['weight_function']:

        # initialize the knn estimator
        knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights=weight_function)
        # print the hyperparameterizations
        print(knn.get_params())
```

输出如下所示:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'distance'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'distance'}
```

这种实现虽然很好地展示了网格搜索过程是如何工作的，但在试图评估具有`3`、`4`、甚至`10`不同类型的超参数(每个超参数都有多种可能的设置)的估计器时，可能并不实用。

以这种方式继续下去将意味着编写和跟踪多个`for`循环，这可能是乏味的。谢天谢地，`scikit-learn`的`model_selection`模块给了我们一个叫做`GridSearchCV`的方法，这个方法更加用户友好。我们将在前面的主题中讨论这个问题。

# GridSearchCV

`GridsearchCV`是一种调整方法，其中可以通过评估网格中提到的参数组合来构建模型。在下图中，我们将看到`GridSearchCV`与手动搜索有何不同，并以表格的形式更详细地了解网格搜索。

## 使用 GridSearchCV 调优

在实践中，通过利用`model_selection.GridSearchCV`，我们可以更容易地进行网格搜索。

为了进行比较，我们将使用与之前相同的乳腺癌数据集和 k-NN 分类器:

```
from sklearn import model_selection, datasets, neighbors
# load the data
cancer = datasets.load_breast_cancer()
# target
y = cancer.target
# features
X = cancer.data
```

加载数据后，我们需要做的下一件事是初始化我们希望在不同超参数化下评估的估计器的类:

```
# initialize the estimator
knn = neighbors.KNeighborsClassifier()
```

然后我们定义网格:

```
# grid contains k and the weight function
grid = {
    'n_neighbors': [1, 3, 5, 7],
    'weights': ['uniform', 'distance']
}
```

为了建立搜索，我们将刚刚初始化的估计器和我们的超参数网格传递给`model_selection.GridSearchCV()`。我们还必须指定一个评分标准，该标准将用于评估搜索过程中尝试的各种超参数化的性能。

最后要做的是通过`cv`参数设置使用交叉验证的数字分割。我们将此设置为`10`，从而进行 10 重交叉验证:

```
# set up the grid search with scoring on precision and number of folds = 10
gscv = model_selection.GridSearchCV(estimator=knn, param_grid=grid, scoring='precision', cv=10)
```

最后一步是通过这个对象的`fit()`方法向其提供数据。一旦完成，网格搜索过程将启动:

```
# start the search
gscv.fit(X, y)
```

默认情况下，与搜索相关的信息将打印到屏幕上，使您可以看到将为 k-NN 估计器评估的确切估计器参数化:

![Figure 8.6: Estimator parameterizations for the k-NN estimator
](image/B15019_08_06.jpg)

图 8.6:k-NN 估计量的估计量参数化

一旦搜索完成，我们可以通过访问和打印`cv_results_`属性来检查结果。`cv_results_`是一个字典，包含关于每个超参数化下的模型性能的有用信息，例如您的评分度量的平均测试集值(`mean_test_score`，越低越好)，尝试的超参数化的完整列表(`params`，以及与`mean_test_score` ( `rank_test_score`)相关的模型排名。

找到的最佳模型的等级为 1，次佳模型的等级为 2，依此类推，如图 8.8 中的*所示。通过`mean_fit_time`报告模型拟合时间。*

虽然通常不考虑较小的数据集，但此值可能很重要，因为在某些情况下，您可能会发现通过某个超参数化模型性能的边际增加与模型拟合时间的显著增加相关，这取决于您可用的计算资源，可能会使超参数化不可行，因为拟合时间太长:

```
# view the results
print(gscv.cv_results_)
```

输出如下所示:

![Figure 8.7: GridsearchCV results
](image/B15019_08_07.jpg)

图 8.7: GridsearchCV 结果

在下图中可以看到模型等级:

![Figure 8.8: Model ranks
](image/B15019_08_08.jpg)

图 8.8:模型等级

注意

出于演示目的，输出被截断。你可以在这里看到完整的输出:[https://packt.live/2uD12uP](https://packt.live/2uD12uP)。

在输出中，值得注意的是，这个字典可以很容易地转换成 pandas DataFrame，这使得信息阅读起来更加清晰，并允许我们有选择地显示我们感兴趣的指标。

例如，假设我们仅对前五个高性能模型的每个超参数化(`params`)和平均交叉验证测试分数(`mean_test_score`)感兴趣:

```
import pandas as pd
# convert the results dictionary to a dataframe
results = pd.DataFrame(gscv.cv_results_)
# select just the hyperparameterizations tried, the mean test scores, order by score and show the top 5 models
print(
results.loc[:,['params','mean_test_score']].sort_values('mean_test_score', ascending=False).head(5)
)
```

运行此代码会产生以下输出:

![Figure 8.9: mean_test_score for top 5 models
](image/B15019_08_09.jpg)

图 8.9:前 5 名模型的平均测试分数

我们也可以使用 pandas 来产生如下结果的可视化:

```
# visualise the result
results.loc[:,['params','mean_test_score']].plot.barh(x = 'params')
```

输出如下所示:

![Figure 8.10: Using pandas to visualize the output
](image/B15019_08_10.jpg)

图 8.10:使用 pandas 来可视化输出

查看上图时，您会发现找到的最佳超参数化是在`n_neighbors = 5`和`weights = 'uniform'`处，因为这会产生最高的平均测试分数(精度)。

注意

这部分的代码可以在[https://packt.live/2uD12uP](https://packt.live/2uD12uP)找到。

### 支持向量机(SVM)分类器

SVM 分类器基本上是一个监督机器学习模型。它是一类常用的估计量，可用于二分类和多分类。众所周知，它在数据有限的情况下表现良好，因此它是一个可靠的模型。与高度迭代或集成方法(如人工神经网络或随机森林)相比，它的训练速度相对较快，如果计算机的处理能力有限，这是一个不错的选择。

它利用一种称为核函数的特殊数学公式进行预测。此函数可以采用多种形式，其中一些函数(如带次数(平方、立方等)的多项式核函数)具有自己的可调参数。

支持向量机已经被证明在图像分类中表现良好，这将在下面的练习中看到。

注意

关于支持向量机的更多信息，参见 https://packt.live/37iDytw，也可参考[https://packt.live/38xaPkC](https://packt.live/38xaPkC)。

## 练习 8.02:SVM 的网格搜索超参数调整

在本练习中，我们将使用一种称为 SVM 分类器的估计器，并使用网格搜索策略调整其超参数。

我们在这里将关注的监督学习目标是仅基于图像的手写数字(0-9)的分类。我们将使用的数据集包含 1，797 个带标签的手写数字图像。

注意

本练习中使用的数据集可以在我们位于 https://packt.live/2vdbHg9 的 GitHub 存储库中找到。

关于数据集属性的细节可以在原始数据集的 URL 上找到:[https://packt.live/36cX35b](https://packt.live/36cX35b)。

这个练习的代码可以在 https://packt.live/36At2MO 找到。

1.  在 Google Colab 中创建新笔记本。
2.  从 scikit-learn 中导入`datasets`、`svm`和`model_selection`:

    ```
    from sklearn import datasets, svm, model_selection
    ```

3.  加载数据。我们称这个物体为图像，然后我们将分离出目标`y`和特征`X`。在训练步骤中，SVM 分类器将学习`y`如何与`X`相关，并因此能够在给定新的`X`值:

    ```
    # load data
    digits = datasets.load_digits()
    # target
    y = digits.target
    # features
    X = digits.data
    ```

    时预测新的`y`值
4.  Initialize the estimator as a multi-class SVM classifier and set the `gamma` argument to `scale`:

    ```
    # support vector machine classifier
    clr = svm.SVC(gamma='scale')
    ```

    注意

    关于伽玛参数的更多信息，请访问[https://packt.live/2Ga2l79](https://packt.live/2Ga2l79)。

5.  定义我们的网格，以覆盖分类器的四个不同的超参数化，这四个超参数化具有线性核和次数为`2`、`3,`和`4`的多项式核。我们想看看四个超参数化中的哪一个导致更准确的预测:

    ```
    # hyperparameter grid. contains linear and polynomial kernels
    grid = [
      {'kernel': ['linear']},
     {'kernel': ['poly'], 'degree': [2, 3, 4]}
     ]
    ```

6.  使用`10`折叠和准确度评分标准建立网格搜索 k 折叠交叉验证。确保它有我们的`grid`和`estimator`对象作为输入:

    ```
    # setting up the grid search to score on accuracy and evaluate over 10 folds
    cv_spec = model_selection.GridSearchCV(estimator=clr, param_grid=grid, scoring='accuracy', cv=10)
    ```

7.  Start the search by providing data to the `.fit()` method. Details of the process, including the hyperparameterizations tried and the scoring method selected, will be printed to the screen:

    ```
    # start the grid search
    cv_spec.fit(X, y)
    ```

    您应该会看到以下输出:

    ![Figure 8.11: GridSearch using the fit() method
    ](image/B15019_08_11.jpg)

    图 8.11:使用。fit()方法

8.  To examine all of the results, simply print `cv_spec.cv_results_` to the screen. You will see that the results are structured as a dictionary, allowing you to access the information you require using the keys:

    ```
    # what is the available information
    print(cv_spec.cv_results_.keys())
    ```

    您将看到以下信息:

    ![Figure 8.12: Results as a dictionary
    ](image/B15019_08_12.jpg)

    图 8.12:作为字典的结果

9.  For this exercise, we are primarily concerned with the test-set performance of each distinct hyperparameterization. You can see the first hyperparameterization through `cv_spec.cv_results_['mean_test_score']`, and the second through `cv_spec.cv_results_['params']`.

    让我们将结果字典转换成一个`pandas`数据帧，并找到最佳的超参数化:

    ```
    import pandas as pd
    # convert the dictionary of results to a pandas dataframe
    results = pd.DataFrame(cv_spec.cv_results_)
    print(
    # show hyperparameterizations
    results.loc[:,['params','mean_test_score']].sort_values('mean_test_score', ascending=False)
    )
    ```

    您将看到以下结果:

    ![Figure 8.13: Parameterization results
    ](image/B15019_08_13.jpg)

    图 8.13:参数化结果

10.  最佳实践是将您产生的任何结果可视化。使这变得容易。运行下面的代码来生成可视化效果:

    ```
    # visualize the result
    (
        results.loc[:,['params','mean_test_score']]
        .sort_values('mean_test_score', ascending=True)
        .plot.barh(x='params', xlim=(0.8))
    )
    ```

输出如下所示:

![Figure 8.14: Using pandas to visualize the results
](image/B15019_08_14.jpg)

图 8.14:使用熊猫来可视化结果

我们可以看到，具有三次多项式核函数的 SVM 分类器在我们的搜索中评估的所有超参数化中具有最高的准确性。请随意向网格中添加更多的超参数化，看看您是否可以提高分数。

## 网格搜索的优缺点

与手动搜索相比，网格搜索的主要优势在于它是一个自动化的过程，人们可以简单地设置并忘记。此外，您有权力指定所评估的精确超参数化，当您事先知道哪种超参数化可能在您的环境中工作良好时，这可能是一件好事。由于网格的明确定义，也很容易准确理解搜索过程中会发生什么。

网格搜索策略的主要缺点是计算量非常大；也就是说，当要尝试的超参数化的数量显著增加时，处理时间可能会非常慢。此外，当您定义网格时，您可能会无意中忽略一个实际上是最佳的超参数化。如果在您的网格中没有指定，它将永远不会被尝试

为了克服这些缺点，我们将在下一节研究随机搜索。

# 随机搜索

在随机搜索中，我们假设每个超参数都是随机变量，而不是像网格搜索那样搜索预定义集合中的每个超参数。在我们深入研究这个过程之前，简要回顾一下什么是随机变量以及分布的含义是有帮助的。

## 随机变量及其分布

随机变量是非恒定的(它的值可以改变),它的可变性可以用分布来描述。有许多不同类型的分布，但每一种都属于两大类之一:离散分布和连续分布。我们使用离散分布来描述随机变量，其值只能取整数，如计数。

一个例子是一个主题公园一天的游客数量，或者一个高尔夫球手一杆进洞的尝试次数。

我们使用连续分布来描述随机变量，其值位于由无限小的增量组成的连续统上。例子包括人的身高或体重，或室外气温。分布通常具有控制其形状的参数。

离散分布可以用所谓的概率质量函数进行数学描述，它定义了随机变量取某个值的确切概率。这个函数左边常见的符号是`P(X=x)`，通俗地说就是随机变量`X`等于某个值`x`的概率是`P`。记住概率的范围在`0`(不可能)和`1`(确定)之间。

根据定义，所有可能的`x`的每个`P(X=x)`的总和将等于 1，或者如果用另一种方式表达，`X`取任何值的概率是 1。这种分布的一个简单例子是离散均匀分布，其中随机变量`X`将只取有限范围值中的一个，并且它取任何特定值的概率对于所有值都是相同的，因此称为均匀分布。

例如，如果有 10 个可能的值，`X`是任何特定值的概率正好是 1/10。如果有 6 个可能的值，如在标准 6 面骰子的情况下，概率将是 1/6，依此类推。离散均匀分布的概率质量函数为:

![Figure 8.15: Probability mass function for the discrete uniform distribution
](image/B15019_08_15.jpg)

图 8.15:离散均匀分布的概率质量函数

下面的代码将允许我们看到 x 有 10 个可能值的分布形式。

首先，我们创建一个列表，列出了`X`可能取的所有值:

```
# list of all xs
X = list(range(1, 11))
print(X)
```

输出如下所示:

```
 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

然后，我们计算`X`将占用`x (P(X=x))`的任何值的概率:

```
# pmf, 1/n * n = 1
p_X_x = [1/len(X)] * len(X)
# sums to 1
print(p_X_x)
```

如前所述，概率的总和等于 1，任何分布都是如此。我们现在有了可视化分布所需的一切:

```
import matplotlib.pyplot as plt
plt.bar(X, p_X_x)
plt.xlabel('X')
plt.ylabel('P(X=x)')
```

输出如下所示:

![Figure 8.16: Visualizing the bar chart
](image/B15019_08_16.jpg)

图 8.16:可视化条形图

在视觉输出中，我们看到`X`是 1 到 10 之间的特定整数的概率等于 1/10。

注意

其他常见的离散分布包括二项分布、负二项分布、几何分布和泊松分布，我们鼓励您研究所有这些分布。在搜索引擎中键入这些术语可以找到更多信息。

连续随机变量的分布更具挑战性，因为我们不能直接计算精确的`P(X=x)`，因为`X`位于连续统上。然而，我们可以用积分来近似一系列值之间的概率，但这超出了本书的范围。使用概率密度函数`P(X)`描述`X`和概率之间的关系。也许最广为人知的连续分布是正态分布，它在视觉上表现为钟形。

正态分布有两个描述其形状的参数，均值(`𝜇`)和方差(`𝜎` 2)。正态分布的概率密度函数为:

![Figure 8.17: Probability density function for the normal distribution
](image/B15019_08_17.jpg)

图 8.17:正态分布的概率密度函数

下面的代码展示了两个均值(`𝜇` `= 0`)相同但方差参数(`𝜎``2 = 1``𝜎``2 = 2.25`)不同的正态分布。让我们首先使用 NumPy 的`.linspace`方法生成从`-10`到`10`的 100 个等距值:

```
import numpy as np
# range of xs
x = np.linspace(-10, 10, 100)
```

然后，我们为两个正态分布生成近似的`X`概率。

使用`scipy.stats`是一种处理分布的好方法，它的`pdf`方法可以让我们很容易地可视化概率密度函数的形状:

```
import scipy.stats as stats
# first normal distribution with mean = 0, variance = 1
p_X_1 = stats.norm.pdf(x=x, loc=0.0, scale=1.0**2)
# second normal distribution with mean = 0, variance = 2.25
p_X_2 = stats.norm.pdf(x=x, loc=0.0, scale=1.5**2)
```

注意

在这种情况下，`loc`对应于𝜇，而`scale`对应于标准差，标准差是`𝜎` `2`的平方根，因此我们对输入求平方。

然后我们想象结果。请注意，`𝜎` `2`控制分布的宽度，从而控制随机变量的可变性:

```
plt.plot(x,p_X_1, color='blue')
plt.plot(x, p_X_2, color='orange')
plt.xlabel('X')
plt.ylabel('P(X)')
```

输出如下所示:

![Figure 8.18: Visualizing the normal distribution
](image/B15019_08_18.jpg)

图 8.18:可视化正态分布

您通常看到的其他离散分布包括伽玛、指数和贝塔分布，我们鼓励您研究这些分布。

注意

这部分的代码可以在[https://packt.live/38Mfyzm](https://packt.live/38Mfyzm)找到。

## Rando m 搜索过程的简单演示

同样，在我们开始随机搜索参数调优的 scikit-learn 实现之前，我们将使用简单的 Python 工具逐步完成这个过程。到目前为止，我们只是使用分类问题来演示调优概念，但是现在我们将研究一个回归问题。我们能否找到一种模型，能够根据身体质量指数和年龄等特征预测患者的糖尿病进展？

注意

原始数据集可以在[https://packt.live/2O4XN6v](https://packt.live/2O4XN6v)找到。

这部分的代码可以在[https://packt.live/3aOudvK](https://packt.live/3aOudvK)找到。

我们首先加载数据:

```
from sklearn import datasets, linear_model, model_selection
# load the data
diabetes = datasets.load_diabetes()
# target
y = diabetes.target
# features
X = diabetes.data
```

为了对数据有所了解，我们可以检查第一个患者的疾病进展:

```
# the first patient has index 0
print(y[0])
```

输出如下所示:

```
 151.0
```

现在让我们来看看它们的特征:

```
# let's look at the first patients data
print(
  dict(zip(diabetes.feature_names, X[0]))
)
```

我们应该看到以下内容:

![Figure 8.19: Dictionary for patient characteristics
](image/B15019_08_19.jpg)

图 8.19:患者特征字典

对于这种情况，我们将尝试一种称为岭回归的技术，它将线性模型拟合到数据中。岭回归是一种特殊的方法，它允许我们直接使用正则化来帮助减轻过度拟合的问题。岭回归有一个关键的超参数𝛼，它控制着模型拟合中的正则化程度。如果𝛼设置为 1，将不会使用任何正则化，这实际上是一种特殊情况，其中岭回归模型拟合将完全等于 OLS 线性回归模型的拟合。增加𝛼的价值，你就增加了正规化的程度。

注意

我们在*第七章*、*中讨论了岭回归对机器学习模型的推广*。

有关岭回归和正则化的更多信息，请参见[https://packt.live/2NR3GUq](https://packt.live/2NR3GUq)。

在随机搜索参数调整的上下文中，我们假设𝛼是一个随机变量，由我们来指定一个可能的分布。

对于这个例子，我们将假设阿尔法遵循伽玛分布。这个分布有两个参数，k 和𝜃，它们分别控制分布的形状和比例。

对于岭回归，我们认为最佳𝛼接近 1，随着远离 1，可能性变得越来越小。反映这一思想的伽马分布的参数化是 k 和𝜃都等于 1。为了形象化这个分布的形式，我们可以运行以下命令:

```
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
# values of alpha
x = np.linspace(1, 20, 100)
# probabilities
p_X = stats.gamma.pdf(x=x, a=1, loc=1, scale=2)
plt.plot(x,p_X)
plt.xlabel('alpha')
plt.ylabel('P(alpha)')
```

输出如下所示:

![Figure 8.20: Visualization of probabilities
](image/B15019_08_20.jpg)

图 8.20:概率的可视化

在图表中，您可以看到𝛼值越小，概率衰减得越快，而值越大，衰减得越慢。

随机搜索过程的下一步是从选定的分布中抽取 n 个值。在这个例子中，我们将绘制 100 个𝛼值。请记住，引出𝛼特定值的概率与其概率相关，如以下分布所定义:

```
# n sample values
n_iter = 100
# sample from the gamma distribution
samples = stats.gamma.rvs(a=1, loc=1, scale=2, size=n_iter, random_state=100)
```

注意

我们设置了一个随机状态，以确保结果的可重复性。

绘制样本的直方图，如下图所示，揭示了一个与我们采样的分布近似一致的形状。请注意，随着样本量的增加，直方图越符合分布:

```
# visualize the sample distribution
plt.hist(samples)
plt.xlabel('alpha')
plt.ylabel('sample count')
```

输出如下所示:

![Figure 8.21: Visualization of the sample distribution
](image/B15019_08_21.jpg)

图 8.21:样本分布的可视化

然后，将为每个𝛼采样值拟合一个模型，并评估其性能。正如我们在本章中看到的超参数调整的其他方法，性能将使用 k 倍交叉验证(使用`k =10`)进行评估，但是因为我们正在处理一个回归问题，所以性能度量将是测试集负 MSE。

使用这个度量意味着值越大越好。我们将把结果存储在一个字典中，每个𝛼值作为键，相应的交叉验证负 MSE 作为值:

```
# we will store the results inside a dictionary
result = {}
# for each sample
for sample in samples:

  # initialize a ridge regression estimator with alpha set to the sample value
  reg = linear_model.Ridge(alpha=sample)

  # conduct a 10-fold cross validation scoring on negative mean squared error
  cv = model_selection.cross_val_score(reg, X, y, cv=10, scoring='neg_mean_squared_error')

  # retain the result in the dictionary
  result[sample] = [cv.mean()]
```

我们不检查原始的结果字典，而是将它转换成 pandas 数据帧，转置它，并给出列名。按负均方误差降序排序揭示了该问题的最佳正则化水平实际上是当𝛼约为 1 时，这意味着我们没有找到证据表明正则化对于该问题是必要的，并且 OLS 线性模型将满足要求:

```
import pandas as pd
# convert the result dictionary to a pandas dataframe, transpose and reset the index
df_result = pd.DataFrame(result).T.reset_index()
# give the columns sensible names
df_result.columns = ['alpha', 'mean_neg_mean_squared_error']
print(df_result.sort_values('mean_neg_mean_squared_error', ascending=False).head())
```

输出如下所示:

![Figure 8.22: Output for the random search process
](image/B15019_08_22.jpg)

图 8.22:随机搜索过程的输出

注意

根据使用的数据，结果会有所不同。

尽可能可视化结果总是有益的。通过负均方误差将𝛼绘制成散点图，可以清楚地看出，冒险远离𝛼 = 1 不会导致预测性能的提高:

```
plt.scatter(df_result.alpha, df_result.mean_neg_mean_squared_error)
plt.xlabel('alpha')
plt.ylabel('-MSE')
```

输出如下所示:

![Figure 8.23: Plotting the scatter plot
](image/B15019_08_23.jpg)

图 8.23:绘制散点图

我们发现最佳𝛼为 1(其默认值)这一事实是超参数调整中的一个特例，因为最佳超参数化是默认值。

## 使用 RandomizedSearchCV 调谐

实际上，我们可以使用 scikit-learn 的`model_selection`模块中的方法来进行搜索。您需要做的只是传递您的估计量、您希望调整的超参数及其分布、您希望从每个分布中抽取的样本数以及您希望用来评估模型性能的度量。这些分别对应于`param_distributions`、`n_iter`和`scoring`自变量。为了便于演示，让我们使用`RandomizedSearchCV`来执行之前完成的搜索。首先，我们加载数据并初始化我们的岭回归估计量:

```
from sklearn import datasets, model_selection, linear_model
# load the data
diabetes = datasets.load_diabetes()
# target
y = diabetes.target
# features
X = diabetes.data
# initialise the ridge regression
reg = linear_model.Ridge()
```

然后，我们指定我们想要调优的超参数是`alpha`，并且我们想要𝛼分布为`gamma`，具有`k = 1`和`𝜃`T4:

```
from scipy import stats
# alpha ~ gamma(1,1)
param_dist = {'alpha': stats.gamma(a=1, loc=1, scale=2)}
```

接下来，我们设置并运行随机搜索流程，该流程将从我们的`gamma(1,1)`分布中抽取 100 个值，拟合岭回归，并使用负均方误差指标的交叉验证评分来评估其性能:

```
# set up the random search to sample 100 values and score on negative mean squared error
rscv = model_selection.RandomizedSearchCV(estimator=reg, param_distributions=param_dist, n_iter=100, scoring='neg_mean_squared_error')
# start the search
rscv.fit(X,y)
```

完成搜索后，我们可以提取结果并生成一个熊猫数据框架，就像我们之前所做的那样。按`rank_test_score`排序并查看前五行与我们的结论一致，即 alpha 应设置为 1，正则化似乎不需要解决这个问题:

```
import pandas as pd
# convert the results dictionary to a pandas data frame
results = pd.DataFrame(rscv.cv_results_)
# show the top 5 hyperparamaterizations
print(results.loc[:,['params','rank_test_score']].sort_values('rank_test_score').head(5))
```

输出如下所示:

![Figure 8.24: Output for tuning using RandomizedSearchCV
](image/B15019_08_24.jpg)

图 8.24:使用 RandomizedSearchCV 进行调优的输出

注意

根据日期的不同，上述结果可能会有所不同。

## 练习 8.03:随机森林分类器的随机搜索超参数调整

在本练习中，我们将再次讨论手写数字分类问题，这次使用随机森林分类器，其超参数通过随机搜索策略进行调整。随机森林是用于单类和多类分类问题的流行方法。它通过生长简单的树模型来学习，每个模型都逐步将数据集分割成能够最好地分离不同类的点的区域。

产生的最终模型可以被认为是 n 个树模型中每一个的平均值。这样，随机森林就是一个`ensemble`方法。在本练习中，我们将调整的参数是`criterion`和`max_features`。

`criterion`是指从类别纯度的角度评估每个分裂的方式(分裂越纯越好),而`max_features`是随机森林在寻找最佳分裂时可以使用的最大特征数。

注意

这个练习的代码可以在 https://packt.live/2uDVct8 找到。

以下步骤将帮助您完成练习。

1.  在 Google Colab 中创建新笔记本。
2.  导入数据并分离特征`X`和目标`y` :

    ```
    from sklearn import datasets
    # import data
    digits = datasets.load_digits()
    # target
    y = digits.target
    # features
    X = digits.data
    ```

3.  初始化随机森林分类器估计器。我们将把`n_estimators`超参数设置为`100`，这意味着最终模型的预测本质上将是`100`简单树模型的平均值。注意使用随机状态以确保结果的可再现性:

    ```
    from sklearn import ensemble
    # an ensemble of 100 estimators
    rfc = ensemble.RandomForestClassifier(n_estimators=100, random_state=100)
    ```

4.  One of the parameters we will be tuning is `max_features`. Let's find out the maximum value this could take:

    ```
    # how many features do we have in our dataset?
    n_features = X.shape[1]
    print(n_features)
    ```

    您应该会看到我们有 64 个功能:

    ```
    64
    ```

    现在我们知道了`max_features`的最大值，我们可以自由地定义随机化搜索过程的超参数输入。在这一点上，我们没有理由相信`max_features`的任何特定值是更优的。

5.  设置覆盖范围`1`到`64`的离散均匀分布。记住这个分布的概率质量函数，`P(X=x) = 1/n`，所以在我们的例子中是`P(X=x) = 1/64`。因为`criterion`只有两个离散选项，这也将作为离散均匀分布用`P(X=x) = ½` :

    ```
    from scipy import stats
    # we would like to smaple from criterion and max_features as discrete uniform distributions
    param_dist = {
        'criterion': ['gini', 'entropy'],
        'max_features': stats.randint(low=1, high=n_features)
    }
    ```

    采样
6.  我们现在已经有了设置随机搜索过程所需的一切。和以前一样，我们将使用准确性作为模型评估的度量。注意随机状态的使用:

    ```
    from sklearn import model_selection
    # setting up the random search sampling 50 times and conducting 5-fold cross-validation
    rscv = model_selection.RandomizedSearchCV(estimator=rfc, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy' , random_state=100)
    ```

7.  Let's kick off the process with the. `fit` method. Please note that both fitting random forests and cross-validation are computationally expensive processes due to their internal processes of iteration. Generating a result may take some time:

    ```
    # start the process
    rscv.fit(X,y)
    ```

    您应该看到以下内容:

    ![Figure 8.25: RandomizedSearchCV results
    ](image/B15019_08_25.jpg)

    图 8.25:随机搜索结果

8.  Next, you need to examine the results. Create a `pandas` DataFrame from the `results` attribute, order by the `rank_test_score`, and look at the top five model hyperparameterizations. Note that because the random search draws samples of hyperparameterizations at random, it is possible to have duplication. We remove the duplicate entries from the DataFrame:

    ```
    import pandas as pd
    # convert the dictionary of results to a pandas dataframe
    results = pd.DataFrame(rscv.cv_results_)
    # removing duplication
    distinct_results = results.loc[:,['params','mean_test_score']]
    # convert the params dictionaries to string data types
    distinct_results.loc[:,'params'] = distinct_results.loc[:,'params'].astype('str')
    # remove duplicates
    distinct_results.drop_duplicates(inplace=True)
    # look at the top 5 best hyperparamaterizations
    distinct_results.sort_values('mean_test_score', ascending=False).head(5)
    ```

    您应该得到以下输出:

    ![Figure 8.26: Top five hyperparameterizations
    ](image/B15019_08_26.jpg)

    图 8.26:五大超参数化

9.  The last step is to visualize the result. Including every parameterization will result in a cluttered plot, so we will filter on parameterizations that resulted in a mean test score > 0.93:

    ```
    # top performing models
    distinct_results[distinct_results.mean_test_score > 0.93].sort_values('mean_test_score').plot.barh(x='params', xlim=(0.9))
    ```

    输出如下所示:

![Figure 8.27: Visualizing the test scores of the top-performing models
](image/B15019_08_27.jpg)

图 8.27:可视化表现最好的模型的测试分数

我们发现最好的超参数化是使用最大特征设置为 4 的`gini`标准的随机森林分类器。

## 随机搜索的优点和缺点

因为随机搜索从一系列可能的超参数化中抽取有限的样本(`model_selection.RandomizedSearchCV`中的`n_iter`)，所以将超参数搜索的范围扩展到网格搜索的实际范围之外是可行的。这是因为网格搜索必须尝试范围内的所有内容，设置大范围的值可能会太慢而无法处理。搜索这个更大的范围给你机会发现一个真正的最优解。

与手动和网格搜索策略相比，您确实牺牲了一定程度的控制来获得这种好处。另一个考虑是，设置随机搜索比其他选项更复杂，因为您必须指定分布。总有出错的可能。也就是说，如果你不确定使用什么样的分布，坚持对各自的变量类型使用离散或连续均匀分布，因为这将为所有选项分配相同的选择概率。

## 活动 8.01:蘑菇有毒吗？

假设你是一名数据科学家，在当地大学的生物系工作。你的同事是一名真菌学家(一名专门研究真菌的生物学家)，她请求你帮助她开发一个机器学习模型，该模型能够识别特定蘑菇物种是否有毒，或者是否具有与其外观相关的属性。

这项活动的目标是采用网格和随机搜索策略来找到一个最佳模型。

注意

本练习中使用的数据集可以在我们位于 https://packt.live/38zdhaB 的 GitHub 存储库中找到。

关于数据集属性的详细信息可以在原始数据集站点上找到:[https://packt.live/36j0jfA](https://packt.live/36j0jfA)。

1.  Load the data into Python using the `pandas.read_csv()` method, calling the object `mushrooms`.

    提示:数据集是 CSV 格式，没有标题。在`pandas.read_csv()`中设置`header=None`。

2.  Separate the target, `y` and features, `X` from the dataset.

    提示:可以在第一列(`mushrooms.iloc[:,0]`)中找到目标，在其余列(`mushrooms.iloc[:,1:]`)中找到特征。

3.  对目标`y`重新编码，使毒蘑菇表示为`1`，食用菌表示为`0`。
4.  Transform the columns of the featureset `X` into a `numpy` array with a binary representation. This is known as one-hot encoding.

    提示:使用`preprocessing.OneHotEncoder()`来变换`X`。

5.  Conduct both a grid and random search to find an optimal hyperparameterization for a random forest classifier. Use accuracy as your method of model evaluation. Make sure that when you initialize the classifier and when you conduct your random search, `random_state = 100`.

    对于网格搜索，请使用以下内容:

    ```
    {
        'criterion': ['gini', 'entropy'],
        'max_features': [2, 4, 6, 8, 10, 12, 14]
    }
    ```

    对于随机搜索，请使用以下内容:

    ```
    {
        'criterion': ['gini', 'entropy'],
        'max_features': stats.randint(low=1, high=max_features)
    }
    ```

6.  Plot the mean test score versus hyperparameterization for the top 10 models found using random search.

    您应该会看到类似下面的图:

![Figure 8.28: Mean test score plot
](image/B15019_08_28.jpg)

图 8.28:平均测试分数图

注意

活动的解决方案可以在这里找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

# 总结

在这一章中，我们讨论了三种超参数调整策略，这三种策略都是基于对提高性能的估计器超参数化的搜索。

手动搜索是三种搜索中最需要动手的，但它给你一种独特的感觉。它适用于所讨论的估计量很简单的情况(超参数数量很少)。

网格搜索是一种自动化方法，是三种方法中最系统的，但是当可能的超参数化范围增加时，运行起来计算量非常大。

随机搜索虽然设置起来最复杂，但它是基于超参数分布的采样，这允许您扩大搜索范围，从而让您有机会发现网格或手动搜索选项可能会错过的好解决方案。在下一章，我们将看看如何可视化结果，总结模型，并阐明特征的重要性和权重。