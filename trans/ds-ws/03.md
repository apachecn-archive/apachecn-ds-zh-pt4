

# 三、二元分类

概观

在本章结束时，你将能够从业务角度阐述数据科学问题陈述；根据影响用例的各种业务驱动因素构建假设，并使用探索性数据分析验证假设；基于通过特征工程从探索性分析中获得的直觉来获得特征；使用逻辑回归函数构建二元分类模型，分析分类指标，并制定改进模型的行动计划。

在本章中，我们将使用一个真实世界的数据集和一种称为分类的监督学习技术来生成业务成果。

# 简介

在前面的章节中，介绍了机器学习，你已经了解了两大类机器学习；监督学习和非监督学习。监督学习可以进一步分为两类问题案例，回归和分类。在上一章中，我们讨论了回归问题。在这一章中，我们将窥视分类问题的世界。

看看下面的*图 3.1* :

![Figure 3.1: Overview of machine learning algorithms
](image/C15019_03_01.jpg)

图 3.1:机器学习算法概述

分类问题是你在现实世界中会遇到的最普遍的用例。与回归问题(预测实数)不同，分类问题处理的是将示例与类别相关联。分类用例将采取如下形式:

*   预测客户是否会购买推荐的产品
*   识别信贷交易是否是欺诈性的
*   确定患者是否患有疾病
*   分析动物图像并预测图像是狗、猫还是Pandas
*   分析文本评论并捕捉潜在的情绪，如快乐、愤怒、悲伤或讽刺

如果你观察前面的例子，前三个和后两个有细微的区别。前三个围绕二元决策:

*   客户可以购买产品，也可以不购买。
*   信用卡交易可能是欺诈性的，也可能是合法的。
*   患者可以被诊断为疾病阳性或阴性。

与前面三种类型一致的用例被称为二进制分类问题。与前三个不同，后两个将一个示例与多个类或类别相关联。这样的问题被称为多类分类问题。本章将讨论二元分类问题。多级分类将在*第 4 章*、*多级分类*中介绍。

# 了解业务环境

使用一个概念的最好方法是用一个你能理解的例子。为了理解业务环境，例如，让我们考虑下面的例子。

你所在银行的营销主管是一名数据科学家，他带着一个他们希望解决的问题来找你。营销团队最近完成了一项营销活动，他们收集了大量现有客户的信息。他们需要您的帮助来确定这些客户中哪些人可能会购买定期存款计划。根据你对客户群的评估，营销团队将制定目标营销策略。营销团队提供了对过去营销活动及其结果的历史数据的访问，即目标客户是否真的购买了定期存款。有了历史数据，你就开始着手识别最有购买定期存款倾向的客户。

## 商业发现

着手解决类似上述数据科学问题的第一个流程是业务发现流程。这需要理解影响业务问题的各种驱动因素。了解业务驱动因素很重要，因为这将有助于形成关于业务问题的假设，这可以在探索性数据分析(EDA)期间得到验证。假设的验证将有助于为特征工程制定直觉，这对我们建立的模型的准确性至关重要。

让我们从用例的上下文中详细理解这个过程。问题陈述旨在识别那些有购买定期存款倾向的客户。你可能知道，定期存款是一种银行工具，你的钱将被锁定一段时间，保证比储蓄账户或计息支票账户有更高的利率。从投资倾向的角度来看，定期存款通常受到风险厌恶型客户的欢迎。了解了业务背景后，我们来看一些关于影响购买定期存款倾向的业务因素的问题:

*   年龄会是一个因素吗，老年人表现出更多的倾向？
*   就业状况和购买定期存款的倾向有关系吗？
*   客户的资产组合——即房屋、贷款或更高的银行存款——会影响购买倾向吗？
*   婚姻状况和教育程度等人口统计数据会影响购买定期存款的倾向吗？如果是这样的话，人口统计学如何与购买倾向相关联？

在业务环境中制定问题是至关重要的，因为这将有助于我们在进行探索性分析时找到各种线索。我们将在下一节处理这个问题。首先，让我们探索与前面的业务问题相关的数据。

## 练习 3.01:加载并浏览数据集中的数据

在本练习中，我们将下载数据集，将其加载到我们的 Colab 笔记本中，并进行一些基本的探索，例如使用`.shape()`函数打印数据集的维度，以及使用`.describe()`函数生成数据集的汇总统计数据。

注意

本练习的数据集是银行数据集，由 S. Moro、P. Cortez 和 P. Rita 提供:一种预测银行电话营销成功的数据驱动方法。

它来自 https://packt.live/2MItXEl 的 UCI 机器学习库:，可以从我们的 GitHub 下载，地址:。

以下步骤将帮助您完成本练习:

1.  打开新的 Colab 笔记本。
2.  现在，`import` `pandas`作为`pd`出现在你的 Colab 笔记本上:

    ```
    import pandas as pd
    ```

3.  将数据集的链接赋给一个名为`file_url`

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter03/bank-full.csv'
    ```

    的变量
4.  Now, read the file using the `pd.read_csv()` function from the pandas DataFrame:

    ```
    # Loading the data using pandas
    bankData = pd.read_csv(file_url, sep=";")
    bankData.head()
    ```

    `pd.read_csv()`函数的参数是字符串形式的文件名和 CSV 的限制分隔符，即`";"`。读取文件后，使用`.head()`功能打印数据帧。

    您应该得到以下输出:

    ![Figure 3.2: Loading data into a Colab notebook
    ](image/C15019_03_02.jpg)

    图 3.2:将数据加载到 Colab 笔记本中

    这里，我们加载了`CSV`文件，然后将其存储为 pandas 数据帧，以供进一步分析。

5.  Next, print the shape of the dataset, as mentioned in the following code snippet:

    ```
    # Printing the shape of the data 
    print(bankData.shape)
    ```

    `.shape`函数用于查找数据集的整体形状。

    您应该得到以下输出:

    ```
    (45211, 17)
    ```

6.  Now, find the summary of the numerical raw data as a table output using the `.describe()` function in pandas, as mentioned in the following code snippet:

    ```
    # Summarizing the statistics of the numerical raw data
    bankData.describe()
    ```

    您应该得到以下输出:

    ![Figure 3.3: Loading data into a Colab notebook
    ](image/C15019_03_03.jpg)

图 3.3:将数据加载到 Colab 笔记本中

从数据的形状可以看出，数据集有带有`17`个变量的`45211`个示例。变量集既有分类变量也有数值变量。上述汇总统计数据仅针对数字数据得出。

在开始我们的旅程之前，您已经完成了需要完成的第一项任务。在本练习中，您学习了如何从数据集中加载数据和导出基本统计信息，例如汇总统计信息。在后续数据集中，我们将深入研究加载的数据集。

## 使用探索性数据分析测试业务假设

在上一节中，您从领域的角度处理了问题陈述，从而确定了一些业务驱动因素。一旦确定了业务驱动因素，下一步就是发展一些关于这些业务驱动因素和你要实现的业务结果之间关系的假设。这些假设需要用你所拥有的数据来验证。这就是探索性数据分析(EDA)在数据科学生命周期中发挥重要作用的地方。

让我们回到我们试图分析的问题陈述。在上一节中，我们确定了一些业务驱动因素，如年龄、人口统计、就业状况和资产组合，我们认为这些因素会影响购买定期存款的倾向。让我们继续阐述我们对其中一些业务驱动因素的假设，然后使用 EDA 验证它们。

## 探索性数据分析的可视化

可视化对于 EDA 来说是必不可少的。有效的可视化有助于从数据中获得业务直觉。在本节中，我们将介绍一些将用于 EDA 的可视化技术:

*   **Line graphs**: Line graphs are one of the simplest forms of visualization. Line graphs are the preferred method for revealing trends in the data. These types of graphs are mostly used for continuous data. We will be generating this graph in *Exercise 3.02*.

    折线图看起来是这样的:

![Figure 3.4: Example of a line graph
](image/C15019_03_04.jpg)

图 3.4:折线图示例

*   **Histograms**: Histograms are plots of the proportion of data along with some specified intervals. They are mostly used for visualizing the distribution of data. Histograms are very effective for identifying whether data distribution is symmetric and for identifying outliers in data. We will be looking at histograms in much more detail later in this chapter.

    下面是直方图的样子:

![Figure 3.5: Example of a histogram
](image/C15019_03_05.jpg)

图 3.5:直方图示例

*   **Density plots**: Like histograms, density plots are also used for visualizing the distribution of data. However, density plots give a smoother representation of the distribution. We will be looking at this later in this chapter.

    这是密度图的样子:

![Figure 3.6: Example of a density plot
](image/C15019_03_06.jpg)

图 3.6:密度图示例

*   **Stacked bar charts**: A stacked bar chart helps you to visualize the various categories of data, one on top of the other, in order to give you a sense of proportion of the categories; for instance, if you want to plot a bar chart showing the values, `Yes` and `No`, on a single bar. This can be done using the stacked bar chart, which cannot be done on the other charts.

    让我们创建一些虚拟数据，并生成一个堆叠条形图来检查不同部门的工作比例。

    导入任务所需的库文件:

    ```
    # Importing library files
    import matplotlib.pyplot as plt
    import numpy as np
    ```

    接下来，创建一些详细列出作业列表的示例数据:

    ```
    # Create a simple list of categories
    jobList = ['admin','scientist','doctor','management']
    ```

    每个工作将有两个类别被绘制，`yes`和`No`，在`yes`和`No`之间有一定的比例。具体如下:

    ```
    # Getting two categories ( 'yes','No') for each of jobs
    jobYes = [20,60,70,40]
    jobNo = [80,40,30,60]
    ```

    在接下来的步骤中，工作列表的长度用于绘制`xlabels`，然后使用`np.arange()`功能排列它们:

    ```
    # Get the length of x axis labels and arranging its indexes
    xlabels = len(jobList)
    ind = np.arange(xlabels)
    ```

    接下来，让我们定义每个条形的宽度并进行绘图。在`p2`图中，我们定义叠加时，`yes`在底部，`No`在顶部:

    ```
    # Get width of each bar
    width = 0.35
    # Getting the plots
    p1 = plt.bar(ind, jobYes, width)
    p2 = plt.bar(ind, jobNo, width, bottom=jobYes)
    ```

    定义 *Y* 轴的标签和绘图标题:

    ```
    # Getting the labels for the plots
    plt.ylabel('Proportion of Jobs')
    plt.title('Job')
    ```

    接下来定义 *X* 和 *Y* 轴的索引。对于 *X* 轴，给出了工作列表，对于 *Y* 轴，索引从`0`到`100`成比例，增量为`10` (0、10、20、30 等):

    ```
    # Defining the x label indexes and y label indexes
    plt.xticks(ind, jobList)
    plt.yticks(np.arange(0, 100, 10))
    ```

    最后一步是定义图例，并将轴标签旋转到`90`度。该图最终显示为:

    ```
    # Defining the legends
    plt.legend((p1[0], p2[0]), ('Yes', 'No'))
    # To rotate the axis labels 
    plt.xticks(rotation=90)
    plt.show()
    ```

以下是基于前面示例的堆积条形图的外观:

![Figure 3.7: Example of a stacked bar plot
](image/C15019_03_07.jpg)

图 3.7:堆积条形图示例

让我们在下面的练习和活动中使用这些图表。

## 练习 3.02:年龄与定期贷款倾向的业务假设测试

本练习的目标是定义一个假设，以检查个人购买定期存款计划的倾向与年龄的关系。在本练习中，我们将使用折线图。

以下步骤将帮助您完成本练习:

1.  Begin by defining the hypothesis.

    验证过程的第一步是定义一个关系假设。假设可以基于你的经验、领域知识、一些已发表的知识或者你的商业直觉。

    让我们首先定义我们关于年龄和购买定期存款倾向的假设:

    *与年轻客户相比，老年客户更倾向于购买定期存款*。这是我们的假设。

    现在我们已经定义了我们的假设，是时候用数据来验证它的真实性了。从数据中获得商业直觉的最佳方式之一是获取数据的横截面并将其可视化。

2.  Import the pandas and altair packages

    ```
        import pandas as pd
        import altair as alt
    ```

    ![Figure 3.8: Installing the necessary packages
    ](image/C15019_03_08.jpg)

    图 3.8:安装必要的包

3.  Next, you need to load the dataset, just like you loaded the dataset in *Exercise 3.01*, *Loading and Exploring the Data from the Dataset*:

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter03/bank-full.csv'
    bankData = pd.read_csv(file_url, sep=";")
    ```

    注意

    在本章的以下练习中，将重复步骤 2-3 。

    我们将核实购买的定期存款是如何按年龄分布的。

4.  Next, we will count the number of records for each age group. We will be using the combination of `.groupby()`, `.agg()`, `.reset_index()` methods from `pandas`.

    注意

    你将在第 12 章、*特征工程*中看到这些方法的更多细节。

    ```
    filter_mask = bankData['y'] == 'yes'
    bankSub1 = bankData[filter_mask].groupby('age')['y'].agg(agegrp='count').reset_index()
    ```

    我们首先获取我们在*练习 3.01* 、*中加载的Pandas`DataFrame`、`bankData`，加载并浏览来自数据集*的数据，然后使用掩码`bankData['y'] == 'yes'`过滤所有定期存款为“是”的情况。这些病例通过`groupby()`方法进行分组，然后通过`agg()`方法根据年龄进行汇总。最后，我们需要使用`.reset_index()`来获得一个结构良好的数据帧，该数据帧将被存储在一个名为`bankSub1`的新的`DataFrame`中。

5.  Now, plot a line chart using altair and the `.Chart().mark_line().encode()` methods and we will define the `x` and `y` variables, as shown in the following code snippet:

    ```
    # Visualising the relationship using altair
    alt.Chart(bankSub1).mark_line().encode(x='age', y='agegrp')
    ```

    您应该得到以下输出:

    ![Figure 3.9: Relationship between age and propensity to purchase
    ](image/C15019_03_09.jpg)

    图 3.9:年龄和购买倾向之间的关系

    从图中我们可以看出，购买定期存款数量最多的是年龄在 25 岁至 40 岁之间的客户，购买倾向随着年龄的增长而逐渐减弱。

    这种关系与我们在假设中的假设是非常违反直觉的，对吗？但是，等一下，我们是不是忽略了重要的一点？我们根据每个年龄段的顾客绝对数量来获取数据。如果银行客户的比例在 25 岁到 40 岁之间更高，那么我们很可能会得到一个像这样的图。我们真正应该绘制的是每个年龄组中购买定期存款的客户比例。

    让我们看看如何通过客户的比例来表示数据。就像您在前面的步骤中所做的那样，我们将根据年龄汇总客户倾向，然后将每个类别的购买倾向除以该年龄组的客户总数，以获得比例。

6.  使用`groupby()`方法对每个年龄组的数据进行分组，并使用`agg()`方法:

    ```
    # Getting another perspective
    ageTot = bankData.groupby('age')['y'].agg(ageTot='count').reset_index()
    ageTot.head()
    ```

    找到每个年龄组的客户总数
7.  现在，根据年龄和购买倾向对数据进行分组，并找出每个倾向类别下的总计数，即`yes`和`no` :

    ```
    # Getting all the details in one place
    ageProp = bankData.groupby(['age','y'])['y'].agg(ageCat='count').reset_index()
    ageProp.head()
    ```

8.  使用`pd.merge()`函数基于`age`变量合并这两个数据框架，然后将每个年龄组内的每个倾向类别除以相应年龄组内的总客户数，以获得客户的比例，如下面的代码片段所示:

    ```
    # Merging both the data frames
    ageComb = pd.merge(ageProp, ageTot,left_on = ['age'], right_on = ['age'])
    ageComb['catProp'] = (ageComb.ageCat/ageComb.ageTot)*100
    ageComb.head()
    ```

9.  Now, display the proportion where you plot both categories (yes and no) as separate plots. This can be achieved through a method within `altair` called `facet()`:

    ```
    # Visualising the relationship using altair
    alt.Chart(ageComb).mark_line().encode(x='age', y='catProp').facet(column='y')
    ```

    该函数可以绘制变量中有多少个类别就绘制多少个图。这里，我们给`'y'`变量，它是`facet()`函数的`yes`和`no`类别的变量名，我们得到两个不同的图:一个用于`yes`，另一个用于`no`。

    您应该得到以下输出:

    ![Figure 3.10: Visualizing normalized relationships
    ](image/C15019_03_10.jpg)

图 3.10:可视化规范化关系

在本练习结束时，你能够得到两个有意义的图表，显示人们购买定期存款计划的倾向。本练习的最终结果是*图 3.10* ，显示了两张图，左图显示了不购买定期存款的人的比例，右图显示了购买定期存款的客户。

我们可以看到，在第一张图中，从`22`到`60`的年龄组，个人不会倾向于购买定期存款。然而，在第二张图中，我们看到了相反的情况，`60`及以上的年龄组更倾向于购买定期存款计划。

在接下来的部分，我们将开始根据我们的直觉来分析我们的情节。

## 来自探索性分析的直觉

我们可以从迄今为止所做的练习中获得什么样的直觉？我们看到了拿用户比例和不拿比例的两个对比图。如您所见，获取用户比例是获得我们必须查看数据的正确视角的正确方法。这更符合我们进化出来的假设。从图中我们可以看出，从`22`到`60`左右的年龄段，购买定期存款的倾向较低。

`60`之后，我们看到定期存款需求呈上升趋势。我们可以观察到的另一个有趣的事实是，年龄小于`20`的人购买定期存款的比例更高。

在*练习 3.02* 、*中，我们发现了如何发展我们的假设，然后使用 EDA 验证该假设。在接下来的部分中，我们将深入研究旅程中的另一个重要步骤，特性工程。*

## 活动 3.01:业务假设测试，找出就业状况与定期存款倾向

你是一家银行的数据科学家。银行管理层向你提供了历史数据，并要求你尝试在就业状况和购买定期存款的倾向之间建立一个假设。

在*练习 3.02* 、*关于年龄与定期贷款倾向的业务假设测试*中，我们研究了一个问题，以找出年龄与购买定期存款倾向之间的关系。在本活动中，我们将使用类似的方法，验证就业状况和定期存款购买倾向之间的关系。

步骤如下:

1.  阐明就业状况和定期存款倾向之间的假设。假设如下:*高薪员工比其他类别员工更喜欢定期存款*。
2.  打开一个类似于在*练习 3.02* 、*中使用的 Colab 笔记本文件，测试年龄与定期贷款倾向*，安装并导入必要的库，如`pandas`和`altair`。
3.  From the banking DataFrame, `bankData`, find the distribution of employment status using the `.groupby()`, `.agg()` and `.reset_index()` methods.

    使用`.groupby()`方法对有关就业状态的数据进行分组，并使用`.agg()`方法找到每种就业状态的倾向总数。

4.  现在，使用`pd.merge()`函数合并两个数据框架，然后通过计算每种就业状态的倾向比例来计算倾向计数。当创建新变量来寻找倾向比例时。
5.  Plot the data and summarize intuitions from the plot using `matplotlib`. Use the stacked bar chart for this activity.

    注意

    本活动中使用的`bank-full.csv`数据集可在[https://packt.live/2Wav1nJ](https://packt.live/2Wav1nJ)找到。

预期输出:相对于就业状况的购买倾向的最终曲线将类似于下图:

![Figure 3.11: Visualizing propensity of purchase by job
](image/C15019_03_11.jpg)

图 3.11:按工作可视化购买倾向

注意

这个活动的解决方案可以在以下地址找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

现在我们已经看到了 EDA，让我们深入特性工程。

# 特色工程

在上一节中，我们介绍了 EDA 的过程。作为早期过程的一部分，我们通过对数据进行切片和可视化来测试我们的业务假设。你可能想知道我们将在哪里使用我们从所有分析中得到的直觉。这个问题的答案将在本节中讨论。

特征工程是将原始变量转化为新变量的过程，这将在本章后面介绍。特征工程是影响我们建立的模型的准确性的最重要的步骤之一。

有两大类特征工程:

1.  在这里，我们从商业角度基于直觉转换原始变量。这些直觉是我们在探索性分析中建立的。
2.  原始变量的转换是从统计和数据规范化的角度进行的。

接下来，我们将研究每种类型的特性工程。

注意

特征工程将在*第 12 章*、*特征工程*和*第 17 章*、*自动化特征工程*中详细介绍。在本节中，您将看到学习分类的目的。

## 业务驱动的特征工程

业务驱动的特征工程是基于在探索性分析期间获得的业务直觉来转换原始变量的过程。它需要根据影响业务问题的业务因素或驱动因素来转换数据和创建新的变量。

在前面的探索性分析练习中，我们探讨了单个变量与因变量之间的关系。在本练习中，我们将组合多个变量，然后派生出新的特征。我们将探讨资产组合和定期存款购买倾向之间的关系。资产组合是客户在银行拥有的所有资产和负债的组合。我们将结合资产和负债，如银行余额、房屋所有权和贷款，得到一个新的特性，称为资产指数。

这些特征工程步骤将分为两个练习。在*练习 3.03* 、*特征工程——探索个体特征*中，我们探索了诸如余额、住房和贷款等个体变量，以了解它们与定期存款倾向的关系。

在*练习 3.04* 、*从现有特征创建新特征*中，我们将转换单个变量，然后将它们组合成一个新特征。

## 练习 3.03:特征工程——探索个体特征

在本练习中，我们将探讨两个变量(个人是否拥有房屋和个人是否有贷款)与这些个人购买定期存款的倾向之间的关系。

以下步骤将帮助您完成本练习:

1.  打开新的 Colab 笔记本。
2.  导入`pandas`包。

    ```
    import pandas as pd
    ```

3.  将数据集的链接分配给一个名为`file_url` :

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter03/bank-full.csv'
    ```

    的变量
4.  使用`.read_csv()`函数读取银行数据集:

    ```
    # Reading the banking data
    bankData = pd.read_csv(file_url, sep=";")
    ```

5.  Next, we will find a relationship between housing and the propensity for term deposits, as mentioned in the following code snippet:

    ```
    # Relationship between housing and propensity for term deposits
    bankData.groupby(['housing', 'y'])['y'].agg(houseTot='count').reset_index()
    ```

    您应该得到以下输出:

    ![Figure 3.12: Housing status versus propensity to buy term deposits
    ](image/C15019_03_12.jpg)

    图 3.12:住房状况与购买定期存款的倾向

    代码的第一部分是根据客户是否拥有房子对他们进行分组。使用`.agg()`方法计算每个类别下的客户数量。从数值上我们可以看出，没有房子的人比有房子的人购买定期存款的倾向要高得多:`( 3354 / ( 3354 + 16727) = 17% to 1935 / ( 1935 + 23195) = 8%)`。

6.  Explore the `'loan'` variable to find its relationship with the propensity for a term deposit, as mentioned in the following code snippet:

    ```
    # Relationship between having a loan and propensity for term deposits
    bankData.groupby(['loan', 'y'])['y'].agg(loanTot='count').reset_index()
    ```

    您应该得到以下输出:

    ![Figure 3.13: Loan versus term deposit propensity
    ](image/C15019_03_13.jpg)

    图 3.13:贷款与定期存款倾向

    在贷款组合的情况下，没有贷款的客户购买定期存款的倾向更高:`( 4805 / ( 4805 + 33162) = 12 % to 484/ ( 484 + 6760) = 6%)`。

    住房和贷款是分类数据，寻找关系很简单。然而，银行余额数据是数值型的，为了分析它，我们需要有不同的策略。一种常见的策略是将连续的数字数据转换为有序数据，并查看每个类别的倾向如何变化。

7.  To convert numerical values into ordinal values, we first find the quantile values and take them as threshold values. The quantiles are obtained using the following code snippet:

    ```
    #Taking the quantiles for 25%, 50% and 75% of the balance data
    import numpy as np
    np.quantile(bankData['balance'],[0.25,0.5,0.75])
    ```

    您应该得到以下输出:

    ![Figure 3.14: Quantiles for bank balance data
    ](image/C15019_03_14.jpg)

    图 3.14:银行余额数据的分位数

    分位数值代表数据分布的某些阈值。例如，当我们说第 25 个百分位数时，我们谈论的是低于该值的数据的值。可以使用 NumPy 中的`np.quantile()`函数计算分位数。在*步骤 4* 的代码片段中，我们计算了第 25、50 和 75 个百分点，结果是`72`、`448`和`1428`。

8.  Now, convert the numerical values of bank balances into categorical values, as mentioned in the following code snippet:

    ```
    bankData['balanceClass'] = 'Quant1'
    bankData.loc[(bankData['balance'] > 72) & (bankData['balance'] < 448), 'balanceClass'] = 'Quant2'
    bankData.loc[(bankData['balance'] > 448) & (bankData['balance'] < 1428), 'balanceClass'] = 'Quant3'
    bankData.loc[bankData['balance'] > 1428, 'balanceClass'] = 'Quant4'
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 3.15: New features from bank balance data
    ](image/C15019_03_15.jpg)

    图 3.15:银行余额数据的新特性

    我们这样做是通过查看我们在*步骤 4* 中获取的分位数阈值，并将数字数据归类到相应的分位数类别中。例如，所有低于第 25 个分位数值 72 的值都被归类为`Quant1`，介于 72 和 448 之间的值被归类为`Quant2`，依此类推。为了存储分位数类别，我们在银行数据集中创建了一个名为`balanceClass`的新特性，并将其默认值设置为`Quan1`。此后，基于每个阈值，数据点被分类到各自的分位数类别。

9.  Next, we need to find the propensity of term deposit purchases based on each quantile the customers fall into. This task is similar to what we did in *Exercise 3.02*, *Business Hypothesis Testing for Age versus Propensity for a Term Loan*:

    ```
    # Calculating the customers under each quantile 
    balanceTot = bankData.groupby(['balanceClass'])['y'].agg(balanceTot='count').reset_index()
    balanceTot
    ```

    您应该得到以下输出:

    ![Figure 3:16: Classification based on quantiles
    ](image/C15019_03_16.jpg)

    图 3:16:基于分位数的分类

10.  Calculate the total number of customers categorized by quantile and propensity classification, as mentioned in the following code snippet:

    ```
    # Calculating the total customers categorised as per quantile and propensity classification
    balanceProp = bankData.groupby(['balanceClass', 'y'])['y'].agg(balanceCat='count').reset_index()
    balanceProp
    ```

    您应该得到以下输出:

    ![Figure 3.17: Total number of customers categorized by quantile and propensity classification
    ](image/C15019_03_17.jpg)

    图 3.17:按分位数和倾向分类的客户总数

11.  Now, `merge` both DataFrames:

    ```
    # Merging both the data frames
    balanceComb = pd.merge(balanceProp, balanceTot, on = ['balanceClass'])
    balanceComb['catProp'] = (balanceComb.balanceCat / balanceComb.balanceTot)*100
    balanceComb
    ```

    您应该得到以下输出:

    ![Figure 3.18: Propensity versus balance category
    ](image/C15019_03_18.jpg)

图 3.18:倾向与平衡类别

从数据分布可以看出，从分位数 1 到分位数 4，购买定期存款的客户比例不断增加。例如，在所有属于`Quant 1`的客户中，有 7.25%购买了定期存款(我们从`catProp`得到这个百分比)。这个比例在`Quant 2`上升到 10.87 %，之后在`Quant 3`和`Quant4`分别上升到 12.52 %和 16.15%。从这一趋势中，我们可以得出结论，余额较高的个人更倾向于定期存款。

在本练习中，我们探讨了每个变量与定期存款购买倾向之间的关系。我们可以观察到的总体趋势是，手里有更多现金的人(无贷款且余额较高)更倾向于购买定期存款。在下一个练习中，我们将使用这些直觉来推导一个新特征。

## 练习 3.04:特征工程——从现有特征中创造新特征

在本练习中，我们将结合我们在*练习 3.03* 、*特征工程-探索单个特征*中分析的单个变量，得出一个称为资产指数的新特征。创建资产指数的一种方法是根据客户的资产或负债分配权重。

例如，较高的银行存款余额或房屋所有权将对整体资产指数产生积极影响，因此将被赋予较高的权重。相比之下，贷款的存在将是一种负债，因此，将不得不具有较低的权重。假设客户有房子，权重为 5，没有房子，权重为 1。类似地，如果客户有贷款，我们可以给权重 1，如果没有贷款，我们可以给权重 5:

1.  打开新的 Colab 笔记本。
2.  导入Pandas和 numpy 包:

    ```
    import pandas as pd
    import numpy as np
    ```

3.  将指向数据集的链接赋给一个名为“file_url”的变量。

    ```
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter03/bank-full.csv'
    ```

4.  使用`.read_csv()`函数读取银行数据集:

    ```
    # Reading the banking data
    bankData = pd.read_csv(filename,sep=";")
    ```

5.  我们将遵循的第一步是标准化数值变量。这是使用下面的代码片段实现的:

    ```
    # Normalizing data
    from sklearn import preprocessing
    x = bankData[['balance']].values.astype(float)
    ```

6.  接下来，创建缩放函数:

    ```
    minmaxScaler = preprocessing.MinMaxScaler()
    ```

7.  通过用`minmaxScaler` :

    ```
    bankData['balanceTran'] = minmaxScaler.fit_transform(x)
    ```

    标准化来转换余额数据
8.  Print the head of the data using the `.head()` function:

    ```
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 3.19: Normalizing the bank balance data
    ](image/C15019_03_19.jpg)

    图 3.19:规范化银行余额数据

    对于包含数值的银行余额数据集，我们需要首先对数据进行规范化。归一化的目的是将我们用来创建新要素的所有变量纳入一个通用范围。我们可以在这里使用的一个有效的方法叫做`MinMaxScaler()`，它将所有的数字数据在 0 到 1 的范围内转换。`MinMaxScaler`函数在`sklearn`中的`preprocessing`方法中可用。在这一步中，我们创建了一个名为`'balanceTran'`的新特性来存储规范化的银行余额值。

9.  After creating the normalized variable, add a small value of `0.001` so as to eliminate the 0 values in the variable. This is mentioned in the following code snippet:

    ```
    # Adding a small numerical constant to eliminate 0 values
    bankData['balanceTran'] = bankData['balanceTran'] + 0.00001
    ```

    添加这个小值的目的是因为，在后续步骤中，我们将把三个转换后的变量相乘，形成一个综合指数。添加小值是为了避免变量值在乘法运算过程中变为 0。

10.  Now, add two additional columns for introducing the transformed variables for loans and housing, as per the weighting approach discussed at the start of this exercise:

    ```
    # Let us transform values for loan data
    bankData['loanTran'] = 1
    # Giving a weight of 5 if there is no loan
    bankData.loc[bankData['loan'] == 'no', 'loanTran'] = 5
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 3.20: Additional columns with the transformed variables
    ](image/C15019_03_20.jpg)

    图 3.20:带有转换变量的附加列

    我们根据加权方法转换了贷款数据的值。当客户有贷款时，它被赋予一个权重`1`，当没有贷款时，被赋予的权重是`5`。`1`和`5`的值是我们正在分配的直观权重。我们分配的价值会根据您可能获得的业务环境而有所不同。

11.  现在，转换`Housing data`的值，就像这里提到的:

    ```
    # Let us transform values for Housing data
    bankData['houseTran'] = 5
    ```

12.  Give a weight of `1` if the customer has a house and print the results, as mentioned in the following code snippet:

    ```
    bankData.loc[bankData['housing'] == 'no', 'houseTran'] = 1
    print(bankData.head())
    ```

    您应该得到以下输出:

    ![Figure 3.21: Transforming loan and housing data
    ](image/C15019_03_21.jpg)

    图 3.21:转换贷款和住房数据

    一旦创建了所有转换后的变量，我们可以将所有转换后的变量相乘来创建一个新的索引`assetIndex`。这是一个综合指数，代表了所有三个变量的综合影响。

13.  Now, create a new variable, which is the product of all of the transformed variables:

    ```
    # Let us now create the new variable which is a product of all these
    bankData['assetIndex'] = bankData['balanceTran'] * bankData['loanTran'] * bankData['houseTran']
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 3.22: Creating a composite index
    ](image/C15019_03_22.jpg)

    图 3.22:创建复合索引

14.  Explore the propensity with respect to the composite index.

    我们观察了资产指数和定期存款购买倾向之间的关系。我们采用类似的策略，通过取分位数，然后将分位数映射到定期存款购买倾向，将资产指数的数值转换为序数值，如*练习 3.03* 、*特征工程-探索个别特征*中所述:

    ```
    # Finding the quantile
    np.quantile(bankData['assetIndex'],[0.25,0.5,0.75])
    ```

    您应该得到以下输出:

    ![Figure 3.23: Conversion of numerical values into ordinal values
    ](image/C15019_03_23.jpg)

    图 3.23:数值到序数值的转换

15.  Next, create quantiles from the `assetindex` data, as mentioned in the following code snippet:

    ```
    bankData['assetClass'] = 'Quant1'
    bankData.loc[(bankData['assetIndex'] > 0.38) & (bankData['assetIndex'] < 0.57), 'assetClass'] = 'Quant2'
    bankData.loc[(bankData['assetIndex'] > 0.57) & (bankData['assetIndex'] < 1.9), 'assetClass'] = 'Quant3'
    bankData.loc[bankData['assetIndex'] > 1.9, 'assetClass'] = 'Quant4'
    bankData.head()
    bankData.assetClass[bankData['assetIndex'] > 1.9] = 'Quant4'
    bankData.head()
    ```

    您应该得到以下输出:

    ![Figure 3.24: Quantiles for the asset index
    ](image/C15019_03_24.jpg)

    图 3.24:资产指数的分位数

16.  计算每个资产类别的总数和类别计数，如下面的代码片段所示:

    ```
    # Calculating total of each asset class
    assetTot = bankData.groupby('assetClass')['y'].agg(assetTot='count').reset_index()
    # Calculating the category wise counts
    assetProp = bankData.groupby(['assetClass', 'y'])['y'].agg(assetCat='count').reset_index()
    ```

17.  Next, merge both DataFrames:

    ```
    # Merging both the data frames
    assetComb = pd.merge(assetProp, assetTot, on = ['assetClass'])
    assetComb['catProp'] = (assetComb.assetCat / assetComb.assetTot)*100
    assetComb
    ```

    您应该得到以下输出:

    ![Figure 3.25: Composite index relationship mapping
    ](image/C15019_03_25.jpg)

图 3.25:综合指数关系映射

从我们创建的新功能中，我们可以看到在`Quant2`的客户中有 18.88%(我们从`catProp`获得该百分比)购买了定期存款，相比之下`Quant1`的客户为 10.57 %，`Quant3`的客户为 8.78%，而`Quant4`的客户为 9.28%。由于`Quant2`购买定期存款的客户比例最高，因此我们可以得出结论，`Quant2`的客户购买定期存款的倾向高于所有其他客户。

与我们刚刚完成的练习类似，您应该根据业务直觉，考虑可以从现有变量中创建的新变量。基于商业直觉创造新的特征是商业驱动的特征工程的本质。在下一节中，我们将看看另一种类型的特征工程，称为数据驱动的特征工程。

# 数据驱动的特征工程

前一节讨论了业务驱动的特性工程。除了我们可以从业务角度获得的特性之外，从数据结构的角度通过特性工程来转换数据也是必要的。我们将研究识别数据结构的不同方法，并了解一些数据转换技术。

## 快速浏览数据类型和描述性摘要

查看数据类型，如分类或数值，然后导出汇总统计数据，这是在执行一些下游特征工程步骤之前快速浏览数据的好方法。让我们来看看数据集中的一个例子:

```
# Looking at Data types
print(bankData.dtypes)
# Looking at descriptive statistics
print(bankData.describe())
```

您应该得到以下输出:

![Figure 3.26: Output showing the different data types in the dataset
](image/C15019_03_26.jpg)

图 3.26:显示数据集中不同数据类型的输出

在前面的输出中，您可以看到数据集中不同类型的信息及其对应的数据类型。例如，`age`是整数，`day`也是整数。

下面的输出是一个描述性的汇总统计，它显示了一些基本的度量，如各个特征的`mean`、`standard deviation`、`count`和`quantile values`:

![Figure 3.27: Data types and a descriptive summary
](image/C15019_03_27.jpg)

图 3.27:数据类型和描述性摘要

描述性汇总的目的是快速了解数据的分布和一些基本统计数据，如平均值和标准差。了解汇总统计数据对于思考每个变量需要哪种转换是至关重要的。

例如，在前面的练习中，我们根据分位数值将数值数据转换为分类变量。转换变量的直觉来自于我们可以从数据集得到的快速汇总统计数据。

在接下来的章节中，我们将会看到相关矩阵和可视化。

# 相关矩阵和可视化

如你所知，相关性是一种衡量两个变量如何一起波动的方法。任何相关值为 1 或接近 1，表明这些变量高度相关。高度相关的变量有时会损害模型的准确性，在许多情况下，我们会决定消除这些变量，或者将它们组合起来形成复合变量或交互变量。

在下面的练习中，我们来看看如何生成数据关联，然后可视化。

## 练习 3.05:找出数据中的相关性，使用银行数据生成相关图

在本练习中，我们将创建一个关联图并分析银行数据集的结果。

以下步骤将帮助您完成练习:

1.  打开一个新的 Colab 笔记本，安装`pandas`包，加载银行数据:

    ```
    import pandas as pd
    file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter03/bank-full.csv'
    bankData = pd.read_csv(file_url, sep=";")
    ```

2.  Now, `import` the `set_option` library from `pandas`, as mentioned here:

    ```
    from pandas import set_option
    ```

    `set_option`功能用于定义许多操作的显示选项。

3.  接下来，创建一个存储数字变量的变量，比如`'age','balance','day','duration','campaign','pdays','previous'`，如下面的代码片段所示。相关图只能用数字数据提取。这就是为什么数字数据必须单独提取:

    ```
    bankNumeric = bankData[['age','balance','day','duration','campaign','pdays','previous']]
    ```

4.  Now, use the `.corr()` function to find the correlation matrix for the dataset:

    ```
    set_option('display.width',150)
    set_option('precision',3)
    bankCorr = bankNumeric.corr(method = 'pearson')
    bankCorr
    ```

    您应该得到以下输出:

    ![Figure 3.28: Correlation matrix
    ](image/C15019_03_28.jpg)

    图 3.28:相关矩阵

    我们用于相关的方法是**皮尔逊**相关系数。我们可以从相关矩阵中看到，对角线元素的相关性为 1。这是因为对角线是一个变量与其自身的相关性，它总是为 1。这是皮尔逊相关系数。

5.  Now, plot the data:

    ```
    from matplotlib import pyplot
    corFig = pyplot.figure()
    figAxis = corFig.add_subplot(111)
    corAx = figAxis.matshow(bankCorr,vmin=-1,vmax=1)
    corFig.colorbar(corAx)
    pyplot.show()
    ```

    您应该得到以下输出:

    ![Figure 3.29: Correlation plot
    ](image/C15019_03_29.jpg)

图 3.29:相关图

我们在这个代码块中使用了许多绘图参数。`pyplot.figure()`是被实例化的绘图类。`.add_subplot()`是用于绘图的网格参数。例如，111 表示第一个子情节的 1 x 1 网格。`.matshow()`功能是显示绘图，而`vmin`和`vmax`参数用于标准化绘图中的数据。

让我们看一下相关矩阵图，以便更快地识别相关变量。一些明显的候选者是`'balance'`和`'balanceTran'`之间的高度相关性，以及`'asset index'`与我们在前面的练习中创建的许多转换变量的高度相关性。除此之外，没有多少变量是高度相关的。

在本练习中，我们开发了一个相关性图，使我们能够可视化变量之间的相关性。

## 数据的偏斜度

特征工程的另一个领域是偏斜度。偏斜数据是指向一个方向或另一个方向移动的数据。偏斜会导致机器学习模型表现不佳。许多机器学习模型假设正态分布的数据或数据结构遵循高斯结构。与假设的高斯结构(流行的钟形曲线)的任何偏差都会影响模型性能。我们可以应用特征工程的一个非常有效的领域是查看数据的偏斜度，然后通过数据的标准化来纠正偏斜度。可以通过使用直方图和密度图绘制数据来可视化偏斜度。我们将研究每一种技术。

让我们看看下面的例子。这里，我们使用`.skew()`函数来寻找数据的偏斜度。例如，为了找到我们的`bank-full.csv`数据集中数据的偏斜度，我们执行以下操作:

```
# Skewness of numeric attributes
bankNumeric.skew()
```

您应该得到以下输出:

![Figure 3.30: Degree of skewness 
](image/C15019_03_30.jpg)

图 3.30:偏斜度

前面的矩阵是偏斜度指数。任何接近 0 的值都表示偏斜度较低。正值表示向右倾斜，负值表示向左倾斜。显示较高左右倾斜值的变量是通过归一化进行进一步特征工程的候选变量。现在让我们通过绘制直方图和密度图来可视化偏斜度。

## 直方图

直方图是绘制数据分布图和识别数据偏斜度(如果有的话)的有效方法。这里列出了两列`bankData`的直方图输出。使用`.hist()`功能，用来自`matplotlib`的`pyplot`包绘制直方图。我们想要包含的支线剧情数量由`.subplots()`功能控制。`(1,2)`在支线剧情中将意味着一行两列。标题由`set_title()`功能设置:

```
# Histograms
from matplotlib import pyplot
fig, axs = plt.subplots(1,2)
axs[0].hist(bankNumeric['age'])
axs[0].set_title('Distribution of age')
axs[1].hist(bankNumeric['assetIndex'])
axs[1].set_title('Distribution of Asset Index')
```

您应该得到以下输出:

![Figure 3.31: Code showing the generation of histograms
](image/C15019_03_31.jpg)

图 3.31:显示直方图生成的代码

从直方图中，我们可以看到`age`变量的分布更接近钟形曲线，偏斜度更低。相比之下，资产指数显示了相对较高的右偏，这使得它更有可能成为归一化的候选对象。

## 密度图

密度图有助于可视化数据的分布。可使用`kind = 'density'`参数创建密度图:

```
# Density plots
bankNumeric['age'].plot(kind = 'density',subplots = False,layout = (1,1))
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Normalised age distribution')
pyplot.show()
```

您应该得到以下输出:

![Figure 3.32: Code showing the generation of a density plot
](image/C15019_03_32.jpg)

图 3.32:显示密度图生成的代码

密度图有助于更平滑地显示数据分布。从年龄的密度图可以看出，它有一个类似于钟形曲线的分布。

## 其他特征工程方法

到目前为止，我们看到了各种描述性统计和可视化，它们是在数据结构上应用许多特征工程技术的先驱。我们在*练习 3.02* 、*中研究了一种这样的特征工程技术，即针对年龄与定期贷款倾向的业务假设测试*，其中我们应用了**最小最大**定标器来标准化数据。

我们现在将研究另外两种类似的数据转换技术，即标准定标器和规格化器。标准定标器将数据标准化为平均值 0 和标准差 1。平均值是数据的平均值，标准偏差是数据分布的量度。通过标准化到相同的平均值和标准偏差，可以对不同分布的数据进行比较。

规格化函数规格化数据的长度。这意味着行中的每个值都除以行向量的规范化，以规范化该行。规格化函数应用于行，而标准定标器应用于列。规格化器和标准缩放器功能是重要的特征工程步骤，在下游建模步骤之前应用于数据。让我们来看看这两种技术:

```
# Standardize data ( 0 mean, 1 stdev)
from sklearn.preprocessing import StandardScaler
from numpy import set_printoptions
scaling = StandardScaler().fit(bankNumeric)
rescaledNum = scaling.transform(bankNumeric)
set_printoptions(precision = 3)
print(rescaledNum)
```

您应该得到以下输出:

![Figure 3.33: Output from standardizing the data
](image/C15019_03_33.jpg)

图 3.33:标准化数据的输出

以下代码使用规范化器数据传输技术:

```
# Normalizing Data ( Length of 1)
from sklearn.preprocessing import Normalizer
normaliser = Normalizer().fit(bankNumeric)
normalisedNum = normaliser.transform(bankNumeric)
set_printoptions(precision = 3)
print(normalisedNum)
```

您应该得到以下输出:

![Figure 3.34: Output by the normalizer
](image/C15019_03_34.jpg)

图 3.34:规格化器的输出

标准缩放器的输出沿列归一化。输出将有 11 列，对应于 11 个数字列(年龄、余额、日期、持续时间等)。如果我们观察输出，我们可以看到一列中的每个值都被归一化，平均值为 0，标准偏差为 1。通过以这种方式转换数据，我们可以轻松地跨列进行比较。

例如，在`age`变量中，我们的数据范围从 18 到 95。相比之下，对于余额数据，我们的数据范围从-8，019 到 102，127。我们可以看到，这两个变量的数据范围不同，无法进行比较。标准定标器功能将这些不同比例的数据点转换为一个通用比例，以便比较数据的分布。规格化器重新缩放每一行，以得到长度为 1 的向量。

我们必须思考的大问题是，为什么我们必须标准化或规范化数据？当特征具有相似的规模或正态分布时，许多机器学习算法收敛得更快。标准化在假设输入变量具有高斯结构的算法中更有用。线性回归、逻辑回归和线性判别分析等算法都属于这种类型。当使用诸如 k-最近邻或神经网络之类的算法时，归一化技术将更适合于稀疏数据集(具有许多零的数据集)。

## 总结特征工程

在本节中，我们从业务角度和数据结构角度研究了特性工程的过程。特征工程是数据科学项目生命周期中非常重要的一步，有助于确定我们构建的模型的准确性。正如在*练习 3.02* 、*中看到的，针对年龄与定期贷款倾向的业务假设测试*我们将我们对领域的理解和直觉转化为构建智能特征。让我们总结一下我们遵循的流程:

1.  我们通过 EDA 从商业角度获得直觉
2.  基于商业直觉，我们设计了一个新功能，它是其他三个变量的组合。
3.  我们验证了新功能的组成变量的影响，并设计了一种应用权重的方法。
4.  将顺序数据转换成相应的权重。
5.  通过使用适当的规格化器对数据进行规格化来转换数据。
6.  将所有三个变量组合成一个新特征。
7.  观察综合指数和购买定期存款倾向之间的关系，并得出我们的直觉。
8.  探索从数据中可视化和提取汇总统计信息的技术。
9.  确定将数据转换为特征工程数据结构的技术。

现在我们已经完成了特性工程步骤，下一个问题是我们从这里去哪里，我们创建的新特性的相关性是什么？正如您将在后续部分中看到的，我们创建的新特性将用于建模过程。前面的练习是我们在创建新要素时可以遵循的轨迹示例。将会有多个这样的踪迹，这应该被认为是基于更多的领域知识和理解。我们建立的模型的准确性将依赖于我们通过将商业知识转化为数据而建立的所有这些智能特征。

## 使用逻辑回归函数构建二元分类模型

数据科学的本质是将业务问题映射到其数据元素中，然后转换这些数据元素以获得我们想要的业务成果。在前面的小节中，我们讨论了如何对数据元素进行必要的转换。数据元素的正确转换可以极大地影响下游建模过程生成正确的业务结果。

让我们从用例的角度来看一下业务成果生成过程。在我们的用例中，期望的业务结果是识别那些可能购买定期存款的客户。为了正确识别哪些客户可能会购买定期存款，我们首先需要了解客户身上有助于识别过程的特质或特征。这种对特征的学习是通过机器学习实现的。

到现在，你可能已经意识到机器学习的目标是估计一个输出变量和输入变量之间的映射函数( *f* )。在数学形式中，这可以写成如下形式:

![Figure 3.35: A mapping function in mathematical form
](image/C15019_03_35.jpg)

图 3.35:数学形式的映射函数

让我们从用例的角度来看这个等式。

*Y* 是因变量，是我们对客户是否有可能购买定期存款的预测。

*X* 是独立变量，即年龄、教育和婚姻状况等属性，是数据集的一部分。

*f()* 是一个函数，它将数据的各种属性与客户是否会购买定期存款的概率联系起来。这个函数是在机器学习过程中学习的。该函数是应用于每个属性的不同系数或参数的组合，以获得定期存款购买的概率。让我们用一个简单的银行数据用例来解释这个概念。

为了简单起见，我们假设我们只有两个属性，年龄和银行余额。利用这些，我们必须预测客户是否可能购买定期存款。假设年龄为 40 岁，余额为 1000 美元。对于所有这些属性值，让我们假设映射等式如下:

![Figure 3.36: Updated mapping equation
](image/C15019_03_36.jpg)

图 3.36:更新的映射方程

使用前面的等式，我们得到以下结果:

*Y = 0.1 + 0.4 * 40 + 0.002 * 1000*

*Y = 18.1*

现在，你可能想知道，我们得到了一个真实的数字，这如何代表客户是否会购买定期存款的决定。这就是决策边界概念的由来。我们还假设，在分析数据时，我们还发现，如果 *Y* 的值超过 15(在这种情况下是一个假设值)，那么客户可能会购买定期存款，否则他们不会购买定期存款。这意味着，按照这个例子，客户可能会购买定期存款。

现在让我们看看这个例子中的动力学，并试着解释这些概念。应用于每个属性的诸如 0.1、0.4 和 0.002 的值是系数。这些系数，以及连接系数和变量的方程，是我们从数据中学习的函数。机器学习的本质是从提供的数据中学习所有这些。所有这些系数和函数也可以用另一个通用名称来调用，称为**模型**。模型是数据生成过程的近似。在机器学习期间，我们试图尽可能接近生成我们正在分析的数据的真实模型。为了学习或估计数据生成模型，我们使用不同的机器学习算法。

机器学习模型可以大致分为两种类型，参数模型和非参数模型。在参数模型中，我们假设要学习的函数的形式，然后从训练数据中学习系数。通过假设函数的形式，我们简化了学习过程。

为了更好地理解这个概念，让我们以线性模型为例。对于线性模型，映射函数采用以下形式:

![Figure 3.37: Linear model mapping function
](image/C15019_03_37.jpg)

图 3.37:线性模型映射函数

术语 *C* *0* 、 *M* *1* 和 *M* *2* 是影响直线截距和斜率的直线系数。*X*1 和*X*2 为输入变量。我们在这里所做的是，我们假设数据生成模型是一个线性模型，然后使用数据，我们估计系数，这将使预测的生成成为可能。通过假设数据生成模型，我们简化了整个学习过程。然而，这些简单的过程也伴随着它们的缺陷。只有底层函数是线性的或近似线性的，才会得到好的结果。如果关于形式的假设是错误的，我们必然会得到不好的结果。

参数模型的一些示例包括:

*   线性和逻辑回归
*   朴素贝叶斯
*   线性支持向量机
*   感知器

不对函数进行强假设的机器学习模型称为非参数模型。在没有假设形式的情况下，非参数模型可以自由地从数据中学习任何函数形式。非参数模型通常需要大量的训练数据来估计基础函数。非参数模型的一些示例包括:

*   决策树
*   k–最近邻居
*   神经网络
*   高斯核支持向量机

## 逻辑回归去神秘化

逻辑回归是一种线性模型，类似于前一章中介绍的线性回归。逻辑回归的核心是 sigmoid 函数，它将任何实数值归约为 0 到 1 之间的值，这使得该函数成为预测概率的理想函数。逻辑回归函数的数学方程可以写成如下形式:

![Figure 3.38: Logistic regression function
](image/C15019_03_38.jpg)

图 3.38:逻辑回归函数

这里， *Y* 是客户是否有可能购买定期存款的概率。

术语*C**0**+M**1*** X**1**+M**2*** X**2*与我们在前面章节中看到的线性回归函数非常相似。您现在应该已经知道，线性回归函数会给出实值输出。为了将实值输出转换成概率，我们使用逻辑函数，其具有以下形式:

![Figure 3.39: An equation to transform the real-valued output to a probability 
](image/C15019_03_39.jpg)

图 3.39:将实值输出转换成概率的等式

这里， *e* 是自然对数。我们不会深究这背后的数学原理；但是，让我们认识到，使用逻辑函数，我们可以将实值输出转换为概率函数。

现在让我们从我们试图解决的业务问题来看逻辑回归函数。在业务问题中，我们试图预测客户是否会购买定期存款的概率。要做到这一点，让我们回到我们从问题陈述中得出的例子:

![Figure 3.40: The logistic regression function updated with the business problem statement
](image/C15019_03_40.jpg)

图 3.40:用业务问题陈述更新的逻辑回归函数

将以下值相加，我们得到 *Y = 0.1 + 0.4 * 40 + 0.002 * 100* 。

为了得到概率，我们必须使用逻辑函数转换这个问题陈述，如下所示:

![Figure 3.41: Transformed problem statement to find the probability of using the logistic function
](image/C15019_03_41.jpg)

图 3.41:转换后的问题陈述，找出使用逻辑函数的概率

在应用中，我们得到值 *Y = 1* ，这是客户购买定期存款的 100%概率。如前一示例中所述，模型的系数(如 0.1、0.4 和 0.002)是我们在训练过程中使用逻辑回归算法学习到的。现在让我们看一下逻辑回归函数的实际实现，首先，通过使用银行数据集进行训练，然后使用我们学习的模型进行预测。

## 评估模型性能的指标

作为一名数据科学家，你总是要对你建立的模型做出决定。这些评估是基于预测的各种指标完成的。在本节中，我们将介绍一些用于评估模型性能的重要指标。

注意

模型性能将在*第 6 章*、*如何评估性能*中详细介绍。本节向您介绍如何使用分类模型。

## 混乱矩阵

正如您将了解到的，我们根据模型在测试集上的表现来评估模型。一个测试集将有它的标签，我们称之为基础事实，并且，使用模型，我们也为测试集生成预测。对模型性能的评估就是对实际情况和预测进行比较。让我们用虚拟测试集来看看这一点:

![Figure 3.42: Confusion matrix generation
](image/C15019_03_42.jpg)

图 3.42:混淆矩阵的生成

上表显示了一个包含七个示例的虚拟数据集。第二列是基本事实，是实际的标签，第三列包含我们预测的结果。从数据中，我们可以看到四个被正确分类，三个被错误分类。

混淆矩阵生成预测和实际情况之间的比较结果，如下表所示:

![Figure 3.43: Confusion matrix
](image/C15019_03_43.jpg)

图 3.43:混淆矩阵

从表中可以看出，有五个例子的标签(基本事实)是 `Yes`，其余的是两个标签为 `No`的例子。

混淆矩阵的第一行是标签`Yes`的评估。`True positive`显示那些基础事实和预测为`Yes`的例子(例子 1、3 和 5)。`False negative`显示了那些基础真值为`Yes`且被错误预测为`No`的示例(示例 2 和 7)。

类似地，混淆矩阵的第二行评估标签`No`的性能。`False positive`是那些地面真值为`No`而被错误归类为`Yes`的例子(例 6)。`True negative`例子是那些基础真值和预测都为`No`的例子(例 4)。

混淆矩阵的生成用于计算许多矩阵，例如准确性和分类报告，这将在后面解释。它基于准确性等指标或分类报告中显示的其他详细指标，如精度或召回模型进行测试。我们通常选择这些指标最高的模型。

## 精确度

准确性是评估的第一级，我们将借助它来快速检查模型性能。参考上表，精度可以表示如下:

![Figure 3.44: A function that represents accuracy
](image/C15019_03_44.jpg)

图 3.44:表示精确度的函数

准确性是所有预测中正确预测的比例。

## 分类报告

分类报告输出三个关键指标:精确度、召回率和 F1 分数。

精度是真阳性与真阳性和假阳性之和的比率:

![Figure 3.45: The precision ratio
](image/C15019_03_45.jpg)

图 3.45:精度比

精度是一个指标，它告诉你，在所有预测的阳性中，有多少是真阳性。

召回率是真阳性与真阳性和假阴性之和的比率:

![Figure 3.46: The recall ratio
](image/C15019_03_46.jpg)

图 3.46:召回率

回忆表明了模型识别所有真阳性的能力。

F1 分数是精确度和召回率的加权分数。F1 分数 1 表示最佳性能，0 表示最差性能。

在下一节中，我们来看看数据预处理，这是在数据分析中处理数据并得出结论的重要过程。

## 数据预处理

数据预处理在数据科学项目的生命周期中扮演着重要的角色。这些过程通常是数据科学生命周期中最耗时的部分。预处理步骤的谨慎实施至关重要，并将对数据科学项目的结果产生重大影响。

各种预处理步骤包括:

*   **数据加载**:将不同来源的数据加载到笔记本中。
*   **数据清理**:数据清理过程需要从可用数据集中移除异常，例如特殊字符、重复数据以及缺失数据的识别。数据清理是数据科学过程中最耗时的步骤之一。
*   **数据插补**:数据插补是用新的数据点填充缺失的数据。
*   **Converting data types**: Datasets will have different types of data such as numerical data, categorical data, and character data. Running models will necessitate the transformation of data types.

    注意

    数据处理将在本书后面的章节中深入讨论。

我们将在后续章节和*练习 3.06* 中实现这些预处理步骤。

## 练习 3.06:预测银行定期存款购买倾向的逻辑回归模型

在本练习中，我们将构建一个逻辑回归模型，用于预测定期存款购买倾向。这个练习将有三个部分。第一部分将是数据的预处理，第二部分将处理训练过程，最后一部分将用于预测、指标分析和推导进一步改进模型的策略。

您从数据预处理开始。

在这一部分中，我们将首先加载数据，将序数数据转换为哑数据，然后将数据拆分为训练集和测试集，以用于后续的训练阶段:

1.  像前面的练习一样，打开 Colab 笔记本，安装驱动器，安装必要的软件包，然后加载数据。
2.  现在，加载库函数和数据:

    ```
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    ```

3.  Now, find the data types:

    ```
    bankData.dtypes
    ```

    您应该得到以下输出:

    ![Figure 3.47: Data types
    ](image/C15019_03_47.jpg)

    图 3.47:数据类型

4.  Convert the ordinal data into dummy data.

    正如您在数据集中看到的，我们有两种类型的数据:数字数据和顺序数据。机器学习算法需要数据的数字表示，因此，我们必须通过创建虚拟变量将有序数据转换为数字形式。虚拟变量将具有对应于该类别是否存在的值 1 或 0。我们用来把序数数据转换成数字形式的函数是`pd.get_dummies()`。该函数将数据结构转换为长格式或水平格式。因此，如果一个变量中有三个类别，将有三个新变量被创建为对应于每个类别的虚拟变量。每个变量的值可以是 1 或 0，这取决于该类别是否出现在变量中。让我们来看看这样做的代码:

    ```
    # Converting all the categorical variables to dummy variables
    bankCat = pd.get_dummies(bankData[['job','marital','education','default','housing','loan','contact','month','poutcome']])
    bankCat.shape
    ```

    您应该得到以下输出:

    ```
    (45211, 44)
    ```

    我们现在有了一个新的数据子集，它对应于转换成数字形式的分类数据。此外，我们在原始数据集中有一些数值变量，不需要任何转换。转换后的分类数据和原始数值数据必须结合起来才能得到所有的原始特征。为了将两者结合起来，让我们首先从原始数据帧中提取数值数据。

5.  Now, separate the numerical variables:

    ```
    bankNum = bankData[['age','balance','day','duration','campaign','pdays','previous']]
    bankNum.shape
    ```

    您应该得到以下输出:

    ```
    (45211, 7)
    ```

6.  Now, prepare the `X` and `Y` variables and print the `Y` shape. The `X` variable is the concatenation of the transformed categorical variable and the separated numerical data:

    ```
    # Preparing the X variables
    X = pd.concat([bankCat, bankNum], axis=1)
    print(X.shape)
    # Preparing the Y variable
    Y = bankData['y']
    print(Y.shape)
    X.head()
    ```

    您应该得到以下输出:

    ![Figure 3.48: Combining categorical and numerical DataFrames
    ](image/C15019_03_48.jpg)

    图 3.48:组合分类和数字数据框架

    一旦创建了数据框架，我们就可以将数据分成训练集和测试集。我们指定数据帧必须分成训练集和测试集的比例。

7.  Split the data into training and test sets:

    ```
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)
    ```

    现在，数据已经为建模任务做好了准备。

    接下来，我们从建模开始。

    在这一部分中，我们将使用在上一步中创建的训练集来训练模型。首先，我们调用`logistic regression` 函数，然后用训练集数据拟合模型。

8.  Define the `LogisticRegression` function:

    ```
    bankModel = LogisticRegression()
    bankModel.fit(X_train, y_train)
    ```

    您应该得到以下输出:

9.  ![Figure 3.49: Parameters of the model that fits
    ](image/C15019_03_49.jpg)

    图 3.49:适合的模型参数

10.  Now, that the model is created, use it for predicting on the test sets and then getting the accuracy level of the predictions:

    ```
    pred = bankModel.predict(X_test)
    print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(bankModel.score(X_test, y_test)))
    ```

    您应该得到以下输出:

    ![Figure 3.50: Prediction with the model
    ](image/C15019_03_50.jpg)

    图 3.50:模型预测

11.  From an initial look, an accuracy metric of 90% gives us the impression that the model has done a decent job of approximating the data generating process. Or is it otherwise? Let's take a closer look at the details of the prediction by generating the metrics for the model. We will use two metric-generating functions, the confusion matrix and classification report:

    ```
    # Confusion Matrix for the model
    from sklearn.metrics import confusion_matrix
    confusionMatrix = confusion_matrix(y_test, pred)
    print(confusionMatrix)
    ```

    您应该得到以下格式的输出:但是，由于建模任务会涉及可变性，这些值可能会有所不同:

    ![Figure 3.51: Generation of the confusion matrix
    ](image/C15019_03_51.jpg)

    图 3.51:混淆矩阵的生成

    注意

    您得到的最终结果将与您在这里看到的不同，因为这取决于您使用的系统。这是因为建模部分本质上是随机的，并且总是会有差异。

12.  Next, let's generate a `classification_report`:

    ```
    from sklearn.metrics import classification_report
    print(classification_report(y_test, pred))
    ```

    您应该得到类似的输出；然而，由于建模过程中的可变性而具有不同的值:

![Figure 3.52: Confusion matrix and classification report
](image/C15019_03_52.jpg)

图 3.52:混淆矩阵和分类报告

从指标中，我们可以看到，在总共 11，969 个`no`示例中，11，640 个被正确分类为`no`，其余 329 个被分类为`yes`。这就给出了*11640/11969*的召回值，接近 97%。从精度的角度来看，在预测为`no`的总共 12，682 个例子中，只有 11，640 个是真正的`no`，这使得我们的精度达到 11，640/12，682 或 92%。

然而，`yes`的指标给出了一幅不同的画面。在总共 1595 例`yes`病例中，只有 553 例被正确识别为`yes`。这使我们回忆起*553/1595 = 35%*。精度为 *553 / (553 + 329) = 62%* 。

从整体准确度水平来看，这可以计算如下:正确分类的*个实例/总实例= (11640 + 553) / 13564 = 90%* 。

当你只关注准确性的时候，这些指标可能看起来不错。然而，从细节上看，我们可以看到分类器实际上在分类`yes`案例方面做得很差。该分类器已被训练为主要预测`no`值，这从商业角度来看是无用的。从商业角度来看，我们主要想要`yes`的估计，这样我们就可以针对这些案例进行有针对性的营销，以尝试销售定期存款。然而，就目前的结果而言，我们似乎没有很好地帮助企业增加定期存款销售收入。

在本练习中，我们对数据进行了预处理，然后执行了训练过程，最后，我们找到了有用的预测、指标分析以及进一步改进模型的策略。

我们现在构建的是第一个模型或基准模型。下一步是尝试通过不同的策略来改进基准模型。一个这样的策略是以工程变量为特征，用新的特征建立新的模型。让我们在下一个活动中实现这一目标。

## 活动 3.02:模型迭代 2–采用特征工程变量的逻辑回归模型

作为银行的数据科学家，您创建了一个基准模型来预测哪些客户可能会购买定期存款。然而，管理层希望改进您在基准模型中获得的结果。在*练习 3.04* 、*从现有功能创建新功能*中，您与营销和运营团队讨论了业务场景，并通过功能工程三个原始变量创建了一个新变量`assetIndex`。您现在正在对特征工程变量拟合另一个逻辑回归模型，并试图改进结果。

在本活动中，您将对一些变量进行特征工程设计，以验证它们对预测的影响。

步骤如下:

1.  打开用于*练习 3.04* 、*中特征工程的 Colab 笔记本，从现有特征创建新特征*执行所有步骤，直到*步骤 18* 。
2.  使用`pd.get_dummies()`功能为分类变量创建虚拟变量。排除原始的原始变量，如贷款和住房，这些变量用于创建新变量`assetIndex`。
3.  选择数值变量，包括已创建的新特性工程变量`assetIndex`。
4.  通过使用`MinMaxScaler()`函数对一些数值变量进行标准化来转换它们。
5.  使用`pd.concat()`函数连接数值变量和分类变量，然后创建`X`和`Y`变量。
6.  使用`train_test_split()`函数分割数据集，然后在新要素上使用`LogisticRegression()`模型拟合新模型。
7.  Analyze the results after generating the confusion matrix and classification report.

    您应该得到以下输出:

![Figure 3.53: Expected output with the classification report
](image/C15019_03_53.jpg)

图 3.53:分类报告的预期输出

分类报告将类似于此处显示的报告。但是，由于建模过程中的可变性，这些值可能会有所不同。

注意

这个活动的解决方案可以在以下地址找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

现在让我们讨论下一步需要采取的步骤，以便改进我们从两次迭代中获得的度量。

## 下一步

我们可以问的下一个明显的问题是，从我们在本章中已经实现的所有过程中，我们将走向何方？让我们讨论一下我们可以采取的进一步改进策略:

*   **Class imbalance**: Class imbalance implies use cases where one class outnumbers the other class(es) in the dataset. In the dataset that we used for training, out of the total 31,647 examples, 27,953 or 88% of them belonged to the `no` class. When there are class imbalances, there is a high likelihood that the classifier overfits to the majority class. This is what we have seen in our example. This is also the reason why we shouldn't draw our conclusions on the performance of our classifier by only looking at the accuracy values.

    类别不平衡在许多用例中非常普遍，例如欺诈检测、医疗诊断和客户流失等。有不同的策略来处理存在类不平衡的用例。我们将在第十三章、*不平衡数据集*中处理*类不平衡场景。*

*   **特征工程**:数据科学是一门迭代科学。得到想要的结果将取决于我们进行的各种实验。对初始模型进行改进的一个重要方面是通过特征工程对原始变量进行修改。我们处理了特征工程，并使用特征工程变量建立了一个模型。在构建新特性的过程中，我们遵循了创建与资产组合相关的新特性的轨迹。类似地，从商业的角度来看，我们可以遵循其他的线索，这些线索有可能产生更多与我们所创建的相似的特性。这种踪迹的识别将依赖于通过我们制定的假设和我们为验证这些业务假设所做的探索性分析来扩展我们应用的业务知识。提高模型准确性的一个非常有效的方法是识别更多的业务踪迹，然后通过创新的特征工程来建立模型。
*   **模型选择策略**:当我们讨论参数和非参数模型时，我们提到了这样一点，如果真实的数据生成过程与我们假设的模型不相似，我们将会得到很差的结果。在我们的案例中，我们假设线性，因此采用了线性模型。如果真实的数据生成过程不是线性的呢？或者，如果有其他参数或非参数模型更适合这个用例呢？这些都是我们在尝试分析结果，尝试改进模型时需要考虑的。我们必须采用一种叫做模型抽查的策略，这需要用不同的模型计算出用例，并在为用例采用模型之前检查初始度量。在随后的章节中，我们将讨论其他建模技术，建议用其他类型的模型来试验这个用例，以抽查哪种建模技术更适合这个用例。

# 总结

在这一章中，我们从解决一个用例的角度学习了使用逻辑回归的二元分类。让我们总结一下本章的学习内容。我们学习了分类问题，特别是二元分类问题。我们还从通过业务发现过程预测定期存款倾向的角度研究了分类问题。在业务发现过程中，我们确定了影响业务成果的不同业务驱动因素。

从探索性分析中获得的直觉被用于从原始变量中创建新特征。构建了一个基准逻辑回归模型，并分析了度量标准以确定未来的行动过程，我们通过合并功能工程变量构建第二个模型来迭代基准模型。

在解决了二元分类问题之后，是时候向前迈出下一步了。在下一章，你将处理多类分类，在那里你将被介绍到解决这类问题的不同技术。