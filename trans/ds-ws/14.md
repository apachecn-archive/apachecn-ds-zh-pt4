

# 14。降维

概观

本章介绍数据科学中的降维。你将使用互联网广告数据集来分析和评估不同的降维技术。本章结束时，你将能够分析高维数据集，并处理这些数据集带来的挑战。除了对大型数据集应用不同的降维技术之外，您还将根据这些数据集拟合模型并分析它们的结果。本章结束时，你将能够处理现实世界中的巨大数据集。

# 简介

在前一章平衡数据集中，我们处理了银行营销数据集，它有 18 个变量。我们能够非常轻松地加载数据集，拟合模型，并获得结果。但是你有没有考虑过当你要处理的变量数量很大时的情况，比如说大约 1800 万个，而不是你在上一章中处理的 18 个？如何加载如此大的数据集并进行分析？如何处理用如此大的数据集建模所需的计算资源？

这是一些现代数据集中的现实，例如:

*   医疗保健，遗传学数据集可能有数百万个特征
*   高分辨率成像数据集
*   与广告、排名和爬行相关的 Web 数据

在处理如此庞大的数据集时，会出现许多挑战:

*   存储和计算挑战:高维大型数据集需要大量存储和昂贵的计算资源来进行分析。
*   探索挑战:当试图探索数据并获得洞察力时，高维数据可能真的很麻烦。
*   算法挑战:许多算法在高维设置中不能很好地扩展。

那么，当我们不得不处理高维数据时，解决方案是什么呢？这就是降维技术的用武之地，我们将在本章中探讨。

降维旨在降低数据集的维度，以克服高维数据带来的挑战。在本章中，我们将研究一些流行的降维技术:

*   反向特征消除或递归特征消除
*   正向特征选择
*   主成分分析
*   独立成分分析
*   要素分析

让我们首先检查我们的业务环境，然后将这些技术应用到问题陈述中。

## 业务背景

你公司的营销主管带着一个她一直在努力解决的问题来找你。许多客户一直在抱怨贵公司网站的浏览体验，因为在浏览过程中弹出的广告数量太多。您的公司希望在您的 web 服务器上构建一个引擎，该引擎可以识别潜在的广告，并在它们出现之前将其消除。

为了帮助您实现这一点，我们给了您一个数据集，其中包含了各种网页上的一组可能的广告。数据集的特征表示可能的广告中的图像的几何形状，以及在 URL、图像 URL、锚文本和锚文本附近出现的单词中出现的短语。这个数据集也被贴上了标签，每个可能的广告都有一个标签，表明它实际上是不是广告。使用这个数据集，你必须建立一个模型来预测某个东西是否是广告。您可能认为这是一个相对简单的问题，可以用任何二进制分类算法来解决。然而，数据集中存在一个挑战。数据集包含大量要素。你已经着手解决这个高维数据集的挑战。

本次练习的数据集可在[https://packt.live/2QtbrSs](https://packt.live/2QtbrSs)获得。文件名为`ad-dataset.zip`。

接下来，解压缩数据集。

将数据集下载到你的 Google drive 中并解压。当数据集解压缩时，应该有三个文件。我们将使用的数据集是`ad.Data`，如图*图 14.1* 所示:

![Figure 14.1: Dataset for the problem
](image/B15019_14_01.jpg)

图 14.1:问题的数据集

这个数据集被上传到 GitHub 存储库中，用于完成所有后续练习。数据集的属性在以下链接中可用:[https://packt.live/36rqiCg](https://packt.live/36rqiCg)。

## 练习 14.01:加载和清理数据集

在本练习中，我们将下载数据集，将其加载到我们的 Colab 笔记本中，并进行一些基本的探索，例如使用`.shape()`和`.describe()`函数打印数据集的维度，以及清理数据集。

注意

`internet_ads`数据集已经上传到我们的 GitHub 知识库，可以在[https://packt.live/2sPaVF6](https://packt.live/2sPaVF6)访问。

以下步骤将帮助您完成本练习:

1.  打开一个新的 Colab 笔记本文件。
2.  现在，`import pandas`进入你的 Colab 笔记本:

    ```
    import pandas as pd
    ```

3.  接下来，设置上传`ad.Data`文件的驱动器路径，如下面的代码片段所示:

    ```
    # Defining file name of the GitHub repository
    filename = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter14/Dataset/ad.data'
    ```

4.  Read the file using the `pd.read_csv()` function from the pandas data frame:

    ```
    adData = pd.read_csv(filename,sep=",",header = None,error_bad_lines=False)
    adData.head()
    ```

    `pd.read_csv()`函数的参数是字符串形式的文件名和 CSV 文件的限制分隔符，即`","`。请注意，由于数据集没有标题。我们使用`header = None`命令特别提到了这一点。最后一个参数`error_bad_lines=False`是跳过文件格式中的任何错误，然后加载数据。

    读取文件后，使用`.head()`功能打印数据帧。

    您应该得到以下输出:

    ![Figure 14.2: Loading data into the Colab notebook
    ](image/B15019_14_02.jpg)

    图 14.2:将数据加载到 Colab 笔记本中

5.  Now, print the shape of the dataset, as shown in the following code snippet:

    ```
    # Printing the shape of the data
    print(adData.shape)
    ```

    您应该得到以下输出:

    ```
    (3279, 1559)
    ```

    从形状上，可以看出我们有大量的特征，`1559`。

6.  Find the summary of the numerical features of the raw data using the `.describe()` function in pandas, as shown in the following code snippet:

    ```
    # Summarizing the statistics of the numerical raw data
    adData.describe()
    ```

    您应该得到以下输出:

    ![Figure 14.3: Loading data into the Colab notebook
    ](image/B15019_14_03.jpg)

    图 14.3:将数据加载到 Colab 笔记本中

    正如我们从数据的形状中看到的，数据集有带有`1559` 变量的`3279`个示例。变量集既有分类变量也有数值变量。汇总统计数据仅针对数字数据。

7.  Separate the dependent and independent variables from our dataset, as shown in the following code snippet:

    ```
    # Separate the dependent and independent variables
    # Preparing the X variables
    X = adData.loc[:,0:1557]
    print(X.shape)
    # Preparing the Y variable
    Y = adData[1558]
    print(Y.shape)
    ```

    您应该得到以下输出:

    ```
    (3279, 1558)
    (3279, )
    ```

    如前所述，数据集中有`1559`个要素。第一个`1558`特征是独立变量。使用`.loc()`功能将它们从初始`adData`数据帧中分离出来，并给出相应特征的索引(`0`至`1557`)。自变量被加载到一个名为`X`的新变量中。因变量是数据集的标签，加载到变量`Y`中。因变量和自变量的形状也会打印出来。

8.  Print the first `15` examples of the independent variables:

    ```
    # Printing the head of the independent variables
    X.head(15)
    ```

    您可以通过在`head()`功能中定义数字来打印尽可能多的数据行。这里，我们已经打印出了数据的前`15`行。

    输出如下所示:

    ![Figure 14.4: First 15 examples of independent variables
    ](image/B15019_14_04.jpg)

    图 14.4:自变量的前 15 个例子

    从输出中我们可以看到数据集中有很多缺失值，用`?`表示。为了进一步分析，我们必须删除这些特殊字符，然后用假定的值替换这些单元格。替换特殊字符的一种流行方法是估算相应特征的平均值。让我们采取这种策略。但是，在此之前，让我们先看看这个数据集的数据类型，以便采用合适的替换策略。

9.  Print the data types of the dataset:

    ```
    # Printing the data types
    print(X.dtypes)
    ```

    我们应该得到以下输出:

    ![Figure 14.5: The data types in our dataset
    ](image/B15019_14_05.jpg)

    图 14.5:数据集中的数据类型

    从输出中，我们可以看到前四列是 object 类型的，它指的是字符串数据，其他的是整数数据。当替换数据中的特殊字符时，我们需要了解数据类型。

10.  Replace special characters with `NaN` values for the first four columns.

    用`NaN`值替换前四列中属于对象类型的特殊字符。`NaN`是“不是数字”的缩写用`NaN`值替换特殊字符使得进一步估算数据变得容易。

    这是通过以下代码片段实现的:

    ```
    # Replacing special characters in first 3 columns which are of type object
    for i in range(0,3):
      X[i] = X[i].str.replace("?", 'nan').values.astype(float)
    print(X.head(15))
    ```

    为了替换前三列，我们使用`for()`循环和`range()`函数遍历这些列。由于前三列是`object`或`string`类型，我们使用了`.str.replace()`函数，代表“字符串替换”。在用`nan`替换数据的特殊字符`?`后，我们用`.values.astype(float)`函数将数据类型转换为`float`，这是进一步处理所需要的。通过打印前 15 个示例，我们可以看到所有特殊字符都被替换为`nan`或`NaN`值

    您应该得到以下输出:

    ![Figure 14.6: After replacing special characters with NaN
    ](image/B15019_14_06.jpg)

    图 14.6:用 NaN 替换特殊字符后

11.  Now, replace special characters for the integer features.

    如同在*步骤 9* 中一样，让我们用下面的代码片段替换`int64`数据类型特性中的特殊字符:

    ```
    # Replacing special characters in the remaining columns which are of type integer
    for i in range(3,1557):
      X[i] = X[i].replace("?", 'NaN').values.astype(float)
    ```

    注意

    对于整数特性，我们在`.replace ()`函数之前没有`.str`，因为这些特性是整数值而不是字符串值。

12.  Now, impute the mean of each column for the `NaN` values.

    既然我们已经用`NaN`值替换了数据中的特殊字符，我们可以使用 pandas 中的`fillna()`函数用列的平均值替换`NaN`值。这是使用以下代码片段执行的:

    ```
    import numpy as np
    # Impute the 'NaN'  with mean of the values
    for i in range(0,1557):
      X[i] = X[i].fillna(X[i].mean())
    print(X.head(15))
    ```

    在前面的代码片段中，`.mean()`函数计算每列的平均值，然后用该列的平均值替换`nan`值。

    您应该得到以下输出:

    ![Figure 14.7: Mean of the NaN columns
    ](image/B15019_14_07.jpg)

    图 14.7:NaN 列的平均值

13.  Scale the dataset using the `minmaxScaler()` function.

    正如第三章、*二进制分类*中的*一样，缩放数据在建模步骤中非常有用。让我们使用第 3 章*、*二进制分类*中的*函数来缩放数据集。*

    这显示在下面的代码片段中:

    ```
    # Scaling the data sets
    # Import library function
    from sklearn import preprocessing
    # Creating the scaling function
    minmaxScaler = preprocessing.MinMaxScaler()
    # Transforming with the scaler function
    X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))
    # Printing the output
    X_tran.head() 
    ```

    您应该得到以下输出。这里，我们显示了前 24 列:

![Figure 14.8: Scaling the dataset using the MinMaxScaler() function
](image/B15019_14_08.jpg)

图 14.8:使用 MinMaxScaler()函数缩放数据集

第一个练习已经结束。在本练习中，我们加载了数据集，提取了汇总统计数据，清理了数据，还缩放了数据。您可以看到，在最终的输出中，所有的原始值都被转换成了缩放值。

在下一节中，让我们尝试用更多的功能来扩充该数据集，使其成为一个大规模数据集，然后在该数据集上拟合一个简单的逻辑回归模型作为基准模型。

让我们首先通过一个例子来看看如何扩充数据。

# 创建高维数据集

在前面的部分中，我们使用了一个具有大约`1,558`个特征的数据集。为了演示高维数据集面临的挑战，让我们从现有的互联网数据集创建一个极高维的数据集。

我们将通过多次复制现有数量的要素来实现这一点，从而使数据集变得非常大。为了复制数据集，我们将使用一个名为`np.tile()`的函数，该函数跨我们想要的轴多次复制一个数据帧。我们还将使用`time()`函数计算任何活动所花费的时间。

让我们用一个玩具例子来看看这两个函数的作用。

首先导入必要的库函数:

```
import pandas as pd
import numpy as np
```

然后，为了创建虚拟数据框，我们将在本例中使用一个两行三列的小型数据集。我们使用`pd.np.array()`函数创建一个数据框:

```
# Creating a simple data frame
df = pd.np.array([[1, 2, 3], [4, 5, 6]])
print(df.shape)
df
```

您应该得到以下输出:

![Figure 14.9: Array for the sample dummy data frame
](image/B15019_14_09.jpg)

图 14.9:样本虚拟数据帧的数组

接下来，复制虚拟数据帧，使用下面代码片段中的`pd.np.tile()`函数完成列的复制:

```
# # Replicating the data frame and noting the time
import time
# Starting a timing function
t0=time.time()
Newdf = pd.DataFrame(pd.np.tile(df, (1, 5)))
print(Newdf.shape)
print(Newdf)
# Finding the end time 
print("Total time:", round(time.time()-t0, 3), "s")
```

您应该得到以下输出:

![Figure 14.10: Replication of the data frame
](image/B15019_14_10.jpg)

图 14.10:数据帧的复制

正如我们在代码片段中看到的,`pd.np.tile()`函数接受两组参数。第一个是我们想要复制的数据帧`df`。下一个参数，`(1,5)`，定义了我们想要复制的轴。在这个例子中，我们定义行将保持原样，因为有了`1`参数，而列将使用`5`参数复制`5`次。从`shape()`函数中我们可以看到，形状为`(2,3)`的原始数据帧已经被转换为形状为`(2,15)`的数据帧。

使用`time`库计算总时间。为了开始计时，我们调用了`time.time()`函数。在这个例子中，我们将初始时间存储在一个名为`t0`的变量中，然后从结束时间中减去这个值，得到这个过程所花费的总时间。因此，我们在现有的互联网广告数据集中增加了更多的数据框架。

## 活动 14.01:在高维数据集上拟合逻辑回归模型

当数据集很大时，您希望测试模型的性能。要做到这一点，您需要人为地扩充互联网广告数据集，使数据集的维度比原始数据集大 300 倍。您将对这个新数据集拟合逻辑回归模型，然后观察结果。

**提示**:在本活动中，我们将使用类似于*练习 14.01* 、*加载并清理数据集*的笔记本，我们还将拟合一个逻辑回归模型，如*第 3 章*、*二元分类*中所做的那样。

注意

我们将在本次活动中使用相同的广告数据集。

`internet_ads`数据集已经上传到我们的 GitHub 知识库，可以在[https://packt.live/2sPaVF6](https://packt.live/2sPaVF6)访问。

完成此活动的步骤如下:

1.  打开新的 Colab 笔记本。
2.  执行从*练习 14.01* 、*加载和清理数据集*开始的所有步骤，直到数据正常化。导出转换后的独立`X_tran`变量。
3.  使用`pd.np.tile()`函数将列复制 300 次，创建一个高维数据集。打印新数据集的形状，并观察新数据集中的要素数量。
4.  将数据集分为训练集和测试集。
5.  Fit a logistic regression model on the new dataset and note the time it takes to fit the model. Note the color change for the indicator for RAM on your Colab notebook.

    **预期产量**:

    在新数据集上拟合逻辑回归模型后，您应该会得到类似于以下内容的输出:

    ```
    Total training time: 23.86 s
    ```

    ![Figure 14.11: Google Colab RAM utilization
    ](image/B15019_14_11.jpg)

    图 14.11: Google Colab 内存利用率

6.  Predict on the test set and print the classification report and confusion matrix.

    您应该得到以下输出:

    ![Figure 14.12: Confusion matrix and the classification report results
    ](image/B15019_14_12.jpg)

图 14.12:混淆矩阵和分类报告结果

注意

活动的解决方案可以在这里找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

在本活动中，您将通过复制现有数据库的列来创建一个高维数据集，并发现该高维数据集的资源利用率非常高。由于维度较大，资源利用率指示器的颜色变为橙色。在这个数据集上，建模花费的时间`23.86`秒更长。使用逻辑回归模型，您还可以在测试集上预测得到大约`97%`的准确度。

首先，你需要知道为什么 Colab 上的 RAM 利用率的颜色变成了橙色。由于我们通过复制创建了巨大的数据集，Colab 不得不使用访问 RAM，因此颜色变成了橙色。

但是，出于好奇，如果您将复制从 300 增加到 500，您认为对 RAM 利用率会有什么影响？让我们看看下面的例子。

注意

您不需要在您的 Colab 笔记本上执行此操作。

我们首先定义 GitHub 存储库的数据集到“ads”数据集的路径:

```
# Defining the file name from GitHub
filename = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter14/Dataset/ad.data'
```

接下来，我们简单地使用 pandas 加载数据:

```
# import pandas as pd
# Loading the data using pandas
adData = pd.read_csv(filename,sep=",",header = None,error_bad_lines=False)
```

创建一个比例因子为`500`的高维数据集:

```
# Creating a high dimension dataset
X_hd = pd.DataFrame(pd.np.tile(adData, (1, 500)))
```

您将看到以下输出:

![Figure 14.13: Colab crashing
](image/B15019_14_13.jpg)

图 14.13: Colab 崩溃

从输出中，您可以看到会话崩溃了，因为 Colab 提供的所有 RAM 都被使用了。会话将重新启动，您将丢失所有变量。因此，留意为您提供的资源以及数据集总是有好处的。作为一名数据科学家，如果您觉得数据集很大，包含许多要素，但处理该数据集的资源有限，您需要联系组织并获得所需的资源，或者制定适当的策略来处理这些高维数据集。

# 处理高维数据集的策略

在*活动 14.01* 、*在高维数据集上拟合逻辑回归模型*中，我们见证了高维数据集的挑战。我们看到了当复制因子为 300 时，资源是如何受到挑战的。您还看到，当复制因子增加到 500 时，笔记本崩溃。当复制因子为 500 时，特征的数量约为 750，000。在我们的例子中，我们的资源甚至在达到 100 万个特性之前就无法扩展。一些现代数据集有时具有数亿或数十亿个要素。想象一下从数据集中获得任何可操作的见解需要多少资源和时间。

幸运的是，我们有许多健壮的方法来处理高维数据集。这些技术中的许多都非常有效，并有助于解决大型数据集带来的挑战。

让我们看看处理高维数据集的一些技术。在*图 14.14* 中，您可以看到我们将在本章中遇到的处理此类数据集的策略:

![Figure 14.14: Strategies to address high dimensional datasets
](image/B15019_14_14.jpg)

图 14.14:处理高维数据集的策略

## 向后特征消除(递归特征消除)

后向特征消除算法背后的机制是特征的递归消除，并在所有消除后保留的那些特征上构建模型。

让我们一步一步地看看这个算法:

1.  最初，在给定的迭代中，首先在所有可用的`n`特征上训练所选择的分类算法。例如，让我们以我们拥有的原始数据集为例，它具有`1,558`特征。该算法从第一次迭代中的所有`1,558`特征开始。
2.  在下一步中，我们一次移除一个特征，并用剩余的`n-1`特征训练一个模型。这个过程重复`n`次。例如，我们首先移除特征 1，然后使用所有剩余的 1，557 个变量来拟合模型。在下一次迭代中，我们使用特征`1`，相反，我们消除特征`2`，然后拟合模型。这个过程重复`n`次(`1,558`次)。
3.  对于每个拟合的模型，计算模型的性能(使用诸如精确度的测量)。
4.  其替换导致性能变化最小的特征被永久删除，并且用`n-1`特征重复*步骤 2* 。
5.  然后用`n-2`特征等重复该过程。
6.  该算法不断消除特征，直到达到我们需要的阈值特征数。

在下一个练习中，让我们来看一下针对增强的 ads 数据集的反向特征消除算法。

## 练习 14.02:使用反向特征消除进行维度缩减

在本练习中，我们将在使用反向消除技术消除要素后拟合逻辑回归模型，以确定模型的准确性。我们将使用与之前相同的 ads 数据集，并且我们将在本次练习中使用额外的功能对其进行增强。

以下步骤将帮助您完成本练习:

1.  打开一个新的 Colab 笔记本文件。
2.  实施类似于*练习 14.01* 的所有初始步骤，直到使用`minmaxscaler()`函数缩放数据集。
3.  Next, create a high-dimensional dataset.

    现在让我们通过因子`2`来人工增加数据集。反向特征消除的过程是一个非常计算密集型的过程，并且使用更高的维度将涉及更长的处理时间。这就是增大系数保持在`2`的原因。这是使用以下代码片段实现的:

    ```
    # Creating a high dimension data set
    X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 2)))
    print(X_hd.shape)
    ```

    您应该得到以下输出:

    ```
    (3279, 3116)
    ```

4.  Define the backward elimination model. Backward elimination works by providing two arguments to the `RFE()` function, which is the model we want to try (logistic regression in our case) and the number of features we want the dataset to be reduced to. This is implemented as follows:

    ```
    from sklearn.linear_model import LogisticRegression
    from sklearn.feature_selection import RFE
    # Defining the Classification function
    backModel = LogisticRegression()
    # Reducing dimensionality to 250 features for the backward elimination model
    rfe = RFE(backModel, 250)
    ```

    在该实现中，我们给出的特征数量`250`是通过反复试验确定的。该过程首先假设任意数量的特征，然后根据最终的度量标准，得出模型的最佳特征数量。在这个实现中，我们对`250`的第一个假设意味着我们希望反向消除模型开始消除特征，直到我们得到最好的`250`特征。

5.  Fit the backward elimination method to identify the best `250` features.

    现在，我们已经准备好在高维数据集上应用反向消除方法。我们还会注意到反向消除起作用所需的时间。这是使用以下代码片段实现的:

    ```
    # Fitting the rfe for selecting the top 250 features
    import time
    t0 = time.time()
    rfe = rfe.fit(X_hd, Y)
    t1 = time.time()
    print("Backward Elimination time:", round(t1-t0, 3), "s")
    ```

    使用`.fit()`功能拟合反向消除法。我们给出独立的和非独立的训练集。

    注意

    反向消除法是一个计算密集型过程，因此该过程将花费大量时间来执行。要素数量越多，花费的时间就越长。

    向后消除的时间是在通知结束时:

    ![Figure 14.15: The time taken for the backward elimination process
    ](image/B15019_14_15.jpg)

    图 14.15:反向消除过程所用的时间

    您可以看到，寻找最佳`250`特征的反向消除过程用了`230.35`秒来实现。

6.  Display the features identified using the backward elimination method. We can display the `250` features that were identified using the backward elimination process using the `get_support()` function. This is implemented as follows:

    ```
    # Getting the indexes of the features used
    rfe.get_support(indices = True)
    ```

    您应该得到以下输出:

    ![Figure 14.16: The identified features being displayed
    ](image/B15019_14_16.jpg)

    图 14.16:显示的已识别特征

    这些是使用反向排除过程从整个数据集中最终选择的最佳`250`特征。

7.  Now, split the dataset into training and testing sets for modeling:

    ```
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)
    print('Training set shape',X_train.shape)
    print('Test set shape',X_test.shape)
    ```

    您应该得到以下输出:

    ```
    Training set shape (2295, 3116)
    Test set shape (984, 3116)
    ```

    从输出中，您可以看到训练集和测试集的形状。

8.  Transform the train and test sets. In *step 5*, we identified the top `250` features through backward elimination. Now we need to reduce the train and test sets to those top `250` features. This is done using the `.transform()` function. This is implemented using the following code snippet:

    ```
    # Transforming both train and test sets
    X_train_tran = rfe.transform(X_train)
    X_test_tran = rfe.transform(X_test)
    print("Training set shape",X_train_tran.shape)
    print("Test set shape",X_test_tran.shape)
    ```

    您应该得到以下输出:

    ```
    Training set shape (2295, 250)
    Test set shape (984, 250)
    ```

    我们可以看到，训练集和测试集都减少到了`250`最佳特性。

9.  Fit a logistic regression model on the training set and note the time:

    ```
    # Fitting the logistic regression model 
    import time
    # Defining the LogisticRegression function
    RfeModel = LogisticRegression()
    # Starting a timing function
    t0=time.time()
    # Fitting the model
    RfeModel.fit(X_train_tran, y_train)
    # Finding the end time 
    print("Total training time:", round(time.time()-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    Total training time: 0.016 s
    ```

    正如预期的那样，在缩减的要素集上拟合模型所需的总时间远低于在*活动 14.01* 中拟合较大数据集所需的时间，后者为`23.86`秒。这是一个很大的进步。

10.  Now, predict on the test set and print the accuracy metrics, as shown in the following code snippet:

    ```
    # Predicting on the test set and getting the accuracy
    pred = RfeModel.predict(X_test_tran)
    print('Accuracy of Logistic regression model after backward elimination: {:.2f}'.format(RfeModel.score(X_test_tran, y_test)))
    ```

    您应该得到以下输出:

    ![Figure 14.17: The achieved accuracy of the logistic regression model
    ](image/B15019_14_17.jpg)

    图 14.17:逻辑回归模型达到的精确度

    您可以看到，与我们从更高维度的模型中得到的结果相比，该模型的精确度有所提高，更高维度的模型是*活动 14.01* 中的`0.97`。这种增加可归因于从完整的特征集中识别不相关的特征，这可以提高模型的性能。

11.  Print the confusion matrix:

    ```
    from sklearn.metrics import confusion_matrix
    confusionMatrix = confusion_matrix(y_test, pred)
    print(confusionMatrix)
    ```

    您应该得到以下输出:

    ![Figure 14.18: Confusion matrix
    ](image/B15019_14_18.jpg)

    图 14.18:混淆矩阵

12.  Printing the classification report:

    ```
    from sklearn.metrics import classification_report
    # Getting the Classification_report
    print(classification_report(y_test, pred))
    ```

    您应该得到以下输出:

    ![Figure 14.19: Classification matrix
    ](image/B15019_14_19.jpg)

图 14.19:分类矩阵

正如我们从反向排除过程中看到的，与*活动 14.01* 中的模型相比，我们能够通过`250`特征提高`98%`的准确度，在*活动 14.01【】中，使用了人工增强的数据集，并获得了`97%`的准确度。*

然而，应该注意的是，降维技术不应该被视为一种提高任何模型性能的方法。降维技术必须从使我们能够在具有大量特征的数据集上拟合模型的角度来看待。当维度增加时，拟合模型变得困难。如果在*活动 14.01* 中使用的比例因子从`300`增加到`500`，就可以观察到这种情况。在这种情况下，用当前的资源集是无法拟合模型的。在这种情况下，降维有助于减少特征的数量，从而能够在降维的基础上拟合模型，而不会大幅降低性能，有时还能改善结果。然而，还应该注意，诸如反向消除之类的方法是计算密集型过程。当缩放因子仅为 2 时，您会观察到这种现象，即识别前 250 个特征所花费的时间。使用更高的比例因子，将花费更多的时间和资源来识别前 250 个特征。

已经看到了后向排除法，现在让我们来看下一个技术，即前向特征选择。

## 前进特征选择

正向特征选择的工作顺序与反向消除相反。在这个过程中，我们从一个初始特性开始，然后一个接一个地添加特性，直到性能没有任何提高。具体流程如下:

1.  从一个特征开始建模。
2.  重复模型构建过程 *n* 次，每次选择一个特征。选择性能改善最大的特征。
3.  一旦选择了第一个特征，就该选择第二个特征了。选择第二特征的过程与*步骤 2* 完全相同。剩余的 *n-1* 特征与第一个特征一起被迭代，并且观察模型上的性能。对模型性能产生最大改进的特征被选为第二特征。
4.  特征的迭代将继续，直到提取出我们已经确定的阈值数量的特征。
5.  所选的最终特征集将是提供最大模型性能的特征集。

现在让我们在下一个练习中实现这个算法。

## 练习 14.03:使用前向特征选择进行降维

在本练习中，我们将通过正向特征选择来选择最佳特征，并观察模型的性能，从而拟合逻辑回归模型。我们将使用与之前相同的 ads 数据集，并且我们将在本次练习中使用额外的功能对其进行增强。

以下步骤将帮助您完成本练习:

1.  打开新的 Colab 笔记本。
2.  实施所有类似于*练习 14.01* 的初始步骤，直到使用`MinMaxScaler()`缩放数据集。
3.  Create a high-dimensional dataset. Now, augment the dataset artificially to a factor of `50`. Augmenting the dataset to higher factors will result in the notebook crashing because of lack of memory. This is implemented using the following code snippet:

    ```
    # Creating a high dimension dataset
    X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))
    print(X_hd.shape)
    ```

    您应该得到以下输出:

    ```
    (3279, 77900)
    ```

4.  将高维数据集分成训练集和测试集:

    ```
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)
    ```

5.  Define the threshold features.

    一旦创建了训练集和测试集，下一步就是导入特性选择函数`SelectKBest`。我们给这个函数的参数是我们想要的特性的数量。通过实验选择特征，并且作为第一步，我们假设一个阈值。在本例中，我们假设阈值为`250`。这是使用以下代码片段实现的:

    ```
    from sklearn.feature_selection import SelectKBest
    # feature extraction
    feats = SelectKBest(k=250)
    ```

6.  Iterate and get the best set of threshold features. Based on the threshold set of features we defined, we have to fit the training set and get the best set of threshold features. Fitting on the training set is done using the `.fit()` function. We also note the time it takes to find the best set of features. This is executed using the following code snippet:

    ```
    # Fitting the features for training set
    import time
    t0 = time.time()
    fit = feats.fit(X_train, y_train)
    t1 = time.time()
    print("Forward selection fitting time:", round(t1-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    Forward selection fitting time: 2.682 s
    ```

    我们可以看到前向选择方法用了大约`2.68`秒，比后向选择方法低很多。

7.  创建新的训练集和测试集。一旦我们确定了最佳的特征集，我们就必须修改我们的训练集和测试集，使它们只包含那些选定的特征。这是通过使用`.transform()`功能:

    ```
    # Creating new training set and test sets 
    features_train = fit.transform(X_train)
    features_test = fit.transform(X_test)
    ```

    完成的
8.  Let's verify the shapes of the train and test sets before transformation and after transformation:

    ```
    # Printing the shape of training and test sets before transformation
    print('Train shape before transformation',X_train.shape)
    print('Test shape before transformation',X_test.shape)
    # Printing the shape of training and test sets after transformation
    print('Train shape after transformation',features_train.shape)
    print('Test shape after transformation',features_test.shape)
    ```

    您应该得到以下输出:

    ![Figure 14.20: Shape of the training and testing datasets
    ](image/B15019_14_20.jpg)

    图 14.20:训练和测试数据集的形状

    您可以看到，训练集和测试集都减少到了各自的`250`特性。

9.  现在，让我们在转换后的数据集上拟合一个逻辑回归模型，并记录拟合该模型所需的时间:

    ```
    # Fitting a Logistic Regression Model
    from sklearn.linear_model import LogisticRegression
    import time
    t0 = time.time()
    forwardModel = LogisticRegression()
    forwardModel.fit(features_train, y_train)
    t1 = time.time()
    ```

10.  Print the total time:

    ```
    print("Total training time:", round(t1-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    Total training time: 0.035 s
    ```

    你可以看到训练时间比*活动 14.01* 中的模型少得多，为`23.86`秒。这一较短的时间归因于正向选择模型中的特征数量。

11.  Now, perform predictions on the test set and print the accuracy metrics:

    ```
    # Predicting with the forward model
    pred = forwardModel.predict(features_test)
    print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(forwardModel.score(features_test, y_test)))
    ```

    您应该得到以下输出:

    ```
    Accuracy of Logistic regression model prediction on test set: 0.94
    ```

12.  Print the confusion matrix:

    ```
    from sklearn.metrics import confusion_matrix
    confusionMatrix = confusion_matrix(y_test, pred)
    print(confusionMatrix)
    ```

    您应该得到以下输出:

    ![Figure 14.21: Resulting confusion matrix
    ](image/B15019_14_21.jpg)

    图 14.21:产生的混淆矩阵

13.  Print the classification report:

    ```
    from sklearn.metrics import classification_report
    # Getting the Classification_report
    print(classification_report(y_test, pred))
    ```

    您应该得到以下输出:

    ![Figure 14.22: Resulting classification report
    ](image/B15019_14_22.jpg)

图 14.22:结果分类报告

正如我们在正向选择过程中看到的，我们能够通过`250`特征获得`94%`的准确度分数。该分数低于使用逆向淘汰法(`98%`)以及*活动 14.01* 中内置的基准模型(`97%`)获得的分数。然而，寻找最佳特征所用的时间(2.68 秒)大大少于反向消除法(230.35 秒)。

在下一节中，我们将学习主成分分析(PCA)。

## 主成分分析

PCA 是一种非常有效的降维技术，它在不影响数据信息内容的情况下实现降维。PCA 背后的基本思想是首先识别数据集中不同变量之间的相关性。一旦确定了相关性，算法就决定以保持数据可变性的方式消除变量。换句话说，PCA 的目标是找到不相关的数据源。

对原始变量实施 PCA 导致将它们转换成一组全新的变量，称为主成分。这些分量中的每一个都代表沿着相互正交的轴的数据的可变性。这意味着第一个轴符合数据最大可变性出现的方向。此后，选择第二个轴，使得该轴与第一个选择的轴正交(垂直),并且还覆盖下一个最高可变性。

让我们用一个例子来看看 PCA 的思想。

我们将创建一个包含 2 个变量和每个变量中 100 个随机数据点的样本数据集。使用`rand()`功能创建随机数据点。这在以下代码中实现:

```
import numpy as np
# Setting the seed for reproducibility
seed = np.random.RandomState(123)
# Generating an array of random numbers
X = seed.rand(100,2)
# Printing the shape of the dataset
X.shape
```

注意

使用`RandomState(123)`功能定义随机状态。这是为了确保任何复制这个例子的人得到相同的输出。

让我们使用`matplotlib`来可视化这些数据:

```
import matplotlib.pyplot as plt
%matplotlib inline
plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal')
```

您应该得到以下输出:

![Figure 14.23: Visualization of the data
](image/B15019_14_23.jpg)

图 14.23:数据的可视化

在图表中，我们可以看到数据是均匀分布的。

现在让我们来寻找这个数据集的主要组成部分。我们将这个二维数据集缩减为一维数据集。换句话说，我们将把原始数据集简化成它的一个主要组成部分。

这在代码中实现，如下所示:

```
from sklearn.decomposition import PCA
# Defining one component
pca = PCA(n_components=1)
# Fitting the PCA function
pca.fit(X)
# Getting the new dataset
X_pca = pca.transform(X)
# Printing the shapes
print("Original data set:   ", X.shape)
print("Data set after transformation:", X_pca.shape)
```

您应该得到以下输出:

```
original shape: (100, 2)
transformed shape: (100, 1)
```

正如我们在代码中看到的，我们首先使用`'n_components' = 1`参数定义组件的数量。此后，PCA 算法适合输入数据集。在拟合输入数据之后，初始数据集被转换成只有一个变量的新数据集，该变量是其主成分。

该算法通过使用数据可变性最大的轴，将原始数据集转换为其第一个主成分。

为了形象化这个概念，让我们将`X_pca`数据集的转换反转到它的原始形式，然后将这个数据和原始数据一起形象化。为了反转转换，我们使用了`.inverse_transform()`函数:

```
# Reversing the transformation and plotting 
X_reverse = pca.inverse_transform(X_pca)
# Plotting the original data
plt.scatter(X[:, 0], X[:, 1], alpha=0.1)
# Plotting the reversed data
plt.scatter(X_reverse[:, 0], X_reverse[:, 1], alpha=0.9)
plt.axis('equal');
```

您应该得到以下输出:

![Figure 14.24: Plot with reverse transformation
](image/B15019_14_24.jpg)

图 14.24:反向变换的绘图

正如我们在图中看到的，橙色的数据点代表可变性最高的轴。将所有数据点投影到该轴上，以生成第一主成分。

当变换成各种主成分时生成的数据点将与变换前的原始数据点非常不同。每个主分量将位于与另一个主分量正交(垂直)的轴上。如果为前面的示例生成了第二主分量，则第二主分量将沿着图中蓝色箭头所示的轴。我们选择用于模型构建的主成分数量的方法是通过选择解释某一可变性阈值的成分数量。

例如，如果最初有 1，000 个特征，我们将其减少到 100 个主成分，然后我们发现在这 100 个主成分中，前 75 个成分解释了 90%的数据可变性，我们将挑选这 75 个成分来构建模型。这个过程被称为挑选主成分，并解释方差的百分比。

现在让我们看看如何在我们的用例中使用 PCA 作为降维工具。

## 练习 14.04:使用主成分分析进行降维

在本练习中，我们将通过选择解释数据最大可变性的主成分来拟合逻辑回归模型。我们还将观察特征选择和模型构建过程的性能。我们将使用与之前相同的 ads 数据集，并且我们将在本次练习中使用额外的功能对其进行增强。

以下步骤将帮助您完成本练习:

1.  打开一个新的 Colab 笔记本文件。
2.  实施从*练习 14.01* 开始的初始步骤，直到使用`minmaxscaler()`函数缩放数据集。
3.  Create a high-dimensional dataset. Let's now augment the dataset artificially to a factor of 50\. Augmenting the dataset to higher factors will result in the notebook crashing because of a lack of memory. This is implemented using the following code snippet:

    ```
    # Creating a high dimension data set
    X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))
    print(X_hd.shape)
    ```

    您应该得到以下输出

    ```
    (3279, 77900)
    ```

4.  让我们将高维数据集分成训练集和测试集:

    ```
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)	
    ```

5.  Let's now fit the PCA function on the training set. This is done using the `.fit()` function, as shown in the following snippet. We will also note the time it takes to fit the PCA model on the dataset:

    ```
    from sklearn.decomposition import PCA
    import time
    t0 = time.time()
    pca = PCA().fit(X_train)
    t1 = time.time()
    print("PCA fitting time:", round(t1-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    PCS fitting time: 179.545 s
    ```

    我们可以看到，在数据集上拟合 PCA 函数所用的时间小于后向淘汰模型(230.35 秒)，高于前向选择方法(2.682 秒)。

6.  We will now determine the number of principal components by plotting the cumulative variance explained by all the principal components. The variance explained is determined by the `pca.explained_variance_ratio_` method. This is plotted in `matplotlib` using the following code snippet:

    ```
    %matplotlib inline
    import numpy as np
    import matplotlib.pyplot as plt
    plt.plot(np.cumsum(pca.explained_variance_ratio_))
    plt.xlabel('Number of Principal Components')
    plt.ylabel('Cumulative explained variance');
    ```

    在代码中，`np.cumsum()`函数用于获得每个主成分的累积方差。

    您将获得以下输出图:

    ![Figure 14.25: The variance graph
    ](image/B15019_14_25.jpg)

    图 14.25:差异图

    从图中我们可以看到，前`250`个主成分比`90%`个主成分对方差的解释更多。基于这个图表，我们可以根据它解释的可变性来决定我们想要多少个主成分。让我们选择适合我们模型的`250`组件。

7.  既然我们已经认识到`250`组件解释了很多可变性，让我们重新调整`250`组件的训练集。下面的代码片段对此进行了描述:

    ```
    # Defining PCA with 250 components
    pca = PCA(n_components=250)
    # Fitting PCA on the training set
    pca.fit(X_train)
    ```

8.  我们现在用 200 个主成分来转换训练和测试集:

    ```
    # Transforming training set and test set
    X_pca = pca.transform(X_train)
    X_test_pca = pca.transform(X_test)
    ```

9.  Let's verify the shapes of the train and test sets before transformation and after transformation:

    ```
    # Printing the shape of train and test sets before and after transformation
    print("original shape of Training set:   ", X_train.shape)
    print("original shape of Test set:   ", X_test.shape)
    print("Transformed shape of training set:", X_pca.shape)
    print("Transformed shape of test set:", X_test_pca.shape)
    ```

    您应该得到以下输出:

    ![Figure 14.26: Transformed and the original training and testing sets
    ](image/B15019_14_26.jpg)

    图 14.26:转换后的和原始的训练和测试集

    您可以看到，训练集和测试集都减少到了各自的`250`特性。

10.  现在，让我们在转换后的数据集上拟合逻辑回归模型，并记录拟合模型所花费的时间:

    ```
    # Fitting a Logistic Regression Model
    from sklearn.linear_model import LogisticRegression
    import time
    pcaModel = LogisticRegression()
    t0 = time.time()
    pcaModel.fit(X_pca, y_train)
    t1 = time.time()
    ```

11.  Print the total time:

    ```
    print("Total training time:", round(t1-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    Total training time: 0.293 s
    ```

    你可以看到训练时间远低于*活动 14.01* 中的模型，为 23.86 秒。较短的时间归因于 PCA 中选择的特征数量`250`较少。

12.  Now, predict on the test set and print the accuracy metrics:

    ```
    # Predicting with the pca model
    pred = pcaModel.predict(X_test_pca)
    print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(pcaModel.score(X_test_pca, y_test)))
    ```

    您应该得到以下输出:

    ![Figure 14.27: Accuracy of the logistic regression model
    ](image/B15019_14_27.jpg)

    图 14.27:逻辑回归模型的准确性

    可以看到精度水平优于具有所有特性的基准模型(`97%`)和前向选择模型(`94%`)。

13.  Print the confusion matrix:

    ```
    from sklearn.metrics import confusion_matrix
    confusionMatrix = confusion_matrix(y_test, pred)
    print(confusionMatrix)
    ```

    您应该得到以下输出:

    ![Figure 14.28: Resulting confusion matrix
    ](image/B15019_14_28.jpg)

    图 14.28:产生的混淆矩阵

14.  Print the classification report:

    ```
    from sklearn.metrics import classification_report
    # Getting the Classification_report
    print(classification_report(y_test, pred))
    ```

    您应该得到以下输出:

    ![Figure 14.29: Resulting classification matrix
    ](image/B15019_14_29.jpg)

图 14.29:产生的分类矩阵

从结果中可以明显看出，我们得到了 98%的分数，这比基准模型要好。可归因于较高性能的一个原因可能是使用 PCA 方法创建了不相关的主成分，这提高了性能。

在下一部分，我们将探讨独立分量分析(ICA)。

## 独立成分分析

ICA 是一种降维技术，在概念上遵循与 PCA 相似的路径。ICA 和 PCA 都试图通过线性组合原始数据来获得新的数据源。

然而，它们之间的区别在于它们用来寻找新的数据来源的方法。PCA 试图寻找不相关的数据源，ICA 试图寻找独立的数据源。

ICA 的降维实现与 PCA 非常相似。

让我们看看我们的用例中 ICA 的实现。

## 练习 14.05:使用独立成分分析进行降维

在本练习中，我们将使用 ICA 技术拟合逻辑回归模型，并观察模型的性能。我们将使用与之前相同的 ads 数据集，并且我们将在本次练习中使用额外的功能对其进行增强。

以下步骤将帮助您完成本练习:

1.  打开一个新的 Colab 笔记本文件。
2.  实施从*练习 14.01* 开始的所有步骤，直到使用`MinMaxScaler()`缩放数据集。
3.  Let's now augment the dataset artificially to a factor of `50`. Augmenting the dataset to factors that are higher than `50` will result in the notebook crashing because of a lack of memory. This is implemented using the following code snippet:

    ```
    # Creating a high dimension data set
    X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))
    print(X_hd.shape)
    ```

    您应该得到以下输出:

    ```
    (3279, 77900)
    ```

4.  让我们将高维数据集分成训练集和测试集:

    ```
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)	
    ```

5.  让我们加载 ICA 函数`FastICA`，然后定义我们需要的元件数量。我们将使用与 PCA 相同数量的组件:

    ```
    # Defining the ICA with number of components
    from sklearn.decomposition import FastICA 
    ICA = FastICA(n_components=250, random_state=123)
    ```

6.  Once the ICA method is defined, we will fit the method on the training set and also transform the training set to get a new training set with the required number of components. We will also note the time taken for fitting and transforming:

    ```
    # Fitting the ICA method and transforming the training set
    import time
    t0 = time.time()
    X_ica=ICA.fit_transform(X_train)
    t1 = time.time()
    print("ICA fitting time:", round(t1-t0, 3), "s")
    ```

    在代码中，`.fit()`函数用于拟合训练集，而`transform()`方法用于获得具有所需特征数量的新训练集。

    您应该得到以下输出:

    ```
    ICA fitting time: 203.02 s
    ```

    我们可以看到，实现 ICA 花费的时间比 PCA 多得多(179.54 秒)。

7.  我们现在用`250`组件:

    ```
    # Transforming the test set 
    X_test_ica=ICA.transform(X_test)
    ```

    转换测试集
8.  Let's verify the shapes of the train and test sets before transformation and after transformation:

    ```
    # Printing the shape of train and test sets before and after transformation
    print("original shape of Training set:   ", X_train.shape)
    print("original shape of Test set:   ", X_test.shape)
    print("Transformed shape of training set:", X_ica.shape)
    print("Transformed shape of test set:", X_test_ica.shape)
    ```

    您应该得到以下输出:

    ![Figure 14.30: Shape of the original and transformed datasets
    ](image/B15019_14_30.jpg)

    图 14.30:原始数据集和转换数据集的形状

    您可以看到，训练集和测试集都减少到了各自的`250`特性。

9.  现在，让我们在转换后的数据集上拟合逻辑回归模型，并记录它所花费的时间:

    ```
    # Fitting a Logistic Regression Model
    from sklearn.linear_model import LogisticRegression
    import time
    icaModel = LogisticRegression()
    t0 = time.time()
    icaModel.fit(X_ica, y_train)
    t1 = time.time()
    ```

10.  Print the total time:

    ```
    print("Total training time:", round(t1-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    Total training time: 0.054 s
    ```

11.  Let's now predict on the test set and print the accuracy metrics:

    ```
    # Predicting with the ica model
    pred = icaModel.predict(X_test_ica)
    print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(icaModel.score(X_test_ica, y_test)))
    ```

    您应该得到以下输出:

    ```
    Accuracy of Logistic regression model prediction on test set: 0.87
    ```

    我们可以看到 ICA 模型的结果比其他模型差。

12.  Print the confusion matrix:

    ```
    from sklearn.metrics import confusion_matrix
    confusionMatrix = confusion_matrix(y_test, pred)
    print(confusionMatrix)
    ```

    您应该得到以下输出:

    ![Figure 14.31: Resulting confusion matrix
    ](image/B15019_14_31.jpg)

    图 14.31:产生的混淆矩阵

    我们可以看到 ICA 模型在广告分类方面表现不佳。所有的例子都被错误地归类为非广告。我们可以得出结论，ICA 不适合这个数据集。

13.  Print the classification report:

    ```
    from sklearn.metrics import classification_report
    # Getting the Classification_report
    print(classification_report(y_test, pred))
    ```

    您应该得到以下输出:

    ![Figure 14.32: Resulting classification report
    ](image/B15019_14_32.jpg)

图 14.32:结果分类报告

正如我们所看到的，将数据转换为其前 250 个独立的组成部分并没有捕捉到数据中所有必要的可变性。这导致了该方法的分类结果的退化。我们可以得出结论，ICA 不适合这个数据集。

还观察到，寻找最佳独立特征所花费的时间比 PCA 长。但是需要注意的是，根据输入数据的不同，不同的方法得出的结果也不同。尽管 ICA 不适合这个数据集，但它仍然是一种有效的降维方法，应该是数据科学家的必备技能。

在这个练习中，你可能会提出几个问题:

*   你认为我们如何使用 ICA 来改善分类结果？
*   增加组件数量会导致精度指标略有提高。
*   由于采取了改善结果的策略，还有其他副作用吗？

增加组件的数量也会导致逻辑回归模型的训练时间更长。

## 因素分析

因子分析是一种通过对高度相关的变量进行分组来实现降维的技术。让我们来看一个例子，它来自我们预测广告的上下文。

在我们的数据集中，可能有许多描述网页上图像的几何形状(广告中图像的大小和形状)的特征。这些特征可以相互关联，因为它们指的是图像的特定特征。

类似地，可能有许多描述 URL 中出现的锚文本或短语的特征，这些特征是高度相关的。因素分析从数据中寻找相关组，然后将它们分组为潜在因素。因此，如果有 10 个描述图像几何形状的原始特征，因子分析会将它们分组为一个表征图像几何形状的特征。每一组都称为因子。由于许多相关的要素被组合成一个组，因此与数据集的原始维度相比，生成的要素数量将会少得多。

现在让我们看看如何将因子分析作为一种降维技术来实现。

## 练习 14.06:使用因子分析进行降维

在本练习中，我们将在将原始维度缩减为一些关键因素后拟合一个逻辑回归模型，然后观察模型的表现。

以下步骤将帮助您完成本练习:

1.  打开一个新的 Colab 笔记本文件。
2.  从*练习 14.01* 开始实施相同的初始步骤，直到使用`minmaxscaler()`函数缩放数据集。
3.  Let's now augment the dataset artificially to a factor of `50`. Augmenting the dataset to factors that are higher than `50` will result in the notebook crashing because of a lack of memory. This is implemented using the following code snippet:

    ```
    # Creating a high dimension data set
    X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))
    print(X_hd.shape)
    ```

    您应该得到以下输出:

    ```
    (3279, 77900)
    ```

4.  让我们将高维数据集分成训练集和测试集:

    ```
    from sklearn.model_selection import train_test_split
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)
    ```

5.  An important step in factor analysis is defining the number of factors in a dataset. This step is achieved through experimentation. In our case, we will arbitrarily assume that there are `20` factors. This is implemented as follows:

    ```
    # Defining the number of factors
    from sklearn.decomposition import FactorAnalysis
    fa = FactorAnalysis(n_components = 20,random_state=123)
    ```

    因子的数量通过`n_components`参数定义。我们还为再现性定义了一个随机状态。

6.  Once the factor method is defined, we will fit the method on the training set and also transform the training set to get a new training set with the required number of factors. We will also note the time it takes to fit the required number of factors:

    ```
    # Fitting the Factor analysis method and transforming the training set
    import time
    t0 = time.time()
    X_fac=fa.fit_transform(X_train)
    t1 = time.time()
    print("Factor analysis fitting time:", round(t1-t0, 3), "s")
    ```

    在代码中，`.fit()`函数用于拟合训练集，而`transform()`方法用于获得具有所需因子数量的新训练集。

    您应该得到以下输出:

    ```
    Factor analysis fitting time: 130.688 s
    ```

    因子分析也是一种计算密集型方法。这就是为什么只选择了 20 个因素。我们可以看到,`20`因子用了`130.688`秒。

7.  我们现在用相同数量的因子来转换测试集:

    ```
    # Transforming the test set 
    X_test_fac=fa.transform(X_test)
    ```

8.  Let's verify the shapes of the train and test sets before transformation and after transformation:

    ```
    # Printing the shape of train and test sets before and after transformation
    print("original shape of Training set:   ", X_train.shape)
    print("original shape of Test set:   ", X_test.shape)
    print("Transformed shape of training set:", X_fac.shape)
    print("Transformed shape of test set:", X_test_fac.shape)
    ```

    您应该得到以下输出:

    ![Figure 14.33: Original and transformed dataset values
    ](image/B15019_14_33.jpg)

    图 14.33:原始和转换的数据集值

    您可以看到，训练集和测试集都减少到了`20`个因子。

9.  现在，让我们在转换后的数据集上拟合逻辑回归模型，并记录拟合模型所花费的时间:

    ```
    # Fitting a Logistic Regression Model
    from sklearn.linear_model import LogisticRegression
    import time
    facModel = LogisticRegression()
    t0 = time.time()
    facModel.fit(X_fac, y_train)
    t1 = time.time()
    ```

10.  Print the total time:

    ```
    print("Total training time:", round(t1-t0, 3), "s")
    ```

    您应该得到以下输出:

    ```
    Total training time: 0.028 s
    ```

    我们可以看到，拟合逻辑回归模型所用的时间与其他方法相当。

11.  Let's now predict on the test set and print the accuracy metrics:

    ```
    # Predicting with the factor analysis model
    pred = facModel.predict(X_test_fac)
    print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(facModel.score(X_test_fac, y_test)))
    ```

    您应该得到以下输出:

    ```
    Accuracy of Logistic regression model prediction on test set: 0.92
    ```

    我们可以看到，因子模型比 ICA 模型具有更好的结果，但比其他模型的结果差。

12.  Print the confusion matrix:

    ```
    from sklearn.metrics import confusion_matrix
    confusionMatrix = confusion_matrix(y_test, pred)
    print(confusionMatrix)
    ```

    您应该得到以下输出:

    ![Figure 14.34: Resulting confusion matrix
    ](image/B15019_14_34.jpg)

    图 14.34:产生的混淆矩阵

    我们可以看到，因子模型在广告分类方面比 ICA 模型做得更好。然而，仍然有大量的误报。

13.  Print the classification report:

    ```
    from sklearn.metrics import classification_report
    # Getting the Classification_report
    print(classification_report(y_test, pred))
    ```

    您应该得到以下输出:

    ![Figure 14.35: Resulting classification report
    ](image/B15019_14_35.jpg)

图 14.35:结果分类报告

正如我们在结果中看到的，通过将变量减少到 20 个因子，我们能够得到一个不错的分类结果。即使结果有所下降，我们仍然有一些可管理的功能，这将能够在任何算法上很好地扩展。需要通过更多的实验来探索精度测量和管理功能之间的平衡。

你认为我们如何改善因子分析的分类结果？

增加元件数量会导致精度指标的提高。

# 比较不同的降维技术

既然我们已经学习了不同的降维技术，让我们将所有这些技术应用到我们将从现有 ads 数据集创建的新数据集。

我们将从已知分布中随机抽取一些数据点，然后将这些随机样本添加到现有数据集中，以创建新的数据集。让我们进行一个实验，看看如何从现有的数据集创建新的数据集。

我们导入必要的库:

```
import pandas as pd
import numpy as np
```

接下来，我们创建一个虚拟数据帧。

在这个例子中，我们将使用一个两行三列的小型数据集。我们使用`pd.np.array()`函数创建一个数据框:

```
# Creating a simple data frame
df = pd.np.array([[1, 2, 3], [4, 5, 6]])
print(df.shape)
df
```

您应该得到以下输出:

![Figure 14.36: Sample data frame
](image/B15019_14_36.jpg)

图 14.36:样本数据框

我们接下来要做的是对一些与我们创建的数据框形状相同的数据点进行采样。

让我们从具有平均值`0`和标准偏差`0.1`的正态分布中抽取一些数据点。我们在*第三章“二元分类”中简单提到了正态分布。*。正态分布有两个参数。第一个是平均值，即分布中所有数据的平均值，第二个是标准差，即衡量数据点分布情况的指标。

通过假设平均值和标准差，我们将能够使用`np.random.normal()` Python 函数从正态分布中抽取样本。我们必须给这个函数的参数是新数据集的平均值、标准差和形状。

让我们看看这是如何在代码中实现的:

```
# Defining the mean and standard deviation
mu, sigma = 0, 0.1 
# Generating random sample
noise = np.random.normal(mu, sigma, [2,3]) 
noise.shape
```

您应该得到以下输出:

```
(2, 3)
```

正如我们所看到的，我们给出平均值(`mu`)、标准差(`sigma`)和数据帧的形状`[2,3]`来生成新的随机样本。

打印采样数据帧:

```
# Sampled data frame
noise
```

您将获得以下输出:

```
array([[-0.07175021, -0.21135372,  0.10258917],
       [ 0.03737542,  0.00045449, -0.04866098]])
```

下一步是添加原始数据框和采样数据框以获得新数据集:

```
# Creating a new data set by adding sampled data frame
df_new = df + noise
df_new
```

您应该得到以下输出:

```
array([[0.92824979, 1.78864628, 3.10258917],
      [4.03737542, 5.00045449, 5.95133902]])
```

了解了如何创建新数据集后，让我们在下一个活动中使用这些知识。

## 活动 14.02:增强型 Ads 数据集上的降维技术比较

你已经学会了不同的降维技术。对于您要创建的数据集，您需要确定其中哪种技术是最好的。

**提示**:在本次活动中，我们将使用到目前为止你在所有练习中使用过的不同技巧。您还将创建一个新的数据集，就像我们在上一节中所做的那样。

完成此活动的步骤如下:

1.  打开新的 Colab 笔记本。
2.  将原始 ads 数据归一化，并导出转换后的自变量`X_tran`。
3.  通过使用`pd.np.tile()`函数复制列两次，创建一个高维数据集。
4.  从均值= 0、标准差= 0.1 的正态分布中创建随机样本。使新数据集具有与在*步骤 3* 中创建的高维数据集相同的形状。
5.  将高维数据集和随机样本相加得到新的数据集。
6.  将数据集分为训练集和测试集。
7.  Implement backward elimination with the following steps:

    使用`RFE()`功能执行反向消除步骤。

    使用逻辑回归作为模型，并选择最佳的`300`特征。

    在训练集上拟合`RFE()`函数，并测量在训练集上拟合 RFE 模型所需的时间。

    用 RFE 模型转换训练集和测试集。

    对转换后的训练集拟合逻辑回归模型。

    对测试集进行预测，并打印准确度分数、混淆矩阵和分类报告。

8.  Implement the forward selection technique with the following steps:

    使用`SelectKBest()`功能定义特征的数量。选择最佳的`300`功能。

    使用`.fit()`功能在训练集上拟合正向选择，并记录拟合所用的时间。

    使用`.transform()`功能转换训练集和测试集。

    对转换后的训练集拟合逻辑回归模型。

    对转换后的测试集进行预测，并打印准确性、混淆矩阵和分类报告。

9.  Implement PCA:

    使用`PCA()`功能定义主要组件。使用 300 个组件。

    将`PCA()`安装到训练套件上。注意时间。

    使用`.transform()`函数转换训练集和测试集，以获得这些数据集各自的组件数量。

    对转换后的训练集拟合逻辑回归模型。

    对转换后的测试集进行预测，并打印准确性、混淆矩阵和分类报告。

10.  Implement ICA:

    使用`300`组件通过`FastICA()`功能定义独立组件。

    在训练集上拟合独立组件并转换训练集。记下实施的时间。

    使用`.transform()`函数转换测试集，以获得这些数据集各自的组件数量。

    对转换后的训练集拟合逻辑回归模型。

    对转换后的测试集进行预测，并打印准确性、混淆矩阵和分类报告。

11.  Implement factor analysis:

    使用`FactorAnalysis()`功能和`30`因子定义因子的数量。

    拟合训练集上的因子并转换训练集。记下实施的时间。

    使用`.transform()`函数转换测试集，以获得这些数据集各自的组件数量。

    对转换后的训练集拟合逻辑回归模型。

    对转换后的测试集进行预测，并打印准确性、混淆矩阵和分类报告。

12.  比较表中的所有方法。

**预期产量**:

您应该将结果制成表格，然后进行比较:

![Figure 14.37: Expected output of all the reduction techniques
](image/B15019_14_37.jpg)

图 14.37:所有缩减技术的预期输出

注意

活动的解决方案可以在这里找到:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。

在本次活动中，我们使用从互联网广告数据集创建的新数据集实施了五种不同的维度缩减方法。

从表中的结果，我们可以看到，三种方法(向后消除，向前选择，和主成分分析)得到了相同的准确性分数。因此，最佳方法的选择标准应基于获得降维所需的时间。根据这些标准，前向选择法是最好的方法，其次是 PCA。

第三，我们应该在准确性和降维时间之间取得平衡。我们可以看到，因子分析和逆向淘汰的准确率分数非常接近，分别为 96%和 97%。然而，与因子分析相比，反向消除所用的时间相当长。因此，我们应该考虑将因子分析作为第三个最好的方法，即使它的准确性比向后淘汰法稍低。最后一点应该去 ICA，因为精度远远低于所有其他方法。

# 总结

在这一章中，我们学习了各种降维技术。让我们总结一下本章所学的内容。

在本章的开始，我们介绍了一些现代数据集在可扩展性方面固有的挑战。为了进一步了解这些挑战，我们下载了互联网广告数据集，并开展了一项活动，见证了大型数据集带来的可扩展性挑战。在活动中，我们人工创建了一个大型数据集，并为其拟合了一个逻辑回归模型。

在随后的章节中，我们介绍了五种不同的降维方法。

**后向特征消除**按照逐个消除特征的原则工作，直到精度测量没有出现重大下降。这种方法是计算密集型的，但是我们得到了比基准模型更好的结果。

**前向特征选择**与后向消除方向相反，每次选择一个特征，以获得我们预定的最佳特征集。这种方法也是计算密集型的。我们还发现这种方法的准确性稍低。

**主成分分析** ( **PCA** )旨在找到相互正交的成分，最好地解释数据的可变性。我们用 PCA 得到了比基准模型更好的结果。

**独立成分分析** ( **ICA** )类似于 PCA 但是，它在选择组件的方法上有所不同。ICA 从数据集中寻找独立成分。我们看到 ICA 在我们的环境中取得了最差的结果之一。

**因子分析**就是寻找最能描述数据的因子或相关特征组。通过因子分析，我们取得了比 ICA 更好的结果。

本章的目的是让你掌握一套技术，在模型的可伸缩性具有挑战性的场景中提供帮助。获得好结果的关键是理解在什么场景中使用什么方法。这可以通过大量的实践和对大量大型数据集的实验来实现。

在本章中我们已经学习了一套管理可伸缩性的工具，我们将进入下一章，讨论提高性能的问题。在下一章，你将会被介绍到一种叫做集成学习的技术。这项技术将有助于提高机器学习模型的性能。