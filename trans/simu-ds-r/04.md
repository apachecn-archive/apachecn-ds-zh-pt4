<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 第四章。随机数的模拟

你能想象一个不产生随机数的统计模拟来评估和比较方法吗？你能想象一个不从预测或先验分布中抽取随机数的贝叶斯方法吗？你能想象任何没有随机性概念的机会游戏吗？你能想象一个没有随机性的世界吗？

一般来说，统计和概率论是基于概率空间和随机数的抽象概念。一个优雅的数学理论已经发展起来，它的出发点是随机数。

计算统计学、数据科学和统计模拟等特殊应用研究领域都采用了随机数的概念。由此，通常会产生/需要大量的**个独立且同分布的**(**I . I . d .**)随机数，尤其是为了仿真的目的。

然而，在计算机应用中，令人惊讶的是随机数大多是用确定性算法模拟的，这在某种程度上与基本理论相矛盾。本章的第一部分主要处理确定性随机数生成的问题。但是我们先来看看真正的随机数。

# 实数随机数

生成的*实数(true)* 随机数数应该是独立同分布随机变量的实现，并且应该是不可预测的，也就是说下一个生成的数与之前生成的随机数是不可预测的。例如，彩票和博彩业就依赖于他们。但是真/实随机数在统计学中也有用吗？在回答这个问题之前，我们先讨论一下实数随机数生成。

作为随机数的来源，可以使用以下随机数生成器:

*   抛硬币、掷骰子、轮盘赌等等
*   放射源的衰变
*   来自大气层的噪音([www.random.org](http://www.random.org)

可以观察到，真随机数的产生背后有一个物理过程。

例如，通过生成 0 和 1 的序列，这些比特应该是同等可能的，并且彼此独立出现。为了评估这种特性，可以使用统计测试。

然而，真随机数发生器在数据科学和统计模拟的应用中有一些缺点:

*   生成它们通常计算量很大
*   实施通常困难且耗时，因此成本更高
*   再现性不是给定的
*   通常不检查是否会产生随机数

一个突出的例子是一枚硬币在投掷时受损。这里，落在硬币正面的概率将会改变。一般来说，不可观察和不可控制的过程可能会对真正的随机发生器产生影响，而这种影响很难被证实。例如，从伽马射线在探测器上的撞击中获得的随机数可能偶尔会受到磁场变化的影响。

然而，主要是因为任何结果的再现性是一个非常重要的概念，实数生成器通常不用于统计、统计模拟和数据科学应用。真随机数发生器的应用领域尤其是在密码术和老虎机中，因为通常要求所产生的随机数是绝对不可预测的。

R 包`random`直接访问 www.random.org 的[。借助这个网站的设施，可以收集真正的随机](http://www.random.org)号码。随机数发生器基于大气噪声和撞击(我们和)地球表面的伽马射线的测量。使用`randomNumbers`功能，随机数从该网站流出:

```
library("random")
x <- randomNumbers(n=10000, col=2, min=0,
 max=1e+06, check=TRUE) / 1000000

```

作为随机数发生器正常工作的第一个指标，原始值与滞后值相对应(见*图 4.1* )。这意味着第一个值与第二个值相对应，第二个值与第三个值相对应，依此类推。如果下图中显示了某个结构，则这些值可能是自相关的:

```
n <- length(x)
df <- data.frame(x1 = x[1:(n-1)], x2 = x[2:n])
ggplot(df, aes(x = x1, y = x2)) + geom_point(size = 0.1) +
xlab("random numbers from random.org") + ylab("lag 1")

```

![Real random numbers](Images/B05113_04_01.jpg)

图 4.1:来自 www.random.org 的一系列随机数与它们的移位(滞后 1)值

并不是所有的测试，例如在 https://www.phy.duke.edu/~rgb/General/dieharder.php T2 的顽固测试电池都通过了这个生成的真随机数。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 模拟伪随机数

伪随机数字是几乎所有统计(蒙特卡罗)模拟的基础，它们被用于大量的统计问题。很好地模拟它们对于蒙特卡罗模拟的有效输出是至关重要的，并且重采样方法依赖于伪随机数的质量。

我们可以区分(至少)两种伪随机数发生器:

*   算术随机数生成器:正如名字所暗示的，它们完全基于算术。像![Simulating pseudo random numbers](Images/B05113_04_52.jpg)和 *e* 这样的无理数可以通过使用任何倍数的小数部分来用作随机数发生器。然而，很难确定无理数是否有周期性。另外，无理数在计算机中只能呈现为(有限)机器数；无理数在实际中很少用来生成随机数。
*   **递归算术随机数生成器**:它们基于的计算，从一个或多个先前的数字中产生一个新的随机数。通过设置初始值(种子)来确保恰好在重新运行程序时，产生相同的随机数，从而给出再现性。这些生成随机数的算法是确定性的，因为它们遵循精确的计算规则。因此，伪随机数序列是确定性的。

一个伪随机发生器是由具有初始状态![Simulating pseudo random numbers](Images/B05113_04_72.jpg)(有限数和机器数)的![Simulating pseudo random numbers](Images/B05113_04_53.jpg)、产生初始状态(种子)的![Simulating pseudo random numbers](Images/B05113_04_72.jpg)上的概率分布![Simulating pseudo random numbers](Images/B05113_04_73.jpg)、转移函数![Simulating pseudo random numbers](Images/B05113_04_54.jpg)，以及解空间![Simulating pseudo random numbers](Images/B05113_04_74.jpg)和输出函数![Simulating pseudo random numbers](Images/B05113_04_55.jpg)构成的

从逻辑上讲，![Simulating pseudo random numbers](Images/B05113_04_74.jpg)描述了区间 *(0，1)* 中的机器号。当前状态由![Simulating pseudo random numbers](Images/B05113_04_75.jpg)的![Simulating pseudo random numbers](Images/B05113_04_58.jpg)给出，结果(伪随机数)由:

![Simulating pseudo random numbers](Images/B05113_04_57.jpg)

。

伪随机数发生器的基本特征是周期。

![Simulating pseudo random numbers](Images/B05113_04_56.jpg)

这里，最小的 *j* 表征了伪随机发生器的周期长度。

对于 32 位计算机，最大周期用 *2 ^(32)* 限定。好的伪随机数发生器有一个接近*2^k的周期，其中 *k* 是计算机的位表示(现在通常是 64 位；在较旧的计算机上，它可能是 32 位或更低)。*

## 同余生成元

同余生成器因其简单而广受欢迎。但是，即使它们很简单，它们在某些条件下也能很好地工作，许多软件产品仍然使用某些类型。

同余生成器由以下参数定义:

*   状态值的数量![Congruential generators](Images/B05113_04_62.jpg)，顺序
*   模式![Congruential generators](Images/B05113_04_63.jpg)
*   系数![Congruential generators](Images/B05113_04_64.jpg)
*   增量![Congruential generators](Images/B05113_04_65.jpg)
*   起始值 *$x_1，...，x_n {0，...，m-1 } $(种子)*

对于*我> n* 现在一集:

![Congruential generators](Images/B05113_04_66.jpg)

当时的状态，*我*因此是![Congruential generators](Images/B05113_04_67.jpg)。

通过适当选择 *m* 和 *b* ，可以获得最大周期(例如，参见(Knuth 1998))。

## 线性和乘法同余生成器

最简单的同余生成器是线性和乘法同余生成器。

通常， *n* (见上一次出现)等于 *1* 。如果是这种情况，那么前面的等式简化为:

![Linear and multiplicative congruential generators](Images/B05113_04_68.jpg)

作为一个玩具例子，设 *a = 2* 、 *y [0]* *= 1* 、 *b = 1* 、 *m = 10* 。然后，随机数是 1，3，7，5，1，3，7，5，1，3，7，5，1，等等。周期为 *4* 。

通过适当选择 *b* (其中 *b* 和 *m* 互质) *m* 和 *a* 可以获得最大周期长度。

一个不错的选择是，比如 *m = 2^32，a = 69069，b = 23606797* 。

当 *b = 0* 时，产生所谓的乘法同余。

关于线性和乘法同余生成器的文献指出，这些生成器具有超平面行为。如果我们将序列![Linear and multiplicative congruential generators](Images/B05113_04_76.jpg)滞后于 *k* 元组![Linear and multiplicative congruential generators](Images/B05113_04_77.jpg)，那么这些 *k* 元组(在![Linear and multiplicative congruential generators](Images/B05113_04_78.jpg)中)位于最多![Linear and multiplicative congruential generators](Images/B05113_04_79.jpg)个平行超平面上。

*图 4.2* 显示了使用错误的参数值选择(![Linear and multiplicative congruential generators](Images/B05113_04_81.jpg))的 *k = 3* 的结果，而*图 4.3* 显示了良好的参数值选择(![Linear and multiplicative congruential generators](Images/B05113_04_80.jpg)):

![Linear and multiplicative congruential generators](Images/B05113_04_02.jpg)

图 4.2:线性同余发生器参数选择不当的滞后随机数。滞后序列落入三个二维平面

参数的另一个(更好的)选择可能导致更好的结果；看看*图 4.3* :

![Linear and multiplicative congruential generators](Images/B05113_04_03.jpg)

图 4.3:线性同余发生器参数选择良好的滞后随机数

再举一个例子，我们来看看一个非常著名的线性同余发生器，**杜然**。杜然，由 IBM 构建并使用多年，在 IBM 大型机中充当随机数的标准生成器。

该发生器由参数设置![Linear and multiplicative congruential generators](Images/B05113_04_82.jpg)决定。这导致整数伪随机数![Linear and multiplicative congruential generators](Images/B05113_04_83.jpg)在区间![Linear and multiplicative congruential generators](Images/B05113_04_84.jpg)中。

使用转换![Linear and multiplicative congruential generators](Images/B05113_04_37.jpg)，这些随机数可以很容易地转换成*【0，1】*中的有理数。

为了更好地观察杜然的差异和不良行为，下面的代码产生了交互式 3D 可旋转图形(本书中未显示)。杜然的结果看起来几乎和*图 4.1* 中的结果一样糟糕。通过运行以下代码，可以清楚地看到这些点落在 15 个二维平面中:

```
seed <- 123
randu <- function(n) {
 for (i in 1:n) {
 seed <<- (65539 * seed) %% (2^31)
 result[i] <- seed / 2^31
 }
 return(result)
}
plot3S <- function(func, n=10000) {
 x <- func(n)
 require("rgl")
 x1 <- x[1:(n-2)]
 x2 <- x[2:(n-1)]
 x3 <- x[3:n]
 plot3d(x1, x2, x3, size=3)
 play3d(spin3d())
}

```

要查看结果，请运行以下行:

```
plot3S(randu)
## to compare it with R's standard generator
plot3S(runif) ## (Mersenne-Twister)

```

由于杜然在 20 世纪 70 年代早期被广泛使用，当时的许多结果被认为是可疑的。对这个线性同余发生器使用这些参数值的原因是，可以使用某些计算机硬件的特殊功能快速地抽取随机数。IBM 纠正了它，杜然不再被使用。

简单同余、线性同余和乘法同余通常具有短周期，这就是为什么多个连接的线性同余发生器可能是有利的。

## 滞后的斐波那契生成器

另一种类型的生成器是由斐波那契类型的生成器定义的。这类随机数生成器旨在对前面讨论的线性同余生成器进行改进。

通过选择![Lagged Fibonacci generators](Images/B05113_04_85.jpg)和 b = 0，斐波纳契发生器最初产生的结果是![Lagged Fibonacci generators](Images/B05113_04_36.jpg)。

滞后的斐波那契生成器由![Lagged Fibonacci generators](Images/B05113_04_86.jpg)给出

其中 s < r and ![Lagged Fibonacci generators](Images/B05113_04_87.jpg)是$ +、-、* $或*或*-运算符![Lagged Fibonacci generators](Images/B05113_04_88.jpg)的算术运算。

滞后斐波那契发生器的一个例子是![Lagged Fibonacci generators](Images/B05113_04_89.jpg)，也称为 RAN3。

## 更多发电机

当然，还有比前面讨论的伪随机发生器更先进的方法。好消息是它们已经可以使用了；坏消息是它们通常非常复杂，解释它们超出了本书的范围。在下面的例子中，我们用 r。

R 的默认随机数是 Mersenne Twister 算法(Matsumoto 和 Nishimura 1998)。这个生成器保证了 624 维的均匀分布。这可以通过键入以下命令来查看:

```
RNGkind()
## [1] "Mersenne-Twister" "Inversion"

```

人们意识到 Mersenne Twister 算法是 r 中的默认算法。此外，反演方法(见下一节)使用反演方法将值从均匀分布转换为正态分布。在*图 4.4* 中，从该选择中产生的随机数显示在左侧图形中，而使用 Super-Duper 算法(Reed、Hubert 和 Abrahams，1982 年)和 Box-Muller 方法(Box 和 Muller，1958 年)的结果显示在右侧:

```
ok <- RNGkind()
op <- par(mfrow=c(1,2), mar=c(3,4,2,2))
set.seed(111)
hist(rnorm(1000), main="Mersenne Twister, Inversion", freq=FALSE, xlim=c(-4,4), ylim=c(0,0.43), cex.main=0.7)
curve(dnorm(x), col = 2, lty = 1, lwd = 2, add = TRUE)
RNGkind("Super", "Box-Muller")
RNGkind()
## [1] "Super-Duper" "Box-Muller"
hist(rnorm(1000), main="Super-Duper, Box-Muller", freq=FALSE, xlim=c(-4,4), ylim=c(0,0.43), cex.main=0.7)
curve(dnorm(x), col = 2, lty = 1, lwd = 2, add = TRUE)

```

![More generators](Images/B05113_04_04.jpg)

图 4.4:用 Mersenne Twister 和反转法绘制的随机数(左)和用 Super-Duper 绘制的随机数，以及用 Box-Muller 法转换成标准正态分布的随机数

然而，更多的发生器甚至可以用在基层 r。

当生成随机数时，我们也可以立即想到随机漫步作为一个例子。对于这个例子，200，000 个随机数被生成并存储在 100，000 个观察值和 2 列的矩阵中。

在随机漫步中，假设一个醉得连方向感都没有的醉汉会回到他离开的酒吧，而且只要他还能设法走路，他就能找到回家的路。我们还假设他身体状况良好，可以走很远很远的路(无限远)。为了确保醉汉每次去酒吧后都能找到回家的路，我们重复了 50 次，产生了 50 个轨迹(轨迹从黑色到浅灰色)。

![More generators](Images/B05113_04_05.jpg)

图 4.5:一个醉汉的随意行走

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 非均匀分布随机变量的模拟

至此，对同一随机变量的模拟进行了讨论。实际上，均匀随机数的生成是非常重要的一步。产生非均匀随机数的方法是不同的。主要目的是将随机数从一种均匀分布转换成另一种分布。一般来说，均匀分布的随机变量可以相应地进行变换和修改，以获得其他分布。

## 反转法

条件是在区间[0，1]内已经产生了均匀分布的随机数。反演方法利用了分布函数也在区间[0，1]中定义的事实。

设![The inversion method](Images/B05113_04_90.jpg)为 v 的分布函数，通过将一个均匀随机数![The inversion method](Images/B05113_04_91.jpg)插入到逆分布函数中

![The inversion method](Images/B05113_04_92.jpg)

我们得到一个分布为 v 的随机数。

因此，应用反演过程的先决条件是存在反函数的解析形式，或者至少存在反函数的近似形式。如果是近似的，随机数的质量很大程度上取决于反函数的近似程度。

设 X 是具有分布函数的连续随机变量

![The inversion method](Images/B05113_04_93.jpg)

。通常指分布函数的 p 分位数

![The inversion method](Images/B05113_04_94.jpg)

。

设 U 是区间[0，1]上的均匀分布随机变量。那么![The inversion method](Images/B05113_04_95.jpg)具有分布函数 F (x)的期望分布。这个*定理*我们不给出证明，但是查阅任何一本数理统计的经典书籍，都包括这个证明。

让我们开始实践，从一个非常简单的离散分布开始:伯努利分布。从服从均匀分布的随机数，将它们转换成由一个参数![The inversion method](Images/B05113_04_98.jpg)确定的伯努利分布![The inversion method](Images/B05113_04_96.jpg)是非常简单的。使用 R，这可能看起来像(![The inversion method](Images/B05113_04_97.jpg)):

```
u <- runif(100, 0, 1)
ifelse(u <= 0.2, 0, 1)
##   [1] 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1
##  [36] 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0
##  [71] 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1

```

我们如何用图表展示这一点？下图中我们看到一个等于 **0.554** 的随机数是从 U(0，1)中抽取的。由于大于 **0.2** ，所以预计为 **1** :

![The inversion method](Images/B05113_04_06.jpg)

图 4.6:从均匀 U(0，1)到伯努利![The inversion method](Images/B05113_04_99.jpg)分布。![The inversion method](Images/B05113_04_100.jpg)小于 0.2 的均匀随机数等于 0，否则为 1

以下示例显示了连续分布(指数分布)的生成。

指数的分布由![The inversion method](Images/B05113_04_101.jpg)和![The inversion method](Images/B05113_04_102.jpg)给出。![The inversion method](Images/B05113_04_103.jpg)，现确定如下:

![The inversion method](Images/B05113_04_33.jpg)

由于![The inversion method](Images/B05113_04_34.jpg)的原因，下面的公式适用于分布为 F(X)的![The inversion method](Images/B05113_04_35.jpg)。

这看起来像下面的截图:

![The inversion method](Images/B05113_04_07.jpg)

图 4.7:将随机数从均匀分布转换成指数分布

从*图 4.7* 中，很容易看出指数分布一定是右偏的，因为在这个例子中大多数值都很小。对于与前一个示例中绘制的值相同的值( **0.554** )，得到的投影值约为。`0.807`。

让我们切换到 r，利用反演方法和函数`runif()`，我们可以用反指数函数模拟指数分布的随机数。结果图(*图 4.8* )显示了不同参数![The inversion method](Images/B05113_04_104.jpg)下的模拟指数分布:

```
library("MASS")
invExp <- function(n, lambda = 1) {
 u <- runif(n)
 x <- -(1/lambda) * log(u)
 return(x)
}
lambda <- c(0.5, 1, 2, 5)
par(mar = c(3,3,3,0), mfrow = c(2, 2))
for (l in lambda) {
 sample <- invExp(10000, l)
 truehist(sample, nbins = 20, col = "limegreen", 
 main = paste("lambda =", l), xlab = "")
 curve(dexp(x, l), from = 1e-10, add = TRUE)
}

```

![The inversion method](Images/B05113_04_08.jpg)

图 4.8:使用反演方法模拟服从指数分布的随机数

我们还看到，一旦知道了逆分布函数，反演方法就很容易应用。然而，![The inversion method](Images/B05113_04_105.jpg)通常很难分析或不可能计算。甚至正态分布的逆分布也不是精确已知的，并且必须近似为。逼近通常是通过拟合多项式来完成的。对于其他分布，如 Beta 分布，通常使用其他方法，如后面解释的拒绝方法。

## 别名法

这个方法允许一个人模拟分类变量。

也许你经常用 R 中的函数`sample`来模拟给定概率的离散随机数？下面是一个例子:

```
sample(1:3, 10, replace = TRUE, prob = c(3/7,1/7,3/7))
##  [1] 3 3 1 3 1 3 1 3 3 3

```

但是这种算法如何在非常重要的`sample`函数背后工作呢？

基本上，别名法(Walker 1977)可以用来模拟给定概率函数![The alias method](Images/B05113_04_69.jpg)的离散(单变量)随机数。

基本规则是从一个矩形中抽取*随机数，其中矩形的高度为![The alias method](Images/B05113_04_106.jpg)，宽度由 n 给定，n 为不同类别的个数。*

让我们想象一下(参见*图 4.9* ，左侧)我们之前的例子，其中我们模拟了概率为 3/7、1/3 和 3/7 的从 1 到 3 的离散值。一种所谓的*设置*方法用于将该条形图重新排列成矩形。好消息是这总是可能的:

![The alias method](Images/B05113_04_09.jpg)

图 4.9:左边是三个类别的概率。右边是到矩形格式的转换

因此，对于我们的示例，三个概率必须重新排列以形成一个矩形格式。

对于*图 4.9* 中的矩形表示，我们看到在 *x* 轴方向上，两个类别被表示了两次。例如，在第二个箱中，x [1] 和 x [3] 堆叠在一起。一个区域的替代值(这里是 x [3] )称为别名值。

我们在下表中说明了相应的别名值:

| 

我

 | 

xi

 | 

圆周率

 | 

人工智能

 | 

(美国)罗得岛州(Rhode Island)ˌ剩余收入(residual income)

 |
| --- | --- | --- | --- | --- |
| `1` | `x1` | `3/7` | `x1` | `1` |
| `2` | `x3` | `3/7` | `x1` | `5/7` |
| `3` | `x2` | `1/7` | `x3` | `3/7` |

为了生成随机数，将使用以下算法:

*   模拟从![The alias method](Images/B05113_04_109.jpg)中独立抽取的两个随机数![The alias method](Images/B05113_04_107.jpg)和![The alias method](Images/B05113_04_108.jpg)
*   设置![The alias method](Images/B05113_04_110.jpg)
*   如果![The alias method](Images/B05113_04_111.jpg)返回![The alias method](Images/B05113_04_112.jpg)，否则返回![The alias method](Images/B05113_04_113.jpg)

回到我们的例子，这意味着我们首先从 U(0，1)中抽取一个随机数。想象这是 0.1。我们计算出![The alias method](Images/B05113_04_114.jpg)我们在第一个箱中，并选择![The alias method](Images/B05113_04_115.jpg)。假设从 U(0，1)中抽取的下一个数是 0.5。那么，z = 2。然后我们必须从 U(0，1)中抽取另一个随机数![The alias method](Images/B05113_04_118.jpg)；想象这是 0.2。由于![The alias method](Images/B05113_04_117.jpg)，我们选择![The alias method](Images/B05113_04_116.jpg)。

alias 方法在一个非常简单的示例中显示为，但是应用 alias 方法来模拟具有更多类别的离散值是一个简单的过程。

## 用对数线性模型估计表格中的计数

构建高级独立性测试需要模拟表格的值(如果表格的单元格值彼此独立)，或者如果只有边距可用但需要(预期的)单元格值。

让我们关注一个独立性测试的实际例子。我们使用下表中的数值，这些数值包含了雨量计显示的一个季节的 24 小时降雨量:

|   | 

春天

 | 

夏天

 | 

秋天

 | 

冬天

 | 

总和

 |
| --- | --- | --- | --- | --- | --- |
| `30` | `275` | `302` | `357` | `198` | `1132` |
| `60` | `56` | `20` | `43` | `37` | `156` |
| `125` | `52` | `29` | `53` | `44` | `178` |
| `250` | `65` | `17` | `52` | `69` | `203` |
| `500` | `37` | `15` | `54` | `58` | `164` |
| `1000` | `23` | `5` | `50` | `42` | `120` |
| `Sum` | `508` | `388` | `609` | `448` | `1953` |

例如，对于独立性的 Pearson ![Estimation of counts in tables with log-linear models](Images/B05113_04_119.jpg),估计预期的细胞计数，通常通过乘以裕度并除以总计数。对于第一个单元格值，预期值将是![Estimation of counts in tables with log-linear models](Images/B05113_04_32.jpg)。

但是，也可以使用对数线性模型来估计像元值。这不仅在独立性测试的情况下是有用的，而且对于估计频率的其他应用也是有用的，例如，在统计披露控制的研究领域，而小的估计频率违反了隐私法。

使用对数线性模型来估计像元值(频率)，重要的是假设计数的某种分布假设。通常，细胞计数的基本分布是泊松分布的假设被很好地接受。

关注的是细胞概率。这些像元概率可以用回归模型中的 logit 链接函数来建模:

![Estimation of counts in tables with log-linear models](Images/B05113_04_31.jpg)

用 *k* 作为单元格值的个数。 *o* 代表*赔率*。

公开此公式会导致以下结果:

![Estimation of counts in tables with log-linear models](Images/B05113_04_30.jpg)

logit 函数的逆函数称为逻辑函数。如果 logit ![Estimation of counts in tables with log-linear models](Images/B05113_04_120.jpg)，则![Estimation of counts in tables with log-linear models](Images/B05113_04_121.jpg)

让我们举一个例子，用我们的二维表格计算降水量:

```
x <- data.frame("spring" = c(275,56,52,65,37,23),
 "summer" = c(302,20,29,17,15,5),
 "autumn" = c(375,43,53,52,54,50),
 "winter" = c(198,37,44,69,58,42))

```

使用对数线性模型估计以下像元值:

```
xx <- expand.grid(rownames(x), colnames(x)) # all combinations
x1 <- xx[,1]
x2 <- xx[,2]
y <- as.vector(t(prop.table(x))) # cell probabilites
form <- y ~ x1:x2  # modell
mod <- glm(form, family="poisson") # estimation
pred <- (predict(mod))  # prediction
pred <- exp(pred)/(1+exp(pred))  # transf. with logistic function
round(matrix(pred, ncol=4, byrow=TRUE) * sum(x)) # table
##      [,1] [,2] [,3] [,4]
## [1,]  241  262  302  180
## [2,]   54   20   42   36
## [3,]   51   29   52   43
## [4,]   63   17   51   67
## [5,]   36   15   53   56
## [6,]   23    5   49   41

```

这些期望值现在可以输入到独立性的![Estimation of counts in tables with log-linear models](Images/B05113_04_120.jpg)测试中。

## 拒绝抽样

在不知道正态分布的逆的情况下，如何从正态分布中模拟出随机数？如何从 Beta 分布中模拟随机数？如何从已知密度函数的分布中模拟随机数？

答案是使用拒绝抽样:一种非常直观和简单(但非常强大)的方法来模拟几乎任何分布的随机数。

剔除方法基于密度，而不是反演方法中使用的分布函数。其基础是从一个易于模拟的分布中抽取数字，并根据某种接受标准接受它们(或根据拒绝标准拒绝它们)。

如果生成了具有密度函数 f(x)的随机数，并且存在另一个具有![Rejection sampling](Images/B05113_04_123.jpg)的密度函数(建议密度)h(x)，则以下算法模拟具有密度 f(x)的随机数:

1.  产生独立的随机变量![Rejection sampling](Images/B05113_04_124.jpg)和![Rejection sampling](Images/B05113_04_126.jpg)。
2.  如果![Rejection sampling](Images/B05113_04_61.jpg)接受并放置![Rejection sampling](Images/B05113_04_125.jpg)。
3.  否则，放弃并继续 1。

这个一般概念现在以实际行动来展示，首先是模拟正态分布的值。

#### 模拟正态分布的数值

作为第一个例子，我们使用拒绝抽样来模拟来自正态分布的随机数。

作为建议分布(易于模拟的值分布)，采用柯西分布。对于柯西分布，分布函数的逆可以不用任何近似而解析地表示。

我们希望模拟来自 *N(0，1)* 的随机数。标准正态分布的密度是，![Simulating values from a normal distribution](Images/B05113_04_59.jpg)和来自柯西分布的![Simulating values from a normal distribution](Images/B05113_04_60.jpg)。

人们可以展示以下内容

![Simulating values from a normal distribution](Images/B05113_04_51.jpg)

接受概率由下式给出:

![Simulating values from a normal distribution](Images/B05113_04_50.jpg)

为了说明，计算柯西和正态分布的理论值:

```
x <- seq(-5, 5, length = 200)
dc <- dcauchy(x, location = 0, scale = 1)
dn <- dnorm(x, mean = 0, sd = 1)

```

通过绘制两个密度的估计值来比较这两个密度，使用下面的代码片段。输出显示在*图 4.10* 中:

```
par(mfrow=c(1,2))
plot(x, dn, type="l")
lines(x, dc, col="blue", lty=2)
legend("topright", col=c("black", "blue"), lty=c(1,2), legend = c("normal", "Cauchy"), cex=0.5)
plot(x, dn/dc, type="l")

```

![Simulating values from a normal distribution](Images/B05113_04_10.jpg)

图 4.10:左边是标准正态和标准柯西分布的密度。柯西分布更广泛，但它没有完全覆盖正态分布(因为对于任何密度 f(x) = 1 成立)。右边是正常密度和柯西密度之间的比率。最高比例约为 1 比 1

接下来确定参数 a 的最佳选择。最优选择是 a⋅h(x 完全覆盖 f(x)的最小值，对于任何 x:

```
foo <- function(x) dnorm(x)/dcauchy(x)
opt <- optimize(foo, c(0, 4), maximum=TRUE)
a <- opt$objective
a
## [1] 1.520347
ah <- a * dc

```

对于任何 x 和 h(x)，柯西分布和正态分布的密度都低于 a⋅h(x；参见*图 4.11* :

```
plot(x, dn, type="l", ylim=c(0,0.5), lwd=2)
lines(x, dc, col="blue", lty=2)
lines(x, ah, col="blue", lty=2, lwd=2)
legend("topright", col=c("black", "blue", "blue"), lty=c(1,2,2), lwd=c(1,1,2), legend = c("normal", "Cauchy", "a * Cauchy"), cex=0.5)

```

![Simulating values from a normal distribution](Images/B05113_04_11.jpg)

图 4.11:正常密度、柯西密度和乘以柯西密度

a⋅h(x 之间的区别)如下图所示:

```
plot(x, dn, type="l", ylim=c(0,0.5), lwd=2)
polygon(x, dn, col="gray")
polygon(c(x, rev(x)), c(dn, rev(ah)), col="blue")

```

![Simulating values from a normal distribution](Images/B05113_04_12.jpg)

图 4.12:和之间的差异

接受概率可以写成一个函数，拒绝抽样也是如此:

```
alpha <- function(x){
 dnorm(x)/(1.520347 * dcauchy(x))
}

rejectionNorm <- function(n) {
 x <- rcauchy(10000,0,1)
 u <- runif(10000)
 return(na.omit(ifelse(u <= alpha(x), x, NA)))
}

```

我们现在可以使用拒绝采样来模拟来自正态分布的随机数，并且我们通过直方图显示相应的经验分布，并且我们还显示理论密度曲线；看看*图 4.13* :

```
set.seed(123)
x <- rejectionNorm(10000)
hist(x, prob=TRUE)
curve(dnorm(x), lty = 1, lwd = 2, add = TRUE)

```

![Simulating values from a normal distribution](Images/B05113_04_13.jpg)

图 4.13:使用拒绝抽样的标准正态分布模拟值的直方图。曲线显示了理论密度

#### 模拟来自贝塔分布的随机数

我们已经展示了如何使用拒绝方法来模拟正态分布值。但是，这对于其他发行版是如何工作的呢？答案很简单:和以前一样。我们使用建议分配，其中应满足两个问题:

*   录取概率应该很高。请注意，这个问题不像以前那么重要了，因为在接受率较低的情况下通常可以产生相当好的结果，而且从建议分布中模拟随机数通常并不耗时。
*   建议密度乘以 s 应该覆盖 f(x)的整个范围。

设![Simulating random numbers from a Beta distribution](Images/B05113_04_127.jpg)为随机变量，密度![Simulating random numbers from a Beta distribution](Images/B05113_04_128.jpg)为[0，1]v。

作为建议密度，我们可以(总是)使用 U(0，1)分布，如下图所示:

```
curve(dbeta(x, shape1 = 2, shape2 = 2), from = 0, to = 1,
 xlab = "", ylab = "", main = "")
## a * h(x):
abline(h = 1.5, lty = 2)

```

![Simulating random numbers from a Beta distribution](Images/B05113_04_14.jpg)

图 4.14: f(x) (Beta(2，2)分布)(实线)和 a⋅h(x)(均匀分布乘以 a)(虚线)

如果 Zβ(2，2)分布密度为 f(x)，U 均匀分布在[0，1]上密度为 h(x)，那么![Simulating random numbers from a Beta distribution](Images/B05113_04_129.jpg)。

接受概率由![Simulating random numbers from a Beta distribution](Images/B05113_04_130.jpg)给出:

```
rsBeta <- function(n) {
 z <- runif(n)
 u <- runif(n)
 ind <- (u <= 4 * z * (1 - z))
 return(z[ind])
}
set.seed(123)
sample1 <- rsBeta(10000)
acceptS <- length(sample1) / 10000
acceptS
## [1] 0.6716

```

让我们画出它，我们为它定义一个函数:

```
library(MASS)
plot1 <- function(s, shape1=2, shape2=2){
 truehist(s, h = 0.1, xlim = c(0, 1), #ylim = c(0,2),
 col="white", main = "", xlab = "")
 curve(dbeta(x, shape1 = shape1, shape2 = shape2),
 from = 0, to = 1, add = TRUE)
 d <- density(s, from = 0, to = 1, adjust = 2, 
 kernel = "gaussian")
 lines(d$x, d$y, lty = 2)
 legend("topright", 
 legend = c("true density", "density of simulated values"),
 col = c(1, 1), lty = c(1, 2), cex = 0.6)
}

```

这产生了图 4.15 中所示的结果，显示了我们使用拒绝采样方法模拟 a*β(2，2)* 值的结果:

```
plot1(sample1) # produces a histogram and curve, shown below:

```

![Simulating random numbers from a Beta distribution](Images/B05113_04_15.jpg)

图 4.15:使用拒绝采样的 Beta(2，2)分布模拟值的直方图

在前面的例子中，我们也看到接受率没有那么差；我们甚至采取了最简单的提案分发。

我们可以通过模拟任意区间分布中的随机数来扩展前面的例子。为此，有必要计算上限 a。对于![Simulating random numbers from a Beta distribution](Images/B05113_04_131.jpg)和![Simulating random numbers from a Beta distribution](Images/B05113_04_132.jpg)，以下代码将完成这项工作:

```
rsBeta2 <- function(n, shape1=2.5, shape2=6.5){
 a <- optimize(f=function(x){dbeta(x,shape1,shape2)},
 interval=c(0,1), maximum=TRUE)$objective
 z <- runif(n)
 u <- runif(n, max=a)
 ind <- (u <= dbeta(z,shape1,shape2)) 
 return(z[ind])
}
sample2 <- rsBeta2(10000
)

```

## 截断分布

我们回到逆分布函数已知或可以近似的情况。

为了在区间[a，b]内生成随机数，使用拒绝方法丢弃该区间之外的所有数。但是，当区间与必须接受或拒绝的值的整个范围相比很小时，这可能是非常低效的。那么，如何避免拒绝值但只模拟这样一个区间内的值呢？

更快的拒绝这里是反演方法。让![Truncated distributions](Images/B05113_04_133.jpg)和 Y 的分布限制在一个区间【a，b】。那么，对于 Y 的分布函数 G，它如下:

![Truncated distributions](Images/B05113_04_48.jpg)

![Truncated distributions](Images/B05113_04_49.jpg)

因此是具有分布函数 G(y)的随机数。

作为一个例子，我们看看柯西分布。我们只需要区间[4，5]内的值。使用拒绝方法，我们将拒绝这个百分比的值，为了在这个区间内模拟 10 个值，我们可以这样做:

```
# percentage of rejection
(1- (pcauchy(5) - pcauchy(4))) * 100
## [1] 98.48538
v <- rcauchy(1000)
v <- v[v >= 4 & v <= 5]
v
##  [1] 4.598988 4.117381 4.902618 4.933402 4.453769 4.630756 4.693866
##  [8] 4.785372 4.768864 4.274614 4.471191 4.340737 4.641484 4.059680
## [15] 4.639054 4.135258
v[1:10]
##  [1] 4.598988 4.117381 4.902618 4.933402 4.453769 4.630756 4.693866
##  [8] 4.785372 4.768864 4.274614

```

使用反转方法，这将成为接下来显示的内容:

```
Fa <- pcauchy(4)
Fb <- pcauchy(5)
u <- runif(10, min = 0, max = Fb - Fa)
qcauchy(Fa + u)
##  [1] 4.575576 4.607166 4.717217 4.151208 4.672747 4.582442 4.914843
##  [8] 4.774956 4.962344 4.038282

```

这节省了大量的计算时间，特别是对于大规模的模拟。

## Metropolis - Hastings 算法

几乎所有之前讨论的方法都使用反演(如果分布函数的逆已知或由数值积分确定)或拒绝采样。这些方法的共同点是模拟 i.i.d .随机数。

和前面讨论的方法一样，主要目的是从理论分布中模拟随机数。使用**马尔可夫链蒙特卡罗** ( **MCMC** )方法，我们不能模拟独立同分布随机数，但可以模拟马尔可夫链的相关变量。违反 i.i.d .假设的情况经常被用来解决更困难的问题。接下来，我们将讨论最常用的 MCMC 采样方法——Metropolis-Hastings 算法。

### 关于马尔可夫链的几句话

一些关于马尔可夫链的基本符号和理论有助于理解接下来关于各种大都会黑斯廷斯和吉布斯采样器的章节。

马尔可夫链![A few words on Markov chains](Images/B05113_04_134.jpg)是一个相依随机变量![A few words on Markov chains](Images/B05113_04_135.jpg)的序列，其中给定![A few words on Markov chains](Images/B05113_04_137.jpg)的![A few words on Markov chains](Images/B05113_04_134.jpg)的概率分布只取决于前一状态![A few words on Markov chains](Images/B05113_04_138.jpg)。条件概率称为转移核或马尔可夫核![A few words on Markov chains](Images/B05113_04_139.jpg)。

例如，对于一个简单的随机游走马尔可夫链，下面适用:

![A few words on Markov chains](Images/B05113_04_140.jpg)同独立于![A few words on Markov chains](Images/B05113_04_141.jpg)。

```
## Simple random walk Markov chain:
n <- 10; set.seed(123)
x <- numeric(n)
for(i in 2:n){
 x[i] <- x[i-1] + rnorm(1)
}
x
##  [1]  0.0000000 -0.1488433 -0.2237392  0.4429488  0.3460754  2.4281334
##  [7]  2.2873937  3.4153892  2.2176013  3.1395324

```

马尔可夫核![A few words on Markov chains](Images/B05113_04_142.jpg)在这里对应于![A few words on Markov chains](Images/B05113_04_143.jpg)密度，因此下面的代码等同于前面的代码行:

```
set.seed(123)
x <- numeric(n)
for(i in 2:n){
 x[i] <- rnorm(1, mean = x[i-1])
}
x
##  [1]  0.0000000 -0.1488433 -0.2237392  0.4429488  0.3460754  2.4281334
##  [7]  2.2873937  3.4153892  2.2176013  3.1395324

```

马尔可夫核总是可以公式化的。从前面的例子中，我们看到，用马尔可夫核表示等价于一个外定义的马尔可夫链。我们还可以观察到，下一个状态(值)只取决于前一个状态(值)。

当一个概率函数满足以下条件时，给出一个平稳随机过程:if ![A few words on Markov chains](Images/B05113_04_144.jpg)，then ![A few words on Markov chains](Images/B05113_04_145.jpg)。

从平稳过程的存在可以得出结论，只要周期足够长，平稳分布的样本就会产生，而与初始值无关。稍后，这将被称为老化阶段 MCMC 需要一定量的*时间*来产生来自稳定分布的值，因此属于老化阶段的第一个值将被删除。关于这个老化阶段的更多细节将在后面介绍。

此外，每个“区域”对于所定义的区域可以以正概率实现。而有了无限的虚马尔可夫链，就可以再次到达任意一点(递归)。将本章第一节中的例子与一个从酒吧回家的醉汉进行比较。他很有可能会回到酒吧。

再者，![A few words on Markov chains](Images/B05113_04_146.jpg)的极限分布是 f(遍历性)。这个属性有着深远的影响。一个马氏链产生一个平稳分布的遍历马氏链。然后，对于积分函数，h 保持在*平均值*中:

![A few words on Markov chains](Images/B05113_04_47.jpg)

大数定律和对于 MCMC 方法适用是什么意思(遍历定理)。这也意味着一条链可能不会收敛于期望值![A few words on Markov chains](Images/B05113_04_147.jpg)，但是当一遍又一遍地重复(独立地生成一系列马尔可夫链)时，大数定律成立。这相当于标准理论。

从现在起，我们将不再继续 MCMC 方法的收敛理论，而是从文献中得到这个结果:只要给定一个链的递归，MCMC 就收敛。接下来的问题是 MCMC 将多快收敛。换句话说，一个 MCMC 往往需要很长时间才能收敛，所以总是需要评估收敛速度。

至于拒绝采样，下一种方法的目的是找到一种从目标密度 f 生成样本的方法。然而，如前所述，MCMC 方法有一个缺点:违反了 i.i.d .假设，因为下一个模拟值总是依赖于前一个值。换句话说，样本是相关的。此外，在很长一段时间内，模拟值并不遵循期望的分布，也就是说，初始样本可能遵循非常不同的分布。如果算法的起始点在低密度的区域中，情况尤其如此。因此，有必要进行一段时间的预烧，在此期间，最初数量的样本会被丢弃(通常是前 1000 个左右)。

那么，为什么我们要使用下面描述的 Metropolis-Hastings 方法呢？

这是因为大多数简单的拒绝抽样方法都受到“维数灾难”的困扰，因为拒绝的概率随着维数呈指数增长。由于 Metropolis-Hastings 和其他 MCMC 方法不存在这种程度的问题，因此它们通常是唯一可行的解决方案。因此，MCMC 方法通常是能够为高维统计模型产生样本的唯一方法。此外，举例来说，Metropolis-Hastings 算法将总是有效的，即使有时很难设置该算法。

注意，自相关可以通过稀疏模拟的随机数序列来降低。这意味着在老化阶段之后，大约每 10 个模拟值被保留。然而，这也意味着需要大量的样本。

Metropolis 等人于 1953 年发表了基本的 Metropolis-Hastings 算法。目前的版本是基于(黑斯廷斯 1970)的通用算法。

该算法一般描述如下:

*   它必须指定给定状态![A few words on Markov chains](Images/B05113_04_221.jpg)如何生成下一个状态
*   有一个从建议分布中生成的候选点 Y
*   如果该候选点被接受，则->链在时间![A few words on Markov chains](Images/B05113_04_222.jpg)和![A few words on Markov chains](Images/B05113_04_223.jpg)移动到状态 Y
*   否则，链保持在状态
*   所需条件如下:

    *   不可约性:从任何状态到任何其他状态的非零概率转移(即使它发生在多个步骤中)
    *   正递归:如果直到我们返回的预期时间是有限的
    *   非周期性

通过一个具有站平稳分布 f 的马尔可夫核 K，生成一个马尔可夫链![A few words on Markov chains](Images/B05113_04_215.jpg)，使得![A few words on Markov chains](Images/B05113_04_214.jpg)的极限分布等于 f。难点在于寻找一个核 K。

设 f 是一个有关联条件概率密度![A few words on Markov chains](Images/B05113_04_212.jpg)的密度，在实际中很容易模拟。从 q，我们只需要基本上知道比率![A few words on Markov chains](Images/B05113_04_216.jpg)是已知的，并且![A few words on Markov chains](Images/B05113_04_213.jpg)有足够的可变性来覆盖 f

它总是成立的:对于每个条件密度 q，可以构造一个具有平稳分布 f 的 Metropolis-Hasting 核。

Metropolis-Hastings 算法细分如下(也可参考 Robert 和 Casella 2010 年的研究):

1.  选择一个建议分配![A few words on Markov chains](Images/B05113_04_217.jpg)
2.  从 q 生成![A few words on Markov chains](Images/B05113_04_218.jpg)并设置 t = 0
3.  重复(至少直到链收敛到平稳分布):

    1.  从![A few words on Markov chains](Images/B05113_04_219.jpg)

    2.  抽取![A few words on Markov chains](Images/B05113_04_220.jpg)
    3.  集合:![A few words on Markov chains](Images/B05113_04_70.jpg)

4.  用![A few words on Markov chains](Images/B05113_04_71.jpg)
5.  *t = t + 1*

让我们看一个例子，用 Metropolis-Hastings 算法模拟 Rayleigh 分布中的随机数(参见 Rizzo 2007)。

瑞利分布用于寿命建模。密度由下式给出:![A few words on Markov chains](Images/B05113_04_46.jpg)

对于建议分布，选择![A few words on Markov chains](Images/B05113_04_193.jpg)分布。这种分布就像瑞利分布一样偏斜。在下面的列表中，我们显示了软件 R 中的实现，因此我们用 1(而不是 0)来索引。这会导致以下顺序:

1.  取![A few words on Markov chains](Images/B05113_04_195.jpg)的密度为![A few words on Markov chains](Images/B05113_04_194.jpg)。
2.  从![A few words on Markov chains](Images/B05113_04_196.jpg)生成![A few words on Markov chains](Images/B05113_04_197.jpg)，设 t = 1，结果存储在 x[1]中。
3.  从![A few words on Markov chains](Images/B05113_04_198.jpg) :

    1.  重复从![A few words on Markov chains](Images/B05113_04_199.jpg) )
    2.  模拟 Y 画一个随机数，![A few words on Markov chains](Images/B05113_04_200.jpg)
    3.  用 x[i-1]，计算如下:
    4.  ![A few words on Markov chains](Images/B05113_04_201.jpg)
    5.  其中 f 是瑞利分布的密度，用参数![A few words on Markov chains](Images/B05113_04_203.jpg)![A few words on Markov chains](Images/B05113_04_202.jpg)-估算 Y 点的密度，![A few words on Markov chains](Images/B05113_04_204.jpg)![A few words on Markov chains](Images/B05113_04_206.jpg)-估算![A few words on Markov chains](Images/B05113_04_206.jpg)点的密度。
    6.  如果![A few words on Markov chains](Images/B05113_04_207.jpg)则 y 被接受并且![A few words on Markov chains](Images/B05113_04_208.jpg)被置位；否则![A few words on Markov chains](Images/B05113_04_209.jpg)适用。我们保存![A few words on Markov chains](Images/B05113_04_210.jpg)
    7.  t = t+1

此外，它认为(Rizzo 2007 年):

![A few words on Markov chains](Images/B05113_04_45.jpg)

这个公式可以进一步简化。此外，我们通过分别分析瑞利密度和![A few words on Markov chains](Images/B05113_04_211.jpg)密度来进一步简化示例；另见 Rizzo 2007 年。对于瑞利(-)密度，服务于以下 R 代码:

```
f <- function(x, sigma){
 if(any(x < 0)) return(0)
 stopifnot(sigma > 0)
 return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

```

在以下模拟中，通过建议分布对瑞利(`4`)进行采样:

```
i <- 2
xt <- x[i-1] <- rchisq(1, 1)
y <- rchisq(1, df=xt)

```

并且，对于每 y，![A few words on Markov chains](Images/B05113_04_192.jpg)计算。我们在下面的函数中总结了所有这些:

```
rrai <- function(n = 10000, burnin = 1000, thin = 10, sigma = 4, verbose = TRUE){
 ## raileigh density
 f <- function(x, sigma){
 if(any(x < 0)) return(0)
 stopifnot(sigma > 0)
 return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
 }
 x <- numeric(n)
 x[1] <- rchisq(1, df=1)
 k <- 0; u <- runif(n)
 for(i in 2:n){
 xt <- x[i-1]
 y <- rchisq(1, df=xt)
 num <- f(y, sigma) * dchisq(xt, df=y)
 den <- f(xt, sigma) * dchisq(y, df=xt)
 if(u[i] <= num/den){
 x[i] <- y
 } else {
 x[i] <- xt
 k <- k + 1 # y is rejected
 }
 }
 if(verbose) cat("acceptance rate:", (k-burnin)/n/thin, "\n")
 ## burn-in:
 if(burnin > 1) x <- x[(burnin+1):length(x)]
 ## thining:
 return(x[seq(1, length(x), thin)])
}

r <- rrai(n = 10000, thin = 1, burnin = 1)
## acceptance rate: 0.4045
r <- rrai(n = 10000, thin = 10, burnin = 1000)
## acceptance rate: 0.02982
length(r)
## [1] 900

```

我们看到在没有细化和跳过第一个值的情况下，接受率只有大约 40 %,而在细化(`thin=10`)和老化阶段，接受率大约为 3%。

为了将生成的样本视为随机过程的实现，我们可以将它相对于指数绘制成一条多边形线(注意，`qplot`是`ggplot`的缩写)。*图 4.16* 显示了与生成顺序(索引)(*$ x $-轴*)相关的模拟随机数(*$ y $-轴*):

```
qplot(1:length(r), r, geom="line", xlab="", ylab="random numbers from Rayleigh(4)")

```

![A few words on Markov chains](Images/B05113_04_16.jpg)

图 4.16:使用 Metropolis-Hastings 算法的模拟随机数索引。排除了老化，并且应用了细化来降低自相关性

模拟的随机数也可以与理论分位数进行比较，以评估它们是否遵循瑞利分布。我们对此使用 Q-Q 图。如果这些点沿着一条线，我们可以预期我们的模拟样本是从瑞利分布中提取的。正如我们从下图(*图 4.17* 中看到的，应该是这样的。为了生成该图，计算了瑞利分布的理论分位数以及经验分位数，并绘制了它们之间的关系:

```
a <- ppoints(length(r))
sigma <- 4
QR <- sigma * sqrt (-2 * log (1-a)) # quantiles of Rayleigh
Q <- quantile(r, a)
qqplot(QR, Q, main = "", xlab = "Rayleigh quantile", ylab = "sample quantile")

```

![A few words on Markov chains](Images/B05113_04_17.jpg)

图 4.17:Q-Q 图。比较瑞利分布的理论分位数和样本分位数(用 Metropolis-Hastings 算法模拟)

这个例子只是为了看看 Metropolis-Hastings 算法是如何工作的。我们选择这个例子作为(Rizzo 2007)中的相似例子仅仅是出于教导的原因。请注意，为此目的使用拒绝采样会更便宜(就拒绝率而言)且更有利(就 i.i.d .样本而言)。

### 大都会采样器

Metropolis 采样器是基本算法的一个简单版本，因为它涵盖了对称生成分布的特殊情况。

如果我们考虑一个对称分布![The Metropolis sampler](Images/B05113_04_188.jpg)，接受概率只取决于比率![The Metropolis sampler](Images/B05113_04_189.jpg)。

这让我们有机会再次解释 Metropolis-Hastings 基于这一简化版本的主要功能。

在下文中，我们对新值![The Metropolis sampler](Images/B05113_04_190.jpg)进行采样，并决定是否接受该值的比值![The Metropolis sampler](Images/B05113_04_191.jpg):

![The Metropolis sampler](Images/B05113_04_18.jpg)

图 4.18:从建议分布中模拟出一个新值![The Metropolis sampler](Images/B05113_04_186.jpg)，通过![The Metropolis sampler](Images/B05113_04_188.jpg)计算出接受概率。在左边，新值![The Metropolis sampler](Images/B05113_04_186.jpg)以概率 1 被接受，因为比率大于 1。在右边，新值被接受，f(yt)/f(xt) = 0.56。

在下面的两个例子中，我们也在每种情况下取一个对称分布。

首先，我们想要用 Metropolis-Hastings 算法模拟来自 Beta 分布的随机值(我们已经用拒绝方法完成了)。被认为再次模拟它的密度是 B(2，2)和 B(1，2)。提议密度 q 的一个候选是(再次)U(0，1)。使用`function`参数`n`(模拟值的数量)、`burnin`(老化阶段)、`thin`(细化)、`cand`(候选分布)、`target`(目标分布)、`shape1`(形状参数 1)和`shape2`(形状参数 2)，在以下代码中实现该算法:

```
mh <- function(n=10000, burnin=1000, thin=10, cand=runif,
 target=dbeta, shape1=2, shape2=2){
 if(burnin >= n){
 stop("burnin is larger than the number of simulations")
 }
 x <- rep(cand(1), n) # initialization
 for(i in 2:n){
 y <- cand(1)
 rho <- target(y,shape1,shape2)/
 target(x[i-1], shape1, shape2)
 x[i] <- x[i-1] + (y - x[i-1]) * (runif(1) < rho)
 }
 # burn-in
 x <- x[(burnin+1):n]
 return(x[seq(1, length(x), thin)])
}

```

来自两个 Beta 分布的模拟随机数的密度如图*图 4.19* 所示:

```
par(mfrow=c(1,2))
plot(density(mh()), main = "", xlab = "")
plot(density(mh(shape1=1)), main = "", xlab = "")

```

![The Metropolis sampler](Images/B05113_04_19.jpg)

图 4.19:从 Beta(2，2)和 Beta(1，2)得到的模拟随机数

有了前面的函数，现在很容易模拟来自其他分布的随机数。让我们为伽玛分布，特别是来自 G(2，4)的随机数做这件事。密度 q 的候选值是 N(1，2):

```
rgamma <- mh(cand = rnorm, target = dgamma)

```

根据 Metropolis-Hastings 算法的所有给定示例都可以很容易地用拒绝采样来实现，并且应该用拒绝采样来实现，因为拒绝采样防止了非同标识样本的缺点。我们举了这两个例子来说明大都市黑斯廷斯是如何运作的。

为更复杂的问题实现 Metropolis-Hastings 算法是困难的、特定于问题的，超出了本书的范围。关于基本阅读，请参考(A. Gelman 等人，2013 年)和(Robert 和 Casella，2010 年)，并查看 Metropolis-Hastings 算法的特定问题实现。

## 吉布斯采样器

Gibbs 采样器也属于 MCMC 方法类。最初，以医生吉布斯命名的算法是由(S. Geman 和 D.Geman 1984)描述的。它可以被视为一步 Metropolis-Hastings 算法，其中每个值都被接受。通过依次对每个变量/参数进行采样，吉布斯采样器可以完美地用于从大量变量/参数中进行采样。

出于动机，我们从两相吉布斯采样器开始。

### 两相吉布斯采样器

给定随机变量 X 和 Y 具有联合分布 p(X，Y)和条件分布![The two-phase Gibbs sampler](Images/B05113_04_172.jpg)和![The two-phase Gibbs sampler](Images/B05113_04_173.jpg)。

模拟马尔可夫链![The two-phase Gibbs sampler](Images/B05113_04_174.jpg)的两相吉布斯采样器如下:

*   修复![The two-phase Gibbs sampler](Images/B05113_04_175.jpg)
*   对于![The two-phase Gibbs sampler](Images/B05113_04_176.jpg)绘制:

    *   ![The two-phase Gibbs sampler](Images/B05113_04_177.jpg)
    *   ![The two-phase Gibbs sampler](Images/B05113_04_178.jpg)

这是假设可以从条件分布中抽取样本。

对于二元正态分布，如下:![The two-phase Gibbs sampler](Images/B05113_04_179.jpg)

这是为![The two-phase Gibbs sampler](Images/B05113_04_180.jpg) : ![The two-phase Gibbs sampler](Images/B05113_04_181.jpg)确定的吉布斯采样器

以下成立(无需证明):![The two-phase Gibbs sampler](Images/B05113_04_182.jpg)

递归后![The two-phase Gibbs sampler](Images/B05113_04_183.jpg):

并且![The two-phase Gibbs sampler](Images/B05113_04_184.jpg)处的马尔可夫链收敛到![The two-phase Gibbs sampler](Images/B05113_04_185.jpg)。

让我们为一个 2 相 Gibbs 抽样器写一个程序来模拟前面描述的二元正态分布。我们从低密度区域选择一个坏的开始，以观察老化阶段的需要；看一看图 4.20 中*的左侧图形。注意，可以应用细化来降低自相关。在*图 4.20* 的中间图中可以看到自相关性——下一个值取决于前一个值:*

```
gibbs_bivariate <- function(n = 1000, rho = 0.9, start = 0, burnin = 100, thin = 1){
 x <- y <- numeric(n)
 s <- 1 - rho^2
 x[1] <- start # to show effect of burnin
 for(t in 1:(n-1)){
 y[t+1] <- rnorm(1, rho*x[t], s)
 x[t+1] <- rnorm(1, rho*y[t+1], s)
 }
 s <- seq(burnin+1, n, thin)
 return(cbind(x[s], y[s]))
}
par(mfrow=c(1,3))
set.seed(123)
## bad start:
b0 <- gibbs_bivariate(n=200, start = 30, burnin=0)
## plot the results
plot(b0, type="o", xlab="x", ylab="y", main="burnin 0",
 cex.main=1.3, cex.lab=1.3)
set.seed(123)
plot(b0[20:200,], type="o", xlab="x", ylab="y", main="burnin 20",
 cex.main=1.3, cex.lab=1.3, col=grey(20:200/200))
set.seed(123)
plot(b0[20:200,], pch=20, xlab="x", ylab="y", main="burnin 20",
 cex.main=1.3, cex.lab=1.3)

```

![The two-phase Gibbs sampler](Images/B05113_04_20.jpg)

图 4.20:用吉布斯采样器模拟的二元正态分布数据。左边是所有模拟的随机数。中间是老化后，显示数据点的自相关。右边是老化后的模拟随机数

### 多相吉布斯采样器

为了解释多相吉布斯采样器，使用以下符号:让![The multiphase Gibbs sampler](Images/B05113_04_162.jpg)的![The multiphase Gibbs sampler](Images/B05113_04_163.jpg)的一维或多维分量具有相应的条件密度$ f1，ldots，![The multiphase Gibbs sampler](Images/B05113_04_164.jpg)，用![The multiphase Gibbs sampler](Images/B05113_04_165.jpg)

对于![The multiphase Gibbs sampler](Images/B05113_04_166.jpg)。

对于![The multiphase Gibbs sampler](Images/B05113_04_167.jpg)到![The multiphase Gibbs sampler](Images/B05113_04_168.jpg)的过渡，多相吉布斯采样器可以描述如下:

对于迭代![The multiphase Gibbs sampler](Images/B05113_04_169.jpg)给定的![The multiphase Gibbs sampler](Images/B05113_04_170.jpg)模拟，![The multiphase Gibbs sampler](Images/B05113_04_171.jpg)

### 在线性回归中的应用

吉布斯采样器通常用于回归分析和回归参数拟合的上下文中。下面是一个简单的例子，它展示了回归环境中 Gibbs 抽样的基本原理。

让数据对![Application in linear regression](Images/B05113_04_157.jpg)定义为以下线性相关性![Application in linear regression](Images/B05113_04_158.jpg)，其中![Application in linear regression](Images/B05113_04_159.jpg)和![Application in linear regression](Images/B05113_04_160.jpg)是未知的并且必须被估计，以及![Application in linear regression](Images/B05113_04_161.jpg)。它是:

![Application in linear regression](Images/B05113_04_42.jpg)

由下式给出的可能性:

![Application in linear regression](Images/B05113_04_43.jpg)

以下是估计参数的条件分布结果(无需证明):

![Application in linear regression](Images/B05113_04_44.jpg)

这个简单回归问题的代码如下。参数`alpha`、`beta`和`tau`定义了起始值:

```
lreg <- function(y, x, time, alpha = 0, beta = -2, tau = 1, burnin = 0, thin = 1){
 n <- length(y)
 ## alpha, beta, tau defining varibles
 res <- matrix(, ncol=3, nrow=time)
 for(i in 1:time){
 alpha <- rnorm(1, mean(y)  -beta * mean(x), 1 / (n  *tau))
 m <- (sum(x * y) - alpha * n * mean(x)) / sum(x**2)
 s <- 1 / (tau * sum(x**2))
 beta <- rnorm(1, m, s)
 w <- y - alpha - beta * x
 tau <- rgamma(1, ((n / 2) + 1), (sum(w**2) / 2))
 res[i,] <- c(alpha, beta, tau)
 }
 s <- seq(1, length((burnin + 1):nrow(res)), thin)
 res <- res[((burnin+1):nrow(res))[s], ]
 res <- data.frame(res)
 colnames(res) <- c("alpha", "beta", "tau")
 return(res)
}

```

我们举了一个与(罗伯特和卡塞拉，2010)相似的例子，因为它是完美的教学理由。*图 4.21* 显示了来自包`MASS`的`Cars93`数据集的 Gibbs 采样器的结果。在实践中，我们会迭代超过 100 次，我们跳过老化。此外，我们可以再次应用稀疏来降低参数估计的自相关性。图 4.21 中*的线条越深，迭代次数越高，也就是说，我们开始时很糟糕，但很快就收敛到黑色的解:*

```
data(Cars93, package = "MASS")
set.seed(123)
time <- 100
res <- lreg(Cars93$Price, Cars93$Horsepower, time = time)
par(mar = c(4,4,0.1,0.1))
plot(Cars93$Horsepower, Cars93$Price, pch=20, xlab = "Horsepower", ylab = "Price", type = "n")
range <- 1 - sqrt(1:time/time)
range <- range + 0.1
#range <- range/sum(2*range)
for(i in 1:time){
 abline(a = res[i, 1], b = res[i, 2], col=gray(range[i]))#sqrt(1-i/size)))
}
abline(a = res[i, 1], b = res[i, 2], col="red", lty=2,lwd=3)#sqrt(1-i/size)))
points(Cars93$Horsepower, Cars93$Price, pch=20)

```

![Application in linear regression](Images/B05113_04_21.jpg)![Application in linear regression](Images/B05113_04_156.jpg)

图 4.21:开始时不良的![Application in linear regression](Images/B05113_04_156.jpg)的回归拟合。线越黑，t 越大。虚线 I 是 100 次拟合后的解

## MCMC 样品的诊断

监控收敛对于以下情况很重要:

*   当一些生成的链没有收敛到目标分布时
*   当足够的迭代次数未知时
*   当老化样本的长度未知时
*   当通过检查简单链不能检测到缓慢收敛时

通过检查几个平行链，缓慢收敛应该更明显地观察到。

一般来说，以下诊断是有帮助的:

*   根据指数 1 至 T 绘制模拟值，T 为模拟随机数的长度。这给出了关于老化时间和可能收敛的指示。如果模拟的随机数序列(链)不收敛，增加链的长度，即模拟更多的随机数。
*   重复前一项几次。这让您对老化时间更有信心。
*   使用**自相关函数** ( **ACF** )绘制自相关图。其测量模拟随机数与滞后(lag)模拟随机数的相关性。ACF 缓慢下降表示收敛缓慢，高值表示自相关。这可以用来寻找独立的子样本。换句话说，如果自相关可见，则应用模拟链的细化。
*   如果可能的话，使用非常高的 T 独立地运行 MCMC 算法多次(例如，5-20)。这表明了趋同。
*   对于一个单一的收敛指标，可以使用盖尔曼-鲁宾方法(r .盖尔曼 A. 1992)。该方法基于比较几个生成的链相对于单变量汇总统计方差的行为。我们需要(B)和(W)序列之间的方差。序列间方差为![The diagnosis of MCMC samples](Images/B05113_04_151.jpg)，其中![The diagnosis of MCMC samples](Images/B05113_04_152.jpg)和![The diagnosis of MCMC samples](Images/B05113_04_153.jpg)。在第 I 个序列内，样本方差为![The diagnosis of MCMC samples](Images/B05113_04_154.jpg)。
*   并且，样本内方差的合并估计如下:![The diagnosis of MCMC samples](Images/B05113_04_41.jpg)
*   方差的序列间和序列内估计被组合以估计一个上限:![The diagnosis of MCMC samples](Images/B05113_04_40.jpg)
*   GB 统计是估计的潜在规模缩减:![The diagnosis of MCMC samples](Images/B05113_04_39.jpg)
*   这测量了![The diagnosis of MCMC samples](Images/B05113_04_155.jpg)的 SD 可能延长链的因子。
*   当链的长度趋于无穷大时，该指标降低到 1，如果链已经近似收敛到目标分布，则该指标应该接近 1。

让我们再次切换到 r。我们已经对二元 Gibbs 抽样器模拟的随机数以及回归中的示例进行了一些简单的诊断。此外，我们现在将针对指数绘制模拟值(对于我们的回归问题)；看看*图 4.22* 。我们在演示问题上又开始糟糕了。我们观察到显示了短的老化阶段和快速收敛。为了更加确定，可以重新运行这个例子(使用其他种子和其他起始值):

```
set.seed(123)
g <- lreg(Cars93$Price, Cars93$Horsepower, time = 500)
g1 <- cbind(g, "index" = 1:nrow(g))
g1 <- reshape2::melt(g1, id=c("index"))
ggplot(g1, aes(x = index, y = value)) + geom_line() + facet_wrap(~variable, scales = "free_y")

```

![The diagnosis of MCMC samples](Images/B05113_04_22.jpg)

图 4.22:吉布斯采样器的模拟值(用 Cars93 数据回归)与所有三个估计参数的指数

我们清楚地看到，吉布斯采样器对于不同的滞后具有自相关性。为了更接近 I . I . d .样本，应进行约 15 倍或更大的减薄。这将导致需要模拟更多的随机数(15 倍或更大)。*图 4.23* 由`acf()`结果绘制而成。下图中的 ACF 图应该表示自相关问题:

```
plot(acf(g))

```

![The diagnosis of MCMC samples](Images/B05113_04_23.jpg)

图 4.23:α、β和τ对角线的 ACF 图，以及它们的组合

接下来，我们在不同的起始值下用一个更大的序列运行 *M* 链( *M = 5* )。在基本绘图之后，我们将它们转换成`mcmc`对象(包`coda`)，因为 Gelman-Rubin 方法以及 Brooks 和 Gelman (1998)的诊断图很容易用这些对象产生。我们允许相对糟糕的开始——这是不必要的，从`rnorm(1, 0, 1)`开始会更好，但我们希望看到更多关于预烧阶段的演示问题:

```
library("coda")
time <- 2000; M <- 5
set.seed(12345)
df <- lreg(Cars93$Price, Cars93$Horsepower, time = time)
for(i in 2:M){
 df <- rbind(df, lreg(Cars93$Price, Cars93$Horsepower, time = time))
}
df$M <- factor(rep(1:M, each = time))
df$index <- rep(1:time, M)
df <- reshape2::melt(df, id = c("M", "index"))
ggplot(df, aes(x = index, y = value, group = M, colour=M)) + geom_line(alpha = 0.5) + facet_wrap(~variable, scales = "free_y")

```

*图 4.24* 显示了所有 5 条独立绘制的链条的结果。老化阶段似乎很短:

![The diagnosis of MCMC samples](Images/B05113_04_24.jpg)

图 4.24:使用吉布斯采样器对回归问题的三个参数进行多次随机采样

```
## Brooke-Gelman
gl <- list()
M <- 15
set.seed(12345)
for(i in 1:M){
 gl[[i]] <- lreg(Cars93$Price, Cars93$Horsepower, time = time)
}
gl <- lapply(gl, function(x) mcmc(as.matrix(x)))
## look also at summary(g) (not shown here)
gelman.diag(gl, autoburnin = FALSE)

## Potential scale reduction factors:
##
##      Point est. Upper C.I.
## alpha       1.07       1.07
## beta        1.07       1.07
## tau         1.00       1.00
##
## Multivariate psrf

```

结果给出了潜在规模缩减系数的中值及其 97.5%的分位数。我们还得到了(Brooks 和 Gelman 1998)提出的多元潜在规模缩减因子。函数`gelman.plot`显示了随着迭代次数的增加，Gelman 和 Rubin 的收缩因子的演变。注意，如果`autoburnin`被设置为`TRUE`，那么模拟随机数的前半部分被删除:

```
gelman.plot(gl, autoburnin = FALSE)

```

![The diagnosis of MCMC samples](Images/B05113_04_25.jpg)

图 4.25:随着迭代次数的增加，收缩因子的演变

*图 4.25* 显示了`gelman.plot`。我们看到，对于较大的迭代，收缩因子几乎收敛到 1。这可能是老化应该保持更长时间的指示。

有了所有这些发现，我们可以得出结论，一个好的链应该如下:

```
burnin <- 1000
time <- burnin + time * 20
g <- lreg(Cars93$Price, Cars93$Horsepower, time = time, burnin = burnin, 
thin = 20)

```

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 随机数测试

在本章前面的部分，已经用不同的方法模拟了随机数。随机数生成的大多数测试是测试基本随机数生成器(模拟均匀分布随机数的随机数生成器)是否运行良好。

我们在本章开始时已经看到了图，其中我们显示了一个模拟随机值序列与滞后 1 的序列(对于三维图，滞后 2)。

随机数发生器通常在由不同测试定义的整个测试电池上进行测试。测试的原始集合可以在 http://stat.fsu.edu/pub/diehard/找到。这些是来自马萨利亚的所谓的*顽固*测试。

由于不再维护 Marsaglia 的代码，并且已经有更多的测试可用，我们将额外引用到在[http://www.phy.duke.edu/~rgb/General/dieharder.php](http://www.phy.duke.edu/~rgb/General/dieharder.php)的*死忠*测试。R 包的 Linux 版本`RDieHarder`可以在 CRAN 获得。

例如，它们包括生日间隔测试，在该测试中，测试大区间上的点之间的间隔是否渐近地、指数地分布。这个名字是基于生日悖论。停车场测试在一个 100 x 100 的正方形中随机放置单位圆，如果一个圆与另一个圆不重叠，则该圆被成功停放。生成了 12，000 个圈，成功停放的圈的数量应该遵循某个正态分布。最小距离测试在一个 10000×10000 的正方形内随机放置 8000 个点；线对之间的最小平方距离应该以某一平均值呈指数分布。简而言之:随机球体测试随机选择立方体中的点，建造特定的球体，球体的体积应该以一定的平均值呈指数分布。挤压试验结果应遵循一定的分布。重叠检验和应正态分布，具有特征均值和方差。特定时间间隔的频率计数应遵循一定的分布。，等等，等等。

几乎所有测试的共同点是结果都要遵循一定的分布，也就是几乎所有测试的基本思路都是一样的。

零假设通常如下:

![Tests for random numbers](Images/B05113_04_38.jpg)

这意味着模拟随机数应该来自均匀分布和相同的独立分布。

## 随机数的评估——测试的一个例子

呈现的测试是随机数发生器的典型测试。它基于对计数分布的评估。

对于半径为`1`(比如说`big_circle`)的圆上产生的随机数，可以定义大量半径为`r`的圆。下面的函数结果是一个逻辑向量，表示点是否位于一个圆内，该圆完全在`big_circle`内:

```
circle <- function(x, r=0.05){
 repeat{
 x1 <- runif(1,-1,1)
 x2 <- runif(1,-1,1)
 if( sqrt(x1^2 + x2^2) <= (1 - r) ) break
 }
 inCircle <- ((x[,1] - x1)^2 + (x[,2] - x2)^2) <= r^2
 return(inCircle)
}

```

圆中的点数应具有平均值为![The evaluation of random numbers – an example of a test](Images/B05113_04_150.jpg)的泊松分布。因此，我们计算每个圆的点数，并为这些点数创建一个测试。

下图直观地解释了圆的切割:

```
set.seed(123)
## take possible radii
x <- matrix(runif(10000, -1, 1), ncol=2)
## radii to the square
r2 <- rowSums(x^2)
## r2 smaller than 1 are kept
x1 <- x[r2 <= 1, ]
par(mar = c(2,2,0.1,0.1))
plot(data.frame(x1), pch=20)
for(k in 1:8) points(data.frame(x1[circle(x1, 0.2),]), col=k, pch=20)

```

![The evaluation of random numbers – an example of a test](Images/B05113_04_26.jpg)

图 4.25:大圆内的八个圆，在任意位置

我们重复切割圆 2000 次，并计算每个圆包含的点数:

```
set.seed(123)
z <- replicate(2000, sum(circle(x1)))

```

列表显示，这会产生以下值:

```
TAB <- table(z)
TAB
## z
##   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
##   4  20  40  81 115 190 233 278 253 220 195 152  91  53  43  19  11   2

```

这意味着在这个模拟中，曾经在一个特定的圈内只有一次观察。大多数圆包含大约 9 或 10 个值，即众数为`9`，中值计数如下:

```
laeken::weightedMedian(as.numeric(names(TAB)), as.numeric(TAB))
## [1] 10

```

然而，方差可能很高。我们现在想测试极端事件是否是偶然发生的。观察到的计数也可以直观地与泊松分布的理论计数进行比较。这个看起来不错；参见*图 4.27* :

```
lambda <- nrow(x1) * 0.05^2
PROB <- dpois(as.numeric(names(TAB)), lambda)
b <- barplot(TAB / length(z))
points(b, PROB, col="red", pch=16)

```

![The evaluation of random numbers – an example of a test](Images/B05113_04_27.jpg)

图 4.27:观察到的计数(柱状图)与泊松分布的理论值(点)的对比

我们现在希望减少到 6 个类别，并根据这些类别分配我们的模拟值:

```
## the five classes:
QP <- qpois(seq(0,1,by=1/6), lambda)
QP
## [1]   0   7   8  10  11  13 Inf
## frequency counts in those classes
TAB1 <- table(cut(z, QP, include.lowest=TRUE))
TAB1
##
##    [0,7]    (7,8]   (8,10]  (10,11]  (11,13] (13,Inf]
##      450      233      531      220      347      219

```

由于我们不知道总体，并且只有一个样本(我们的模拟)被模拟，我们必须考虑不确定性并应用统计测试。我们使用![The evaluation of random numbers – an example of a test](Images/B05113_04_149.jpg)拟合优度测试。接下来，计算理论分位数和理论类别宽度，它们精确地表达了落入类别的概率:

```
ppois(QP, lambda)
## [1] 5.235307e-05 2.333321e-01 3.490914e-01 6.008619e-01 7.128626e-01
## [6] 8.746304e-01 1.000000e+00
## 0 should be in the left class:
QP1 <- QP
QP1[1] <- -1
## probablities for each class:
PROB1 <- diff(ppois(QP1, lambda))
PROB1
## [1] 0.2333321 0.1157593 0.2517704 0.1120008 0.1617678 0.1253696
## goodness-of-fit test:
chisq.test(TAB1, p=PROB1)
##
##  Chi-squared test for given probabilities
##
## data:  TAB1
## X-squared = 7.8928, df = 5, p-value = 0.1622

```

我们不在拒绝区域，因此不能拒绝零假设——半径为![The evaluation of random numbers – an example of a test](Images/B05113_04_148.jpg)的圆中的点数是泊松分布的。因此，Mersenne-Twister 随机数发生器通过了这项测试。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 总结

一般在科学上，每一个结果都应该是可重现的，尤其是定量分析。这可以通过设置确定性伪随机数生成器的*种子*来实现。其次，也是非常关键的一点是，要有一个运行良好的随机数生成器来模拟均匀分布的随机数。r 的默认随机数生成器，基于 Mersenne-Twister 寄存器的算法，工作得相当好。模拟随机数可能不应该是自相关的，它们应该有一个很长的周期。否则，结果可能会有偏差，不可信。

基于均匀随机数，可以模拟其他分布的随机数。重要的方法是反演法和拒绝抽样法。尤其是拒绝抽样，它可以被广泛地使用并产生独立的相同的分布随机数。

对于非常具体的任务，必须拒绝这种同分布假设，其他方法是模拟(多元)分布的唯一方法。我们提出了 Metropolis-Hastings 算法的变体:基本的，独立的，最后是 Gibbs 抽样器。拒绝率低于拒绝方法(对于 Gibbs，在没有变薄和老化的情况下，拒绝率为零)。从一个关于回归的长例子中，报告了模拟链的质量。最终结论是，可能需要长时间的老化和细化来降低自相关性。这导致用吉布斯采样器模拟了 61，000 个随机数，其中仅保存了 3，000 个。

为了评估随机数发生器，给出了一个测试。进一步的测试超出了我们的范围。然而，几乎所有的测试都基于相同的方案。计数或测量某物并检验其结果是否符合某种分布。

几乎所有后续章节都必须依赖随机数发生器的质量。下一章中的方法(如重采样方法、蒙特卡罗优化或蒙特卡罗测试)使用随机数生成器，并且通常必须模拟来自特定分布的随机数。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 参考文献

*   博克斯，G.E.P .和 M.E .穆勒。1958.“关于正态随机离差生成的一个注记”，*数理统计年鉴*29:610–11。
*   布鲁克斯，S.P .和 a .吉尔曼。1998.“监测迭代模拟收敛的一般方法”，*《计算和图形统计杂志》*7(4):434–55。
*   赫尔曼、J.B .卡林、H.S .斯特恩、D.B .邓森、a .维赫塔里和 D.B .鲁宾。2013.*贝叶斯数据分析，第三版*。查普曼&霍尔/CRC 统计科学教科书。泰勒&弗朗西斯。
*   葛尔曼，鲁宾，1992。“使用多个序列进行迭代模拟的推断”，*统计科学* 7 (4)。*数理统计研究所*:457–72。
*   格曼 s 和 d 还有格曼。1984.“图像的随机松弛、吉布斯分布和贝叶斯恢复”。肛门模式。马赫。英特尔 6(6):721–41。
*   威斯康辛州黑斯廷斯，1970 年。“使用马尔可夫链的蒙特卡罗抽样方法及其应用”，*Biometrika*57(1):97–109。
*   Knuth，D.E. 1998。计算机编程的艺术，第 2 卷:半数值算法。艾迪生-韦利，第三版。
*   m .松本和 t .西村。1998.“Mersenne Twister:一个 623 维均匀分布的均匀伪随机数发生器”， *ACM 建模与计算机模拟汇刊*8(1):3–30。
*   纽约大都会，A. W .罗森布鲁斯，M. N .罗森布鲁斯，A. H .泰勒和 e .泰勒。1953.“通过快速计算机器计算状态方程”， *J. Chem。物理*21:1087–92。
*   里德，j . s .休伯特和 m .亚伯拉罕。1982 年至公元 4 年。SuperDuper 的 C 实现。加州大学伯克利分校。
*   里索，2007 年法学硕士。*统计计算用 R* 。查普曼&霍尔/CRC R 系列。泰勒&弗朗西斯。http://books.google.at/books?id=BaHhdqOugjsC[。](http://books.google.at/books?id=BaHhdqOugjsC)
*   罗伯特 c .和 g .卡塞拉。2010，*引入带 R* 的蒙特卡罗方法，纽约:施普林格。
*   沃克，A.J. 1977。“生成具有一般分布的离散随机变量的有效方法”， *ACM 数学软件汇刊*3(3):253–56。