

# 第十一章。深度学习简介

创新者总是渴望制造会思考的机器。在可编程个人电脑首次被考虑的时候，个人思考他们是否会变得聪明，在一百多年前一台被制造出来(Lovelace 于 1842 年)。

今天，**人工智能** ( **AI** )是一个蓬勃发展的领域，有许多合理的应用和动态的探索点。我们期待智能编程来自动化日常工作，处理图像和音频并从中提取意义，自动化几种疾病的诊断，等等。

最初，当人工智能(AI)兴起时，这个领域处理和解决的问题对个人来说在精神上很困难，但对计算机来说却相当简单。这些问题可以用一个正式的科学原理纲要来描述。对人工智能的真正考验是解开那些对个人来说很容易完成，但对个人来说很难正式描述的任务。我们自然地解释这些问题，例如人类理解言语(和讽刺)的能力，以及我们识别图像，尤其是人脸的能力。

这种安排是为了让计算机通过获得经验来学习，并通过一系列或一棵树的事实来理解世界，每个事实都根据其与更直接的事实的联系来定义。通过理解这些事实，这种方法与人类管理员正式指出计算机需要的大部分信息的要求保持了战略距离。

事实的渐进系统允许计算机从更直接的想法中构建出复杂的想法。如果我们画一个图表来说明这些想法是如何相互建立的，这个图表是深刻的，有许多层。于是，我们把这种方式称为处理 AI 深度学习。

人工智能的许多早期成就发生在适度无菌和正式的情况下，计算机没有必要对世界有太多的了解。让我们举个例子:

*   IBM 1997 年的深蓝下棋框架，打败了当时的世界冠军加里·卡斯帕罗夫先生。

我们还应该考虑这些因素:

*   象棋显然是一个极其基础的世界。
*   它只包含 64 个方块和 32 个只能以预定方式移动的元素。
*   虽然构思一个卓有成效的国际象棋系统是一个巨大的成就，但测试并不是因为向计算机描述国际象棋元素的排列和可通过的走法的难度。
*   国际象棋可以完全由一个非常简短的完全形式化的原则纲要来描述，这个纲要由软件工程师毫不费力地给出。

在一些任务中，计算机比人表现得更好，而在另一些任务中则更差:

*   Abstract tasks that are among the most difficult mental endeavors for a person are among the simplest for a computer. Computers are much better suited for such tasks.

    *   这方面的一个例子是执行复杂的数学任务。

*   Subjective and natural tasks are performed much better by the average human being than a computer.

    *   一个人的日常生活需要大量的关于世界的信息。
    *   许多这种学习是主观和自然的，因此很难正式表达。
    *   计算机需要捕捉同样的信息，以便采取明智的行动。人工智能的一个关键困难是你将这种随意的学习输入计算机的方式。

一些人工智能企业已经开始用正式的方言对世界信息进行硬编码。计算机可以对这些正式方言中的发音进行推理，从而利用合法的推理规则。这就是所谓的信息库处理人工智能的方式。这些活动都没有取得显著的成功。

依赖硬编码信息的框架所面临的困难表明，人工智能框架需要通过从原始信息中提取模式来获得自己特定的学习能力。这被称为机器学习，我们在前面的章节中学习过。

这些直截了当的机器学习计算的执行强烈地依赖于它们被给予的信息的表示。

例如，当利用逻辑回归来建议未来天气时，AI 框架不会直接看着患者:

*   专家告诉框架一些重要的数据，例如，变化的温度、风向和风速、湿度等等。
*   包含在天气表现中的每一点数据都被称为一个特征。逻辑回归计算出这些天气特征与不同季节或其他地方的不同天气之间的关系。
*   在任何情况下，它都不能以任何方式影响特性的定义方式。

这个问题的一个答案是利用机器，弄清楚如何找到从表示到 yield 的映射以及表示本身。这种方法被称为表征学习。与手工计划的表征相比，学习的表征经常带来更好的执行。此外，它们还允许人工智能框架快速适应新任务，而无需人工干预。

表征学习计算可以在几分钟内为简单的任务或几小时到几个月的复杂任务找到合适的特征排列。为一件复杂的作品勾勒出精彩的轮廓需要大量的人力和时间，比用计算机要多得多。

在本章中，我们将讨论多个主题，从基本介绍开始:

*   基本基础
*   机器学习和深度学习的区别
*   什么是深度学习？
*   深度前馈网络
*   单层和多层神经网络
*   卷积网络
*   实用方法和应用

# 重温线性代数

线性代数是一个应用广泛的数学分支。线性代数是离散数学的一部分，不是连续数学的一部分。理解机器学习和深度学习模型需要很好的理解。我们只会修改数学对象。

## 标量的要点

标量只是单个数字(与线性代数中检查的大部分交替对象相反，后者通常是各种数字的数组)。

## 载体的简要概述

向量是一个有组织的集合或一组数字。我们可以通过列表中的索引来识别每个数字。例如:

> *x = [x1，x2，x3，x4.....xn]*

*   向量也可以被认为是识别空间中的点。
*   每个元素代表沿不同轴的坐标值。
*   我们还可以索引这些值在向量中的位置。因此，访问数组的特定值更容易。

## 矩阵的重要性

*   矩阵是数字的二维数组。
*   每个组件都由两个指数而不是一个指数来识别。
*   例如，2D 空间中的点可以被标识为(3，4)。这意味着该点是在 *x* 轴上的 3 个点和在 *y* 轴上的 4 个点。
*   我们也可以有像[(3，4)，(2，4)，(1，0)]这样的数字数组。这样的阵列称为矩阵。

## 什么是张量？

如果需要二维以上的矩阵，我们就使用张量。

这是一个没有定义轴数的数字数组。

这样的对象具有如下结构: *T (x，y，z)*

*[(1，3，5)，(11，12，23)，(34，32，1)]*



# 概率论与信息论

概率论是一种科学体系，用来反驳有问题的解释。它给出了评估不稳定性的方法和推断新的不确定语句的依据。

在人工智能的应用中，我们利用概率论如下:

*   概率法则定义了人工智能框架应该如何推理，因此算法被设计成利用概率论推断出不同的表达式
*   概率和统计可以用来假设调查提出的人工智能框架的行为

虽然概率论允许我们在不确定的情况下提出不确定的表达式和推理，但数据允许我们测量概率分布的不确定程度。

## 为什么是概率？

与主要依赖于计算机系统的确定性的计算机科学的其他分支不同，机器学习充分利用了概率论:

*   这是因为机器学习必须可靠地管理不确定的量。
*   有时，管理随机(非确定性)数量也是必要的。不确定性和随机性可能来自许多方面。

所有的练习都需要在不确定的情况下有一定的推理能力。事实上，根据定义，过去的数字解释是有效的，很难想到任何建议是完全有效的，或者任何场合是完全肯定会发生的。

不确定性有三个可能的来源:

*   Existing stochasticity in the framework that is being modeled.

    *   例如，在玩纸牌游戏时，我们假设纸牌是真正随机洗牌的。

*   Fragmented observability. When a greater part of the variables that drive the conduct of the framework cannot be observed then even deterministic frameworks can seem stochastic.

    *   例如，在一个有多项选择作为答案的问题中，一项选择会导致正确的答案，而其他选择则不会导致任何结果。挑战者的决定的结果是确定的，但是从候选人的角度来看，结果是不确定的。

*   Fragmented modeling. When we utilize a model that must dispose of a portion of the data we have observed, the disposed-of data results in instability in the model's expectations.

    *   例如，假设我们制造了一个机器人，可以精确地观察它周围的每件物品。如果机器人在预测这些物体的未来区域时离散化空间，那么离散化使机器人很快变得不确定物品的确切位置:每个物品都可能是它被看到拥有的离散单元内的任何地方。

概率可以被看作是管理不确定性的基本原理的扩充。基本原理给出了一种正式的规则安排，用于在怀疑其他一些建议安排为真或假的情况下，计算出哪些建议被推断为真或假。

概率假设给出了形式原则的安排，用于在给定不同建议的概率的情况下决定建议为真实的概率。



# 机器学习和深度学习的区别

机器学习和深度学习旨在完成相同的目标，但是，它们是不同的，相当于各种思想。机器学习是这两者中最主要的，科学家和数学家已经对它进行了几十年的研究。深度学习是一个相对较新的想法。深度学习是基于通过神经网络(多层)的学习来实现目标。了解两者之间的区别对于了解我们应该在哪里应用深度学习以及使用机器学习可以解决哪些问题非常重要。

可以理解的是，通过利用仅依赖于区域和决定性目标就可以毫不费力地挖掘的信息，可以实现构建模式识别算法的更强的方法。

例如，在图像识别中，我们积累各种图片，并在此基础上扩展算法。利用这些信息作为这些图片的一部分，我们的模型可以被训练来识别生物、人类外表或不同的例子。

机器学习连接到不同的领域，现在它不局限于图像或字符识别。它目前被广泛用于机器人、金融市场、自动驾驶汽车和基因组分析。我们在之前的章节中学习了机器学习，现在我们可以进一步了解它与深度学习有多大的不同。

## 什么是深度学习？

深度学习在 2006 年开始流行。它也被称为分层学习。它的应用非常广泛，并且扩大了人工智能和机器学习的范围。社区对深度学习有着浓厚的兴趣。

深度学习是指一类机器学习技术，它:

*   执行无监督或有监督的特征提取。
*   通过利用多层非线性信息处理来执行模式分析或分类。

它由一系列特征或因素组成。在这个层次结构中，较低级别的功能有助于定义较高级别的功能。人工神经网络通常用于深度学习。

*   传统的机器学习模型学习模式或聚类。深度神经网络通过非常少的步骤来学习计算。
*   一般来说，神经网络越深入，它就越强大。
*   神经网络根据可用的新数据进行更新。
*   人工神经网络是容错的，这意味着如果网络的某个部分被破坏，那么这可能会影响网络的性能，但是网络的关键功能仍然可以保留。
*   深度学习算法学习多个级别的表示，并并行进行计算，这可能会越来越复杂。

如果我们快进到今天，人们对许多人称为深度学习的东西有着广泛的热情。最突出的深度学习模型，因为它们被用作大规模图像识别任务的一部分，被称为卷积神经网络，或本质上称为 ConvNets。

深度学习强调我们需要利用的模型类型(如深度卷积多层神经系统)，我们可以利用信息来填充缺失的参数。

随着深度学习而来的是难以置信的责任。由于我们从一个高维度的世界模型开始，我们确实需要大量的信息，我们也称之为大数据，以及相当大的计算能力(通用 GPUs 高性能计算)。卷积被广泛用作深度学习(尤其是计算机视觉应用)的一部分。

![What is deep learning?](graphics/B05321_11_01-1.jpg)

在之前的图像中，我们看到了三层:

*   **输出层**:这里预测一个被监督的目标
*   **隐藏层**:中间函数的抽象表示
*   **输入层**:原始输入

人工模拟神经元代表多层人工神经系统的构建模块。基本的想法是模拟人脑以及它如何解决一个复杂的问题。制造神经系统的主要思想是基于这些理论和模型。

在过去的几十年里，关于深度学习算法，出现了许多更重要的飞跃。这些可以用于从未标记的信息中制作特征指示器，也可以用于预训练深度神经网络，深度神经网络是由许多层组成的神经系统。

神经网络在学术探索和大型创新组织中都是一个有趣的问题，例如脸书、微软和谷歌等公司都在深度学习研究方面进行了大量投资。

由深度学习计算推动的复杂神经网络被认为是解决关键问题的最佳选择。例如:

*   **谷歌的图片搜索**:我们可以使用谷歌图片搜索工具在互联网上搜索图片。这可以通过上传图像或给出图像的 URL 以便在互联网上搜索来实现。
*   谷歌翻译(Google Translate):这个工具可以阅读图像中的文本，并理解语音，以翻译或说出几种语言的意思。

另一个非常著名的应用是由谷歌或特斯拉开发的无人驾驶汽车。它们由深度学习提供动力，以找出最佳路径，实时穿过交通，并执行必要的任务，就像由人类司机驾驶一样。

## 深度前馈网络

深度前馈网络是最著名的深度学习模型。这些也称为以下内容:

*   前馈神经网络。
*   **多层感知器** ( **多层感知器**)

![Deep feedforward networks](graphics/B05321_11_02.jpg)

前馈神经网络的目的是通过其参数进行学习，并定义映射到输出 *y:* 的函数

> *y = f(x，θ)*

如图所示，前馈神经网络之所以如此命名，是因为它们的数据流是单向的。它从 *x* 开始，通过函数进行中间计算，生成 *y* 。

当这样的系统还包括到前一层的连接(反馈)时，这些被称为递归神经网络。

前馈系统对于机器学习专家来说意义重大。它们构成了许多必要的业务应用程序的前提。例如，用于从语音进行自然语言处理的卷积系统就是一种特殊的前馈系统。

前馈系统是通向循环网络的合理垫脚石。这些系统有许多自然语言应用。前馈神经网络之所以被称为网络，是因为它们通过一起形成许多不同的函数来表示。该模型与描绘如何一起创建功能的有向非循环图相连接。

例如，我们有三个函数- *f(1)* 、 *f(2)* 和 *f(3)* 。

它们按如下方式链接或关联在一起:

> *f(x) = f(3)(f(2)(f(1)(x)))*

这些链结构是神经系统最常用的结构。对于这种情况:

*   *f(1)* 被称为网络的第一层。
*   *f(2)* 称为第二层，以此类推。
*   链条的大致长度给出了模型的深度。“深度学习”这个名称就是从这个措辞中产生的。
*   前馈网络的最后一层称为输出层。

在神经网络训练中，我们遵循以下步骤:

1.  驱动 *f(x)* 到坐标*f∫(x)*。训练信息具有在不同训练集评估的带有噪声的数据和不精确数据 off*∫(x)*。
2.  每一个 *x* 的例子都关联着一个标签*y≈f∫(x)*。
3.  训练案例直接决定了屈服层在每一点 *x* 必须做什么。也就是说，它必须创造一个接近 *y* 的值。

### 了解神经网络中的隐藏层

训练信息没有直接规定交替层的行为。学习算法必须选择如何利用这些层来产生期望的产量，然而训练信息并没有说明每个层应该做什么。

相反，学习算法必须选择如何利用这些层来最好地执行估计。由于训练信息并没有展示这些层中每一层的期望产量，因此这些层被称为隐藏层。

### 神经网络的动机

*   这些系统被称为神经系统，因为它们大致是由神经科学激发的。
*   系统的每个隐藏层通常是向量值的。
*   这些隐藏层的尺寸 *y* 决定了模型的宽度。
*   向量的每一个分量都可以被翻译成相当于神经元的一部分。
*   与将该层视为展示单个矢量到矢量功能相反，应该认为该层由并行工作的多个单元组成，每个单元展示一个矢量到标量功能。
*   每个单元看起来像一个神经元，因为它从许多不同的单元获得贡献，并记录自己的激活值。
*   使用多层向量值表示源自神经科学。
*   用于计算这些表征的函数 *f(i)(x)* 的决定在某种程度上受关于有机神经元处理功能的神经科学观察的指导。

我们在前几章研究了正则化。让我们来研究一下为什么这对于深度学习模型来说是重要的和必需的。

## 了解正规化

机器学习中的主要问题是通过何种方法来制定一种算法，该算法将对训练信息以及新的输入表现良好。用作机器学习的一部分的许多技术都明确地旨在减少测试错误，这潜在地损害了增加的训练错误。这些技术统称为正则化。

深度学习专家可以使用许多类型的正则化。更有效的正则化系统一直是该领域的研究工作之一。

有许多正规化系统。

*   Additional constraints on a machine learning model

    *   例如，包括对参数值的约束。

*   目标函数中的附加项，可视为与参数值的精确要求相比较
*   如果有策略地、谨慎地完成，这些额外的需求和约束可以提高测试数据的性能
*   这些约束和限制也可以用于编码特定种类的先前学习
*   这些约束和限制也可能导致模型的泛化
*   集成方法也使用正则化来产生更好的结果

关于深度学习，大多数正则化过程依赖于正则化估计器。要调整估计值:

*   我们需要用增加的偏差换取减少的方差
*   一个有效的正则化是一个有利可图的交换，这意味着它大幅减少方差，同时不会过度扩大偏差

在过度拟合和泛化过程中，我们关注我们正在训练的模型的这些情况:

*   避免生产过程中的真实信息，以考虑过度拟合和诱导偏差
*   包括生产过程的真实信息
*   包括关于生产过程的信息以及关于生产过程的许多其他信息，以考虑过拟合，其中方差而不是偏差控制估计误差

正则化的目标是将一个模型用于提到的第二个过程。

一个过于复杂的模型族当然不会包含目标函数或真实的信息生成过程。在任何情况下，深度学习算法的大多数利用都是在真正的信息产生过程很可能在模型家族之外的地方。深度学习算法通常在很大程度上与复杂的用例相关，如图像识别、语音识别、无人驾驶汽车等。

这意味着控制模型性质的复杂性不仅仅是找到一个具有正确参数集的适当大小的模型的问题。

## 优化深度学习模型

优化方法在设计从海量数据中提取所需知识的算法中至关重要。深度学习是一个快速发展的领域，其中产生了新的优化技术。

深度学习算法包括在许多连接中的增强。例如，在模型中执行演绎，例如 PCA，包括处理改进问题。

我们定期利用诊断优化来构建验证或配置计算。在深度学习所需的众多优化问题中，最困难的是准备神经网络。

为许多机器贡献几天甚至几个月的时间来解决哪怕是一个神经系统训练问题的特定最终目标是非常常见的。由于这一问题非常关键，且成本高昂，因此我们制定了一套具体的优化策略来加强这一问题。

### 优化的案例

为了找到显著减少成本函数 J(θ)的神经网络的参数θ，通常结合对整个训练集评估的执行测量以及额外的正则化项。

用作机器学习任务的训练算法的优化不同于完美优化。更复杂的算法在训练中调整它们的学习率，或者影响包含在成本函数的二阶导数中的数据。最后，通过将基本优化算法加入到更高层次的策略中，创建了一些优化方法。

用于训练深度学习模型的优化算法在几个方面不同于传统的优化算法:

*   机器学习通常以迂回的方式进行。在大多数机器学习的情况下，我们会考虑一些执行度量 *P* ，这是为测试集定义的，同样可能是顽固的。我们相应地迂回升级 *P* 。我们降低不同的成本函数 *J(θ)* ，期望这样做将增强 *P* 。

这与纯粹的优化相反，在纯粹的优化中，最小化 *J* 本身就是一个目标。用于准备深度学习模型的优化算法同样通常包含对机器学习目标函数的特定结构的一些专门化。



# 在朱莉娅中实现

在流行的编程语言中，有许多很好的经过测试的深度学习库:

*   Theano (Python)可以利用 CPU 和 GPU(来自蒙特利尔大学 MILA 实验室)
*   Torch (Lua)是一个类似 Matlab 的环境(来自 Ronan Collobert、Clement Farabet 和 Koray Kavukcuoglu)
*   Tensorflow (Python)利用数据流图
*   MXNet (Python，R，Julia，C++)
*   咖啡是最受欢迎和广泛使用的
*   基于 Theano 的 Keras (Python)
*   摩卡(朱莉娅)张

我们将主要通过摩卡为茱莉亚，这是一个惊人的软件包写的池源张，一个在麻省理工学院的博士生。

首先，按如下方式添加包:

```
Pkg.update() 
Pkg.add("Mocha") 

```

## 网络架构

Mocha 中的网络架构指的是一组层:

```
data_layer = HDF5DataLayer(name="data", source="data-list.txt", batch_size=64, tops=[:data]) 
ip_layer   = InnerProductLayer(name="ip", output_dim=500, tops=[:ip], bottoms=[:data]) 

```

*   `ip_layer`的输入与`data_layer`的输出同名
*   同一个名字把他们联系在一起

Mocha 对一组层进行拓扑排序。

## 层的类型

*   Data layers

    *   这些层从源中读取信息，并将它们提供给顶层

*   Computation layers

    *   它们从基础层获取输入流，进行计算，并将生成的结果提供给顶层

*   Loss layers

    *   这些层从基础层获取处理后的结果(和地面实况名称/标签),并计算标量损失值
    *   网络中所有层和正则项的损失值都包括在内，以表征网络的最终损失函数
    *   在损失函数的帮助下训练反向传播中的网络参数

*   Statistics layers

    *   这些工具从基础层获取信息，并生成有价值的见解，如分类准确性
    *   洞察力是通过各种迭代收集的
    *   `reset_statistics`可用于明确重置统计汇总

*   实用程序层

## 神经元(激活功能)

让我们来理解真正的神经网络(大脑)。神经科学是对大脑功能的研究，并为我们提供了关于其工作方式的良好证据。神经元是大脑真正的信息存储。理解它们的连接强度也是非常重要的，即一个神经元可以有多强烈地影响与其连接的那些神经元。

学习或重复一项任务以及接触新的刺激程序或环境通常会导致大脑活动，大脑实际上是根据接收到的新数据进行活动的神经元。

神经元，因此也是大脑，对不同的刺激和环境有非常不同的反应。在某些情况下，他们可能会比其他人反应更激烈或更兴奋。

了解这一点对于了解人工神经网络非常重要:

*   神经元可以连接到任何层
*   除非是同一神经元，否则每一层的神经元都会影响前向通路的产量和后向通路的斜率
*   默认情况下，层有一个标识神经元

让我们来看看可以用来构建网络的各种类型的神经元:

*   `class Neurons.Identity`

    *   这是一个输入不变的激活功能。

*   `class Neurons.ReLU`

    *   整流线性单元。在向前传球时，这将所有约束限制在某个极限 *ϵ* 以下，通常为 0。
    *   它逐点处理 *y=max(ϵ,x)* 。

*   `class Neurons.LreLU`

    *   泄漏整流线性单元。一个泄漏的 ReLU 可以解决“垂死的 ReLU”问题。
    *   如果一个足够大的梯度改变了权重，使得神经元不再激活新信息，那么 ReLU 就会“死亡”。

*   `class Neurons.Sigmoid`

    *   Sigmoid 是平滑的阶跃函数
    *   对于绝对值很大的负面信息，它大约产生 0，对于大量的正面输入，它估计产生 1
    *   逐点方程为*y = 1/(1+e x)y = 1/(1+e x)*

*   `class Neurons.Tanh`

    *   Tanh 是 Sigmoid 的变体
    *   它采用 *1 1* 中的值，而不是单位期中值
    *   逐点公式为*y =(1 e1x)/(1+e1x)*

![Neurons (activation functions)](graphics/image_11_003.jpg)

## 理解人工神经网络的正则化子

我们在前面的章节中研究了正则项。正则项包括对网络参数的附加惩罚或限制，以限制模型的复杂性。在一个流行的深度学习框架 caffe 中，它被称为 decay。

权重衰减和正则化在反向传播中是可比较的。向前传递中的理论对比是，当被认为是重量衰减时，它们不被认为是目标函数的一部分。

默认情况下，Mocha 类似地消除了正则化子的正向计算，以减少计算量。我们使用术语正则化而不是权重衰减，因为它不太容易理解。

*   类别`NoRegu`:无正则化
*   L2 正则化者
*   L1 正则化者

## 定额约束

范数限制是一种更直接的方法，用于限制模型的复杂性，如果参数的标准或范数超过给定的阈值，则通过在 n 个循环的每一个中明确地收缩参数。

*   类别`NoCons`:无约束
*   类`L2Cons`:限制参数的欧氏范数。阈值和收缩适用于每个参数。具体而言，阈值应用于卷积图层的滤镜参数的每个滤镜。

## 在深度神经网络中使用解算器

Mocha 包含广泛有用的基于随机(次)梯度的解算器。这些可以用来制备深度神经网络。

求解器是通过指示给出重要配置的求解器参数词典来开发的:

*   停止条件等常规设置
*   特定计算的特定参数

此外，通常建议在训练迭代之间进行一些短暂的休息，以打印进度或保存快照。这在摩卡咖啡中被称为工间休息。

**求解算法**

*   class `SGD`: Stochastic Gradient Descent with momentum.

    *   `lr_policy`:学习率政策。
    *   `mom_policy`:气势政策。

*   class `Nesterov`: Stochastic Nesterov accelerated gradient method.

    *   `lr_policy`:学习率政策。
    *   `mom_policy`:气势政策。

*   class `Adam`: A Method for Stochastic Optimization

    *   `lr_policy`:学习率政策。
    *   `beta1`:一阶矩估计的指数衰减因子。 *0 < =beta1 < 1* ，默认 *0.9*
    *   `beta2`:二阶矩估计的指数衰减因子，*0<=β1<1*，默认 *0.999* 。
    *   `epsilon`:影响数值调节的参数更新比例，默认为 *1e-8* 。

## 喝咖啡的时间

训练可以变成几个循环的非常计算密集的迭代。通常建议在训练迭代之间稍作休息，以打印进度或保存快照。这在摩卡咖啡中被称为工间休息。它们的执行过程如下:

```
# report training progress every 1000 iterations 
add_coffee_break(solver, TrainingSummary(), every_n_iter=1000) 

# save snapshots every 5000 iterations 
add_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000) 

```

这将每 1，000 次迭代打印一次训练摘要，每 5，000 次迭代保存一次快照。

## 使用预训练的 Imagenet CNN 进行图像分类

MNIST 是一个手写数字识别数据集。它包含以下内容:

*   60，000 个培训示例
*   10，000 个测试示例
*   28 x 28 单通道灰度图像

我们可以使用`get-mnist.sh`脚本来下载数据集

它调用`mnist.convert.jl`将二进制数据集转换成 Mocha 可以读取的 HDF5 文件。

转换完成后会产生`data/train.hdf5`和`data/test.hdf5`。

我们在这里使用 Mocha 的本地扩展来获得更快的卷积:

```
ENV["MOCHA_USE_NATIVE_EXT"] = "true" 

using Mocha 

backend = CPUBackend() 
init(backend) 

```

这将 Mocha 配置为使用原生后台，而不是 GPU (CUDA)。

现在，我们将继续定义网络结构。我们将首先定义一个读取 HDF5 文件的数据层。这将是网络的输入。

`source`包含一个真实数据文件列表:

```
data_layer  = HDF5DataLayer(name="train-data", source="data/train.txt", 
batch_size=64, shuffle=true) 

```

形成小批量来处理数据。随着批量的增加，方差减小，但会影响计算性能。

洗牌降低了训练过程中排序的效果。

现在我们将继续定义卷积层:

```
conv_layer = ConvolutionLayer(name="conv1", n_filter=20, kernel=(5,5), 
bottoms=[:data], tops=[:conv1]) 

```

*   `name`:用于识别的层的名称。
*   `n_filter`:卷积滤波器的数量。
*   `kernel`:滤镜的大小。
*   `bottoms`:定义从哪里获取输入的数组。(我们定义的 HDF5 数据层。)
*   `tops`:卷积层的输出。

如下定义更多卷积层:

```
pool_layer = PoolingLayer(name="pool1", kernel=(2,2), stride=(2,2), 
    bottoms=[:conv1], tops=[:pool1]) 
conv2_layer = ConvolutionLayer(name="conv2", n_filter=50, kernel=(5,5), 
    bottoms=[:pool1], tops=[:conv2]) 
pool2_layer = PoolingLayer(name="pool2", kernel=(2,2), stride=(2,2), 
    bottoms=[:conv2], tops=[:pool2]) 

```

在卷积层和池层之后，这是两个完全连接的层。

创建图层的计算是输入权重和图层权重之间的内积。这些也被称为`InnerProductLayer`。

层权重也是已知的，因此我们也给这两层命名:

```
fc1_layer  = InnerProductLayer(name="ip1", output_dim=500, 
    neuron=Neurons.ReLU(), bottoms=[:pool2], tops=[:ip1]) 
fc2_layer  = InnerProductLayer(name="ip2", output_dim=10, 
    bottoms=[:ip1], tops=[:ip2]) 

```

最后一个内积层的维数为 10，代表类的个数(数字 0~9)。

这是 LeNet 的基本结构。为了训练该网络，我们将通过添加损失层来定义损失函数:

```
loss_layer = SoftmaxLossLayer(name="loss", bottoms=[:ip2,:label]) 

```

我们现在可以构建我们的网络:

```
common_layers = [conv_layer, pool_layer, conv2_layer, pool2_layer, 
    fc1_layer, fc2_layer] 

net = Net("MNIST-train", backend, [data_layer, common_layers..., loss_layer]) 

```

用随机梯度下降训练神经网络如下进行:

```
exp_dir = "snapshots" 
method = SGD() 

params = make_solver_parameters(method, max_iter=10000, regu_coef=0.0005, 
    mom_policy=MomPolicy.Fixed(0.9), 
    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75), 
    load_from=exp_dir) 

solver = Solver(method, params) 

```

使用的参数有:

*   `max_iter`:这是求解器为训练网络而执行的最大迭代次数
*   `regu_coef`:正则化系数
*   `mom_policy`:势头政策
*   `lr_policy`:学习率政策
*   `load_from`:在这里我们可以从文件或目录中加载保存的模型

添加一些咖啡休息时间，如下所示:

```
setup_coffee_lounge(solver, save_into="$exp_dir/statistics.hdf5", every_n_iter=1000) 

add_coffee_break(solver, TrainingSummary(), every_n_iter=100) 

add_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000) 

```

在单独的验证集上定期检查性能，以便我们可以看到进度。我们拥有的验证数据集将被用作测试数据集。

为了执行评估，定义了具有相同架构但不同数据层的新网络，该网络将从验证集获取输入:

```
data_layer_test = HDF5DataLayer(name="test-data", source="data/test.txt", batch_size=100) 

acc_layer = AccuracyLayer(name="test-accuracy", bottoms=[:ip2, :label]) 

test_net = Net("MNIST-test", backend, [data_layer_test, common_layers..., acc_layer]) 

```

添加一个休息时间来获取验证性能的报告，如下所示:

```
add_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=1000) 

```

最后，开始培训，如下所示:

```
solve(solver, net) 

destroy(net) 
destroy(test_net) 
shutdown(backend)  

```

这是我们创建的两个网络:

![Image classification with pre-trained Imagenet CNN](graphics/image_11_004.jpg)

现在，我们运行根据现有测试数据生成的模型。我们得到以下输出:

```
Correct label index: 5
Label probability vector:
Float32[5.870685e-6
0.00057068263
1.5419962e-5
8.387835e-7
0.99935246
5.5915066e-6
4.284061e-5
1.2896479e-6
4.2869314e-7
4.600691e-6]

```



# 总结

在这一章中，我们了解了深度学习以及它与机器学习的不同之处。深度学习是指一类机器学习技术，通过利用多层非线性信息处理来执行无监督或有监督的特征提取和模式分析或分类。

我们研究了深度前馈网络、正则化和优化深度学习模型。我们还学习了如何在 Julia 中创建一个神经网络来对使用摩卡的手写数字进行分类。



# 参考文献

*   [http://docs.julialang.org/en/release-0.4/manual/](http://docs.julialang.org/en/release-0.4/manual/)
*   [https://github.com/pluskid/Mocha.jl](https://github.com/pluskid/Mocha.jl)
*   [http://psych.utoronto.ca/users/reingold/courses/ai/nn.html](http://psych.utoronto.ca/users/reingold/courses/ai/nn.html)
*   [https://www . Microsoft . com/en-us/research/WP-content/uploads/2016/02/deep learning-now publishing-vol 7-SIG-039 . pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf)
*   [http://www.deeplearningbook.org/contents/intro.html](http://www.deeplearningbook.org/contents/intro.html)
*   [http://deeplearning.net/tutorial/deeplearning.pdf](http://deeplearning.net/tutorial/deeplearning.pdf)