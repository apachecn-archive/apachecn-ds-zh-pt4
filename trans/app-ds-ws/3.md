<title>B15916_02_Final_SMP_ePub</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 2。使用 Jupyter 进行数据探索

概观

在这一章中，我们将最终获得一些数据，并通过探索性分析工作，我们将计算一些信息指标和可视化。在本章结束时，你将能够使用 pandas Python 库加载表格数据并对其进行计算，以及使用`seaborn` Python 库创建可视化。

# 简介

到目前为止，我们已经看了一眼数据科学生态系统，并开始学习 Jupyter，这是我们将在本书的编码练习和活动中使用的工具。现在，我们将把注意力从学习 Jupyter 转移到实际使用它进行分析上来。

**数据可视化**和**探索**是数据科学过程中的重要步骤。这就是你如何了解你的数据，并确保你完全理解它。**可视化**可以用来发现数据集中不寻常的记录，并将这些信息呈现给其他人。

除了理解和获得对数据的基本信任，您的分析可能会发现数据中的模式和见解。在某些情况下，这些模式可以促进进一步的研究，并最终对您的业务非常有益。

Python 或 R 等高级编程语言的应用知识将使您能够访问数据集，从顶级聚合到粒度细节。然而，使用更容易掌握和使用的工具，如 Tableau 或 Microsoft Power BI，也可以从数据中学到很多东西。

除了学习创建可视化工具之外，从概念上理解不同类型的可视化及其用途也很重要。类似地，有一些与数据探索相关的重要技术，例如异常值或缺失样本的聚集和过滤。

在这一章中，我们将通过使用 pandas DataFrames 来学习在 Jupyter 中处理数据集的一些基础知识。然后，我们将学习使用 Seaborn 可视化库探索数据集，并使用 scikit learn 进行基本建模。

# 我们的第一项分析——波士顿住房数据集

我们将在本节中查看的数据集是所谓的波士顿住房数据集。它包含了有关波士顿市周围不同地区的房屋的美国人口普查数据。每个样本对应于一个独特的领域，并有大约一打的措施。我们应该把样本看作行，把测量看作列。这一数据首次发表于 1978 年，规模相当小，仅包含约 500 个样本。

现在，我们已经了解了数据集的上下文，让我们为探索和分析阶段确定一个粗略的计划。如果适用，该计划将考虑正在研究的相关问题。在这种情况下，目标不是回答一个问题，而是展示 Jupyter 的实际操作，并说明一些基本的数据分析方法。

我们进行这种分析的一般方法是:

*   使用熊猫数据框架将数据加载到木星
*   定量理解特征
*   寻找模式并提出问题
*   回答问题的问题

在开始分析之前，让我们花点时间来设置我们的笔记本电脑环境。

## 练习 2.01:导入数据科学库并设置笔记本绘图环境

在本练习中，我们将设置舞台，以便准备将数据加载到笔记本中。执行以下步骤来完成本练习:

1.  If you haven't done so already, start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

2.  然后，按照终端中的提示，通过复制并粘贴 URL，在 web 浏览器中打开您选择的平台。
3.  Import `pandas` and pull up the `docstring` for a pandas DataFrame, the main data structure you will use to store tables and run calculations on them. This should be familiar to you from *Activity 1.01*, *Using Jupyter to Learn about Pandas DataFrames*, from *Chapter 1*, *Introduction to Jupyter Notebooks*, where you learned about some basics of DataFrames:

    ```
    import pandas as pd
    pd.DataFrame?
    ```

    输出如下所示:

    ![Figure 2.1: The docstring for pd.DataFrame
    ](image/B15916_02_01.jpg)

    图 2.1:PD 的文档字符串。数据帧

4.  Import `seaborn` and pull up examples of the plot functions you have access to:

    ```
    import seaborn as sns
    sns.*?
    ```

    您将在列表中看到熟悉的图表，如条形图、箱线图和散点图，如下所示:

    ![Figure 2.2: The output of running sns.*?
    ](image/B15916_02_02.jpg)

    图 2.2:运行 sns 的输出。*?

5.  Load the library dependencies and set up the plotting environment for the notebook.

    注意

    您可以将这些库键入到新的单元格中，并使用 Jupyter tab 补全功能。

    导入库的代码如下:

    ```
    # Common standard libraries
    import datetime
    import time
    import os
    # Common external libraries
    import pandas as pd
    import numpy as np
    import sklearn # scikit-learn
    import requests
    from bs4 import BeautifulSoup
    # Visualization libraries
    %matplotlib inline
    import matplotlib.pyplot as plt
    import seaborn as sns
    ```

    库不需要在笔记本的顶部导入，但是为了在一个地方总结依赖关系，这样做是一个好习惯。不过，有时候在笔记本中途加载东西是有意义的，这完全没问题。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2N1riF2](https://packt.live/2N1riF2)。

    你也可以在[https://packt.live/37DzuVK](https://packt.live/37DzuVK)在线运行这个例子。

6.  Set the plot's appearance, as follows:

    ```
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    ```

    注意

    在笔记本上工作时，很容易因为不按顺序运行单元格而遇到不可再现性的问题。因此，您可以留意单元格运行计数，它列在单元格左侧的方括号中，单元格每运行一次就递增 1。

    例如，考虑一下，如果您在笔记本中间导入熊猫(`import pandas as pd`)。由于 Jupyter 笔记本在单元格之间共享内存，您可以在任何单元格中使用熊猫，甚至是它上面的单元格。现在，假设您这样做了，并使用 pandas 在导入单元格上方编写了一些代码。在这种情况下，如果您要从头重新运行笔记本(或者发送给其他人来运行)，那么它将抛出一个`NameError`异常，并在遇到该单元格时停止执行。这是因为笔记本试图在导入库之前执行 pandas 代码。

    在转换数据时也会出现类似的问题，例如对笔记本中后来转换的数据重新运行图表。在这种情况下，解决方法是小心你运行事物的顺序。如果时间允许的话，试着偶尔从头开始重新运行你的笔记本，这样你就可以在这些错误进入你的分析之前抓住它们。

现在我们已经加载了外部库，我们可以继续分析了。请打开笔记本，继续下一个练习，继续浏览笔记本。

## 使用 pandas 数据框架将数据加载到 Jupyter

通常，数据存储在表格中，这意味着它可以保存为一个由逗号分隔的变量文件。这种格式以及许多其他格式可以使用 pandas 库作为 DataFrame 对象读入 Python。其他常见格式包括**制表符分隔变量** ( **TSV** )、**结构化查询语言** ( **SQL** )表格、 **JavaScript 对象表示法** ( **JSON** )数据结构。事实上，熊猫支持所有这些。

我们现在不需要担心像这样加载数据，因为我们的第一个数据集可以直接通过 scikit-learn 获得；然而，我们将在本书的后面看到一些加载和保存这些类型的数据结构的例子。

注意

加载数据进行分析后的一个重要步骤是确保数据是干净的。例如，我们通常需要处理缺失数据，并确保所有列都具有正确的数据类型。我们将在本节中使用的数据集已经被清理过了，所以我们不需要担心这个问题。然而，我们将在*第 3 章*、*为预测建模准备数据*中看到一个需要清理的数据示例，并探索处理它的技巧。

## 练习 2.02:加载波士顿住房数据集

要开始使用波士顿住房数据集，您需要将该表加载到 pandas 数据框架中，并查看其字段和一些汇总统计数据。执行以下步骤来完成本练习:

1.  使用`load_boston`方法:

    ```
    from sklearn import datasets
    boston = datasets.load_boston()
    ```

    从`sklearn.datasets`模块加载 Boston Housing 数据集
2.  Check the data structure type, as follows:

    ```
    type(boston)
    ```

    输出如下所示:

    ```
    sklearn.utils.Bunch
    ```

    输出表明这是一个 scikit-learn `Bunch`对象，但是您仍然需要获得更多的相关信息来理解您正在处理的内容。

3.  Import the base object from scikit-learn `utils` and print the docstring in your notebook:

    ```
    from sklearn.utils import Bunch
    Bunch?
    ```

    输出如下所示:

    ![Figure 2.3: The docstring for sklearn.utils.Bunch
    ](image/B15916_02_03.jpg)

    图 2.3:sk learn . utils . bunch 的 docstring

4.  Print the field names (that is, the keys to the dictionary) as follows:

    ```
    boston.keys()
    ```

    输出如下所示:

    ```
    dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename']) 
    ```

5.  Print the dataset description contained in `boston['DESCR']` as follows:

    ```
    print(boston['DESCR'])
    ```

    注意，在这个调用中，您希望显式地打印字段值，以便笔记本以比字符串表示更可读的格式呈现内容(也就是说，如果我们只键入`boston['DESCR']`而不将其包装在`print`语句中)。通过这样做，我们可以看到之前总结的数据集信息:

    ![Figure 2.4: Description of the Boston dataset
    ](image/B15916_02_04.jpg)

    图 2.4:波士顿数据集的描述

    简要通读功能描述和/或自己描述。出于本书的目的，需要理解的最重要的字段是`RM`、`AGE`、`LSTAT`和`MEDV`。这里特别重要的是特性描述(在`Attribute Information`下)。您将在分析过程中以此作为参考。

6.  Now, create a pandas DataFrame that contains the data. This is beneficial for a few reasons: all of our data will be contained in one object, there are useful and computationally efficient DataFrame methods we can use, and other libraries, such as seaborn, have tools that integrate nicely with DataFrames.

    在这种情况下，您将使用标准构造函数方法创建数据框架。为了提醒您这是如何完成的，再一次调出 docstring:

    ```
    import pandas as pd
    pd.DataFrame?
    ```

    输出如下所示:

    ![Figure 2.5: The docstring for pd.DataFrame
    ](image/B15916_02_05.jpg)

    图 2.5:PD 的文档字符串。数据帧

    docstring 揭示了 DataFrame 输入参数。您希望使用`boston['data']`输入数据，使用`boston['feature_names']`输入标题。

7.  Print the data, as follows:

    ```
    boston['data']
    ```

    输出如下所示:

    ![Figure 2.6: Printing the data
    ](image/B15916_02_06.jpg)

    图 2.6:打印数据

8.  Print the shape of the data, as follows:

    ```
    boston['data'].shape
    ```

    输出如下所示:

    ```
    (506, 13)
    ```

9.  Print the feature names, as follows:

    ```
    boston['feature_names']
    ```

    输出如下所示:

    ```
    array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 
           'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')
    ```

    查看输出，您可以看到您的数据位于 2D NumPy 数组中。运行`boston['data'].shape`命令返回长度(样本数)和特征数，分别作为第一个和第二个输出。

10.  Load the data into a pandas DataFrame, `df`, by running the following code:

    ```
    # Load the data
    df = pd.DataFrame(data=boston['data'], \
                      columns=boston['feature_names'],)
    ```

    在机器学习中，被建模的变量被称为**目标变量**；这是你试图预测的，给定特征。对于这个数据集，建议的目标是`MEDV`，这是以千美元为单位的中值房价。

11.  Check the shape of the target, as follows:

    ```
    boston['target'].shape
    ```

    输出为`(506,)`。

    您可以看到它与特征具有相同的长度，这是意料之中的。因此，它可以作为新列添加到 DataFrame 中。

12.  将目标变量添加到`df`，如下:

    ```
    df['MEDV'] = boston['target']
    ```

13.  Move the target variable to the front of `df`, as follows:

    ```
    y = df['MEDV'].copy()
    del df['MEDV']
    df = pd.concat((y, df), axis=1)
    ```

    这样做是为了通过将目标存储在数据帧的前面来区分目标和我们的特征。

    在这里，您引入了一个虚拟变量`y`，在我们将目标列从 DataFrame 中移除之前，用来保存它的副本。然后，使用 pandas 串联函数将其与第一个轴上的剩余数据帧合并(与第 0 个轴相反，第 0 个轴合并行)。

    注意

    您将经常看到用于引用数据帧列的点符号。比如以前，我们可以做`y = df.MEDV.copy()`。虽然这对于访问列数据很有效，但通常不能用来代替方括号符号，例如当列名中有空格时。此外，不能使用点符号删除列。例如，`del df.MEDV`会引发一个错误。删除`MEDV`列的正确方法是运行`del df['MEDV']`。

14.  Implement `df.head()` or `df.tail()` to glimpse the data and `len(df)` to verify that the number of samples is what we expect. Run the next few cells to see the head, tail, and length of `df`:![Figure 2.7: The first 10 and last 10 rows of the Boston Housing dataset
    ](image/B15916_02_07.jpg)

    图 2.7:波士顿住房数据集的前 10 行和后 10 行

15.  Verify that the number of samples is what you expect, as follows:

    ```
    len(df)
    ```

    输出为`506`。

    每一行都标有一个索引值，如表左侧的粗体所示。默认情况下，这些是一组整数，从 0 开始，每行递增 1。

16.  Check the data type contained within each column, as follows:

    ```
    df.dtypes
    ```

    输出如下所示:

    ![Figure 2.8: Data types of each column in the Boston Housing dataset
    ](image/B15916_02_08.jpg)

    图 2.8:波士顿住房数据集中每一列的数据类型

    对于这个数据集，您可以看到每个字段都是一个浮点数，包括目标。具体来说，查看目标(`MEDV`)的样本，您可以清楚地看到它分布在一个连续的范围内，正如浮点数据类型所期望的那样。考虑到该字段表示每条记录的中值房价，这是有意义的。根据这些信息，您可以确定预测这个目标变量是一个回归问题。

17.  You can run `df.isnull()` to clean the dataset as pandas automatically sets missing data as NaN values. To get the number of NaN values per column, execute the following code:

    ```
    df.isnull().sum()
    ```

    输出如下所示:

    ![Figure 2.9: Number of missing records in each column of the Boston Housing dataset
    ](image/B15916_02_09.jpg)

    图 2.9:波士顿住房数据集中每一列缺失记录的数量

    `df.isnull()`返回一个与`df`长度相同的布尔帧。

    对于这个数据集，您可以看到没有 NaN 值，这意味着您在清理数据方面没有立即要做的工作，可以继续前进。

18.  Remove some columns, as follows:

    ```
    for col in ['ZN', 'NOX', 'RAD', 'PTRATIO', 'B']:
        del df[col]
    ```

    这样做是为了简化分析。当我们深入研究数据时，我们将更详细地关注剩余的列。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2N1riF2](https://packt.live/2N1riF2)。

    你也可以在[https://packt.live/37DzuVK](https://packt.live/37DzuVK)在线运行这个例子。

## 数据探索

因为这是一个我们从未见过的全新数据集，所以这里的第一个目标是理解数据。我们已经看到了数据的文本描述，这对定性理解很重要。现在，我们将计算一个定量描述。

## 练习 2.03:分析波士顿住房数据集

在本练习中，您将从自上而下的角度了解有关数据集的更多信息，从汇总指标开始，然后深入了解特定列的更多详细信息。你将探索不同领域之间的关系，描绘出视觉化图像来增加我们的理解，并研究出现的问题。执行以下步骤来完成本练习:

1.  Check some statistics of the DataFrame, as follows:

    ```
    df.describe().T
    ```

    输出如下所示:

    ![Figure 2.10: Summary statistics for the Boston Housing dataset
    ](image/B15916_02_10.jpg)

    图 2.10:波士顿住房数据集的汇总统计数据

    前面的命令计算各种属性，包括每列的平均值、标准差、最小值和最大值。这张表让我们从高层次了解了所有东西是如何分布的。请注意，您已经通过向输出添加一个`.T`后缀对结果进行了转换；这将交换行和列。

    继续进行分析，您将指定一组要关注的列。

2.  定义一组特定的列，如下:

    ```
    cols = ['RM', 'AGE', 'TAX', 'LSTAT', 'MEDV']
    ```

3.  Display this subset as the columns of the DataFrame, as follows:

    ```
    df[cols].tail()
    ```

    输出如下所示:

    ![Figure 2.11: A subset of columns from the Boston Housing dataset
    ](image/B15916_02_11.jpg)

    图 2.11:波士顿住房数据集中列的子集

    回想一下每一列代表什么。从数据集文档中，您可以看到以下内容:

    `RM`:每套住宅的平均房间数

    `AGE`:1940 年以前建造的业主自用单元的比例

    `TAX`:每万美元的全值财产税税率

    `LSTAT`:被归类为“低地位”的人口百分比

    `MEDV`:以千美元计的自有住房中值

    要在这些数据中寻找模式，可以从使用`pd.DataFrame.corr`计算成对相关性开始。

4.  Calculate the pairwise correlations for your selected columns, as follows:

    ```
    df[cols].corr()
    ```

    该命令的输出如下:

    ![Figure 2.12: Pairwise correlation scores between select columns
    ](image/B15916_02_12.jpg)

    图 2.12:选择列之间的成对相关分数

    这个结果表显示了每对列之间的相关分数。大的正分数表示强的正(即，在同一方向)相关性。正如所料，您可以在对角线上看到最大值 1。

    默认情况下，pandas 计算每对列的标准相关系数，也称为`Pearson coefficient`。这被定义为两个变量之间的协方差，除以它们的标准差的乘积:

    ![Figure 2.13: The Pearson correlation coefficient equation
    ](image/B15916_02_13.jpg)

    图 2.13:皮尔逊相关系数方程

    在这里，您应该将 *X* 视为一列，将 *Y* 视为另一列。标准差的计算方法通常是将每个数据点与该列平均值之间的平方差相加。

    反过来，协方差定义如下:

    ![Figure 2.14: The covariance equation
    ](image/B15916_02_14.jpg)

    图 2.14:协方差方程

    这里， *n* 是记录的数量(即表中的行数)， *x* i 和 *y* i 分别对应每条记录被求和的单个值， *x* *̄* 和 *y* *̄* 分别对应列 *X* 和 *Y* 记录的平均值。

    回到分析，使用热图可视化您之前计算的相关系数。这将产生一个更好的结果供您查看，而不是为了解释前面的表格而劳神费力。

5.  导入绘制热图所需的库，并按如下方式设置外观设置:

    ```
    # Visualization libraries
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    # Setting plot appearance
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    ```

6.  To create the heatmap using Seaborn, execute the following code:

    ```
    ax = sns.heatmap(df[cols].corr(), \
                     cmap=sns.cubehelix_palette(20, \
                                                light=0.95, \
                                                dark=0.15),)
    ax.xaxis.tick_top() # move labels to the top
    plt.savefig('../figures/chapter-2-boston-housing-corr.png', \
                bbox_inches='tight', dpi=300,)
    ```

    注意

    要使用前面的代码保存图像，您需要确保在您的工作目录中设置了一个`figures`文件夹。或者，您可以编辑代码中的路径，将图像保存到您选择的不同文件夹中。

    以下屏幕截图显示了热图:

    ![Figure 2.15: Heatmap of correlation scores
    ](image/B15916_02_15.jpg)

    图 2.15:关联分数热图

    调用`sns.heatmap`并传递成对相关矩阵作为输入。您将在这里使用一个自定义调色板来覆盖 Seaborn 缺省值。该函数返回一个由`ax`变量引用的`matplotlib.axes`对象。

    最终的图形会以高分辨率 PNG 格式保存到`figures`文件夹中。

    注意

    使用`plt.savefig`功能将每个图表导出为 PNG 文件。为了保存高质量的图像，我们设置了`bbox_inches='tight'`和`dpi=300`参数。

    对于数据集探索练习的最后一步，我们将使用 seaborn 的`pairplot`函数可视化我们的数据。

7.  Visualize the DataFrame using Sseaborn's `pairplot` function, as follows:

    ```
    sns.pairplot(df[cols], plot_kws={'alpha': 0.6}, \
                 diag_kws={'bins': 30},)
    plt.savefig('../figures/chapter-2-boston-housing-pairplot.png', \
                bbox_inches='tight', dpi=300,)
    ```

    这是可视化的输出:

    ![Figure 2.16: Pairplot visualization of the Boston Housing dataset
    ](image/B15916_02_16.jpg)

图 2.16:波士顿住房数据集的 Pairplot 可视化

查看对角线上的直方图，您可以看到以下内容:

*   `a` : `RM`和`MEDV`具有最接近正态分布的形状。
*   `b` : `AGE`向左倾斜，`LSTAT`向右倾斜(这可能看起来违反直觉，但倾斜是根据平均值相对于最大值的位置来定义的)。
*   `c`:对于`TAX`，我们发现大量的分布在 700 左右。从散点图中也可以明显看出这一点。

仔细查看右下方的`MEDV`直方图，我们实际上可以看到类似于`TAX`的东西，其中有一个 5 万美元左右的大上限仓位。回想一下，我们用`df.describe()`的时候，`MEDV`的最小值和最大值分别是 5k 和 50k。这表明`MEDV`(代表每个社区的中值房价)已经被限制在 5000 英镑的低端和 50000 英镑的高端。这可能是出于道德原因，以帮助保护离群社区的隐私。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2N1riF2](https://packt.live/2N1riF2)。

你也可以在[https://packt.live/37DzuVK](https://packt.live/37DzuVK)在线运行这个例子。

既然我们在探索数据方面有了一个良好的开端，让我们将注意力转向数据集提供的建模问题。

## 【Jupyter 笔记本预测分析简介

继续我们对波士顿住房数据集的分析，我们可以看到它给我们提出了一个回归问题。在回归中，给定一组特征，我们试图预测一个数字目标变量。在这种情况下，我们将使用之前练习中的 pairplot 中的一些特征来预测**中值房价** ( **MEDV** )。

我们将训练只将一个特征作为输入的模型来进行预测。这样，模型在概念上就容易理解，并且我们可以更多地关注 scikit-learn API 的技术细节。然后，在下一章中，你将更容易处理我们将要在那里训练的相对复杂的模型。

## 练习 2.04:使用 Seaborn 和 scikit 训练线性模型-学习

现在，您将开始使用 Seaborn 训练模型。为此，请执行以下步骤:

1.  Take a look at the pairplot that you created in the last step of *Exercise 2.03*, *Analyzing the Boston Housing Dataset*. In particular, look at the scatter plots in the bottom-left corner:![Figure 2.17: Scatter plots for MEDV and LSTAT
    ](image/B15916_02_17.jpg)

    图 2.17:MEDV 和 LSTAT 的散点图

    请注意每栋房子的房间数与低收入阶层人口的比例与房屋价值中值高度相关。让我们提出以下问题:给定这些变量，我们能多好地预测 MDEV？

    为了帮助回答这个问题，使用 seaborn 来可视化这些关系。您将沿着最佳拟合线性模型线绘制散点图。

2.  To use the `sns.regplot` function (which stands for "regression" plot), pull up that docstring, as follows:

    ```
    sns.regplot?
    ```

    输出如下所示:

    ![Figure 2.18: The docstring for sns.regplot
    ](image/B15916_02_18.jpg)

    图 2.18:SNS . regplot 的文档字符串

    阅读前几个参数，注意函数如何接受数据帧作为输入。

3.  Draw scatter plots for the linear models, as follows:

    ```
    fig, ax = plt.subplots(1, 2)
    sns.regplot(x='RM', y='MEDV', data=df, \
                ax=ax[0], scatter_kws={'alpha': 0.4},)
    sns.regplot(x='LSTAT', y='MEDV', data=df, \
                ax=ax[1], scatter_kws={'alpha': 0.4},)
    plt.savefig('../figures/chapter-2-boston-housing-scatter.png', \
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.19: Scatter plots with linear regression trends
    ](image/B15916_02_19.jpg)

    图 2.19:具有线性回归趋势的散点图

    最佳拟合线是通过最小化普通最小二乘误差函数来计算的，这是 seaborn 在我们调用`regplot`函数时自动完成的。此外，请注意线条周围的阴影区域，它代表 95%的置信区间。

    注意

    由`sns.regplot`函数呈现的 95%置信区间，如前图所示，是通过取垂直于最佳拟合线的条柱中数据的标准偏差来计算的，有效地确定了最佳拟合线上每个点的置信区间。在实践中，这涉及到 seaborn 引导数据，这是一个通过随机抽样和替换来创建新数据的过程。自举样本的数量根据数据集的大小自动确定，也可以通过传递`n_boot`参数手动设置。

4.  Plot the residuals using seaborn, as follows:

    ```
    fig, ax = plt.subplots(1, 2)
    ax[0] = sns.residplot(x='RM', y='MEDV', data=df, \
                          ax=ax[0], scatter_kws={'alpha': 0.4},)
    ax[0].set_ylabel('MDEV residuals $(y-\hat{y})$')
    ax[1] = sns.residplot(x='LSTAT', y='MEDV', data=df, \
                          ax=ax[1], scatter_kws={'alpha': 0.4},)
    ax[1].set_ylabel('')
    plt.ylim(-25, 40)
    plt.savefig('../figures/chapter-2-boston-housing-residuals.png',\
                bbox_inches='tight', dpi=300,)
    ```

    这将产生以下图表:

    ![Figure 2.20: Residual charts for linear regression models
    ](image/B15916_02_20.jpg)

    图 2.20:线性回归模型的残差图

    这些残差图上的每个点是观察值(`y`)和线性模型预测值(`ŷ`)之间的差。大于零的残差是被模型低估的数据点。同样，小于零的残差是被模型高估的数据点。

    这些图中的模式可能表示次优建模。在前面的每种情况下，您可以在正区域中看到对角排列的分散点。这些都是由`MEDV`的 5 万美元上限造成的。两个残差图都显示数据主要集中在 0 附近，这是良好拟合的指标。然而，`LSTAT`似乎在略低于 0 的位置聚集，表明线性模型可能不是最佳选择。通过查看上面的散点图可以看到同样的事实，可以看到最适合`LSTAT`的线通过了大部分的点。

5.  Define a function using scikit-learn that calculates the line of best fit and **mean squared error** (**MSE**):

    ```
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error
    def get_mse(df, feature, target='MEDV'):
        # Get x, y to model
        y = df[target].values
        x = df[feature].values.reshape(-1,1)
        print('{} ~ {}'.format(target, feature))
        # Build and fit the model
        lm = LinearRegression()
        lm.fit(x, y)
        msg = ('model: y = {:.3f} + {:.3f}x' \
               .format(lm.intercept_, lm.coef_[0]))
        print(msg)
    ```

    注意

    这一步的完整代码可以在[https://packt.live/2UIzwq8](https://packt.live/2UIzwq8)找到。

    在`get_mse`函数中，将`y`和`x`变量分别分配给目标 MDEV 和从属特征。通过调用 values 属性将它们转换为 NumPy 数组。从属特征数组被重新整形为 scikit-learn 所期望的格式；只有在对一维特征空间建模时才需要此步骤。然后将模型实例化，并根据数据进行拟合。对于线性回归，拟合包括使用普通最小二乘法计算模型参数(最小化每个样本的误差平方和)。最后，在确定参数后，您将预测目标变量，并使用结果来计算 MSE。MSE 是每个数据点的误差平方和，其中误差定义为观察值和预测值之间的差异。

6.  Call the `get_mse` function for both `RM` and `LSTAT`, as follows:

    ```
    get_mse(df, 'RM')
    get_mse(df, 'LSTAT')
    ```

    调用`RM`和`LSTAT`的输出如下:

    ```
    MEDV ~ RM
    model: y = -34.671 + 9.102x
    mse = 43.60
    MEDV ~ LSTAT
    model: y = 34.554 + -0.950x
    mse = 38.48
    ```

    通过比较 MSE，可知`LSTAT`的误差略低于`RM`的误差。然而，回过头来看散点图，似乎我们对`LSTAT`使用多项式模型可能会取得更好的成功。在*活动 2.01* 、*构建三阶多项式模型*中，我们将通过使用 scikit-learn 计算一个三阶多项式模型来对此进行测试。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2N1riF2](https://packt.live/2N1riF2)。

    你也可以在[https://packt.live/37DzuVK](https://packt.live/37DzuVK)在线运行这个例子。

现在我们已经介绍了如何使用 scikit-learn 对数据建模，让我们回到对波士顿住房数据集的探索。在下一节中，我们将介绍一些关于分类特征的概念，然后将这些概念应用到我们的数据集，以便更详细地探索变量关系。

## 使用分类特征进行分割分析

通常，我们会发现数据集混合了连续字段和分类字段。在这种情况下，我们可以通过用分类字段分割连续变量来了解我们的数据并找到模式。

作为一个具体的例子，假设你正在评估一个广告活动的投资回报。您可以访问的数据包含一些计算出的**投资回报** ( **ROI** )指标。这些值是每天计算和记录的，并且您正在分析前一年的数据。你的任务是寻找数据驱动的见解，以改进广告活动。查看 ROI 每日时间序列，您会看到数据每周都有波动。按照一周中的每一天进行细分，您会发现以下 ROI 分布(其中`0`代表一周的第一天，`6`代表最后一天):

![Figure 2.21: A violin plot with ROI on the vertical axis and the day of the week 
on the horizontal axis
](image/B15916_02_21.jpg)

图 2.21:垂直轴是 ROI，水平轴是星期几的 violin 图

由于我们正在处理的波士顿住房数据集中没有任何分类字段，因此我们将通过有效地离散化连续字段来创建一个分类字段。在我们的例子中，这将涉及宁滨的数据分为“低”，“中”，和“高”类别。需要注意的是，我们并不是简单地创建一个分类数据字段来说明本节中的数据分析概念。正如我们将看到的，这样做可以从数据中揭示出洞察力，否则很难注意到或完全不可用。

## 练习 2.05:从连续变量中创建分类字段并进行分段可视化

在开始这个练习之前，再看一下*练习 2.03* 、*的最终配对图，分析 Boston Housing 数据集*，其中比较了`MEDV`、`LSTAT`、`TAX`、`AGE`和`RM`:

![Figure 2.22: Scatter charts comparing AGE to MEDV, LSTAT, TAX, and RM
](image/B15916_02_22.jpg)

图 2.22:年龄与 MEDV、LSTAT、税收和 RM 的散点图比较

看一下包含`AGE`的面板。提醒一下，这一特征被定义为 1940 年以前建造的自有住房的比例。我们将把这个特征转换成一个分类变量。一旦它被转换，你将能够重新绘制这个数字，每个面板根据年龄分类按颜色分段。执行以下步骤来完成本练习:

1.  Plot the `AGE` cumulative distribution, as follows:

    ```
    sns.distplot(df.AGE.values, bins=100, \
                 hist_kws={'cumulative': True}, \
                 kde_kws={'lw': 0},)
    plt.xlabel('AGE')
    plt.ylabel('CDF')
    plt.axhline(0.33, color='red')
    plt.axhline(0.66, color='red')
    plt.xlim(0, df.AGE.max())
    plt.savefig('../figures/chapter-2-boston-housing-age-cdf.png',\
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.23: Plot for cumulative distribution of AGE
    ](image/B15916_02_23.jpg)

    图 2.23:年龄累积分布图

    这里，您使用`sns.distplot`来绘制分布图，并设置`hist_kws={'cumulative': True}`来计算和绘制累积分布图。在这些类型的分布中，每个条形计算当前 x 轴仓和左侧每个仓中有多少个值，从而显示该点的累积量。

    注意

    Seaborn 图表的各种属性可以使用`x_kws`进行调优，其中`x`代表一个参数名，比如`hist`。通常，这些用于设置图表的美学元素。例如，在前面的图中，我们设置`kde_kws={'lw': 0}`以绕过绘制核密度估计，这将是对应于密度的平滑线。

    查看该图，具有低`AGE`分布值的样本非常少，而具有非常大的`AGE`的样本则多得多。这由最右侧分布的陡度表示。

    红线表示累积分布中的 1/3 和 2/3 点。看看我们的分布截取这些水平线的地方，可以看到只有大约 33%的样本的`AGE`值小于 55，33%的样本的`AGE`值大于 90。换句话说，三分之一的住宅社区只有不到 55%的房屋建于 1940 年之前。这些被认为是相对较新的社区。另一方面，另外三分之一的住宅社区 90%以上的房屋建于 1940 年之前。这些会被认为是非常古老的。您将使用红色水平线与分布相交的地方作为向导，将要素分成几类:`Relatively New`、`Relatively Old`和`Very Old`。

2.  Create a new categorical feature and set the segmentation points, as follows:

    ```
    def get_age_category(x):
        if x < 50:
            age = 'Relatively New'
        elif 50 <= x < 85:
            age = 'Relatively Old'
        else:
            age = 'Very Old'
        return age
    df['AGE_category'] = df.AGE.apply(get_age_category)
    ```

    这里，您使用的是非常方便的 pandas `apply`方法，它将一个函数应用于给定的一列或一组列。所应用的函数——在本例中是`get_age_category`——应该接受一个表示一行数据的参数，并为新列返回一个值。在这种情况下，传递的数据行只是一个值，即样本的`AGE`。

    注意

    `apply`方法很棒，因为它可以解决各种各样的问题，并允许代码易于阅读。不过，通常像`pd.Series.str`这样的矢量化方法可以更快地完成同样的事情。因此，建议尽可能避免使用它，尤其是在处理大型数据集时。我们将在*第 3 章*、*为预测建模准备数据*中看到一些矢量化方法的例子。

3.  Verify the number of samples you've grouped into each age category, as follows:

    ```
    df.groupby('AGE_category').size()
    ```

    输出如下所示:

    ```
    AGE_category
    Relatively New    147
    Relatively Old    149
    Very Old          210
    dtype: int64
    ```

    查看结果，您可以看到两个班级的规模几乎相等，而`Very Old`组大约大 40%。您感兴趣的是保持类在大小上的可比性，以便每个类都得到很好的表示，并且可以直接从分析中做出推断。

    注意

    不可能总是将样本均匀地分配给各个类，在现实世界中，发现高度不平衡的类是很常见的。在这种情况下，重要的是要记住，很难对代表性不足的阶层提出具有统计意义的主张。具有不平衡类别的预测分析可能特别困难。下面的博客文章提供了一个关于在执行机器学习时处理不平衡类的方法的出色总结:[https://svds.com/learning-imbalanced-classes/](https://svds.com/learning-imbalanced-classes/)。

    现在来看看当被新特征`AGE_category`分割时，目标变量是如何分布的。

4.  Construct a violin plot, as follows:

    ```
    sns.violinplot(x='MEDV', y='AGE_category', data=df, \
                   order=['Relatively New', 'Relatively Old', \
                          'Very Old'],)
    plt.xlim(0, 55)
    plt.savefig('../figures/chapter-2-boston-housing-'\
                'age-medv-violin.png', \
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.24: Violin plot comparing MEDV distributions by age
    ](image/B15916_02_24.jpg)

    图 2.24:按年龄比较 MEDV 分布的小提琴图

    前面的 violin 图显示了每个年龄类别的中值房价分布的核密度估计。您可以将每个分类级别的分布与正态分布进行比较，您可以观察到三者都有一定程度的偏态。`Very Old`组包含最低的中值房价记录，并且具有相对较大的宽度，而其他组更紧密地集中在它们的平均值上。年轻群体偏向高端，这从放大的右半部分和分布主体内粗黑线中白点的位置可以明显看出。

    这个白点代表平均值，粗黑线跨越大约 50%的人口(它填充到白点两侧的第一个分位数)。细黑线代表箱线图须，覆盖 95%的人口。可以通过传递`inner='point' to sns.violinplot()`来修改这个内部可视化，以显示单独的数据点。

5.  Reconstruct the violin plot by adding the `inner='point'` argument to the `sns.violinplot` call. The code for this is as follows:

    ```
    sns.violinplot(x='MEDV', y='AGE_category', data=df, \
                   order=['Relatively New', 'Relatively Old', \
                          'Very Old'], \
                   inner='point',)
    plt.xlim(0, 55)
    plt.savefig('../figures/chapter-2-boston-housing-'\
                'age-medv-violin-points.png',\
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.25: Including points for individual samples in the violin plot
    ](image/B15916_02_25.jpg)

    图 2.25:包括小提琴图中单个样本的点

    出于测试目的制作这样的图是很好的，这样可以看到底层数据是如何连接到视觉效果的。例如，您可以看到,`Relatively New`段没有低于大约 16，000 美元的中值房价，因此，分布尾部实际上不包含任何数据。由于我们的数据集很小(只有大约 500 行)，您可以看到每个段都是如此。

6.  Reconstruct the `pairplot` visualization from earlier, but now include color labels for each `AGE` category. This is done by simply passing the hue argument, as follows:

    ```
    cols = ['RM', 'AGE', 'TAX', 'LSTAT', 'MEDV', 'AGE_category']
    sns.pairplot(df[cols], hue='AGE_category',\
                 hue_order=['Relatively New', 'Relatively Old', \
                            'Very Old'], \
                 plot_kws={'alpha': 0.5},)
    plt.savefig('../figures/chapter-2-boston-housing-'\
                'age-pairplot.png', \
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.26: Pairplot visualization of the Boston Housing dataset, with AGE segments
    ](image/B15916_02_26.jpg)

    图 2.26:波士顿住房数据集的成对图可视化，带有年龄段

    查看前面的直方图，您可以看到对于`RM`和`TAX`，每个片段的基本分布看起来是相似的。另一方面，`LSTAT`分布看起来更加独特。你可以通过小提琴的情节来更详细地关注它们。

7.  Reconstruct a violin plot to compare the LSTAT distributions for each `AGE_category` segment, as follows:

    ```
    sns.violinplot(x='LSTAT', y='AGE_category', data=df, \
                   order=['Relatively New', 'Relatively Old', \
                          'Very Old'],)
    plt.xlim(-5, 40)
    plt.savefig('../figures/chapter-2-boston-housing-'\
                'lstat-violin.png',\
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.27: Violin plot comparing LSTAT distributions by age
    ](image/B15916_02_27.jpg)

图 2.27:按年龄比较 LSTAT 分布的小提琴图

不同于`MEDV` violin 图，其中每个分布具有大致相同的宽度，在这里，您可以看到宽度随着`AGE`增加。以老房子为主的社区(`Very Old`部分)包含从很少到很多较低阶层的居民，而`Relatively New`社区更有可能主要是较高阶层，超过 95%的样本比`Very Old`社区的较低阶层百分比要低。这是有道理的，因为`Relatively New`街区会更贵。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2N1riF2](https://packt.live/2N1riF2)。

你也可以在[https://packt.live/37DzuVK](https://packt.live/37DzuVK)在线运行这个例子。

## 活动 2.01:建立一个三阶多项式模型

在本章的前面，您使用 scikit-learn 独立地为作为`RM`和`LSTAT`的函数的中值房价(`MEDV`)创建了线性模型。特别是，由于`MEDV`是`RM`的函数，您知道建模想要构建一个三阶多项式模型来与线性模型进行比较。回想一下我们试图解决的实际问题:在给定较低阶层人口百分比的情况下，预测中值房价。这种模式可能有利于波士顿的潜在购房者，他们关心自己社区中有多少人是下层阶级。

这里的目的是使用 scikit-learn 拟合多项式回归模型，在给定`LSTAT`值的情况下预测中值房价(`MEDV`)，并构建一个具有较低 MSE 的模型。为了实现这一点，必须执行以下步骤:

1.  Load the necessary libraries and set up the plot settings for the notebook.

    注意

    完成此活动时，您将需要使用笔记本中的许多单元格。请根据需要插入新的单元格。

2.  像本章前面所做的那样，将波士顿住房数据集加载到 pandas 数据框架中。回想一下，您访问了内置 scikit-learn 数据集的数据。不要忘记添加名为`MEDV`的目标变量列。
3.  从`df`中取出从属特征和目标变量。将目标分配给`y`变量，将特征分配给`x`变量。确保`x`的形状适合训练 scikit-learn 模型。它应该像这样:`[[x1], [x2], [x3], ...]`。
4.  通过打印前三个样品，验证`x`具有正确的形状。
5.  从 scikit-learn 的预处理文件夹中导入`PolynomialFeatures`类。然后，用`degree=3`实例化。完成后，在笔记本中显示实例化的对象。
6.  通过运行`fit_transform`方法转换`LSTAT`特征(分配给`x`变量)。将新创建的多项式特征集分配给`x_poly`变量。
7.  通过打印前几项来验证`x_poly`的样子。为了表示 0 到 3 次的多项式，它们每个都应该具有 4 维。
8.  从 scikit 中导入`LinearRegression`类——学习并训练一个线性分类模型，就像我们在计算 MSE 时所做的一样。当实例化模型时，设置`fit_intercept=False`并给它命名为`clf`。
9.  使用`clf`的`coef_`属性提取模型系数。用这些系数写出我们多项式模型的方程(即 y = a + bx + cx2 + dx3)。
10.  确定每个样本的预测值，然后使用这些值计算残差。
11.  打印前 10 个残差。您应该看到一些大于 0，一些小于 0。
12.  计算三阶多项式模型的 MSE。
13.  创建多项式模型图，作为平滑线，覆盖样本的散点图。
14.  Plot the residuals as a scatter chart. Compare this to the linear model residual chart we plotted earlier for this feature. The polynomial model should yield a better fit.

    注意

    第 266 页介绍了详细的步骤以及该活动的解决方案。

# 总结

在这一章中，我们在一个真实的 Jupyter 笔记本环境中进行了探索性分析。在此过程中，我们使用了散点图、直方图和小提琴图等可视化工具来加深对数据的理解。我们还执行了简单的预测建模，这将是本书后续章节的重点。

在下一章中，我们将讨论如何进行预测分析，以及在为建模准备数据时需要考虑的事项。我们将使用 pandas 来探索数据预处理的方法，例如填充缺失数据、将分类特征转换为数字特征，以及将数据分成训练集和测试集。