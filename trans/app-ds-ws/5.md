<title>B15916_04_Final_VK_ePub</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 4。训练分类模型

概观

在这一章中，你将学习诸如支持向量机、随机森林和 k-最近邻分类器等算法。在训练和比较各种模型时，您将在决策边界图的帮助下了解过度拟合的概念。在本章结束时，你将能够使用 scikit-learn 来应用这些算法，以便为现实世界的分类问题训练模型。

# 简介

在前面的章节中，我们介绍了在训练机器学习模型之前，我们需要在数据科学项目中采取的步骤。这包括规划阶段，即识别业务问题、评估数据源的适用性以及决定建模方法。

已经决定了一个通用的建模方法，我们应该小心避免**训练 ML 模型的常见陷阱**当我们继续建模的时候。首先，记住训练数据非常重要。事实上，增加训练数据的数量比模型选择对评分性能的影响更大。一个问题是可能没有足够的数据可用，这可能使模式难以找到，并导致模型在测试数据上表现不佳。**数据质量**对模型性能也有巨大影响。一些可能的问题包括:

*   非代表性训练数据(采样偏差)
*   记录组中的错误(例如记录的重量是千克而不是磅)
*   极端值
*   未填充的缺失值
*   不良特征

当预处理数据集时，这些问题应该让我们想起上一章所做的工作。

在建模方面，您应该避免**过度拟合**，这种情况发生在模型与训练数据拟合得太好，以至于无法推广到测试样本的时候。类似地，你也应该尽量避免**欠拟合**，在这种情况下，模型无法捕捉到能够产生更高精度的有趣模式。对此的解决方案是尝试许多不同类型的模型，并小心遵循最佳实践，如 k-fold 交叉验证，这将在*第 5 章*、*模型验证和优化*中讨论。为了让你为此做好准备，本章将着重于理解过度拟合实际上是什么样子，以及如何避免过度拟合的一般概念。

在第三章、*准备预测建模数据*的*接近尾声时，我们介绍了人力资源分析数据集(【https://www.kaggle.com/giripujar/hr-analytics】T4)。当训练和比较我们的模型时，我们将在为本章设计的练习中继续使用这个数据集。之前，我们加载了数据集的混乱版本，并使用 pandas 和 Jupyter 对其进行了清理，而在本章中，我们将加载数据集的清理版本。*

这一章是非常实际的，我们将学到的大部分内容将来自我们在与 Jupyter 一起工作时所做的练习。我们将通过使用 scikit-learn 训练各种分类算法来对员工流动进行建模。然而，首先，我们将花一点时间从概念的角度简要介绍算法是如何工作的。

# 了解分类算法

回想一下有监督的机器学习的两种类型:回归和分类。在回归中，我们预测一个数字目标变量。例如，回忆一下第二章、*中的线性和多项式模型。在这里，我们将重点关注另一种类型的监督机器学习——分类——其目标是使用可用的度量来预测记录的类别。在最简单的情况下，只有两个可能的类，这意味着我们正在进行二元分类。这就是本章中的示例问题，我们将尝试预测一名员工是否会离职。如果我们有两个以上的类别标签，那么我们正在进行多类别分类。*

虽然在使用 scikit-learn 训练模型时，二分类和多分类之间几乎没有区别，但算法可能会有显著不同。特别是，多类分类模型通常使用一对多的方法。对于具有三个类别标签的情况，工作如下。当模型与数据“吻合”时，将训练三个模型，每个模型预测该记录是单个类的一部分还是某个其他类的一部分。然后，在进行预测时，将评估每个模型，并返回具有最高置信度的类标签。

在本章中，我们将训练三种类型的分类模型:**支持向量机** ( **支持向量机**)、**随机森林**和**k-最近邻分类器** ( **KNN** )。这些算法各不相同。然而，正如我们将看到的，由于 scikit-learn 的简单性，它们非常类似于训练和用于预测。在打开本章的 Jupyter 笔记本之前，让我们简要讨论一下这些算法是如何工作的。

支持向量机试图找到划分类别的最佳超平面。这是通过最大化超平面和每个类的最近记录之间的距离来实现的，这些记录被称为支持向量。

虽然基本实现最适合线性可分数据，但通过使用核技巧，支持向量机也可用于建模非线性依赖关系。简而言之，内核技巧将数据集特征映射到一个更高维的空间。一旦在这个更高维的空间中，数据就被认为是线性可分的，这样就可以找到一个超平面。

这个超平面也被称为决策面，我们将在本章的练习中训练我们的模型时可视化它。事实上，我们将看到线性和非线性(核)支持向量机的例子。

随机森林是决策树的集合，其中每一个都在训练数据的不同子集上被训练。例如，为了进行分类，随机森林分类器可能会取数百棵决策树的平均结果。分类**决策树**基于一系列级联决策预测给定记录的类别。例如，第一个决策可能是`"if feature x_1 is less than or greater than 0"`。然后，在这种情况下，数据将被拆分，并馈入树的下行分支。当“训练”决策树时(即，当将训练数据输入到算法中时)，基于最大化信息增益的特征分割来选择每个决策的细节。本质上，该算法试图在每个分支分裂处分离最大数量的类标签，使得树底部的每个桶将包含具有相同目标标签的训练记录。

训练随机林包括为一组决策树创建引导数据集(即，带有替换的随机采样数据，以便可以在训练数据集中多次复制记录)。然后根据多数票做出预测。这些方法的好处是更少的过度拟合和更好的泛化能力。

注意

决策树可以用来模拟连续数据和分类数据的混合，这使得它们非常有用。此外，正如我们将在本章后面看到的，可以限制树的深度以减少过度拟合。要详细(但简短)地了解决策树算法，请查看这个流行的 StackOverflow 答案，其中作者展示了一个简单的示例，并讨论了节点纯度、信息增益和熵等概念:https://stack overflow . com/questions/1859554/what-is-entropy-and-information-gain/1859910 # 1859910。

KNN 分类算法记忆训练数据，并根据特征空间中的 k-最近记录进行预测。通过三个特征，这可以被可视化为预测样本周围的球体。然而，通常我们会处理三个以上的特征，因此绘制超球面是为了找到最接近的 k 个记录。

在介绍了我们将在本书中用来训练模型的算法后，我们准备继续本章的动手操作部分。

## 练习 4.01:用 scikit-learn 训练双特征分类模型

在本练习中，我们将继续研究我们在上一章中介绍的员工保留问题。

回想一下这个问题的背景，一家公司向我们寻求帮助，以了解和防止员工流失。为了刷新您的记忆，您不妨回顾并通读*第三章*、*为预测建模准备数据*中的问题描述和建模计划。正如我们在那里描述的，我们帮助解决这个问题的方法是确定一个员工离开公司的概率。使用提供的人力资源分析数据集，我们制定了一个策略来模拟员工是否会离开公司。

正如在数据集中看到的，该模型可以依赖于与员工相关的各种信息(特征),例如满意度、他们正在进行的项目数量以及他们在公司中的部门。然而，首先，我们将只关注两个特性:满意度和上次评估分数。这样，我们可以专注于建模的技术细节，并说明不同建模方法之间的比较。在下一章中，我们将在我们已经完成的工作的基础上，根据我们所掌握的全部特性来训练模型。执行以下步骤来完成本练习:

1.  创建新的 Jupyter 笔记本。
2.  在第一个单元格中，添加以下代码行，以加载必要的库并为笔记本设置绘图环境:

    ```
    import numpy as np
    import pandas as pd
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (8, 8)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    ```

3.  In the next cell, enter the following code to print the date, version numbers, and hardware information:

    ```
    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

    您应该得到以下输出:

    ![Figure 4.1: Screenshot showing all the libraries loaded
    ](image/B15916_04_01.jpg)

    图 4.1:显示所有加载的库的屏幕截图

    我们将从仔细查看我们的目标变量`left`开始，它具有以下值之一:

    `1` (True)如果员工已经离职

    `0` (False)如果员工仍在公司工作

    在上一章中，您处理了原始数据集并将转换后的表保存在 CSV 文件中。你可以回头看看`chapter_3_workbook.ipynb`笔记本的底部，看看这是什么时候完成的。

4.  Load the processed dataset by running the following code in a new cell:

    ```
    df = pd.read_csv('[PATH_TO_THE_data_FOLDER]/hr-'\
                     'analytics/hr_data_processed.csv')
    ```

    注意

    在完成第三章、*中*的数据清理阶段，为预测建模*准备数据时，很容易意外地将错误引入到您处理的数据集中。为了避免这种可能性，或者万一您在完成本章的练习时遇到问题，请确保您使用的是本书原始资料中提供的经过处理的数据集。*

    处理后的数据集可从位于 https://packt.live/3hEBQIy 的[GitHub 下载。](https://packt.live/3hEBQIy)

    在本章中，我们将在两组连续特征上训练分类模型:`satisfaction_level`和`last_evaluation`。根据我们在*第三章*、*准备预测建模数据*([https://packt.live/2YEsiUX](https://packt.live/2YEsiUX))中绘制的直方图，我们可以看到它们在`0`和`1`之间分布得相当均匀。现在，让我们看看它们是如何相互分配的。

5.  Draw the bivariate (and univariate) graphs of the two feature variables by running the following code:

    ```
    sns.jointplot(x='satisfaction_level', y='last_evaluation', \
                  data=df, kind='hex')
    plt.savefig('../figures/chapter-4-hr-analytics-jointplot.png', \
                bbox_inches='tight', dpi=300,)
    ```

    您将获得以下输出。如您所见，数据中有一些非常独特的模式:

    ![Figure 4.2: Bivariate and univariate distributions for satisfaction_level and last_evaluation
    ](image/B15916_04_02.jpg)

    图 4.2:满意度和最后评估的二元和一元分布

    前面的密度图是有用的，但更有趣的是，当比较离开和留在公司的员工时，这种分布有什么不同。

6.  Replot the bivariate distribution, but this time, segment the chart on the target variable so that each target class is represented by a different color. This can be done with the following code:

    ```
    fig, ax = plt.subplots()
    plot_args = dict(shade=True, shade_lowest=False)
    for i, c in zip((0, 1), ('Reds', 'Blues')):
        sns.kdeplot(df.loc[df.left==i, 'satisfaction_level'], \
                    df.loc[df.left==i, 'last_evaluation'], \
                    cmap=c, **plot_args)
    ax.text(0.05, 1.05, 'left = 0', size=16, \
            color=sns.color_palette('Reds')[-2])
    ax.text(0.25, 1.05, 'left = 1', \
            size=16, color=sns.color_palette('Blues')[-2])
    plt.savefig('../figures/chapter-4-hr-analytics-bivariate-'\
                'segmented.png', bbox_inches='tight', dpi=300,)
    ```

    你会看到数据的分段。输出如下所示:

    ![Figure 4.3: Bivariate distribution for satisfaction_level and last_evaluation, segmented by the target variable
    ](image/B15916_04_03.jpg)

    图 4.3:满意度和最后评估的二元分布，按目标变量划分

    现在，您可以看到模式是如何与目标变量相关联的。对于本练习的剩余部分，尝试利用这些模式来训练有效的分类模型。

7.  Split the data into training and test sets by running the cell containing the following code:

    ```
    from sklearn.model_selection import train_test_split
    features = ['satisfaction_level', 'last_evaluation']
    X_train, X_test, \
    y_train, y_test = train_test_split(df[features].values, \
                                       df['left'].values, \
                                       test_size=0.3, \
                                       random_state=1)
    ```

    当涉及到机器学习时，通常情况下，当输入数据被缩放以使所有特征处于相同的顺序时，前两个模型(SVM 和 kNN 分类器)是最有效的。使用 scikit-learn 的`StandardScaler`类来实现这一点

8.  Load the `StandardScaler` class and create a new instance, as referenced by the `scaler` variable. Then, fit this on the training set and transform it, before finally transforming the test set:

    ```
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.transform(X_test)
    ```

    注意

    在训练机器学习模型时，一个容易犯的错误是在整个数据集上*拟合*缩放器，而实际上它应该只在训练数据上*拟合*。例如，在将数据分成训练集和测试集之前对其进行缩放是错误的。

    我们不希望这种情况发生，因为那样的话，我们的训练数据的缩放将受到我们的测试集的细节的影响，并且我们不希望模型训练以任何方式受到测试数据的影响。通过允许模型训练受到测试数据的影响，我们将给予它学习测试集的机会，从而导致它成为真实的“看不见的”数据的不良表示。

9.  Import the scikit-learn SVM class, SVC, and fit the model on the training data with the following code:

    ```
    from sklearn.svm import SVC
    svm = SVC(kernel='linear', C=1, random_state=1, gamma='scale')
    svm.fit(X_train_std, y_train)
    ```

    您将获得以下输出:

    ```
    SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='scale', 
        kernel='linear', max_iter=-1, probability=False, 
        random_state=1, shrinking=True, 
        tol=0.001, verbose=False)
    ```

10.  Compute the accuracy of this model on unseen (test) data by running the following code:

    ```
    from sklearn.metrics import accuracy_score
    y_pred = svm.predict(X_test_std)
    acc = accuracy_score(y_test, y_pred)
    print('accuracy = {:.1f}%'.format(acc*100))
    ```

    这将打印以下输出:

    ```
    accuracy = 75.9%
    ```

    这里，您已经使用了 scikit-learn 的`accuracy_score`函数，它接受两个参数:测试集中每条记录的正确值和测试集中每条记录的预测值(分别为`y_test`和`y_pred`)。

    结果看起来有希望达到 75%,对于我们的第一个模型来说，这已经不错了。然而，您可能还记得*第 3 章*、*为预测建模*准备数据，我们的目标类`left`是高度不平衡的，因为数据集中的大多数记录对应于仍在公司工作的员工。考虑到这一点，除了整体精度之外，我们还需要考虑单个类的精度。

11.  To look at the accuracy within each target class, use a `confusion matrix`. This is a 2 x 2 table with actual classes on the horizontal axis and predicted classes on the vertical axis, as follows:![Figure 4.4: Confusion matrix for the linear SVM model
    ](image/B15916_04_04.jpg)

    图 4.4:线性 SVM 模型的混淆矩阵

    一个完美的分类器将所有的预测作为真阳性或真阴性，因此对于假阳性和假阴性，在非对角线条目上显示零。

    通过运行以下命令，打印我们模型的混淆矩阵:

    ```
    from sklearn.metrics import confusion_matrix
    cmat = confusion_matrix(y_test, y_pred)
    ```

    这将打印以下输出:

    ```
    array([[3416,    0],
           [1084,    0]])
    ```

    将此与混淆参考表(*图 4.4* )进行比较，您可以看到我们的模型将所有测试记录预测为 0 类，因此我们得到了超过 1084 个假阴性。这可不好！

12.  Using the preceding confusion matrix, derive the class accuracies by running the following code:

    ```
    print('percent accuracy score per class:')
    cmat = confusion_matrix(y_test, y_pred)
    scores = cmat.diagonal() / cmat.sum(axis=1) * 100
    print('left = 0 : {:.2f}%'.format(scores[0]))
    print('left = 1 : {:.2f}%'.format(scores[1]))
    ```

    这将打印以下输出:

    ```
    percent accuracy score per class:
    left = 0 : 100.00%
    left = 1 : 0.00%
    ```

    正如所料，您可以看到您的模型只是将每个样本分类为 0，这意味着它预测测试集中没有员工会离开公司。显然，这一点帮助都没有。

    让我们使用等值线图来显示特征空间中每一点的预测类。这通常被称为决策区域图。

13.  Plot the decision regions using a helpful function from the `mlxtend` library, as follows:

    ```
    from mlxtend.plotting import plot_decision_regions
    N_samples = 200
    X, y = X_train_std[:N_samples], y_train[:N_samples]
    plot_decision_regions(X, y, clf=svm)
    ```

    以下是决策区域的输出图:

    ![Figure 4.5: Decision region plot for the linear SVM
    ](image/B15916_04_05.jpg)

    图 4.5:线性 SVM 的决策区域图

    前面的函数绘制决策区域，以及作为参数传递的一组记录。为了正确地看到决策区域，而不会有太多的记录妨碍我们的视图，我们只将一个 200 条记录的测试数据样本传递给`plot_decision_regions`函数。在这种情况下，当然没关系。我们看到结果完全是红色的，表明特征空间中的每个点都将被分类为 0。

    线性模型不能很好地描述这些非线性模式，这并不奇怪。如果数据集是线性可分的，我们将能够使用直线从每个类中分离出大多数记录。回想一下使用支持向量机分类非线性问题的核心技巧。让我们看看这样做是否能改善结果。

14.  Print the docstring for scikit-learn's SVM by running the cell containing `SVC?`:![Figure 4.6: The docstring for sklearn.svm.SVC
    ](image/B15916_04_06.jpg)

    图 4.6:sk learn . SVM . SVC 的文档字符串

    向下滚动并查看参数描述。这应读作如下:

    ```
    kernel : string, optional (default='rbf')
        Specifies the kernel type to be used in the algorithm.
        It must be one of 'linear', 'poly', 'rbf', 'sigmoid',
        'precomputed' or a callable.
    ```

    注意内核选项，默认情况下它被启用为`rbf`。这代表径向基函数，其细节超出了本书的范围。

15.  Use the `rbf` kernel option to train a new SVM by running the following code:

    ```
    svm = SVC(kernel='rbf', C=1, random_state=1, gamma='scale')
    svm.fit(X_train_std, y_train)
    ```

    为了继续快速构建和比较模型，我们将把前面的几个步骤包装到一个名为`check_model_fit`的 Python 函数中，该函数将接受一个经过训练的模型，以及测试数据，并生成我们感兴趣的准确度分数和决策边界图。通过运行以下代码来定义该函数:

    第 4 章 _ 工作簿. ipynb

    ```
    from sklearn.metrics import accuracy_score
    from sklearn.metrics import confusion_matrix
    from IPython.display import display
    from mlxtend.plotting import plot_decision_regions
    def check_model_fit(clf, X_test, y_test):
        # Print overall test-set accuracy
        y_pred = clf.predict(X_test)
        acc = accuracy_score(y_test, y_pred, normalize=True) * 100
        print('total accuracy = {:.1f}%'.format(acc))
    ```

    这一步的完整代码可以在[https://packt.live/3e6JYPJ](https://packt.live/3e6JYPJ)找到。

16.  Call `check_model_fit` for your newly trained kernel SVM by running the following command:

    ```
    check_model_fit(svm, X_test_std, y_test)
    ```

    这将产生以下输出:

    ![Figure 4.7: Decision region plot for the kernel SVM
    ](image/B15916_04_07.jpg)

图 4.7:内核 SVM 的决策区域图

这个结果好多了。我们可以看到总体准确率接近 90%，而 1 级准确率现在是 67%，相比之下，线性 SVM 的准确率为 0%。我们能够捕捉数据中的非线性模式，并正确地对大多数离职员工进行分类。

注意

要访问该特定部分的源代码，请参考[https://packt.live/30FSdOZ](https://packt.live/30FSdOZ)。

你也可以在[https://packt.live/2ACdbUc](https://packt.live/2ACdbUc)在线运行这个例子。

在本章的剩余练习中，我们将继续使用 scikit-learn 训练模型，并看看它们与本节中的 SVM 相比如何。然而，在此之前，我们将简要讨论在前面的图表中决策界限是如何绘制的。

## 绘图 _ 决策 _ 区域功能

在这一节中，我们将探索 Jupyter 如何帮助我们更深入地研究外部 Python 库函数，以了解它们是如何工作的。特别是，我们将关注由`mlxtend`外部库提供的`plot_decision_regions`函数。

在本章的练习中，我们将使用`plot_decision_regions`函数来可视化我们的模型如何学习训练数据。有必要看一眼源代码(用 Python 编写)，以了解这些图是如何绘制的:

```
from mlxtend.plotting import plot_decision_regions
plot_decision_regions?
```

通过在 Jupyter 笔记本中运行前面的代码，可以看到如下的 docstring:

![Figure 4.8: The docstring for mlxtend.plotting.plot_decision_regions
](image/B15916_04_08.jpg)

图 4.8:mlx tend . plotting . plot _ decision _ regions 的文档字符串

通过在 Jupyter 中滚动到 docstring 打印输出的底部，可以找到定义该函数的文件的位置。这将看起来像下面这样:

![Figure 4.9: The file location and type, as seen at the bottom of the docstring in Jupyter
](image/B15916_04_09.jpg)

图 4.9:文件位置和类型，如 Jupyter 中 docstring 底部所示

通过记下前面截图中的文件路径，它的内容可以打印在笔记本中，如下所示(如果 bash 可用):

```
%%bash
cat /anaconda3/lib/python3.7/site-packages/mlxtend/plotting/decision_regions.py
```

`bash`命令给出了以下输出:

![Figure 4.10: Printing the head of the plot_decision_regions source code
](image/B15916_04_10.jpg)

图 4.10:打印绘图 _ 决策 _ 区域源代码的头部

这允许我们查看代码，但是由于没有颜色标记，所以不太容易查看。一个好的解决方案是从笔记本中复制它(或者复制文件本身)，并在您喜欢的文本编辑器中打开它。事实上，如果你愿意，这可以直接在 Jupyter 中完成，因为除了笔记本编辑器之外，Jupyter 还是一个纯文本编辑器。

完成这些之后，我们可以找到负责创建决策边界图的代码部分。内容如下:

![Figure 4.11: Interesting parts of the plot_decision_regions source code
](image/B15916_04_11.jpg)

图 4.11:plot _ decision _ regions 源代码中有趣的部分

通过研究这段代码，我们可以看到下面发生的事情，按照注释的数字进行解释:

*   `1`:在特征空间上生成的网格。
*   `2`:然后在分类模型`clf`上使用 scikit-learn 的`predict`方法在每个点进行预测。
*   `3`:最后，基于这些预测生成等高线图。这个等高线图就是我们在绘制决策边界图时看到的函数输出。

详细研究和理解这段代码并不重要。相反，这次讨论的目的是拉开帷幕，展示如何更深入地了解 Python 库正在执行的逻辑。

在下一个练习中，我们将回到我们之前开始的对员工保留问题的建模。

## 练习 4.02:用 scikit-learn 训练 k 近邻分类器

在人力资源分析数据集上训练我们的第一个模型时，我们看到了线性 SVM 和核 SVM 在对数据集中的两个选定特征进行建模时表现出的显著差异。这里，我们将继续使用 KNN 算法对这两个特征`satisfaction_level`和`last_evaluation`进行建模。在本书中，我们将第一次想象过度拟合是什么样子，并学习处理它的策略。执行以下步骤来完成本练习:

1.  Starting at the point in the notebook where the previous exercise ended, load the scikit-learn KNN classification model:

    ```
    from sklearn.neighbors import KNeighborsClassifier
    KNeighborsClassifier?
    ```

    前面的代码将打印文档字符串:

    ![Figure 4.12: The docstring for sklearn.neighbors.KNeighborsClassifier
    ](image/B15916_04_12.jpg)

    图 4.12:sk learn . neighbors . kneighborsclassifier 的文档字符串

    `n_neighbors`参数决定分类时要使用多少最近邻记录。如果`weights`参数设置为`uniform`，那么类别标签由多数投票决定。

    对于`weights`参数，您可以考虑的另一个选项是`distance`，其中更接近的样本在投票中具有更高的权重。与大多数模型参数一样，最佳选择取决于特定的数据集。

2.  Train the KNN classifier with `n_neighbors=3`, and then compute the accuracy and decision regions. Since you have built all of this logic into the `check_model_fit` function, do this as follows:

    ```
    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(X_train_std, y_train)
    check_model_fit(knn, X_test_std, y_test)
    ```

    下面是代码的输出:

    ![Figure 4.13: Training a KNN classifier with n_neighbors=3
    ](image/B15916_04_13.jpg)

    图 4.13:训练 n_neighbors=3 的 KNN 分类器

    在这里，您可以看到总体准确性的提高，现在得分超过 90%，特别是类 1 的显著提高，得分约为 96%。然而，决策区域图表明我们过度拟合了数据。这一点通过坚硬的、*起伏的*决策边界以及分散在整个特征空间中的第 1 类预测范围(橙色轮廓)的小口袋来证明。您可以通过增加用于分类的最近邻的数量来软化决策边界并减少过度拟合。

3.  To reduce overfitting, train a KNN model with `n_neighbors=25` by running the following code:

    ```
    knn = KNeighborsClassifier(n_neighbors=25) 
    knn.fit(X_train_std, y_train)
    check_model_fit(knn, X_test_std, y_test)
    ```

    您将看到以下输出:

    ![Figure 4.14: Training a KNN classifier with n_neighbors=25
    ](image/B15916_04_14.jpg)

图 4.14:训练 n_neighbors=25 的 KNN 分类器

如您所见，与`n_neighbors=3`模型的图相比，决策边界明显不那么起伏，并且第 1 类预测范围(橙色轮廓)的口袋也少得多。

查看指标，类别 1 的准确性稍低，但我们需要使用更全面的方法(如 k-fold 交叉验证)来确定该模型与我们之前训练的`n_neighbors=3`模型之间是否存在显著差异。

注意，增加`n_neighbors`对训练时间没有影响，因为模型只是记忆数据。然而，随着`n_neighbors`的增加，预测时间变得更长。

注意

要访问该特定部分的源代码，请参考[https://packt.live/30FSdOZ](https://packt.live/30FSdOZ)。

你也可以在[https://packt.live/2ACdbUc](https://packt.live/2ACdbUc)在线运行这个例子。

注意

当使用机器学习解决现实世界的问题时，算法运行足够快以达到目的是很重要的。例如，预测明天天气的脚本不可能需要一天以上的时间来运行。

在对大量数据进行模型定型时，内存限制也是应该考虑的一个因素。当这成为一个问题时，您可以研究可以在更小的数据块上迭代训练的模型。

完成 KNN 分类器的训练后，我们将在下一个练习中通过训练随机森林继续探索用 scikit-learn 建模。

## 练习 4.03:用 scikit-learn 训练随机森林分类器

我们将从上一个练习停止的地方开始这个练习。到目前为止，我们已经训练了支持向量机和 KNNs，并且看到了过拟合和欠拟合在决策边界和由此产生的分类精度方面是什么样子的。

在这里，您将学习如何训练随机森林，并将它们对我们的建模问题的结果与前面算法的结果进行比较。您还将学习如何在笔记本中呈现决策树可视化，这将为我们的随机森林模型的内部工作提供见解。执行以下步骤来完成本练习:

1.  Starting at the point in the notebook where the previous exercise ended load the scikit-learn Random Forest classification model and print the docstring by running the following command:

    ```
    from sklearn.ensemble import RandomForestClassifier
    RandomForestClassifier?
    ```

    您将看到以下输出:

    ![Figure 4.15: The docstring for sklearn.ensemble.RandomForestClassifier
    ](image/B15916_04_15.jpg)

    图 4.15:sk learn . ensemble . randomforestclassifier 的文档字符串

    这里，scikit-learn 认为`RandomForestClassifier`是一个集成算法，从前面的`import`语句中可以看出，它是从`ensemble`文件夹中加载的。回想一下，在本章前面，我们讨论了随机森林实际上是其他分类器的集合，称为决策树。当训练我们的随机森林时，我们实际上是在训练一组决策树。类似地，在进行预测时，我们实际上是将记录输入到随机森林的每个决策树中，并计算平均结果。

    我们将训练一个由 50 个决策树组成的随机森林分类模型，每个决策树的最大深度为 5。查看下面的代码，注意 Python 命令与我们之前训练的 SVM 和 KNN 模型非常相似，尽管每个算法在 scikit-learn 中的工作方式都非常不同。通过设置`max_depth=5`参数，正如我们之前输出的 docstring 所描述的，我们将*连续的*分割的最大数量限制为 5。当我们在本练习的后面查看决策树可视化时，这一点会变得很清楚。

2.  Train the Random Forest classifier we just described by running the following code:

    ```
    forest = RandomForestClassifier(n_estimators=50, \
                                    max_depth=5, random_state=1,)
    forest.fit(X_train, y_train)
    check_model_fit(forest, X_test, y_test)
    plt.xlim(-0.1, 1.2)
    plt.ylim(0.2, 1.2)
    plt.savefig('../figures/chapter-4-hr-analytics-forest.png', \
                bbox_inches='tight', \
                dpi=300,)
    ```

    您将看到总精度为`92.0%`。以下是输出结果:

    ![Figure 4.16: Training a Random Forest with a maximum depth of 5
    ](image/B15916_04_16.jpg)

    图 4.16:训练一个最大深度为 5 的随机森林

    在查看决策边界图时，请注意由决策树机器学习算法产生的独特的轴平行决策边界。

    我们可以访问任何用于构建随机森林的决策树。这些树存储在模型的`estimators_`属性中。让我们画一个决策树来感受一下发生了什么。

    注意

    用 Python 和 scikit-learn 生成决策树图形可视化需要`graphviz`依赖，这有时很难安装。如果你在这一步有困难，你应该打开`chapter-4-hr-analytics-tree-graph.png`图形文件，它可以在源代码的`figures`目录中找到。这个文件可以在以下链接找到:[https://packt.live/30IjVuh](https://packt.live/30IjVuh)。

3.  通过运行以下代码，从笔记本中的随机森林中为一个决策树构建一个图:

    ```
    from sklearn.tree import export_graphviz
    import graphviz
    dot_data = export_graphviz(forest.estimators_[0], \
                               out_file=None, \
                               feature_names=features, \
                               class_names=['no', 'yes'], \
                               filled=True, rounded=True, \
                               special_characters=True,)
    graph = graphviz.Source(dot_data)
    ```

4.  然后，通过运行以下命令将图形保存为 PNG 文件:

    ```
    graph.render(filename='../figures/chapter-4-hr-analytics-'\
                 'tree-graph', format='png',)
    ```

5.  最后，通过运行`graph`命令在您的笔记本中渲染图形。这将返回以下输出:![Figure 4.17: A decision tree from a Random Forest ensemble, where max_depth=5
    ](image/B15916_04_17.jpg)

图 4.17:来自随机森林集合的决策树，其中 max_depth=5

从上图中，我们可以看到，由于设置了`max_depth=5`，每条路径被限制为五个连续的节点。在每个分支，scikit-learn 的决策树算法决定了最大化训练数据中类的可分性的特征分割。考虑树的以下部分:

![Figure 4.18: A section of the decision tree where a split is made on the last_evaluation ≤ 0.445 condition
](image/B15916_04_18.jpg)

图 4.18:在 last_evaluation ≤ 0.445 条件下进行分割的决策树部分

在这里，我们可以看到来自顶层节点的 1，926 个训练样本在`last_evaluation ≤ 0.445`条件下被拆分，产生了一个纯的(左侧)子节点，其中有 208 个“否”样本，以及一个混合了(右侧)1，544 个“否”样本和 1，149 个“是”样本的子节点。回想一下，“否”对应的是还在公司工作的员工，“是”对应的是已经离职的员工。

橙色框表示大多数样本标记为“否”的节点，蓝色框表示大多数样本标记为“是”的节点。每个框的阴影(亮、暗等)表示置信度，这与该节点的纯度有关。

注意

要访问该特定部分的源代码，请参考[https://packt.live/30FSdOZ](https://packt.live/30FSdOZ)。

你也可以在[https://packt.live/2ACdbUc](https://packt.live/2ACdbUc)在线运行这个例子。

我们对随机森林的练习到此结束，并带我们结束对人力资源分析数据集的初始建模研究。在本练习中，我们学习了如何训练随机森林，并探索了它们的决策树成分是如何组成的。

尽管我们在本节中训练了各种模型，但我们只完成了一个端到端的示例，其中数据被加载、分成训练集和测试集、用于训练模型，然后进行评分。之后，我们依靠以前的工作来简化建模过程。

在下一节中，您将有机会完成完整的建模活动，从加载预处理数据集到评分和比较最终结果。

## 活动 4.01:用 scikit-learn 训练和可视化 SVM 模型

在本活动中，您将使用人力资源分析数据集中的两个新功能构建模型，以预测我们之前描述的相同目标变量(员工是否会离职)。

您将从数据集中选择这些要素，将它们分成训练集和测试集，对它们进行缩放，对一些 SVM 进行训练和评分，并可视化生成的决策边界。

在完成这个活动的同时复制和粘贴代码可能很有诱惑力，但是通过自己打出解决方案，你会学得更有效率。即使在参考笔记本的早期部分时，也要尽量避免复制和粘贴。执行以下步骤来完成本练习:

注意

本练习的详细步骤，以及解决方案和附加注释，请参见第 285 页。

1.  启动新的 Jupyter 笔记本，加载您使用的库，并为笔记本设置打印环境。
2.  将预处理后的数据集从 https://packt.live/2YE90iC(`hr_data_processed.csv`)加载到笔记本中，并将其赋给变量`df`。
3.  在本活动中，您将使用`number_project`和`average_montly_hours`功能。在这些列上过滤`df`，并使用`pd.DataFrane.describe`函数显示它们的汇总指标。比较每个的平均值、最小值和最大值。
4.  进行训练测试分割，就像在*练习 4.01* 、*用 Scikit-Learn* 训练双特征分类模型中所做的那样，用这两个新特征代替旧特征。
5.  使用 scikit-learn 的`MinMaxScaler`缩放训练数据，并确定此`scaler`与我们之前在笔记本中使用的`scaler`有何不同。
6.  使用`rbf`内核训练一个 SVM。设置`C=1`和`gamma='scale'`参数。
7.  确定该模型对测试集的分类准确性。
8.  为测试集确定该模型的分类准确性。
9.  使用`plot_decision_regions`功能可视化该模型的决策区域。
10.  在实例化 SVM 模型时，您传入了一个参数`C=1`。这是模型的一个属性，可以通过调整来优化模型的准确性。通过增加`C`，SVM 将尝试更紧密地拟合训练数据。用`C=50`训练一个新的 SVM，并想象它的决策区域。将这个与你为`C=1` SVM 绘制的图表进行比较。

# 总结

在本章中，我们学习了 SVM、KNN 和随机森林分类算法，并将它们应用于我们预处理的人力资源分析数据集，以构建预测模型。给定一组员工指标，这些模型被训练来预测员工是否会离开公司。

为了保持事情的简单和专注于算法，我们建立的模型只依赖于两个特征，即满意度和最后的评估值。这个二维特征空间还允许我们可视化决策边界，并识别过度拟合看起来像什么。

下一章，我们将介绍机器学习中的两个重要课题:k-fold 交叉验证和验证曲线。这样，我们将讨论更高级的主题，如参数调整和模型选择。然后，为了优化员工保留问题的最终模型，我们将使用降维技术 PCA 探索特征提取，并在数据集中可用的所有特征上训练模型。