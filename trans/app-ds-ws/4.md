

# 三、为预测建模准备数据

概观

在这一章中，你将学习规划一个机器学习策略，并评估数据是否适合建模。您还将对中的数据执行操作，以准备数据，使其可用于定型模型。这将包括填充缺失记录、将分类特征转换为数字特征，以及将数据集分成训练组和测试组。本章结束时，您将能够处理数据集并为预测分析做好准备。

# 简介

在上一章中已经完成了一个相当复杂的分析，现在您应该可以轻松地使用 Jupyter 笔记本来处理数据了。除了数据探索和可视化之外，我们的分析还包括几个相对简单的建模问题，其中我们训练了线性回归模型。这些最佳拟合的*线非常容易创建，因为只涉及两个维度，数据非常清晰。*

正如我们将在后面的章节中看到的，训练更高级的模型(如决策树)也同样容易，因为像`scikit-learn`这样的开源软件非常简单。然而，准备数据所涉及的工作可能要困难得多，这取决于相关数据集的细节。

训练数据的质量对于创建能够很好地推广到未来样本的模型非常重要。例如，训练数据集中的错误会导致模型学习不反映数据背后真实过程的模式，或者充当降低其准确性的噪声。

由于数据准备是机器学习过程中如此重要的一部分，本章的练习将集中在我们可以用于 Python 数据处理的学习方法上。我们将通过在 Jupyter 笔记本上运行各种示例和活动来继续我们的动手学习方法。

然而，我们不会直接进入数据清理的细节，我们将介绍机器学习本身，并总结如何处理一般的数据科学问题。具体来说，我们将讨论如何观察业务问题，识别可能有用的数据，并为训练预测模型制定计划。

# 机器学习过程

**机器学习** ( **ML** )是数据科学的核心。它是一组庞大的算法的总称，这些算法在数据中寻找模式并对其建模。这些算法可以分为不同的类别，如监督、非监督和强化学习。

在**监督的**问题中，我们可以访问标记记录的历史视图，并拟合模型来预测它们——例如，已经标记了测试结果的血液测试数据。在**无监督**问题中，没有这样的数据可用，标签可能需要使用聚类技术来创建。在后面的章节中，我们将对它们进行更详细的分解，并使用每个例子。

**强化学习**关注的是通过迭代过程(如模拟)最大化奖励函数。与其他类型的学习算法类似，强化学习可以应用于广泛的问题，例如通过调整移动算法来教会机器人如何在砾石路上行走，以便在摔倒之前最大化它可以获得的距离。这将大致如下进行:

*   为智能体(机器人)定义一组与环境(碎石路)交互的规则。
*   运行模拟并记录评分标准(移动的距离)。
*   根据评分标准调整规则，并重复此过程，直到无法对评分标准进行进一步改进。

这种类型的算法在概念上是简单的，并且可以是非常强大的；然而，在有多个因素需要调整的大型环境中，这也是计算密集型的。一个著名的例子是 AlphaStar，这是一个使用强化学习成为电脑游戏《星际争霸》专家的程序。据估计，AlphaStar 拥有相当于人类 200 年游戏时间的知识。

注意

强化学习超出了本书的范围，不再进一步讨论。

机器学习中的术语**学习**代表系统从可用数据中自动选择模型参数和设计的能力。这个过程通常被称为**将模型拟合**到训练数据。由于我们关注的是应用数据科学，您将会看到使用 Python 生态系统中的现代工具，这个过程是多么简单。

相比之下，支撑每种建模方法的数学和统计概念可能非常复杂。除了在项目过程中使用的任何东西之外，全面的数据科学家应该对基础模型(如线性和逻辑回归)的这些算法细节有坚实的理解。

# 解决数据科学问题

在开始分析和建模阶段之前，确保您的数据科学项目有一个结构良好的计划非常重要。我们将概述在制定这个计划时要记住的一些因素，然后在下一节中讨论一些关于准备建模数据的技术细节。

由于本书以 Jupyter 笔记本为中心，我们将首先强调它们对于数据科学项目的规划阶段有多么有用。它们为记录您的分析和建模计划提供了一个非常方便的媒介，例如，通过编写关于数据的粗略笔记或我们有兴趣训练的模型列表。把这些笔记放在你进行分析的地方，可以帮助其他人在看到你的工作时理解你在做什么，或者在你离开一段时间后回头看时为你提供背景。

数据科学的很大一部分涉及使用机器学习来建立预测模型。当制定预测建模的计划时，您应该从考虑利益相关者的需求开始。一个完美的模型如果不能解决一个相关的问题，它将是无用的。围绕业务需求规划战略可以确保一个成功的模型能够带来可操作的见解。

虽然原则上解决许多业务问题是可能的，但是交付解决方案的能力总是依赖于必要数据的可用性。因此，在可用数据源的上下文中考虑业务需求非常重要。当数据充足时，这不会有什么影响，但是随着可用数据量的减少，可以解决的问题的范围也会变小。

注意

当在与项目目标无关或不可能产生有用见解的部分分析上花费太多时间时，要小心。在处理一小部分数据或尝试实现特定功能时，很容易迷失方向。

你可能会花上几个小时试图让情节看起来恰到好处，但最终，除了你没有人会看到它们。做这种事情当然不是没有好处；例如，将来你可以更快地制作出好看的图表。然而，我们不应该忘记问自己，对于当前的项目，我们正在做的工作是否真的值得我们花费时间。

这些想法可以形成处理数据科学问题的一般流程，例如预测建模，如下所示:

*   **通过与关键利益相关者交谈来确定**业务需求。为研究方向产生想法。找出一个问题，其解决方案将导致可操作的业务决策。
*   **查看可用数据，并尝试将其与业务需求联系起来。确保您了解可用的数据字段及其适用的时间范围。尝试选择一个要建模的目标指标(如果适用)和一组能够洞察业务需求的功能。**

这些步骤应该重复进行，直到一个现实的计划成形。此时，您将已经确定了您的模型输入以及预期的输出。

缺少可用数据可能会导致困难，或者阻止您确定合适的建模方法。在这些情况下，请记住找到适合您的特定问题的新数据源的可能性，例如，通过调查生成样本或购买第三方数据集。

在确定了可能通过数据建模解决的一系列问题以及合适的数据源之后，我们需要为每个模型设计一个框架。在下一节中，我们将研究一系列关于数据的问题，以及每个答案对机器学习训练模型的影响。

# 从建模的角度理解数据

当试图在机器学习的背景下理解一个商业问题时，我们需要确定该问题是适合监督学习还是非监督学习。问题可以通过建模一个目标变量来解决吗？如果是，那么这个变量在数据集中可用吗？是数字还是范畴？这些问题的答案将允许我们确定哪些建模算法将与我们的问题相关。下图概述了这一过程:

![Figure 3.1: A flowchart for identifying types of modeling problems
](image/B15916_03_01.jpg)

图 3.1:识别建模问题类型的流程图

前面的流程图描述了我们可以遵循的各种路径，以便对数据集进行分类以进行建模。在第一个分支，我们感兴趣的是识别目标变量是否存在。例如，在天气预报模型中，它可以是记录历史降雨量的列。或者，目标变量可能是一个标记某一天是否下雨的列。像这样的变量的存在将决定它是一个有监督的还是无监督的学习问题。

**监督学习**可以是分类问题，也可以是回归问题。在回归中，变量是数字，例如，以厘米为单位的降雨量。可以通过回归建模的另一种情况是范围从 1-5 颗星的电影评级。

在分类中，变量是分类的，我们预测类别标签。最简单的分类问题类型是二进制的——例如，是否经过训练(`yes` / `no`)。多类分类的一个例子是更一般地预测天气类型:晴天、多云、下雨或暴风雨。

**无监督学习**问题可能更难可靠地解决，与有监督学习相比，解决问题的方法更少。一种可以应用于无监督问题的建模技术是聚类分析，其中基于特征空间中度量之间的距离自动识别记录组。这种模型可以将未来的记录分配给最近的聚类。

无监督学习的一个很好的例子是对男性或女性躯干测量值(高度、宽度、臂长等)进行聚类，以确定服装系列(S、M、L、XL)中每个尺寸的良好测量值。训练数据应该包括从人口中随机抽样的人的测量值，这样我们就有了各种体型的数据。

当创建模型时，应该将它限制为四个聚类，以便可以选择 S、M、L 和 XL 尺寸的服装尺寸作为多维建模空间中每个聚类的中心点。例如，小尺寸可能最终是(x，y，x) = (65 cm，45 cm，43 cm)，对应于三维聚类的中心，其中 x =“躯干长度”，y =“躯干宽度”，z =“手臂长度”。

一旦您了解了要建模的问题的类型，还有其他因素需要考虑，如下所示:

*   数据的大小，用宽度(列数)和高度(行数)来表示。
*   Algorithm selection, depending on the details of the features. Some features apply themselves to certain algorithms over others. For example, sparse datasets (lots of zeros) are better modeled with **Lasso regression**, rather than standard ordinary least squares regression.

    注意

    Lasso 算法通过向取决于输入样本值的成本函数添加惩罚来正则化模型。这具有将权重参数降为零的效果，因此对模型隐藏了某些特征，即维数减少。

*   通常，与相同底层流程的较小数据集相比，较大的数据集在准确性方面表现更好。
*   对大量数据进行模型训练可能非常耗时。有时，可以通过使用降维技术来减少要素的数量，从而节省大型数据集的时间。
*   对于给定的问题，应该训练和比较一组各种不同的模型。使用集合平均技术组合高精度模型可以提高整体精度，从而使模型在处理未知数据时表现更好。

这些考虑因素假设建模数据存在于单个表中。然而，多个互连数据集的存在将需要额外的高级模型设计选择。

例如，考虑以下情况，我们有两个数据源:

*   包含 AAPL 股票每日收盘价的数据馈送(例如，表格)
*   包含每月 iPhone 销售数据的数据馈送(例如，表格)

这里的困难是处理每个数据集的不同时间尺度。

一种方法是合并它们，将每月销售数据添加到每日时间刻度表中的每个样本中，或者将每日数据按月分组。

另一个想法是建立两个模型，每个数据集一个，并在最终的集合预测模型中使用每个模型的结果的组合。

这些早期的建模决策应该在准备阶段仔细考虑，以避免走上错误的道路。然而，在规划阶段，当最佳选择不明显时，往往会出现设计选择。在这种情况下(在考虑了准确性、计算复杂性和可解释性等权衡因素后)，应该通过训练多个模型并使用相关评分指标对它们进行比较来确定最佳选项。

既然我们已经讨论了数据建模的高级概念，我们将把注意力集中在为预测建模准备数据上。

## 准备建模数据

为了建立在看不见的数据上表现良好的模型(也就是说，在生产中表现良好)，我们必须在仔细处理的数据上训练它们。就像俗话说的那样“*你吃什么就是什么*”，模型的表现直接反映了它被训练的数据。当然，它的性能也将取决于您避免模型过拟合或欠拟合的能力，这将在*第 5 章*、*模型验证和优化*和*第 6 章*、*用 Jupyter 笔记本进行 Web 抓取*中讨论。

机器学习的数据预处理通常需要最长时间的方面是清理杂乱的数据。一些估计表明，数据科学家花费大约三分之二的工作时间来清理和组织数据集。这包括如下任务:

*   合并公共字段上的数据集，将所有数据放入单个表中
*   提高数据质量的特征工程
*   删除或填充不正确或缺失的值，例如，用该字段现有值的平均值或中值替换缺失的数据
*   删除重复记录
*   通过标准化或规范化所需数据、应用特征变换和应用训练测试拆分来构建训练数据集

现在，让我们学习如何使用 Jupyter 笔记本和熊猫完成这些任务的基本知识。

在下面的练习中，我们将在本书中第一次遇到缺失(NaN)值。所以，让我们讨论一下这些在 Python 中是如何工作的。

你可以通过做`a = float('nan')`来定义安南变量。这是 pandas 用来表示缺失值的数据结构。

与 NaNs 一起工作的一个棘手问题是测试平等性。你不能简单地使用标准的比较方法，因为它们会有意想不到的表现。相反，最好与库中的高级函数进行比较，比如 pandas 或 NumPy。下面的代码说明了这一点:

![Figure 3.2: Working with missing values in Python
](image/B15916_03_02.jpg)

图 3.2:在 Python 中处理缺失值

这些结果中的一些可能看起来违反直觉。然而，这种行为背后是有逻辑的，为了更深入地理解标准比较返回`False`的根本原因，看看这个极好的 StackOverflow 答案:[https://stackoverflow.com/a/1573715/3511819](https://stackoverflow.com/a/1573715/3511819)。

## 练习 3.01:熊猫机器学习的数据清理

在本章的剩余部分，我们将通过一个非常简单的演示数据集来探索如何用 pandas 库转换数据。

在*活动 3.01* 、*准备训练员工保留预测模型*中，我们将在实践中应用本练习*中的技术，并清理一个大得多的数据集。*

但是现在，我们将关注熊猫图书馆的一些基础知识。我们将学习如何合并表、处理重复记录以及处理缺失值。执行以下步骤来完成本练习:

1.  If you haven't done so already, start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

    然后，按照终端中的提示，通过复制并粘贴 URL，打开您在 web 浏览器中选择的平台:

2.  加载所需的库并为笔记本设置绘图设置，如下:

    ```
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    ```

3.  现在，我们将从展示熊猫和 scikit-learn 的一些基本工具开始。加载水印魔法扩展，如下所示:

    ```
    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

4.  Generate two sample datasets and merge them. Display the docstring for the `merge` function, as follows:

    ```
    pd.merge?
    ```

    输出如下所示:

    ![Figure 3.3: The docstring for pd.merge
    ](image/B15916_03_03.jpg)

    图 3.3:PD . merge 的文档字符串

    如您所见，该函数接受一个`left`和`right`数据帧进行合并。您可以指定一个或多个要分组的列，以及它们的分组方式，即使用左、右、外部或内部值集。通过实验这个函数来开始这个练习，这样我们可以学习如何使用它。

5.  Now, define our example datasets:

    ```
    df_1 = pd.DataFrame({'product': ['red shirt', 'red shirt', \
                                     'red shirt', 'white dress',], \
                         'price': [49.33, 49.33, 32.49, 199.99,],})
    df_2 = pd.DataFrame({'product': ['red shirt', 'blue pants', \
                                     'white tuxedo', 'white dress',], \
                         'in_stock': [True, True, False, False,],})
    ```

    在这里，您将从头开始构建两个简单的数据框架。可以看到，它们包含一个公共键(product 列)，这两个表之间有一些共享条目。

6.  Display the first DataFrame as follows:

    ```
    df_1
    ```

    输出如下所示:

    ![Figure 3.4: The first product's DataFrame
    ](image/B15916_03_04.jpg)

    图 3.4:第一个产品的数据框架

7.  Display the second DataFrame as follows:

    ```
    df_2
    ```

    输出如下所示:

    ![Figure 3.5: The second product's DataFrame
    ](image/B15916_03_05.jpg)

    图 3.5:第二个产品的数据框架

8.  Now, use the `merge` function from pandas to perform an inner merge and display the result, as follows:

    ```
    df = pd.merge(left=df_1, right=df_2, \
                  on='product', how='inner')
    df
    ```

    下面的截图显示了内部`merge`函数的输出:

    ![Figure 3.6: Inner merge on products
    ](image/B15916_03_06.jpg)

    图 3.6:产品的内部合并

    请注意如何只包括共享项目(即红衬衫和白裙子)。要包含两个表中的所有条目，您可以改为执行外部合并，这将在下一步中执行。

9.  Perform an outer merge and display the result, as follows:

    ```
    df = pd.merge(left=df_1, right=df_2, \
                  on='product', how='outer')
    df
    ```

    下面的截图显示了外部`merge`函数的输出:

    ![Figure 3.7: Outer merge on products
    ](image/B15916_03_07.jpg)

    图 3.7:产品的外部合并

    这将返回每个表中的所有数据，其中缺少的值都标有`NaN`。

10.  You may have noticed that our most recently merged table has duplicated data in the first few rows. To handle this, return a version of the DataFrame with no duplicate rows by using the following code:

    ```
    df.drop_duplicates()
    ```

    输出如下所示:

    ![Figure 3.8: Merged products' table after dropping duplicates
    ](image/B15916_03_08.jpg)

    图 3.8:删除重复项后的合并产品表

    这是删除重复行的最简单的方法。要将这些更改应用到`df`，您可以设置`inplace=True`参数或运行`df = df.drop_duplicated()`。

    现在考虑另一种使用屏蔽来选择或删除重复行的方法。

11.  Print the `True`/`False` series in order to mask duplicate rows, as follows:

    ```
    df.duplicated()
    ```

    输出如下所示:

    ```
    0    False
    1     True
    2    False
    3    False
    4    False
    5    False
    dtype: bool
    ```

12.  Sum the result to determine how many rows have been duplicated. Use the following code to do so:

    ```
    df.duplicated().sum()
    ```

    这将返回`1`。

13.  Display this duplicated row as follows:

    ```
    df[df.duplicated()]
    ```

    输出如下所示:

    ![Figure 3.9: Displaying a duplicated row
    ](image/B15916_03_09.jpg)

    图 3.9:显示重复的行

14.  In the previous step, you used a mask to filter the DataFrame on duplicated rows. By using the tilde symbol, `~`, you can use the same technique to show the other set of rows from the DataFrame. Use the following code to do so:

    ```
    df[~(df.duplicated())]
    ```

    显示在`duplicated`功能之前使用`~`的输出如下:

    ![Figure 3.10: Displaying non-duplicated rows
    ](image/B15916_03_10.jpg)

    图 3.10:显示非重复行

15.  Using this masking technique allows for more granular control over how duplicates can be treated. For example, eliminate duplicate products using the following code:

    ```
    mask = ~(df['product'].duplicated())
    df[mask]
    ```

    用于删除重复项的上述命令的输出如下:

    ![Figure 3.11: Merged products' table after dropping duplicate products
    ](image/B15916_03_11.jpg)

    图 3.11:删除重复产品后的合并产品表

    通过运行此行，您可以执行以下操作:

    为产品行创建一个掩码(一个`True` / `False`系列)，重复的用`True`标记。

    用波浪符号(`~`)取那个蒙版的反面，这样重复的用`False`标记，其他的都是`True`。

    使用该蒙版过滤出`df`的`False`行，对应于复制的产品。

    正如所料，我们现在可以看到，只有第一个红色衬衫行保留，因为重复的产品行已被删除。

16.  At this point, you have tested various methods for dropping duplicates, but have not actually changed `df`. In order to proceed with this exercise, replace `df` with a deduplicated version of itself. This can be done as follows:

    ```
    df.drop_duplicates(inplace=True)
    df
    ```

    删除重复项后，以下输出显示`df`:

    ![Figure 3.12: Merged products' table after dropping duplicate rows
    ](image/B15916_03_12.jpg)

    图 3.12:删除重复行后的合并产品表

    可以看到，重复行现在已经从`df`永久删除。

    您将了解的预处理的下一个方面是处理丢失的数据。这很重要，因为许多模型不能在不完整的记录上训练。

17.  One option is to drop the rows, which might be a good idea if your NaN records are missing the majority of their values. Use the following code to do so:

    ```
    df.dropna()
    ```

    与您之前运行的删除重复项的大多数命令类似，调用`df.dropna()`不会改变数据帧。相反，它返回一个新的 DataFrame，该 data frame 在笔记本中的单元格下方呈现给您。不出所料,“blue pants”和“white tuxedo”产品缺少数据的行不会出现在这里:

    ![Figure 3.13: Merged products' table after dropping rows with missing values
    ](image/B15916_03_13.jpg)

    图 3.13:删除缺少值的行后的合并产品表

18.  If most of the values are missing for a feature, it may be best to drop that column entirely. This can be done with the same method as before, by setting the axes argument to `1`, in order to specify columns instead of rows. This can be done as follows:

    ```
    df.dropna(axis=1)
    ```

    输出如下所示:

    ![Figure 3.14: Merged products' table after dropping columns with missing values
    ](image/B15916_03_14.jpg)

    图 3.14:删除缺少值的列后的合并产品表

    简单地删除`NaN`值，就像我们对上两个例子所做的那样，通常不是最好的选择。这是因为这会导致我们丢失可能有价值的训练数据。

    我们可以考虑填充缺失的条目，而不是删除它们。Pandas 提供了一种以多种不同方式填充`NaN`条目的方法，我们现在将举例说明其中的一些。

19.  Print the docstring for the pandas `NaN-fill` method, as follows:

    ```
    df.fillna?
    ```

    输出如下所示:

    ![Figure 3.15: The docstring for pd.DataFrame.fillna
    ](image/B15916_03_15.jpg)

    图 3.15:PD 的文档字符串。数据框架

    注意`value`参数的选项；例如，这可以是基于索引的单个值或字典/序列类型映射。或者，您可以将值保留为`None`，并传递一个`fill`方法来代替。稍后我们将看到每一个的例子。

20.  Print the DataFrame as follows:

    ```
    df
    ```

    输出如下所示:

    ![Figure 3.16: The merged products' table
    ](image/B15916_03_16.jpg)

    图 3.16:合并产品表

21.  One strategy for filling missing data is to use the average value for that field. Do this for the price column by running the following code:

    ```
    fill_value = df.price.mean()
    df.fillna(value=fill_value)
    ```

    输出如下所示:

    ![Figure 3.17: Merged products' table after filling in the missing prices with the mean
    ](image/B15916_03_17.jpg)

    图 3.17:用平均值填充缺失价格后的合并产品表

22.  In some cases, you may want to fill the missing values using the previous non-null record. Note that this strategy will depend on the sorting order of the DataFrame.

    对`price`栏进行如下操作:

    ```
    df.fillna(method='pad')
    ```

    输出如下所示:

    ![Figure 3.18: Merged products' table after filling in the missing prices with the "pad" method
    ](image/B15916_03_18.jpg)

    图 3.18:使用“填充”方法填充缺失价格后的合并产品表

    请注意`white dress`价格是如何被用来填充其下缺失的值的。

23.  Now that you have looked at a couple of the ways in which pandas enables you to fill missing data, go with the mean value method. Permanently replace the missing data in `df`, as follows:

    ```
    df = df.fillna(value=df.price.mean())
    ```

    我们关于为机器学习准备数据的第一个练习到此结束。我们已经学习了如何合并表，识别诸如重复行和丢失值之类的问题，然后使用 Pandas 解决这些问题。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2YzXB3j](https://packt.live/2YzXB3j)。

    你也可以在[https://packt.live/2Y3vvi4](https://packt.live/2Y3vvi4)在线运行这个例子。

## 练习 3.02:为熊猫的机器学习准备数据

在本练习中，我们将继续使用前面的简单表格，并完成准备工作，以便它可以用于训练机器学习算法。不过，我们实际上不会尝试在如此小的数据集上训练任何模型！当我们在*第 4 章*、*中开始训练模型*时，我们将使用一个更加真实的表格，大约有 15k 行。

我们将通过对分类数据的分类标签进行编码来开始这个过程，然后通过对数据执行训练/测试分割来完成这个练习。执行以下步骤来完成本练习:

注意

本练习建立在*练习 3.01* 、*用熊猫*进行机器学习的数据清理之上，应该在同一个笔记本中执行。

1.  Before learning about label encoding, create a new field that represents the average product ratings by running the following code:

    ```
    ratings = ['low', 'medium', 'high']
    np.random.seed(2)
    df['rating'] = np.random.choice(ratings, len(df))
    df
    ```

    输出如下所示:

    ![Figure 3.19: The table after adding a new column for rating
    ](image/B15916_03_19.jpg)

    图 3.19:添加评级新列后的表格

2.  Now, encode all non-numeric data types present in the table. The simplest column to handle is the Boolean list called `in_stock`. This can easily be mapped to binary numeric values (that is, `0` and `1`). Use the following code to do so:

    ```
    df.in_stock = df.in_stock.map({False: 0, True: 1})
    df
    ```

    输出如下所示:

    ![Figure 3.20: The table after converting the in_stock column into numeric form
    ](image/B15916_03_20.jpg)

    图 3.20:将 in_stock 列转换成数字形式后的表格

3.  Another option for encoding feature labels is to use scikit-learn's `LabelEncoder` library. This gives you a high-level abstraction to perform encoding using scikit-learn's API. Test this using the following code:

    ```
    from sklearn.preprocessing import LabelEncoder
    rating_encoder = LabelEncoder()
    df.rating = rating_encoder.fit_transform(df.rating)
    df
    ```

    输出如下所示:

    ![Figure 3.21: The table after converting rating into a numeric form
    ](image/B15916_03_21.jpg)

    图 3.21:将评分转换为数字形式后的表格

    这可能会让人想起在构建多项式模型时，您在前一章中所做的预处理。这里，实例化一个标签编码器，然后“训练”它并使用`fit_transform`方法“转换”您的数据:

    ```
    rating_encoder.inverse_transform(df.rating)
    ```

    这将显示以下输出:

    ```
    array(['low', 'medium', 'low', 'high', 'high'], dtype=object)
    ```

4.  One benefit of using scikit-learn's `LabelEncoder` is that you can convert the feature values back into their original form using the `inverse_transform` function. Do this by running the following code. Study the output to convince yourself that they are the proper labels:

    ```
    df.rating = rating_encoder.inverse_transform(df.rating)
    df
    ```

    输出如下所示:

    ![Figure 3.22: The table after performing an inverse transform on the rating column
    ](image/B15916_03_22.jpg)

    图 3.22:对评级列执行逆变换后的表格

    您可能会注意到这里的一个问题。我们正在研究一个所谓的**序数特征**，其中标签有一个固有的顺序。在这种情况下，我们应该期望用`0`对`low`评分进行编码，用`2`对`high`评分进行编码。然而，这并不是我们能看到的结果。为了实现正确的顺序标签编码，我们应该自己建立一个映射字典。

5.  Encode the ordinal labels properly, as follows:

    ```
    ordinal_map = {rating: index \
                   for index, rating in enumerate(['low', \
                                                   'medium', \
                                                   'high'])}
    print(ordinal_map)
    df.rating = df.rating.map(ordinal_map)
    df
    ```

    输出如下所示:

    ![Figure 3.23: The table after mapping the ordinal values for rating
    ](image/B15916_03_23.jpg)

    图 3.23:映射等级顺序值后的表格

    在这里，我们创建了`ordinal_map`映射字典。这是使用字典理解和枚举来完成的，但是看看结果，我们可以看到它可以很容易地被手动定义。当然，这只是我们示例的情况，因为在`rating`字段中很少有唯一值。我们可能期望唯一值的数量非常大，在这种情况下，字典理解方法会有用得多。

    正如我们之前对`in_stock`列所做的那样，我们将字典映射应用于该特性。查看结果，我们可以看到 rating 比以前更有意义，其中`low`用`0`、`medium`用`1`、`high`用`2`标记。

    既然我们已经讨论了序数特征，让我们接触另一种叫做**名义特征**的类型。这些是没有内在顺序的领域，在我们的例子中，我们可以看到一个产品是一个完美的例子。

    大多数 scikit-learn 模型可以在这样的数据上训练，其中我们有字符串而不是整数编码的标签。在这种情况下，必要的转换在引擎盖下完成。然而，对于 scikit-learn 或其他机器学习和深度学习库中的所有模型，情况可能并非如此。因此，在预处理阶段自己编码是一个很好的实践。

6.  A commonly used technique to convert class labels from strings into numerical values is called one-hot encoding. This splits the distinct classes into separate features, and can be accomplished elegantly with `pd.get_dummies()`. Do this as follows:

    ```
    df = pd.get_dummies(df)
    df
    ```

    输出如下所示:

    ![Figure 3.24: The result of calling get_dummies on the table
    ](image/B15916_03_24.jpg)

    图 3.24:在表上调用 get_dummies 的结果

    在这里，我们可以看到 one-hot 编码的结果:`product`列被分成四部分，每一部分代表一个唯一值。在每一列中，我们可以找到一个`1`或`0`，表示该行是否包含特定的值或产品。

    注意

    通过像我们在这里所做的那样对变量进行一次性编码，其中四个唯一值被分成四个新列，我们将自己置于将**多重共线性**引入数据集的风险之中。当数据集中的一个或多个变量可以用其他变量的线性组合来描述时，就会出现这种情况。像这样的线性依赖会导致建模时出现问题，所以最好尽量避免。

    修复由一键编码引入的多重共线性问题的一种方法是删除其中一个结果列。这可以用`pd.get_dummies(df, drop_first=True)`来完成。也可以考虑处理多重共线性的替代方法；然而，这个主题超出了本书的范围，我们在这里不担心它的后果。

    继续前进并忽略任何数据缩放(通常应该这样做)，最后一步是将数据分成训练集和测试集，以便我们可以使用它们进行机器学习。这可以使用 scikit-learn 的`train_test_split`来完成。假设我们将尝试预测一个商品是否有库存，给定其他特征值。

7.  Split the data into training and test sets by running the cell containing the following code:

    ```
    features = ['price', 'rating','product_blue pants', \
                'product_red shirt','product_white dress', \
                'product_white tuxedo',]
    X = df[features].values
    target = 'in_stock'
    y = df[target].values
    from sklearn.model_selection import train_test_split
    X_train, X_test, \
    y_train, y_test = (train_test_split(X, y, \
                                        test_size=0.3))
    ```

    在这里，您选择数据的子集并将它们输入到`train_test_split`函数中。该函数有四个输出，这些输出被分解成特征(`X`)和目标(`y`)的训练和测试分割。

8.  Print the shapes of each result that was unpacked from the `train_test_split` function by running the following code:

    ```
    print('Data Shapes')
    print('--------------')
    print('X_train', X_train.shape)
    print('X_test ', X_test.shape)
    print('y_train', y_train.shape)
    print('y_test ', y_test.shape)
    ```

    输出如下所示:

    ```
    Data Shapes
    --------------
    X_train (3, 6)
    X_test  (2, 6)
    y_train (3,)
    y_test  (2,)
    ```

    观察输出数据的形状，其中测试集应该有大约 30%的记录，定型集应该有大约 70%。因为我们这里只有五个记录，scikit-learn 将数据分成两个测试记录和三个训练记录。然而，当准备适当的数据集时，分割将非常接近指定的阈值。

    注意

    当我们在前面的代码中调用 values 属性时，我们将 pandas 系列(即 DataFrame 列)转换为 NumPy 数组。这是一个很好的实践，因为它从 series 对象中去除了不必要的信息，比如索引和名称。

这结束了关于清理数据的训练练习，以便可以在机器学习应用中使用它。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2YzXB3j](https://packt.live/2YzXB3j)。

你也可以在[https://packt.live/2Y3vvi4](https://packt.live/2Y3vvi4)在线运行这个例子。

您注意到我们的 Jupyter 笔记本在测试各种数据转换方法时有多有效了吗？一定要保存像这样的实验室风格的笔记本，以便您可以在以后回顾您的工作，并了解所做的数据清理决策。

该笔记本也可用于重新处理数据的更新版本，如果我们希望对处理过程进行任何更改，可以通过更改适当的单元在笔记本中轻松进行测试。实现这一点的最佳方法可能是将笔记本复制到一个新文件中，这样我们就可以随时保留原始分析的副本以供参考。

在下一节中，我们将在准备大型数据集时将本练习中的概念应用于该数据集，以便它可用于训练预测模型。

# 人力资源分析数据集简介

了解了基本的数据清理概念，并看到了它们在 pandas 和 scikit-learn 中的实现，我们将把我们所学到的知识应用到具有真实环境的多样化数据集上。在接下来的章节中，我们将使用各种机器学习技术对这个数据集进行建模，所以现在让我们花一些时间来熟悉它。让我们设想以下情况:

假设你受雇为一家公司做自由职业者的工作，这家公司想了解员工离职的原因。他们汇编了一组他们认为在这方面会有帮助的数据。它包括员工满意度、评估、工作时间、部门和工资的详细信息。

该公司通过向你发送一个名为`hr_data.csv`的文件来与你分享他们的数据，并询问你认为可以做些什么来帮助阻止员工离职。

我们的目标是将我们迄今为止讨论过的概念应用于现实生活中的问题。特别是，我们努力做到以下几点:

*   在给定可用数据的情况下，确定使用数据建模来提供有影响力的业务洞察力的计划。
*   准备用于机器学习模型的数据集。

在确定了相关的业务问题是员工流动后，我们通常会希望围绕为什么会发生这种情况以及哪些数据有助于制定解决方案进行全面的讨论。

出于本例的目的，让我们从查看公司提供的数据集开始。我们需要了解数据集的基本属性，比如样本数量和列描述。该数据集的基本属性如下:

*   行:15，000 个样本(行)，代表以前或现在的雇员
*   Columns: The columns and data types are as follows:

    `left`(员工是否已经离开公司【布尔变量】)

    `satisfaction_level`、`last_evaluation`、`average_montly_hours`、`time_spend_company`、`number_project`(员工指标【浮动变量】)

    `work_accident`、`promotion_last_5years`、`is_smoker`(员工指标【布尔变量】)

    `department`、`salary`(员工指标【字符串变量】)

查看这些信息后，我们可以看到存在一个大型数据集(15k 行),其中包含员工流动的相关信号。也就是说，这些是诸如满意度、工作时间和部门等指标，它们可能揭示有趣的模式，揭示员工离开公司的原因。

这个数据集标有一个`yes` / `no`(布尔型)变量，表示员工是否已经离职(`left`)。通过参考本章前面的数据流程图，我们可以看到像这样的标记数据很好地应用于监督学习模型。换句话说，给定其他变量，我们可以尝试预测`yes` / `no`变量`left`列。但是，这对业务有帮助吗？

让我们再考虑一下企业的需求:公司想要减少离职员工的数量。如果我们能够预测一名员工辞职的可能性有多大，企业就可以有选择地针对这些员工给予特殊待遇；例如，他们的工资可能会提高，或者他们的项目数量可能会减少。此外，这些具体的工资或项目变化的影响可以使用我们的模型进行估计。

鉴于这种以业务为导向的讨论，我们似乎已经就如何进行建模达成了一个决定:我们的目标将是训练一个可以预测员工是否离开的模型。

注意

正如我们之前所讨论的，我们将使用人力资源分析数据集，它最初是在 Kaggle 的开源存储库中找到的。这本书的数据集的链接可以在 GitHub 上找到:[https://packt.live/3hEBQIy](https://packt.live/3hEBQIy)。

这些数据是经过模拟的，这意味着样本是人工生成的，并不代表真实的人。我们将忽略这一事实，并对数据进行建模，就好像它是由现实生活中的过程生成的一样。

请注意，我们在本书中使用的数据集和原始 Kaggle 版本之间有一点点不同。我们的人力资源分析数据集包含一些缺失值(nan ),因此我们可以说明数据清理技术。出于同样的目的，我们还添加了一列名为`is_smoker`的数据。

我们将通过将`left`目标变量建模为前面列出的其他特性(列)的函数来实现这一点。

既然我们已经知道了希望创建什么类型的模型，我们需要熟悉数据集的细节，并为建模做好准备。这两点将是下一个活动的重点，这个活动是本章的结尾。

## 活动 3.01:准备培训一个员工留用预测模型

在本活动中，您将开始探索人力资源分析数据集，并为建模做准备。您将把数据集加载到 Jupyter 笔记本中，并查看每一列中的数据类型、分布和缺失值。然后，您将通过识别和修复建模时会导致问题的问题来清理数据。

1.  If you haven't done so already, start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

    然后，按照终端中的提示，通过复制并粘贴 URL，打开您在 web 浏览器中选择的平台。

2.  第一步是将数据集加载到笔记本中。在此之前，使用 bash 在您的笔记本中打印表格的前 10 行左右的内容(即`head`)。如果您不能在 Jupyter 环境中使用 bash，那么可以用 Python 代码来实现。
3.  用熊猫加载表格，并将其赋给`df`变量。
4.  打印表格的列，然后是前几行(`head`)和最后几行(`tail`)。
5.  用 Python 打开 CSV 文件(用`open`代替熊猫)，统计表格中的行数。
6.  计算`df`的长度，并与前面的结果进行比较。数字匹配吗？如果没有，为什么没有？
7.  检查`left`变量是如何分布的。有多少个`yes`值和多少个`no`值？是否有任何缺失值？
8.  打印每个特征的数据类型。
9.  绘制直方图/条形图，以便可视化每个特征分布。
10.  检查每列中有多少个 NaN 值。对于丢失数据的要素，考虑一下您将如何修复它们。这可能包括填充缺失数据、删除要素或删除样本。
11.  从`df`中删除`is_smoker`列。
12.  用中间值填充`time_spend_company`栏中的 NaN 值。
13.  做一个由`number_project`分割的`average_montly_hours`的箱线图。这张图表如何帮助你填补缺失的`average_montly_hours`数据？
14.  从前面的图表中计算每个区段的平均`average_montly_hours`数据。
15.  使用之前为每个分段计算的平均值填写`average_montly_hours`中的 NaN 值。这可以通过向`df.fillna`传递一个熊猫系列对象来完成。该系列应具有与`df`相同的形状，并为每个 NaN 条目填充适当的值。
16.  确认`df`不再有 NaN 值。
17.  既然已经清理了数据，最后一步就是将非数值字段转换成整数表示。布尔型字段应该映射到`0`和`1`，而分类型字段应该一键编码。
18.  打印转换后的数据集的列。在一键编码之后，应该有新的列可见。
19.  Save our preprocessed data in a CSV file.

    注意

    本练习的详细步骤以及解决方案和附加注释在第 273 页提供。

完成此活动后，人力资源分析数据集现在已准备好建模。

在继续之前，让我们在这里暂停一下，看看 Jupyter 笔记本是多么适合执行这种初始数据分析和清理。

例如，想象一下，我们让这个项目保持当前状态几个月。当我们回到这里时，我们可能不记得我们离开时到底发生了什么。然而，通过回头参考这个笔记本，我们将能够追溯我们的步骤，并快速回忆起我们之前了解到的数据。

此外，我们可以使用新的数据集，并重新运行笔记本来准备新的数据集，供我们的机器学习算法使用。回想一下，在这种情况下，最好先做一份笔记本的复印件，以免丢失最初的分析。

# 总结

在本章中，我们重点关注了训练机器学习模型之前的步骤。我们讨论了如何规划机器学习策略，并了解了我们可以用来准备建模数据集的各种动手方法。

从高层次的视角开始，我们通过查看可用数据、确定业务需求和评估数据的适用性来关注数据科学问题。接下来，我们讨论了如何从建模的角度理解数据，例如能够识别数据集是否适合有监督或无监督的学习问题。

在介绍了这些宏观概念之后，我们特别关注数据准备，这应该在建模之前进行。我们看到了如何合并数据集、删除或填充缺失值、转换分类特征以及将数据集分成训练集和测试集。

最后，我们介绍了人力资源分析数据集，并通过对其进行清理以进行建模，将我们所学到的知识付诸实践。在接下来的章节中，我们将使用这个处理过的数据集来训练各种分类模型。我们将首先介绍我们的建模算法并概述它们是如何工作的，然后使用 Jupyter 来训练和比较它们的预测能力。