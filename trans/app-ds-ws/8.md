<title>B15916_Solution_Final_RK_ePub</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 附录

# 1。Jupyter 笔记本简介

## 活动 1.01:使用 Jupyter 了解熊猫数据帧

解决方案:

1.  Start one of the following platforms to run Jupyter Notebooks:

    Jupyter 笔记本(运行`jupyter notebook`)

    JupyterLab (run `jupyter lab` )

    然后，按照终端中的提示，通过复制并粘贴 URL，在 web 浏览器中打开平台。

2.  如下加载`numpy`库:

    ```
    import numpy as np
    ```

3.  导入`pandas`，如下:

    ```
    import pandas as pd
    ```

4.  Pull up the docstring for the pandas DataFrame object, as follows:

    ```
    pd.DataFrame?
    ```

    输出如下所示:

    ![Figure 1:39: The docstring for pd.DataFrame
    ](image/B15916_01_39.jpg)

    图 1:39:PD 的文档字符串。数据帧

5.  Use a dictionary to create a DataFrame with `fruit` and `score` columns, as follows:

    ```
    fruit_scores = {'fruit': ['apple', 'orange', \
                              'banana', 'blueberry'], \
                    'score': [4, 2, 9, 8],}
    df = pd.DataFrame(data=fruit_scores)
    ```

    数据帧如下:

    ![Figure 1.40: A DataFrame with fruits and their scores
    ](image/B15916_01_40.jpg)

    图 1.40:带有水果及其分数的数据框

6.  Use tab completion to pull up a list of functions available for the DataFrame by typing `df.` and pressing *Tab*. The list of functions should then appear as a list of autocomplete options, as follows:![Figure 1.41: Example of the tab help feature in Jupyter
    ](image/B15916_01_41.jpg)

    图 1.41:Jupyter 中选项卡帮助特性的例子

7.  Pull up the docstring for `sort_values` by running a cell with the following code:

    ```
    df.sort_values?
    ```

    输出如下所示:

    ![Figure 1.42: The docstring for pd.DataFrame.sort_values
    ](image/B15916_01_42.jpg)

    图 1.42:PD 的文档字符串。DataFrame.sort_values

8.  Sort the DataFrame by score in descending order, as follows:

    ```
    df.sort_values(by='score', ascending=False)
    ```

    输出如下所示:

    ![Figure 1.43: Sorted fruits DataFrame
    ](image/B15916_01_43.jpg)

    图 1.43:排序后的水果数据框

9.  这将计算并显示排序后的数据帧；但是，`df`上的原始排序将保持不变。
10.  See how long it takes to compute the preceding sorting operation, as follows:

    ```
    %timeit df.sort_values(by='score', ascending=False)
    ```

    输出如下所示:

    ```
    349 µs ± 6.43 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3ftGze0](https://packt.live/3ftGze0)。

    你也可以在[https://packt.live/2Y49zTQ](https://packt.live/2Y49zTQ)在线运行这个例子。

# 2。使用 Jupyter 进行数据探索

## 活动 2.01:建立一个三阶多项式模型

解决方案:

1.  从 scikit-learn 加载必要的库和数据集，如下:

    ```
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn import datasets
    boston = datasets.load_boston()
    df = pd.DataFrame(data=boston['data'], \
                      columns=boston['feature_names'],)
    df['MEDV'] = boston['target']
    ```

2.  First, we will pull out our dependent feature and target variable from `df`, as follows:

    ```
    y = df['MEDV'].values
    x = df['LSTAT'].values.reshape(-1,1)
    ```

    这与我们之前对线性模型所做的相同。

3.  Verify what `x` looks like by executing the following code:

    ```
    x[:3]
    ```

    输出如下所示:

    ```
    array([[4.98],
           [9.14],
           [4.03]])
    ```

    注意数组中的每个元素本身是一个长度为 1 的数组。这就是`reshape(-1,1)`所做的，也是 scikit-learn 所期望的形式。

4.  Transform `x` into polynomial features by importing the appropriate transformation tool from scikit-learn and instantiating the third-degree polynomial feature transformer:

    ```
    from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(degree=3)
    ```

    这一步骤的基本原理可能不会立即显而易见，但将很快得到解释。

    `poly`的表示如下:

    ```
    PolynomialFeatures(degree=3, include_bias=True,
                       interaction_only=False, order='C')
    ```

5.  Transform the `LSTAT` feature (as stored in the `x` variable) by running the `fit_transform` method, as follows:

    ```
    x_poly = poly.fit_transform(x)
    ```

    这里，我们使用了 transformer 特性的实例来转换`LSTAT`变量。

6.  Verify what `x_poly` looks like by using the following code:

    ```
    x_poly[:3]
    ```

    输出如下所示:

    ```
    array([[  1\.      ,   4.98    ,  24.8004  , 123.505992],
           [  1\.      ,   9.14    ,  83.5396  , 763.551944],
           [  1\.      ,   4.03    ,  16.2409  ,  65.450827]])
    ```

    与 *x* 不同，现在每一行中的数组长度为 4，其中的值被计算为 *x* 、 *x* 、 *x* 和 *x* 。

    现在，我们将使用这些数据来拟合一个线性模型。将特征标记为 *a* 、 *b* 、 *c* 和 *d* ，我们将计算线性模型的系数 *α* ₒ、 *α* 1、 *α* 2 和 *α* 3:

    ![Figure 2.28: Linear model equation
    ](image/B15916_02_28.jpg)

    图 2.28:线性模型方程

    我们可以插入 a、b、c 和 d 的定义，得到下面的多项式模型，其中系数与前面的相同:

    ![Figure 2.29: Third-order polynomial equation
    ](image/B15916_02_29.jpg)

    图 2.29:三阶多项式方程

7.  Import the `LinearRegression` class and train a linear classification mode by running the following command:

    ```
    from sklearn.linear_model import LinearRegression
    clf = LinearRegression(fit_intercept=False)
    clf.fit(x_poly, y)
    ```

    输出如下所示:

    ```
    LinearRegression(copy_X=True, fit_intercept=False,
                     n_jobs=None, normalize=False)
    ```

8.  Extract the coefficients and print the polynomial model using the following code:

    ```
    x_0, x_1, x_2, x_3 = clf.coef_
    msg = ('model: y = {:.3f} + {:.3f}x + {:.3f}x^2 + {:.3f}x^3'\
           .format(x_0, x_1, x_2, x_3))
    print(msg)
    ```

    输出如下所示:

    ```
    model: y = 48.650 + -3.866x + 0.149x^2 + -0.002x^3
    ```

    通过运行以下代码，确定每个样本的预测值并计算残差:

    ```
    y_pred = clf.predict(x_poly)
    resid_MEDV = y - y_pred
    ```

9.  Print the first 10 residual values, as follows:

    ```
    resid_MEDV[:10]
    ```

    输出如下所示:

    ```
    array([-8.84025736, -2.61360313, -0.65577837, -5.11949581,  
           4.23191217, -3.56387056,  3.16728909, 12.00336372,  
           4.03348935,  2.87915437])
    ```

    我们将很快绘制这些图，以便与线性模型残差进行比较，但首先，我们将计算 MSE。

10.  运行以下代码，打印三阶多项式模型的 MSE:

    ```
    from sklearn.metrics import mean_squared_error
    error = mean_squared_error(y, y_pred)
    ```

11.  Print the MSE, as follows:

    ```
    print('mse = {:.2f}'.format(error))
    ```

    输出如下所示:

    ```
    mse = 28.88
    ```

    可以看出，与线性模型(38.5)相比，多项式模型的 **MSE** 明显更小。这个误差指标可以通过平方根转换成以美元为单位的平均误差。对多项式模型这样做，我们发现中值房屋价值的平均误差仅为 5300 美元。

    现在，我们将通过绘制最佳拟合的多项式线以及数据来可视化模型。

12.  Plot the polynomial model along with the samples, as follows:

    ```
    fig, ax = plt.subplots()
    # Plot the samples
    ax.scatter(x.flatten(), y, alpha=0.6)
    # Plot the polynomial model
    x_ = np.linspace(2, 38, 50).reshape(-1, 1)
    x_poly = poly.fit_transform(x_)
    y_ = clf.predict(x_poly)
    ax.plot(x_, y_, color='red', alpha=0.8)
    ax.set_xlabel('LSTAT')
    ax.set_ylabel('MEDV')
    plt.savefig('../figures/chapter-2-boston-housing-poly.png', \
                bbox_inches='tight', dpi=300,)
    ```

    输出如下所示:

    ![Figure 2.30: Plot of the polynomial model for MEDV
    ](image/B15916_02_30.jpg)

    图 2.30:MEDV 多项式模型图

    这里，我们通过计算一组`x`值的多项式模型预测来绘制红色曲线。使用`np.linspace`创建了`x`值的数组，产生了 50 个在 2 和 38 之间均匀排列的值。

    现在，我们将绘制相应的残差。尽管我们之前使用了 seaborn 来实现这一点，但我们将不得不手动完成这一操作，以显示 scikit-learn 模型的结果。因为我们已经计算了残差，正如`resid_MEDV`变量所引用的，我们只需要在散点图上绘制这个值列表。

13.  Plot the residuals by running the following code:

    ```
    fig, ax = plt.subplots(figsize=(5, 7))
    ax.scatter(x, resid_MEDV, alpha=0.6)
    ax.set_xlabel('LSTAT')
    ax.set_ylabel('MEDV Residual $(y-\hat{y})$')
    plt.axhline(0, color='black', ls='dotted')
    plt.savefig('../figures/chapter-2-boston-housing-'\
                'poly-residuals.png', \
                bbox_inches='tight', dpi=300,)
    ```

    这将导致以下输出:

    ![Figure 2.31: Plot of the polynomial model residuals
    ](image/B15916_02_31.jpg)

图 2.31:多项式模型残差图

与线性模型的`LSTAT`残差图相比，多项式模型残差似乎更紧密地聚集在 *y - ŷ = 0 附近。*注意 *y* 是样本`MEDV`，而 *ŷ* 是预测值。仍然有清晰的模式，例如靠近 *x = 7* 和 *y = -7* 的集群表明建模不理想。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2UIzwq8](https://packt.live/2UIzwq8)。

你也可以在[https://packt.live/37DzuVK](https://packt.live/37DzuVK)在线运行这个例子。

# 3。为预测建模准备数据

## 活动 3.01:准备培训一个员工留用预测模型

解决方案:

1.  Check the head of the table by running the following command:

    ```
    %%bash
    head ../data/hr-analytics/hr_data.csv
    ```

    请注意我们是如何指定相对于笔记本位置的路径的。在这种情况下，我们需要后退一个目录(通过在文件路径中使用“`..`”)，这将我们带到项目的根文件夹。然后，我们在`data/hr-analytics`中寻找`hr_data.csv`。

    这将生成以下输出:

    ![Figure 3.25: Printing the head of hr_data.csv with bash
    ](image/B15916_03_25.jpg)

    图 3.25:用 bash 打印 hr_data.csv 的文件头

2.  If you cannot run bash in your notebook, run the following command:

    ```
    with open('../data/hr-analytics/hr_data.csv', 'r') as f:
        for _ in range(10):
            print(next(f).strip())
    ```

    输出如下所示:

    ![Figure 3.26: Printing the head of hr_data.csv with Python
    ](image/B15916_03_26.jpg)

    图 3.26:用 Python 打印 hr_data.csv 文件头

    根据输出判断，确信它看起来是标准的 CSV 格式。对于 CSV 文件，我们应该能够简单地用`pd.read_csv`加载数据。

3.  Load the data with pandas, as follows:

    ```
    df = pd.read_csv('../data/hr-analytics/hr_data.csv')
    ```

    您应该自己写出来，并尝试使用制表符结束来帮助键入文件路径。

4.  Inspect the columns as follows:

    ```
    df.columns
    ```

    输出如下所示:

    ![Figure 3.27: Columns loaded from hr_data.csv
    ](image/B15916_03_27.jpg)

    图 3.27:从 hr_data.csv 加载的列

5.  Ensure that the data has loaded as expected by printing the head of the DataFrame, as follows:

    ```
    df.head()
    ```

    输出类似于以下内容:

    ![Figure 3.28: The head of hr_data.csv in a pandas DataFrame
    ](image/B15916_03_28.jpg)

    图 3.28:熊猫数据帧中 hr_data.csv 的头部

    注意

    为了表示的目的，前面的数据帧被裁剪。完整的输出可以在这里找到:[https://packt.live/2YEsiUX](https://packt.live/2YEsiUX)。

6.  Print the tail of the DataFrame, as follows:

    ```
    df.tail()
    ```

    输出如下所示:

    ![Figure 3.29: The tail of hr_data.csv in a pandas DataFrame
    ](image/B15916_03_29.jpg)

    图 3.29:熊猫数据帧中 hr_data.csv 的尾部

    我们可以看到它似乎已经正确加载。根据尾部索引值，有将近 15，000 行。

    注意

    为了表示的目的，前面的数据帧被裁剪。完整的输出可以在这里找到:[https://packt.live/2YEsiUX](https://packt.live/2YEsiUX)。

7.  Check the number of rows (including the header) in the CSV file with the following code:

    ```
    with open('../data/hr-analytics/hr_data.csv') as f:
        num_lines = len([line for line in f.read().splitlines()\
                         if line.strip()])
    num_lines
    ```

    这将打印以下输出:

    ```
    15000
    ```

8.  See how many rows are in the DataFrame:

    ```
    len(df)
    ```

    这将打印以下输出:

    ```
    14999
    ```

    这个数字比文件中的行数少一，因为它不包括标题(列名)。因此，我们可以得出结论，所有记录都已加载。

9.  Check how the `left` target variable is distributed, as follows:

    ```
    df.left.value_counts()
    ```

    这将打印以下输出:

    ```
    no     11428
    yes     3571
    Name: left, dtype: int64
    ```

10.  This can be visualized as follows:

    ```
    df.left.value_counts().plot('barh')
    plt.show()
    ```

    输出如下所示:

    ![Figure 3.30: Distribution of the left target variable
    ](image/B15916_03_30.jpg)

    图 3.30:左侧目标变量的分布

    大约四分之三的样本是没有离职的员工。离开的小组构成了样本的另四分之一。这告诉我们，我们正在处理一个不平衡的分类问题，这意味着在计算精度时，我们必须采取特殊的措施来考虑每个类别。

11.  Check for missing values, as follows:

    ```
    df.left.isnull().sum()
    ```

    这将打印以下输出:

    ```
    0
    ```

    可以看出，没有丢失值。

12.  Print the data type of each column, as follows:

    ```
    df.dtypes
    ```

    观察我们如何将连续和离散特征结合在一起:

    ![Figure 3.31: Data types of each column
    ](image/B15916_03_31.jpg)

    图 3.31:每一列的数据类型

13.  Display the feature distributions by running the following code:

    ```
    for f in df.columns:
        fig = plt.figure()
        s = df[f]
        if s.dtype in ('float', 'int'):
            num_bins = min((30, len(df[f].unique())))
            s.hist(bins=num_bins)
        else:
            s.value_counts().plot.bar()
        plt.xlabel(f)
    ```

    输出如下所示:

    ![Figure 3.32: Distribution of values for each column (1/2)
    ](image/B15916_03_32.jpg)

    图 3.32:每列的值分布(1/2)

    其余的图如下所示:

    ![Figure 3.33: Distribution of values for each column (2/2)
    ](image/B15916_03_33.jpg)

    图 3.33:每列的值分布(2/2)

    对于许多特征，我们可以看到可能值的广泛分布，表明特征空间中有很好的多样性。这令人鼓舞；对于我们的模型来说，围绕一个小范围的值进行强烈分组的特征可能不会提供很多信息。比如我们可以看到`promotion_last_5years`就是这种情况，绝大多数样本都是`0`。

14.  Check how many `NaN` values are in each column, as follows:

    ```
    df.isnull().sum() / len(df) * 100
    ```

    输出如下所示:

    ![Figure 3.34: Percentage of values that are missing in each column
    ](image/B15916_03_34.jpg)

    图 3.34:每一列中缺失值的百分比

    这里我们可以看到`average_monthly_hours`大约缺失 2.5%`time_spend_company`缺失 1%`is_smoker.`缺失 98%

15.  删除`is_smoker`列，因为该指标中几乎没有任何信息。这可以按如下方式进行:

    ```
    del df['is_smoker']
    ```

16.  Fill in the `NaN` values in the `time_spend_company` column. This can be done with the following code:

    ```
    fill_value = df.time_spend_company.median()
    df.time_spend_company = df.time_spend_company.fillna(fill_value)
    ```

    要处理的最后一列是`average_montly_hours`。我们可以做一些与之前类似的事情，使用中间值或舍入平均值作为整数填充值。相反，让我们试着利用它与另一个变量的关系。这可以让我们更准确地填写缺失的数据。

17.  Check for the following:

    ```
    df.isnull().sum() / len(df) * 100
    ```

    输出如下所示:

    ![Figure 3.35: Boxplot showing how the average monthly hours and number of projects are related
    ](image/B15916_03_35.jpg)

    图 3.35:箱线图显示了平均每月小时数和项目数之间的关系

18.  Make a boxplot of `average_montly_hours` segmented by `number_project`, as follows:

    ```
    sns.boxplot(x='number_project', y='average_montly_hours', data=df)
    plt.savefig('../figures/chapter-3-hr-analytics-hours-num-'\
                'proj-boxplot.png', bbox_inches='tight', dpi=300,)
    ```

    注意

    该路径将根据您想要保存图像的位置而有所不同。

    输出如下所示:

    ![Figure 3.36: Average number of hours worked for each "number of projects" bucket
    ](image/B15916_03_36.jpg)

    图 3.36:每个“项目数量”时段的平均工作小时数

    在这里，我们可以看到项目的数量是如何与`average_monthly_hours`相关联的，这个结果并不令人惊讶。我们将通过基于该记录的项目数量填充`average_montly_hours`的`NaN`值来利用这种关系。

    具体来说，我们将使用每组的平均值。

19.  Calculate the mean of each group, as follows:

    ```
    mean_per_project = (df.groupby('number_project') \
                        .average_montly_hours.mean())
    mean_per_project = dict(mean_per_project)
    mean_per_project
    ```

    输出如下所示:

    ```
    {2: 160.16353543979506,
     3: 197.47882323104236,
     4: 205.07858315740089,
     5: 211.99962839093274,
     6: 238.73947368421054,
     7: 276.015873015873}
    ```

    然后，我们可以将其映射到`number_project`列，并将结果 series 对象作为参数传递给`fillna`。

20.  在`average_montly_hours`中填入`NaN`值，如下:

    ```
    fill_values = df.number_project.map(mean_per_project)
    df.average_montly_hours = (df.average_montly_hours\
                               .fillna(fill_values))
    ```

21.  通过运行以下断言测试，确认`df`不再有`NaN`值。如果它没有引发错误，那么您已经成功地从表中删除了`NaN`值:

    ```
    assert df.isnull().sum().sum() == 0
    ```

22.  将字符串和布尔字段转换为整数表示形式。特别是，我们将手动将`yes`和`no`中的`left`表变量转换成`1`和`0`，并构建一键编码特性。这可以使用下面的代码来完成:

    ```
    df.left = df.left.map({'no': 0, 'yes': 1})
    df = pd.get_dummies(df)
    ```

23.  Show the fields as follows:

    ```
    df.columns
    ```

    输出如下所示:

    ![Figure 3.37: A screenshot of the different fields in the DataFrame
    ](image/B15916_03_37.jpg)

    图 3.37:数据框中不同字段的屏幕截图

    在这里，我们可以看到 department 和 salary 已经被拆分成不同的一次性编码特性。

    为机器学习准备数据的最后一步是缩放特征，但现在不适合这么做。在下一章中，当我们在人力资源分析数据集上训练我们的第一个模型时，我们将了解更多关于特征缩放的信息。

24.  We have completed the data preprocessing stage and are ready to move on to training the models. Let's save our preprocessed data by running the following code:

    ```
    df.to_csv('../data/hr-analytics/hr_data_processed.csv', \
              index=False)
    ```

    注意

    当将熊猫数据帧保存为 CSV 文件时，注意我们传递了`index=False`参数。这样做是为了不让我们的索引写入文件。一般来说，索引可能包含多个信息字段，但是在我们的例子中，它只是一个标签范围，从 0 到数据长度。因此，我们不应该将这些信息写入输出 CSV。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2YEsiUX](https://packt.live/2YEsiUX)。

    你也可以在[https://packt.live/2Y3vvi4](https://packt.live/2Y3vvi4)在线运行这个例子。

# 4。训练分类模型

## 活动 4.01:用 Scikit-learn 训练和可视化 SVM 模型

## 解决方案:

1.  创建新的 Jupyter 笔记本。
2.  在第一个单元格中，添加以下代码行来加载我们将使用的库，并为笔记本设置我们的绘图环境:

    ```
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (8, 8)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    ```

3.  In the next cell, enter the following code to print the date, version numbers, and hardware information:

    ```
    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

    您应该得到以下输出:

    ![Figure 4.19: Output of loading all the required libraries
    ](image/B15916_04_19.jpg)

    图 4.19:加载所有需要的库的输出

4.  Load the preprocessed Human Resource Analytics dataset by running the following command:

    ```
    df = pd.read_csv('../data/hr-analytics/hr_data_processed.csv')
    ```

    注意

    该路径将根据数据的存储位置而有所不同。如果您从不同的文件夹工作，请提供绝对路径。

5.  Describe the `number_project` and `average_monthly_hours` features by running the following command:

    ```
    df[['number_project', 'average_montly_hours']].describe()
    ```

    这会产生以下输出:

    ![Figure 4.20: Summary description of values for number_project and average_monthly_hours
    ](image/B15916_04_20.jpg)

    图 4.20:数字 _ 项目和平均 _ 每月 _ 小时值的概要描述

    比较`mean`、`min`、`max`各值，注意`number_project`是如何限制在`2`-`7`范围内，而`average_monthly_hours`是从`96`-`310`范围内。

6.  通过运行下面的代码将`number_project`和`average_monthly_hours`特性分成训练集和测试集:

    ```
    from sklearn.model_selection import train_test_split
    features = ['number_project', 'average_montly_hours']
    X_train, X_test, \
    y_train, y_test = train_test_split(df[features].values, \
                                       df['left'].values, \
                                       test_size=0.3, \
                                       random_state=1,)
    ```

7.  Scale the data using a `MinMaxScaler` library by running the following code:

    ```
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    ```

    在笔记本的早期训练模型时，我们使用了一个`StandardScaler`库，它缩放每个特征，使它们的方差相同，并且以 0 为中心。

    `MinMaxScaler`库将每个特征缩放到 0 到 1 的范围内。我们可以通过运行以下代码来观察这一点:

    ```
    (X_train_scaled.flatten().mean(),
     X_train_scaled.flatten().min(),
     X_train_scaled.flatten().max())
    ```

    这将打印以下输出:

    ```
    (0.4231614367047082, 0.0, 1.0)
    ```

8.  通过运行下面的代码用`rbf`内核训练一个 SVM:

    ```
    from sklearn.svm import SVC
    svm = SVC(kernel='rbf', C=1, random_state=1, gamma='scale')
    svm.fit(X_train_scaled, y_train)
    ```

9.  Calculate the classification accuracy on the test set by running the following code:

    ```
    from sklearn.metrics import accuracy_score
    y_pred = svm.predict(X_test_scaled)
    accuracy_score(y_test, y_pred) * 100
    ```

    这将打印出以下输出，表明总体精度约为 88.8%:

    ```
    88.84444444444445
    ```

10.  Calculate the class accuracies on the test set by running the following code:

    ```
    from sklearn.metrics import confusion_matrix
    cmat = confusion_matrix(y_test, y_pred)
    cmat.diagonal() / cmat.sum(axis=1) * 100
    ```

    这将打印以下输出，表明 0 类的精度约为 96%，1 类的精度约为 66%。

    ```
    array([95.98946136, 66.32841328])
    ```

11.  Plot the decision regions for the model by running the following code:

    ```
    from mlxtend.plotting import plot_decision_regions
    N_samples = 200
    X, y = X_train_scaled[:N_samples], y_train[:N_samples]
    plot_decision_regions(X, y, clf=svm)
    plt.xlim(-0.2, 1.2)
    plt.ylim(-0.2, 1.2)
    ```

    以下是显示决策区域的输出:

    ![Figure 4.21: Decision region plot for a kernel SVM with C=1
    ](image/B15916_04_21.jpg)

    图 4.21:C = 1 的核 SVM 的决策区域图

12.  Train an SVM with `C=50` and plot the resulting decision regions by running the following code:

    ```
    svm = SVC(kernel='rbf', C=50, random_state=1, gamma='scale')
    svm.fit(X_train_scaled, y_train)
    X, y = X_train_scaled[:N_samples], y_train[:N_samples]
    plot_decision_regions(X, y, clf=svm)
    plt.xlim(-0.2, 1.2)
    plt.ylim(-0.2, 1.2)
    ```

    下面是 SVM 的剧情:

    ![Figure 4.22: Decision region plot for a kernel SVM with C=50
    ](image/B15916_04_22.jpg)

图 4.22:C = 50 的核 SVM 的决策区域图

比较这两个决策区域图，我们可以看到`C=50` SVM 是如何试图更接近地拟合训练数据中的模式的。这相对于`x, y = (0.4, 1)`处的点是值得注意的，如橙色三角形所示。决策面已经过调整，可以在`C=50` SVM 的训练集中正确分类该记录，但不能在`C=1` SVM 的训练集中正确分类。

注意

要访问该特定部分的源代码，请参考[https://packt.live/3e6JYPJ](https://packt.live/3e6JYPJ)。

你也可以在[https://packt.live/2ACdbUc](https://packt.live/2ACdbUc)在线运行这个例子。

# 5。模型验证和优化

## 活动 5.01:超参数调整和模型选择

解决方案:

1.  创建一个新的 Jupyter 笔记本并加载以下库:

    ```
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    %load_ext watermark
    %watermark -d -v -m -p \
    numpy,pandas,matplotlib,seaborn,sklearn
    ```

2.  Load the preprocessed Human Resource Analytics dataset by running the following code:

    ```
    df = pd.read_csv('../data/hr-analytics/hr_data_processed_pca.csv')
    df.columns
    ```

    这将显示以下输出:

    ![Figure 5.10: The columns of hr_data_processed_pca.csv
    ](image/B15916_05_10.jpg)

    图 5.10:HR _ data _ processed _ PCA . CSV 的列

3.  选择要包含在模型中的特性，并通过运行以下代码对数据执行训练测试分割:

    ```
    from sklearn.model_selection import train_test_split
    features = ['satisfaction_level', 'last_evaluation', \
                'time_spend_company', 'number_project', \
                'average_montly_hours', 'first_principle_component', \
                'second_principle_component', \
                'third_principle_component',]
    X, X_test, \
    y, y_test = train_test_split(df[features].values, \
                                 df['left'].values, \
                                 test_size=0.15, \
                                 random_state=1)
    ```

4.  通过运行以下代码，在 2 到 52 的范围内，用`n_estimators=50`计算`RandomForestClassifier`的验证曲线:

    ```
    from sklearn.ensemble import RandomForestClassifier
    np.random.seed(1)
    clf = RandomForestClassifier(n_estimators=50)
    max_depth_range = np.arange(2, 52, 2)
    print('Training {} models ...'.format(len(max_depth_range)))
    train_scores, test_scores = \
    validation_curve(estimator=clf, X=X, y=y, param_name='max_depth', \
                     param_range=max_depth_range, cv=5,);
    ```

5.  Plot the validation curve by running the following code:

    ```
    plot_validation_curve(train_scores, test_scores, \
                          max_depth_range, xlabel='max_depth',)
    plt.ylim(0.97, 1.0)
    ```

    以下是该图的输出结果:

    ![Figure 5.11: Validation curve for a Random Forest with PCA features
    ](image/B15916_05_11.jpg)

    图 5.11:具有 PCA 特征的随机森林的验证曲线

    在这里，我们有一个有趣的结果。

    在某些方面，这条验证曲线非常类似于我们之前看到的决策树的图表，它是在与这个随机森林相同的功能上训练的。特别是，我们可以看到训练集(蓝色圆圈)很快接近 100%的准确性，而验证集(红色方块)则被限制在较低的最大准确性。

    然而，与之前的验证曲线不同，随着`max_depth`的增加，这里的验证集似乎收敛到*最大*精度。这不是我们之前在决策树中看到的行为，在决策树中，验证集的准确性达到最大值大约为`max_depth=8`，然后随着`max_depth`值的增加而略微下降。

    验证集的这种观察到的行为可以用所使用的建模算法的性质来解释。在我们之前的案例中，我们正在训练决策树，它对大的`max_depth`值的训练数据进行了过度拟合，这导致了较低的验证准确性。另一方面，我们在这里使用的随机森林模型不太容易对大的`max_depth`值过度拟合，因为它的每个组成决策树的数据采样方式。

    在这种情况下，对于我们的随机森林模型，重要的是将`max_depth`设置得足够高，以避免欠拟合，我们不应该担心在大的`max_depth`值时过拟合。考虑到这一点，以及验证精度似乎集中在 20 左右的事实，我们将选择`max_depth=25`作为这个超参数的最佳值。

6.  Calculate the k-fold cross validation accuracy of our selected model for each class by running the following code:

    ```
    clf = RandomForestClassifier(n_estimators=50, max_depth=25)
    np.random.seed(1)
    scores = cross_val_class_score(clf, X, y)
    print('accuracy = {} +/- {}'.format(scores.mean(axis=0), \
                                        scores.std(axis=0),))
    ```

    这将打印以下输出(或类似的输出，取决于您的随机种子在每个步骤中是如何设置的):

    ```
    fold: 1 accuracy: [0.99897119 0.94407895]
    fold: 2 accuracy: [0.99897119 0.96369637]
    fold: 3 accuracy: [0.99897119 0.96039604]
    fold: 4 accuracy: [0.99794239 0.98349835]
    fold: 5 accuracy: [0.99897119 0.96039604]
    fold: 6 accuracy: [0.99691358 0.95379538]
    fold: 7 accuracy: [0.99794239 0.95709571]
    fold: 8 accuracy: [0.99897119 0.95709571]
    fold: 9 accuracy: [0.9969104  0.94059406]
    fold: 10 accuracy: [0.99485067 0.97689769]
    accuracy = [0.99794154 0.95975443] +/- [0.00130286 0.01239563]
    ```

    将此结果与之前练习中的决策树结果进行比较，我们可以看到每个类别的准确性都有显著提高，类别 0 从 99.4%提高到 99.8%，类别 1 从 92.4%提高到 95.9%。

7.  Evaluate the performance of this model on the test set by running the following code:

    ```
    from sklearn.metrics import confusion_matrix
    clf = RandomForestClassifier(n_estimators=50, max_depth=25)
    clf.fit(X, y)
    y_pred = clf.predict(X_test)
    cmat = confusion_matrix(y_test, y_pred)
    cmat.diagonal() / cmat.sum(axis=1) * 100
    ```

    这将打印以下输出:

    ```
    array([99.70760234, 97.03703704])
    ```

    通过将这些数字与之前的 k 倍精度进行比较，我们可以看到它们都位于预期范围内，因此验证了该模型。

8.  通过运行以下代码，根据`df`中的全套记录训练模型:

    ```
    features = ['satisfaction_level', 'last_evaluation', \
                'time_spend_company', 'number_project', \
                'average_montly_hours', 'first_principle_component', \
                'second_principle_component', \
                'third_principle_component',]
    X = df[features].values
    y = df['left'].values
    clf = RandomForestClassifier(n_estimators=50, max_depth=25)
    clf.fit(X, y)
    ```

9.  Save the model to disk by running the following code:

    ```
    import joblib
    joblib.dump(clf, 'hr-analytics-pca-forest.pkl')
    ```

    然后，重新加载模型，如下所示:

    ```
    clf = joblib.load('hr-analytics-pca-forest.pkl')
    clf
    ```

    这将返回训练模型的字符串表示形式:

    ![Figure 5.12: Random Forest model representation
    ](image/B15916_05_12.jpg)

    图 5.12:随机森林模型表示

10.  Check the model's performance for an imaginary employee, Alice, by selecting the appropriate features from row 573 of `df` with the following code:

    ```
    alice = df.iloc[573][features]
    alice
    ```

    这将打印以下输出，代表 Alice 的员工指标和派生的主要成分:

    ```
    satisfaction_level              0.360000
    last_evaluation                 0.470000
    time_spend_company              3.000000
    number_project                  2.000000
    average_montly_hours          148.000000
    first_principle_component       0.742801
    second_principle_component     -0.514568
    third_principle_component      -0.677421
    ```

11.  Predict the class label for Alice, as follows:

    ```
    clf.predict([alice.values])
    ```

    这将打印以下输出:

    ```
    array([1])
    ```

    然后，计算分配给该预测的概率，如下所示:

    ```
    clf.predict_proba([alice.values])
    ```

    这将打印以下输出:

    ```
    array([[0., 1.]])
    ```

    这些结果表明，该模型以 100%的概率预测 Alice 将离开公司。

12.  In order to improve the chance of the company being able to retain Alice as an employee, they could try to reduce the amount of time she needs to spend at work. Using our model, we can test the effect that might have on her likelihood of leaving.

    设置`average_montly_hours=100`和`time_spend_company=2`，然后通过运行以下代码重新评估模型的预测概率:

    ```
    alice.average_montly_hours = 100
    alice.time_spend_company = 2
    clf.predict_proba([alice.values])
    ```

    这将打印以下输出:

    ```
    array([[0.84, 0.16]])
    ```

    使用以下命令预测新的类标签:

    ```
    clf.predict([alice.values])
    ```

    这将打印以下输出:

    ```
    array([0])
    ```

    这个结果表明，通过将每月工作时间减少到 100 小时，将在公司的时间减少到 2 级，Alice 有 84%的机会不会离开公司。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/37vgad6](https://packt.live/37vgad6)。

    你也可以在[https://packt.live/2BcG5tP](https://packt.live/2BcG5tP)在线运行这个例子。

这是一个很好的例子，说明了预测建模如何被企业用来做出数据驱动的决策。

# 6。使用 Jupyter 笔记本进行网页抓取

## 活动 6.01:使用 Jupyter 笔记本进行网页抓取

解决方案:

1.  在您的笔记本中运行以下代码来加载必要的库:

    ```
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os

    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    import requests
    from bs4 import BeautifulSoup
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")

    %load_ext watermark
    %watermark -d -v -m -p \
    requests,numpy,pandas,matplotlib,seaborn,sklearn
    ```

2.  After defining the `url` variable, load that page in the notebook using an IFrame. This can be done by running the following code:

    ```
    url = 'https://en.wikipedia.org/wiki/List_of_countries_and'\
          '_dependencies_by_population'
    from IPython.display import IFrame
    IFrame(url, height=300, width=800)
    ```

    下面是显示 IFrame 的输出:

    ![Figure 6.29: Running the live web page in Jupyter
    ](image/B15916_06_29.jpg)

    图 6.29:在 Jupyter 中运行动态网页

    浏览完页面后，点击单元格并选择`Cell` | `Current Outputs` | `Clear`关闭页面。

3.  Request the page by running the following code:

    ```
    resp = requests.get(url)
    resp
    ```

    这应该会打印出以下输出:

    ```
    <Response [200]>
    ```

4.  通过运行以下命令，使用响应内容实例化一个`BeautifulSoup`对象:

    ```
    soup = BeautifulSoup(resp.content, 'html.parser')
    ```

5.  Get the `h1` of the page by running the following command:

    ```
    soup.find_all('h1')
    ```

    这应该会打印出以下输出:

    ```
    [<h1 class="firstHeading" id="firstHeading" lang="en">List of countries and dependencies by population</h1>]
    ```

6.  通过运行以下命令选择带有`id=`“`bodyContent`的`div`:

    ```
    body_content = soup.find('div', {'id': 'bodyContent'})
    ```

7.  Search this div element for the table headers by running the following code:

    ```
    table_headers = body_content.find_all('th')
    table_headers
    ```

    以下是显示显示标题的输出:

    ![Figure 6.30: The table header elements
    ](image/B15916_06_30.jpg)

    图 6.30:表格标题元素

    然后，通过运行以下代码，打印每个元素的元素文本及其索引:

    ```
    for i, t in enumerate(table_headers):
        print(i, t.text.strip())
        print('-'*10)
    ```

    该循环显示如下:

    ![Figure 6.31: Printing the text of each table header element
    ](image/B15916_06_31.jpg)

    图 6.31:打印每个表格标题元素的文本

8.  通过运行以下代码手动设置表头:

    ```
    table_headers = ['Country(or dependent territory)', 'Population', \
                     '% of WorldPopulation', 'Date']
    ```

9.  通过运行以下代码从索引`2`处的行元素中选择数据条目:

    ```
    row_number = 2
    row_data = body_content.find_all('tr')[row_number]\
               .find_all(['td', 'th'])
    ```

10.  通过运行`len(row_data)`找到该行中数据条目的数量。结果是`6`。
11.  Print out the row's data entries by running `row_data`:![Figure 6.32: The data elements in a selected row
    ](image/B15916_06_32.jpg)

    图 6.32:选定行中的数据元素

12.  Print the element text for each row data entry, along with their index, by running the following code:

    ```
    for i, row in enumerate(row_data):
        print(i, row.text)
    ```

    输出如下图所示:

    ![Figure 6.33: Printing the text for each data element in a selected row
    ](image/B15916_06_33.jpg)

    图 6.33:打印选定行中每个数据元素的文本

13.  通过运行下面的代码选择我们感兴趣的字段:

    ```
    row_data = row_data[1:5]
    ```

14.  First, assign `d1` to the first element and parse the entry value using the following code:

    ```
    d1 = row_data[0]
    d1.find('a').text
    ```

    这里，我们使用了在获取文本之前在数据条目中搜索一个`<a>`元素的技巧。

15.  接下来，对第二个条目做同样的操作:

    ```
    d2 = row_data[1]
    d2.text
    ```

16.  然后，对第三个条目做同样的操作:

    ```
    d3 = row_data[2]
    d3.text
    ```

17.  Finally, do the same for the last entry:

    ```
    d4 = row_data[3]
    d4.text
    ```

    对于最后三个条目，我们现在只需要简单地获取元素文本就可以了。稍后可以对它们进行进一步处理(例如，通过将字符串转换为数值)。

18.  通过运行下面的代码执行国家人口的全面搜集:

    ```
    # Perform the full scrape by iterating over the rows
    pop_data = []
    row_elements = body_content.find_all('tr')
    for i, row in enumerate(row_elements):
        row_data = row.find_all(['td','th'])
        if len(row_data) < 5:
            print('Ignoring row {} because length < 5'.format(i))
            continue
        d1, d2, d3, d4 = row_data[1:5]
        errs = []
        try:
            d1 = d1.find('a').text
        except Exception as e:
            d1 = ''
            errs.append(str(e))
        try:
            d2 = d2.text
        except Exception as e:
            d2 = ''
            errs.append(str(e))
        try:
            d3 = d3.text
        except Exception as e:
            d3 = ''
            errs.append(str(e))
        try:
            d4 = d4.text
        except Exception as e:
            d4 = ''
            errs.append(str(e))
        data = [d1, d2, d3, d4]
        print(data)
        pop_data.append(data)
        if errs:
            print('Errors in row {}: {}'.format(i, ', '.join(errs)))
    ```

19.  Print out the data we parsed previously, by running `pop_data`. The output is as follows:![Figure 6.34: Output of the pop_data command
    ](image/B15916_06_34.jpg)

    图 6.34:pop _ data 命令的输出

20.  运行`table_headers`打印出表头。输出如下:

    ```
    ['Country(or dependent territory)',
     'Population',
     '% of WorldPopulation',
     'Date']
    ```

21.  通过运行以下代码将数据保存在 CSV 文件中:

    ```
    f_path = '../data/countries/populations_raw.csv'
    pd.DataFrame(pop_data, columns=table_headers)\
                 .to_csv(f_path, index=False)
    ```

22.  If your Jupyter environment supports bash, then you can view the head of your table by running the following code:

    ```
    %%bash
    head ../data/countries/populations_raw.csv
    ```

    `head`显示如下:

    ![Figure 6.35: The first 10 rows of extracted Wikipedia data in a CSV file
    ](image/B15916_06_35.jpg)

图 6.35:CSV 文件中提取的前 10 行维基百科数据

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。

总的来说，我们已经看到了 Jupyter 笔记本如何用于网页抓取。我们通过学习 HTTP 方法和状态代码开始了这一章。然后，我们使用`requests`库通过 Python 实际执行 HTTP 请求，并了解了如何使用`BeautifulSoup`库解析响应 HTML。

## 活动 6.02:分析国家人口和利率

解决办法

1.  Start up one of the following platforms for running Jupyter Notebooks:

    JupyterLab (run `jupyter lab` )

    Jupyter 笔记本(运行`jupyter notebook`)

    然后，按照终端中的提示，通过复制并粘贴 URL，在 web 浏览器中打开所选的平台。

2.  运行下面的代码来加载一些库，我们将使用这些库来配置笔记本的绘图设置:

    ```
    import pandas as pd
    import numpy as np
    import datetime
    import time
    import os
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    %config InlineBackend.figure_format='retina'
    sns.set() # Revert to matplotlib defaults
    plt.rcParams['figure.figsize'] = (9, 6)
    plt.rcParams['axes.labelpad'] = 10
    sns.set_style("darkgrid")
    %load_ext watermark
    %watermark -d -v -m -p numpy,pandas,matplotlib,seaborn
    ```

3.  Load the processed country population and interest rate dataset by running the following code:

    ```
    df = pd.read_csv('../data/countries/country_data_merged.csv')
    df.head()
    ```

    显示头部的输出如下面的屏幕截图所示:

    ![Figure 6.36: The head of country_data_merged.csv
    ](image/B15916_06_36.jpg)

    图 6.36:country _ data _ merged . CSV 的头

4.  Print the datatypes of each column by running `df.dtypes`:![Figure 6.37: The datatypes of each column
    ](image/B15916_06_37.jpg)

    图 6.37:每一列的数据类型

5.  通过运行以下代码将日期列条目转换为日期时间:

    ```
    df['date_population_update'] = \
    pd.to_datetime(df['date_population_update'])
    df['date_interest_rate_last_change'] = \
    pd.to_datetime(df['date_interest_rate_last_change'])
    ```

6.  Check for missing values by running the following command:

    ```
    df.isnull().sum()
    ```

    以下屏幕截图显示了该命令的输出:

    ![Figure 6.38: The number of missing values in each column
    ](image/B15916_06_38.jpg)

    图 6.38:每一列中缺失值的数量

7.  Plot comparable histograms of the date columns, that is, `date_population_update` and `date_interest_rate_last_change`, by running the following code:

    ```
    col = 'date_population_update'
    df[col].hist(bins=10, alpha=0.7, label=col)
    col = 'date_interest_rate_last_change'
    df[col].hist(bins=45, alpha=0.7, label=col)
    plt.xticks(rotation=45)
    plt.legend()
    plt.show()
    ```

    从此代码绘制的直方图如下所示:

    ![Figure 6.39: Histograms for the date fields in the dataset
    ](image/B15916_06_39.jpg)

    图 6.39:数据集中日期字段的直方图

8.  Find the country with the oldest date since the last interest rate change by running the following code:

    ```
    min_date = df['date_interest_rate_last_change'].min()
    min_date_mask = df['date_interest_rate_last_change'] == min_date
    df[min_date_mask]
    ```

    下面是这段代码的输出:

    ![Figure 6.40: Country wth the oldest date since last interest rate change
    ](image/B15916_06_40.jpg)

    图 6.40:自上次利率变化以来日期最早的国家

    注意

    为了表示的目的，前面的数据帧被裁剪。你可以在[https://packt.live/2ACHg63](https://packt.live/2ACHg63)查阅完整的数据框。

9.  Find any countries that have negative interest rates by running the following code:

    ```
    neg_rates_mask = df['interest_rate_pct'] < 0
    df[neg_rates_mask][['country', 'interest_rate_pct']]
    ```

    负利率的国家如下所示:

    ![Figure 6.41: Displaying the countries with negative interest rates
    ](image/B15916_06_41.jpg)

    图 6.41:显示负利率国家

10.  Plot a bar chart of the top countries by population, by running the following code:

    ```
    df_plot = df.sort_values('population', ascending=False).head(10)
    df_plot['population'].plot.bar()
    plt.xticks(rotation=45)
    plt.ylabel('Population')
    ax = plt.gca()
    labels = df_plot['country'].values
    ax.set_xticklabels(labels)
    plt.show()
    ```

    如此绘制的条形图如下所示:

    ![Figure 6.42: A bar chart comparing the top countries by population
    ](image/B15916_06_42.jpg)

    图 6.42:按人口比较排名靠前的国家的条形图

11.  Plot a scatter chart of `population_pct` versus `interest_rate_pct` by running the following code:

    ```
    sns.scatterplot(data=df, x='population_pct', y='interest_rate_pct')
    plt.show()
    ```

    使用此代码绘制的散点图显示如下:

    ![Figure 6.43: A scatter chart showing interest rates as a function of population
    ](image/B15916_06_43.jpg)

    图 6.43:散点图显示了利率作为人口的函数

12.  Select the rows that correspond to the outlier points along the population axis shown in the preceding chart by running the following command:

    ```
    df[df['population_pct'] > 15]
    ```

    以下是显示异常值的输出:

    ![Figure 6.44: The countries that have over 15% of the world population
    ](image/B15916_06_44.jpg)

    图 6.44:占世界人口 15%以上的国家

    注意

    为了表示的目的，前面的数据帧被裁剪。你可以在[https://packt.live/2ACHg63](https://packt.live/2ACHg63)查阅完整的数据框。

13.  Plot a scatter chart of `average_inflation_rate_pct` versus `interest_rate_pct` by running the following code:

    ```
    sns.scatterplot(data=df, x='average_inflation_rate_pct', \
                    y='interest_rate_pct')
    plt.show()
    ```

    显示的散点图如下所示:

    ![Figure 6.45: A scatter chart showing interest rates as a function of average inflation
    ](image/B15916_06_45.jpg)

    图 6.45:一个散点图，显示了利率作为平均通货膨胀的函数

    本次活动到此结束。但是，我们可以进一步演示一旦数据被加载、浏览和可视化后可以执行的建模类型。让我们看看如何使用前面的数据来拟合 k-means 聚类模型。k-means 聚类没有在这些章节中涉及，因为它超出了本书的范围；但是，我们鼓励您遵循该准则，并将其作为进一步学习和探索的基础。我们将在这里使用的一些工具和技术是不熟悉的，但是您可以进行自己的研究来加深理解。步骤如下:

14.  Use scikit-learn's `StandardScaler` class to prepare the features for modeling by running the following code:

    ```
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    features = ['population_pct', 'interest_rate_pct', \
                'average_inflation_rate_pct']
    X = df[features].values
    X_scaled = scaler.fit_transform(X)
    df['scaled_population_pct'] = X_scaled[:,0]
    df['scaled_interest_rate_pct'] = X_scaled[:,1]
    df['scaled_average_inflation_rate_pct'] = X_scaled[:,2]
    ```

    在导入和实例化 scaler 类之后，我们从 DataFrame 中选择我们的特征值，并将它们输入到`fit_transform`方法中。通过这样做，缩放器了解数据的样子，并根据底层算法确定如何适当地缩放数据。它返回我们的特征的缩放版本，然后映射回数据帧。

15.  现在，让我们训练一个关于`population_pct`、`interest_rate_pct`和`average_inflation_rate_pct`特征的聚类模型。我们将从下面的代码开始:

    ```
    from sklearn.cluster import KMeans
    clf = KMeans(n_clusters=5)
    clf.fit(X_scaled)
    ```

16.  通过运行以下命令来标记训练数据的每一行:

    ```
    df['kmeans_cluster'] = clf.predict(X_scaled)
    ```

17.  Plot a scatter chart of `population_pct` versus `interest_rate_pct`, where colors are assigned according to clusters, by running the following code:

    ```
    sns.scatterplot(data=df, x='population_pct', \
                    y='interest_rate_pct', hue='kmeans_cluster',)
    plt.show()
    ```

    这将显示以下散点图:

    ![Figure 6.46: Segmenting the interest rate versus population chart with KMeans clusters
    ](image/B15916_06_46.jpg)

    图 6.46:用 k 均值聚类分割利率与人口图表

18.  Plot a scatter chart of `average_inflation_rate_pct` versus `interest_rate_pct`, where colors are assigned according to clusters, by running the following code:

    ```
    sns.scatterplot(data=df, x='average_inflation_rate_pct', \
                    y='interest_rate_pct', hue='kmeans_cluster',)
    plt.show()
    ```

    这将显示以下散点图:

    ![Figure 6.47: Segmenting the interest rate versus inflation rate chart with KMeans clusters
    ](image/B15916_06_47.jpg)

    图 6.47:用 k 均值聚类分割利率与通货膨胀率图表

19.  For the previous chart, add point sizes corresponding to country populations by running the following code:

    ```
    sns.scatterplot(data=df, x='average_inflation_rate_pct', \
                    y='interest_rate_pct', size='population_pct', \
                    hue='kmeans_cluster', sizes=(20, 500),)
    plt.xlim(-1, 15)
    plt.ylim(-1, 15)
    plt.show()
    ```

    生成的散点图如下所示:

    ![Figure 6.48: Overlaying population data of the KMeans segmented chart of interest rates versus inflation rates
    ](image/B15916_06_48.jpg)

    图 6.48:利率与通货膨胀率的克均值分段图的叠加人口数据

20.  Create a three-dimensional version of the preceding chart by plotting the scatter chart of `average_inflation_rate_pct` versus `interest_rate_pct` versus `population_pct` by defining the following function:

    ```
    def render_3d_plot():
        from mpl_toolkits.mplot3d import Axes3D
        import matplotlib.pyplot as plt
        plt.rcParams['figure.figsize'] = (9, 9)
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(df['interest_rate_pct'], \
                   df['average_inflation_rate_pct'], \
                   df['population_pct'], \
                   c=df['kmeans_cluster'].values,)
        ax.set_xlabel('interest_rate_pct')
        ax.set_ylabel('average_inflation_rate_pct')
        ax.set_zlabel('population_pct')
    ```

    使用 Jupyter Notebook 平台时，可以通过运行以下代码来呈现静态版本的绘图:

    ```
    %matplotlib inline
    render_3d_plot()
    ```

    此代码的输出如下所示:

    ![Figure 6.49: Visualizing the KMeans clustering model with a three-dimensional scatter chart
    ](image/B15916_06_49.jpg)

    图 6.49:用 thr 三维散点图可视化 KMeans 聚类模型

21.  When using the Jupyter Notebook platform, an interactive version of the plot can be rendered by running the following code:

    ```
    %matplotlib notebook
    render_3d_plot()
    ```

    为了让它在 JupyterLab 中工作，您可能需要安装`jupyter-matplolib`扩展。你可以在 https://packt.live/2UNrzzQ 的 GitHub 上找到安装说明。安装后，您可以在 JupyterLab 中运行以下代码来呈现交互式图表:

    ```
    %matplotlib widget
    render_3d_plot()
    ```

    输出如下所示:

    ![Figure 6.50: Visualizing the k-Means clustering model with an interactive three-dimensional scatter chart
    ](image/B15916_06_50.jpg)

    图 6.50:用交互式三维散点图可视化 k 均值聚类模型

22.  Save the model using `joblib` by running the following code:

    ```
    import joblib
    joblib.dump(scaler, 'kmeans-5-cluster-scaler.pkl')
    joblib.dump(clf, 'kmeans-5-cluster-model.pkl')
    ```

    为了使用这个模型进行分类，我们需要保存训练好的模型`clf`和训练好的`scaler`。这样，未来的数据可以在我们输入`clf`之前进行适当的缩放。这最后一步总结了额外的材料和完整的活动解决方案。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2ACHg63](https://packt.live/2ACHg63)。

    你也可以在[https://packt.live/2zDrqYu](https://packt.live/2zDrqYu)在线运行这个例子。