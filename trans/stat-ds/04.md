

# 数据挖掘和数据库开发人员

本章向数据开发人员介绍了数据挖掘(不要与查询混淆),提供了对什么是数据挖掘以及为什么它是数据科学不可或缺的一部分的理解。

我们将提供一些工作示例，帮助读者轻松地将 R 用于最常见的统计数据挖掘方法:降维、频繁模式和序列。

在本章中，我们将事情分为以下几个主题:

*   数据挖掘的定义和目的
*   让开发人员为数据挖掘而不是数据查询做好准备
*   使用 R 进行降维、频繁模式和序列挖掘



# 数据挖掘技术

用一个高层次的定义开始解释事情总是谨慎的。

数据挖掘可以简单地解释为以一种可理解的(并且进一步可用的)格式收集关于特定主题或信念的信息。但是请记住，所收集的信息不是数据本身(就像数据查询一样)，而是来自数据的信息(在本章的后面会详细介绍)。

数据挖掘也不应该与分析、信息提取或数据分析相混淆。此外，它可以是手动的或手动的、半自动的或自动的过程。处理新数据时，数据科学家通常会执行手动流程。稍后，当处理同一数据(源)的新版本时，它可能会在某种程度上实现自动化。

数据挖掘是由数据科学家执行的探测，以发现数据中以前未知的信息，例如:

*   模式，如数据记录组，称为**簇**
*   异常记录，称为**异常**
*   关联规则或顺序模式形式的依赖关系

这种新的信息(或见解)可以被认为是一种数据汇总，可以用于进一步的分析，例如，机器学习和预测分析。例如，通过数据挖掘，数据科学家可以识别各种组，然后决策支持系统可以使用这些组来获得更准确的预测结果。

数据开发人员可以将从数据挖掘中获得的洞察力比作描述性或结构性元数据，在行业内被理解为定义数据的数据。

一旦探测完成，数据科学家挖掘了信息，那么在给定工作目标的情况下，该信息必须被转换成可理解和可用的结构。

数据收集、数据准备、来自解释和可视化的结果以及报告不是数据挖掘的一部分。



# 常用技术

一些最常见、最广泛接受和使用的数据挖掘(统计)分析方法将在以下小节中进行解释。



# 形象化

平均值、变化量、计数、百分比、交叉制表和简单相关性的可视化有助于数据科学家理解数据的结构。这也被称为**数据剖析**。

区域、时态、多维和层次是数据可视化的典型、常用和易于理解的格式。



# 聚类分析

数据科学家使用聚类分析将数据变量放入定义的集合(即聚类)中，作为汇总数据的一种方式。集群应该是内部同质的(变量彼此相似)和外部异质的(变量不像其他集群的成员)。

层次凝聚法、划分法和基于模型法都是聚类分析的常用方法。



# 相关分析

相关性分析是数据科学家测量两个数据变量之间关系的一种方法。这就产生了所谓的**相关系数**，它显示了一个变量(自变量)的变化是否会导致另一个变量(因变量)的变化。

常见的相关性方法有正/负、线性和非线性。



# 判别分析

当没有明显的组的自然排序时，使用判别分析来确定数据变量是否是成员。通过这种方法，在数据变量的分类或分组过程中使用具有特定分数或度量的预定组。

**Linear discriminant analysis** (**LDA**) is one of the most common approaches to discriminate analysis where the data scientist attempts to find a linear combination of features that characterize or separates a data variable (into groups).

# 要素分析

因子分析有助于理解(一组数据变量之间)关联的原因。主要目标是尝试减少变量的数量，并检测它们之间关系的结构(这种方法也导致总体数据减少)。

数据科学家使用的因子分析类型有主成分、常见、图像、alpha 和因子回归。



# 回归分析

回归分析使用两个或多个定量变量之间的关系，这样一个变量(因变量)可以从其他变量(自变量)中预测出来。

回归分析有很多种，包括简单线性、多元线性、曲线和多元曲线，以及逻辑回归模型。



# 逻辑分析

逻辑分析是一种在响应变量为二元或定性变量时使用的方法，它试图使用最大似然法找到最佳拟合方程，从而在给定拟合回归系数的情况下最大化获得观察结果的概率。

一些常见的逻辑回归模型包括简单、多元、多同态和泊松逻辑回归模型。



# 目的

通过数据挖掘的实践，数据科学家可以实现(前面提到的)从长信息或数据中获取可操作信息的目标。

有人说数据挖掘的目标是发现非结构化(数据)中的结构。例如，您可以使用数据挖掘来识别客户群，以设计针对高价值客户的促销活动或库存控制计划，从而确保产品保质期较短。

人们可能会混淆数据查询和数据挖掘。但是，如果我们考虑为一家新开的家装商店生成库存控制计划的例子，那么通过简单地查询销售交易来确定过去几个月(从其他商店位置)销售最快的产品，我们可能不会成功。然而，挖掘人口统计信息可能会产生更好的结果，因为我们可能会在数据中识别有效的新颖、潜在有用和可理解的相关性和模式，然后可以用于预测当地消费者的购买。换句话说，数据挖掘的目标或目的不是报告，而是揭示。

在下一节中，我们将进一步了解数据挖掘和数据查询之间的区别。



# 挖掘与查询

数据查询是询问特定的结构化数据问题以寻找特定答案的过程，而数据挖掘是使用统计算法筛选数据以确定模式和关系的过程。

下面的矩阵可以帮助数据开发人员理解数据查询和数据挖掘之间的区别:

| **例子** | **数据查询或挖掘** |
| 上个月全球售出的技术书籍总数是多少？ | 数据查询 |
| 哪些因素影响了上个月全球销售的技术书籍的类型？ | 数据挖掘技术 |
| 上个季度卖出了多少本不同技术的技术书？ | 数据查询 |
| 哪些技术是作为一套技术的一部分购买的？ | 数据挖掘技术 |
| 一项技术倾向于购买硬拷贝还是电子版本？ | 数据挖掘技术 |
| 哪些技术书籍有回头客？ | 数据挖掘技术 |
| 总体而言，哪本技术书籍销量最高？ | 数据查询 |

同样，数据查询是关于报告事件的结果，而数据挖掘是识别关系的过程，这可能有助于理解哪些因素影响了那些事件的结果，或者它们可用于预测类似事件的未来结果。



# 为数据挖掘选择 R

虽然有许多好的选项可供选择，但 R 是一种学习曲线较短的语言和环境，本质上非常灵活，并且非常专注于统计计算，这使得它非常适合于操作、清理、汇总、生成概率统计等(以及实际上用您的数据创建可视化)；因此，对练习数据进行挖掘是一个很好的选择。

此外，这里还有一些在数据挖掘项目中学习和使用 R 的理由:

*   许多学术统计学家都使用 r，所以它是一个不会消失的工具
*   r 基本上是独立于平台的；你开发的东西几乎可以在任何地方运行
*   r 有很棒的帮助资源。谷歌一下就知道了！

为了说明这一点，我们将在本章的剩余部分使用 R 编程语言探索一些实际的数据挖掘例子。



# 形象化

首先，让我们看看如何使用 r 创建一个简单的数据可视化。在这个用例场景中，我们从一家理论上的医院收集数据，然后通过在线调查收集入院和患者病史信息。在提供治疗的同时，信息也被添加到患者的档案中。该文件包括许多字段，包括:

*   患者的基本描述性数据，如性别、出生日期、身高、体重、血型等
*   生命统计数据，如血压、心率等
*   病史，如医院就诊次数、手术次数、重大疾病或状况、当前正在接受医生治疗等
*   人口统计，如职业、家庭所在地、教育背景等
*   文件中还收集了一些额外的信息，以发展患者的特征和习惯，例如患者在他或她的每周饮食中包括牛肉、猪肉和家禽的次数，他或她是否通常使用黄油替代产品，等等

假设我们没有得到关于数据的进一步信息(除了一个简短的字段名列表和医院人员在患者入院时捕获数据的知识)，下一步将执行一些数据挖掘，即识别或分组数据，并可能定位变量之间的关系。

首先，我们可以将医院调查数据读入一个 R 数据框，然后使用两个可用的 R 函数来显示关于我们文件的信息:

![](assets/c17b8bdc-1859-4b64-8e34-67adfe67a348.png)

这里显示的代码将我们的文本文件(名为`Chapter4.txt`)读入一个 R 数据帧(也名为`chapter4`)，然后使用函数`dim`和`names`。`dim`函数向我们展示了文件的数据结构(该文件中有`5994`条记录或案例和`107`个数据点或变量，如我们刚刚看到的截图所示)。`names`函数简单地列出了我们文件中的所有字段或变量名(部分显示在我们刚刚看到的截图中)。

R function attributes 和`str`也是一些非常有用的 R 数据挖掘函数，值得读者花时间进一步研究和试验。

最初，数据科学家可能会从查看字段名开始寻找一些想法；也许常见的群体，如性别、年龄和州(如今，保险也是一个非常有趣的属性！).



# 当前吸烟者

通常，数据科学家在执行数据挖掘时心中有一个目标。因此，在这个例子中，让我们假设我们感兴趣的是将吸烟患者分成不同的年龄组。使用变量`current_smoker`，我们可以使用 R 表函数并运行下面的代码:

```
table(chapter4["current_smoker"]) 
```

这将产生以下信息:

![](assets/03bc2b88-0a16-4fed-9a37-42e946cbe88a.png)

从这里显示的结果来看，似乎不吸烟者(`5466`)比吸烟者(`528`)多，至少在这个文件或人群中是这样。

接下来，我们希望看到(也就是可视化)的是我们人群中按年龄分组的吸烟患者。要做到这一点，作为一名数据科学家，合乎逻辑的下一步是理解出现在`age`变量中的值的`range`。换句话说，我们数据挖掘工作的一部分将是查看我们人群中最年轻患者的年龄，以及最年长患者的年龄。我们可以使用 R range 函数来查找这些信息，而不必对数据进行分割，如下面的屏幕截图所示:

![](assets/c195d470-d7a7-4b21-80ff-56f740902f49.png)

从这些结果中，数据科学家现在可以看到，我们的病例的患者年龄范围从 1 岁到 99 岁！另一个好主意是将我们患者的年龄频率可视化。数据科学家可能希望再次使用 R 表函数来创建直方图:

```
hist(table(Chapter4["age"]))
```

上述代码的输出是:

![](assets/e8f2bf7f-f955-4dc2-9ab2-8a32226ae45d.png)

此 R 代码将生成以下可视化效果，这将为我们的患者年龄提供更多可见性:

![](assets/c5fc8996-9db1-4cb8-9ad4-0dbec85f7254.png)

另一个有趣的信息是密度估计。不需要太多努力，我们就可以嵌套三个 R 函数，`plot`、`density`和`table`，来创建另一个病人年龄的视图。

我们可以运行下面的代码:

![](assets/60b3d4e8-c31e-49e7-bdba-3ed218f5d83c.png)

这将生成以下可视化效果:

![](assets/5f53daff-9b86-4932-9824-b53a92c22efd.png)

鉴于所有这些新发现的知识，数据科学家可能会想继续将我们的案例分成六个不同的年龄组:

*   22 岁以下
*   22 至 34 岁
*   35 至 44 岁
*   45 至 54 岁
*   55 至 64 岁
*   65 岁及以上

为了开始使用数据科学家的语言，数据开发人员应该开始使用单词 cases 而不是 records(在文件中)和 population 而不是 file。

下面的 R 程序代码根据记录的年龄将我们的案例分为当前吸烟者，然后创建一个简单的饼图来可视化结果:

```
# -- read our data into a data frame object 
Chapter4<-read.csv('c:/chapter4/Chapter4.txt') 

# -- initialize holders for counting cases 
a1 <-0;a2 <-0;a3 <-0;a4 <-0;a5 <-0;a6 <-0 
# -- read through the cases and count smokers by age group 
for(i in 2:nrow(Chapter4)) 
{ 
if (as.numeric(Chapter4[i,"age"]) < 22 & Chapter4[i,"current_smoker"]=="Yes") {a1 <- a1 + 1} 
if (as.numeric(Chapter4[i,"age"]) > 21 & as.numeric(Chapter4[i,"age"]) < 35 & Chapter4[i,"current_smoker"]=="Yes") {a2 <- a2 + 1} 
if (as.numeric(Chapter4[i,"age"]) > 34 & as.numeric(Chapter4[i,"age"]) < 45 & Chapter4[i,"current_smoker"]=="Yes") {a3 <- a3 + 1} 
if (as.numeric(Chapter4[i,"age"]) > 44 & as.numeric(Chapter4[i,"age"]) < 55 & Chapter4[i,"current_smoker"]=="Yes") {a4 <- a4 + 1} 
if (as.numeric(Chapter4[i,"age"]) > 54 & as.numeric(Chapter4[i,"age"]) < 65 & Chapter4[i,"current_smoker"]=="Yes") {a5 <- a5 + 1} 
if (as.numeric(Chapter4[i,"age"]) > 64) {a6 <- a6 + 1} 
} 

# -- build a pie chart 
slices <- c(a1, a2, a3, a4, a5, a6) 
lbls <- c("under 21", "22-34","35-44","45-54","55-64", "65 & over") 

# -- create the actual visualization 
pie(slices, labels = lbls, main="Smokers by Age Range") 
```

下面是我们使用 R `pie`函数生成的简单饼图:

![](assets/cc677925-792b-4f15-a210-09c4f69192d1.png)

# 缺少值

任何分析结果的关键是数据的可用性。

假设在我们的总体中存在缺失值的情况。您可能希望在分析中忽略(或省略)这些案例。与其花时间编写代码来处理这些情况，不如使用方便的 R 通用函数`na`。`na.omit`函数评估文件中的每个案例，如果它缺少任何变量的任何值，它会自动删除该案例。

下面的例子显示了 R 函数`na.omit`和`nrow`在缺少值的文件上的使用:

![](assets/de473aa2-3934-4518-bfa5-57f4f820c384.png)

注意使用`na.omit`前后的行(例)数(丢弃 5 条记录)。

我已经用更新的文件覆盖了对象`Chapter4`；实际上，创建一个新对象是一个好习惯，这样您就可以在任何处理之前和之后对数据进行审计。



# 聚类分析

在下一个示例中，数据科学家想要更仔细地查看我们的案例，但仅限于那些吸烟者。因此，对于 R，让我们首先创建原始病例的子集，仅包括当前吸烟者的病例。正如我们在前面的例子中所做的，在我们创建我们的子集(名为`mysub`)之后，我们将使用 R `nrow`函数来验证我们的新总体中的记录数，这样我们就可以了解我们的新总体中的病例数:

```
# --- create a subset of smokers only cases 
mysub <- subset(Chapter4,Chapter4["current_smoker"]=="Yes") 

# --- confirm the row count 
nrow(mysub) 
```

上述代码的输出是:

![](assets/68a391a3-d805-415b-8bd3-70a2e947658d.png)

根据我们刚刚看到的输出，在我们的新人群中仍有超过 500 个病例。因此，作为一名数据科学家，我们决定从我们的案例中随机抽取一个样本(然后我们将对其执行聚类分析)，然后再次验证我们最新人口中的记录数量。

我们可以使用 R `sample`命令创建一个只有 30 个案例的样本:

```
# --- create a random sample of 30 smokers 
mysample <- mysub[sample(1:nrow(mysub), 30, 
   replace=FALSE),] 
# --- confirm the row count in our random case sample 
nrow(mysample) 
```

上述代码的输出是:

![](assets/3c0859b9-022c-41a4-92b6-0a264ad1186d.png)

最后，我们的数据科学家认为，他现在已经有了足够小的案例样本，很容易处理，所以让我们继续进行聚类分析。正如本章前面提到的，层次凝聚法是最流行的聚类分析技术之一，因此我们将在随机样本案例中使用它。

我们可以使用 R 的`dist`和`hclust`函数的组合对我们的随机案例样本进行层次凝聚聚类分析。`dist`函数计算数据集的距离矩阵，给出任意两个观测值之间的欧氏距离。`hclust`函数对该距离矩阵执行分层聚类。

总之，查看和理解层次聚类分析结果的最简单方法是可视化结果。这种可视化被称为**树状图**(一种经常用来说明集群排列的树形图)，所以我们也将添加该代码:

```
# -- perform the hierarchical cluster analysis 
smokerclust<-hclust(dist(mysample)) 

# -- create results in a dendrogram 
plot(smokerclust)
```

上述代码的输出是:

![](assets/b1fd4a93-df6c-42c1-94c9-a65bb0bac182.png)

此代码示例创建了以下可视化效果:

![](assets/91f8dfde-6227-4ec6-9edf-32c479187cd0.png)

r 提供了一长串选项，用于基于数据和统计创建丰富的可视化。对于数据科学家来说，熟悉这些选项很重要，更重要的是，了解哪种类型的可视化最适合分析目标。



# 维度缩减

聚类旨在根据对属性值的观察，对发现相互关联的数据变量进行分组。但是，如果场景中有大量属性，数据科学家会发现某些属性通常对给定的集群没有意义。在我们在本章前面使用的例子(处理病人案例)中，我们本可以发现这种情况。回想一下，我们只对吸烟者进行了层次聚类分析。这些病例包括许多属性，例如，性别、年龄、体重、身高、无医院访问、心率、状态、关系、保险血型、血压、教育程度、出生日期、当前饮酒者、当前药物治疗、已知过敏、当前医生治疗、曾经动过手术、职业、心脏病发作、风湿热、心脏杂音、动脉疾病等等。

作为一名数据科学家，您可以使用 R 函数`names`，就像我们在本章前面所做的那样，来查看属性的完整列表。

降维是数据科学家试图减少或限制案例中属性或维度数量的过程。这被称为减少考虑中的随机变量的数量，但这解释为根据科学理论简单地从数据文件中删除列。

目前公认且常用的消除维度的方法包括:

*   缺失数据:如果一个变量(列)有许多没有值的事例(记录)，它不会增加多少值；因此，可以删除该列。
*   记住，在本章的前面，我们使用了 R 函数`na.omit`。这个函数在删除整个案例时非常方便；然而，通过降维，我们希望在所有情况下省略整个变量。
*   差异小:就像一个变量有大量的缺失值一样，差异小的变量不会增加值，可以被删除。
*   高度相关:趋势非常相似的数据列也可能携带非常相似的信息。在这种情况下，只需要其中一个。
*   决策树:这是一种可能需要更多工作的技术。这是一种降维方法，数据科学家根据目标属性生成一组决策树，然后使用每个属性的使用统计信息来查找最具信息性的特征(或列)。具有最低统计值的列可能会被删除。
*   **主成分分析** ( **PCA** ):是将一个数据集中的变量转化为一组新的变量称为**主成分**的过程。组件按变量的可能方差排序，仅保留方差最大的组件。
*   向后消除和向前构建:这些技术涉及关注一个或多个变量，然后一次依次删除或添加一个额外的变量，并观察其效果。向后消除通过可容忍的错误率来度量效果，而向前构造通过对性能的效果来度量。



# 计算统计显著性

现在让我们看一个简单的例子，使用数据变量计算方差来确定它是否应该从分析中删除。

同样，使用我们在本章中使用的同一个患者案例，我们可以使用 R 函数`var`来确定我们总体中变量的统计显著性。

R 函数`var`仅适用于数值。

在接下来的代码中，我们使用 R `var`函数来计算名为:

```
"no_servings_per_week_skim_milk". 
```

我们可以看到它的方差百分比很低(它不经常变化，或者没有发现它有很多不同的值，具体情况具体分析):

![](assets/404c6dfb-bd5f-48d9-a877-d3c78726c28b.png)

如果我们查看另一个变量(名为:`No_servings_per_week_regular_or_diet_soda`)的计算方差百分比，我们会发现它的计算方差(比前一个变量)更高:

![](assets/88aff8c5-5d12-4cb1-b94a-4e9436d7102b.png)

最后，如果我们看第三个变量，这个变量叫做`No_servings_per_week_water`，我们得到第三个计算的方差:

![](assets/825297f7-380c-48c4-afe3-ef822cbd85a5.png)

从这些单个方差计算中，我们可以看到这些变量在我们的案例分析中的统计显著性:

| **数据变量** | **计算方差** |
| `No_servings_per_week_skim_milk` | .003160316 |
| `No_servings_per_week_regular_or_diet_soda` | 8.505655 |
| `No_servings_per_week_water` | 24.10477 |

名为`No_servings_per_week_skim_milk`的数据变量当然可以从分析中排除，并且根据我们的数据科学家的容忍水平，名为`No_servings_per_week_regular_or_diet_soda`的数据变量也可以从我们的分析中排除。

使用简单的 R 函数，我们可以将计算的方差数据可视化，以便更好地理解:

![](assets/ff81e81d-e46f-4bdd-b39a-307320bdbedf.png)

因此，我们生成以下可视化:

![](assets/0d9230a8-bf0e-432a-b95a-dab538a5dd62.png)

当我们消除一个变量时，它将从我们总体中的所有案例中被消除。



# 频繁模式化

为了理解统计模式，让我们从思考当一个城市地区受到恶劣天气和潜在危险旅行的威胁时会发生什么开始——所有的当地商店都卖完了面包、牛奶和鸡蛋！

模式化(这是数据挖掘的一个子领域)是一个浏览数据的过程，旨在识别以前未知但可能有用的模式，这些模式由频繁同时发生的事件(如引发面包、牛奶和鸡蛋销售的暴风雨天气事件)或对象(如通常一起购买或捆绑在同一购物车中的产品面包、牛奶和鸡蛋)组成。

模式挖掘是由使用或开发自定义模式挖掘逻辑组成的过程。这种逻辑可以应用于各种类型的数据源(如事务和序列数据库、流、字符串、空间数据、图形等)，以寻找各种类型的模式。

在更高的层面上，数据科学家寻求:

*   有趣的模式
*   频繁模式
*   罕见的图案
*   具有高置信度的模式
*   顶级模式和其他模式

数据中可能存在的一些更具体的模式类型包括:

*   **子图**:在一个图或一组图中发现有趣的图
*   **直接和间接关联**:识别对象或事件之间的耦合或依赖关系；隐式或显式定义
*   趋势:这有时也被称为趋势分析，是收集看似不相关的信息并试图发现一种模式的实践
*   **周期模式**:这被定义为一个元素的特征在一个时期或一组中的趋势或变化
*   **顺序规则**:这是对顺序模式挖掘的一个补充，考虑了一个已识别的模式被遵循的概率
*   **格**:一个偏序集，其中每两个元素有一个唯一的最小上界和一个唯一的最大下界
*   **序列模式**:出现在几个数据序列中的子序列
*   **高效用模式**:高效用模式是那些被确定为具有更高、更大或等于阈值的模式



# 频繁项目设置

基于上一节的想法(寻找频繁模式)是频繁项目设置。对于数据开发人员来说，目前最适用的模式概念是频繁项目设置或查找经常成为组或集合成员的项目。

使用我们在本章前面的暴风雨天气的例子，可以想象在销售交易的文件或数据库中搜索的过程，寻找牛奶、面包和鸡蛋作为一次销售(或一组产品)一起购买的场合(即事件)。

频繁项设置还包括确定在分析中使用的最小支持阈值。这意味着数据科学家将确定组成一个集合的项目的最小出现次数。

再次回到我们的暴风雨天气的例子，如果数据科学家设置了两个要使用的 minsup，那么销售，其中只有两个成员产品存在，将被认为是一个集合或模式。

让我们考虑以下销售交易:

| **销售 ID** | **购买的物品** | **合格(作为频繁项目集)** |
| 销售 1 | 牛奶、面包、鸡蛋 | 是 |
| 销售 2 | 牛奶，土豆 | 不 |
| 销售 3 | 面包、鸡蛋、茶 | 是 |
| 销售 4 | 鸡蛋，橙汁 | 不 |

毫无疑问，最著名的模式挖掘算法是 Apriori 算法，该算法被设计用于交易数据库，以发现商店中顾客所做交易的模式。该算法将用户设置的最小集阈值和包含一组事务的事务数据库作为输入，并输出所有频繁项集。



# 序列挖掘

序列挖掘进一步发展了前面的概念。这是数据科学家用来发现一组模式的过程，这些模式在对象之间是共享的，但它们之间也有特定的顺序。

对于序列挖掘，我们承认存在与已识别序列相关联的序列规则。这些规则定义了模式的对象和顺序。一个序列可以有多个规则。序列规则的支持度可以由数据科学家通过包含该规则的序列数除以序列总数来计算或确定。序列规则的置信度是包含该规则的序列数除以包含其前身的序列数。

总的来说，序列规则挖掘的目标是发现所有支持度和置信度不小于两个阈值的序列规则，这两个阈值由名为 minsup 和 minconf 的用户给出。



# 摘要

在这一章中，我们为数据挖掘提供了一个通用的定义，列出了数据科学家最常用的技术，并陈述了工作的总体目标。还将数据挖掘与数据查询进行了比较，并使用 R 给出了各种工作示例来说明某些关键技术。最后，探讨了降维、频繁模式和序列挖掘的概念。

下一章将从数据开发人员的角度对数据的统计分析进行实际操作介绍，提供描述数据本质、探索数据中呈现的关系、从数据创建汇总模型、证明数据模型的有效性以及在数据开发模型上使用预测分析的说明。