

# 二、确定目标

本章介绍并解释(再次从开发人员的角度)数据科学统计学背后的基本目标，并向读者介绍整本书中使用的重要术语和关键概念(带有解释和示例)。

在这一章中，我们将事情分为以下几个主题:

*   数据科学关键目标入门
*   将统计学引入数据科学
*   统计学和数据科学常用术语



# 数据科学的关键目标

正如在[第 1 章](e74075ca-5c7e-4cb4-b1d9-df4b23809f1e.xhtml)、*从数据开发人员转变为数据科学家*中提到的，如何定义数据科学是一个见仁见智的问题。

我个人喜欢这样的解释，数据科学是一种进步，或者更好地说，是思想或步骤的进化，如下图所示:

![](assets/af5dbf5e-3f4f-441e-a7f2-9515457293f8.png)

这种数据科学发展(如上图所示)由数据科学家跟踪的一系列步骤或阶段组成，包括以下内容:

*   收集数据
*   处理数据
*   探索和可视化数据
*   分析(数据)和/或(对数据)应用机器学习
*   基于获得的洞察力进行决策(或计划)

虽然进展或进化意味着一个连续的旅程，但在实践中，这是一个极其不稳定的过程；每个阶段都可能激发数据科学家逆转并重复一个或多个阶段，直到他们满意为止。换句话说，可以重复该过程的所有或某些阶段，直到数据科学家确定达到了期望的结果。

例如，在仔细检查生成的可视化(在*探索和可视化数据*阶段)之后，可以确定需要对数据进行额外的处理，或者在任何合理的分析或学习有价值之前需要收集额外的数据。

您可能会粗略地将数据科学过程与敏捷软件开发神话进行比较，在敏捷软件开发神话中，开发人员执行各种任务，分析结果，完成更多工作，再次评审工作，并重复该过程，直到获得期望的结果或成果。

让我们解释一下数据科学发展的每个阶段。



# 收集数据

这应该是显而易见的——没有(至少一些)数据，我们无法执行任何后续步骤(尽管有人可能会争论推论的观点，那是不恰当的。数据科学没有魔法。作为数据科学家，我们不会从任何东西中制造出任何东西。推理(我们将在本章后面定义)至少需要一些数据来开始。

收集数据的一些新概念包括可以从大量来源收集数据，并且数据源的数量和类型每天都在增长。此外，如何收集数据可能需要一个对数据开发人员来说全新的视角；数据科学的数据并不总是来自关系数据库，而是来自机器生成的日志文件、在线调查、性能统计等等；同样，这个列表也在不断变化。

另一个值得思考的问题是——收集数据也涉及到补充。例如，数据科学家可能会确定他或她需要向之前收集、处理和审查的特定应用程序数据池添加额外的人口统计数据。



# 处理数据

数据的处理(或转换)是数据科学家的编程技能发挥作用的地方(尽管您经常会发现数据科学家在其他步骤中执行某种处理，如收集、可视化或学习)。

请记住，数据科学中存在许多方面的处理。最常见的是格式化(和重新格式化)，它涉及机械地设置数据类型、聚合值、重新排序或删除列等活动；清理(或处理数据质量)，它解决缺省值或缺失值、不完整或不合适的值等问题；以及分析，它通过创建对数据的统计理解来为数据添加上下文。

要对数据完成的处理可以很简单(例如，可以是非常简单的手动事件，需要对 MS Excel 工作表中的数据进行重复更新)，也可以很复杂(如使用 R 或 Python 等编程语言)，甚至更复杂(如将处理逻辑编码到例程中，然后可以对新的数据群进行调度和自动重新运行)。



# 探索和可视化数据

在整个数据科学管道流程的这个阶段或步骤中，数据科学家将使用各种方法来更深入地挖掘数据。通常，会创建几个图形表示(同样，手动或通过编程脚本或工具)来强调或验证数据科学家的观察、特定观点或信念。这是整个数据科学过程中的一个重要步骤，因为数据科学家可能会理解应该对数据进行额外的处理，或者需要收集额外的数据，或者原始理论似乎得到了验证。这些发现将导致暂停，思考需要采取的下一步措施。数据科学家是否应该继续进行正式的分析过程，也许是为自动学习创建一个预测模型？或者，科学家是否应该回到先前的步骤，收集额外的(或不同的)数据进行处理？

**Data visualization** is a key technique permitting data scientists to perform analyses, identify key trends or events, and make more confident decisions much more quickly.

# 分析数据和/或对数据应用机器学习

在这一阶段，当数据科学家(受高度的科学好奇心和经验驱动)试图根据观察或他们对数据的理解(到目前为止)的解释来构建一个故事时，会发生相当多的分析。数据科学家继续使用分析或 BI 包(如 Tableau 或 Pentaho，或 R 或 Python 等开源解决方案)对数据进行分割，以创建具体的数据故事线。同样，基于这些分析结果，数据科学家可以选择再次回到先前的阶段，提取新的数据，进行处理和再处理，并创建额外的可视化。在某些时候，当取得适当的进展时，数据科学家可能会决定数据处于可以开始数据分析的点。随着时间的推移，机器学习(本章后面将进一步定义)已经从更多的模式识别练习发展到现在被定义为利用选定的统计方法进行更深入的挖掘，使用该阶段的数据和分析结果来学习和预测项目数据。

数据科学家通过机器学习从数据中提取定量结果并将其表达为每个人(不仅仅是其他数据科学家)都能立即理解的能力是一种非常宝贵的技能，我们将在本书中对此进行更多讨论。



# 基于获得的洞察力决定(或计划)

在这一步中，数据科学家希望以洞察的形式从他们的工作中获得价值。洞察力是通过执行前面描述的阶段获得的，旨在获得对特定情况或现象的理解。这个想法是，这种洞察力可以用来作为输入，以作出更好的决定。

一个有趣的例子是 IBM Watson 支持的 Roztayger 个性匹配过程(在撰写本文时，这还是实验性的),它展示了从数据中挖掘的洞察力的创造性应用。使用你的脸书或推特信息(或者你可以输入一个简短的简历)，沃森将会即时对你的个性进行分析。结果很有趣，也很准确，然后这些见解被用来建议最适合你和你个人风格的设计师标签。

你可以在 http://roztayger.com/match 找到这个特色。个性洞察服务根据一个人的写作方式提取个性特征。您可以使用该服务将个人与其他个人、机会和产品相匹配，或者通过个性化的信息和推荐来定制他们的体验。特征包括五大个性特征、价值观和需求。使用此服务时，建议输入至少 1，200 个单词的文本。

一旦(实时)数据科学分析完成，上述网站不仅提供其建议，还共享其见解背后的数据，显示易于理解、组织有序的结果表格视图，以及引人注目的可视化效果，如下图所示:

![](assets/a16dbddb-e1bb-4822-85a8-ccc431a3a728.png)

这说明了数据科学发展的这一阶段的另一个关键方面，即，一旦数据科学家确定了一种见解，他必须清楚地展示和传达这些数据见解/发现。



# 像数据科学家一样思考

正如我们已经强调的，关于数据科学家是什么和做什么的概念的一致意见仍然刚刚出现。数据科学的整个领域充其量只是粗略的定义。向数据科学过渡不仅是为了了解数据科学涉及哪些技能和概念，然后努力发展这些技能，也是为了找到一个需求与您的技能相匹配的组织或团体。

正如一个数据开发人员需要了解数据操作和访问的最新趋势和工具一样，未来的数据科学家也应该如此。



# 将统计学引入数据科学

根据你的来源和个人信仰，你可能会说:

*统计学是数据科学，数据科学是统计学*。

为了澄清这一点，请注意，有一种流行的观点认为，统计可以被认为是一种研究或过程，包括数据的收集、分析、解释、展示和组织。如你所见，这个定义与我们在本章前一节描述的数据科学过程非常相似。

深入挖掘这个主题，你会发现统计总是涉及(或收集)用于帮助分析和呈现数据的技术或方法(同样，这种理解也可以用于描述数据科学)。

人们普遍认为，数据科学和统计学这两个术语有着相同的含义，至少在某些圈子里是这样。同样，术语和概念的一致性仍在数据科学家中不断发展。



# 常用术语

根据个人经验、研究和各种行业专家的建议，钻研数据科学艺术的人应该抓住每一个机会来理解和掌握以下常见数据科学术语列表，并积累经验:

*   统计人口
*   可能性
*   假阳性
*   统计推断
*   回归
*   适合的
*   分类数据
*   分类
*   使聚集
*   统计比较
*   编码
*   分布
*   数据挖掘技术
*   决策树
*   机器学习
*   争吵和争吵
*   形象化
*   D3
*   正规化
*   评价
*   交叉验证
*   神经网络
*   助推
*   电梯
*   方式
*   局外人
*   预测建模
*   大数据
*   置信区间
*   写作



# 统计人口

您也许可以将统计总体视为一个记录集(或一组记录)。这组或一组记录将是数据科学家对某些实验感兴趣的类似项目或事件。

对于数据开发人员来说，一组数据可能是一个月内所有销售交易的记录集，他们的兴趣可能是向组织的高级管理人员报告一年中哪些产品在哪个时间销售最快。

对于数据科学家来说，人口可能是一个月内所有急诊室入院的记录集，而感兴趣的领域可能是确定急诊室使用的统计人口统计数据。

通常，术语**统计总体**和**统计模型**可以互换使用。再一次，数据科学家继续在他们的通用术语的使用上保持一致。

关于统计总体的另一个要点是，记录集可能是一组(实际上)现有的对象，也可能是一组假设的对象。使用前面的示例，您可以将实际对象作为当月记录的实际销售交易，而将假设对象作为预计、预测或假定(基于观察或经验假设或其他逻辑)在当月发生的销售交易进行比较。

最后，通过使用统计推断(将在本章后面解释)，数据科学家可以选择记录集(或总体)的一部分或子集，旨在表示感兴趣的特定领域的总体。这个子集称为**统计样本**。

如果准确地选择了一个总体样本，就可以从该样本的相应特征中估计出整个总体(样本所来自的)的特征。



# 可能性

概率与支配随机事件的规律有关。
-www.britannica.com

当想到概率时，你会想到可能即将发生的事件以及它们实际发生的可能性。这与统计思维过程不同，统计思维过程包括分析过去事件的频率，试图解释或理解观察结果。此外，数据科学家将关联各种单个事件，研究这些事件的关系。当我们研究它们的概率时，这些不同的事件如何相互联系决定了需要遵循的方法和规则。

概率分布是一种表格，用于显示样本总体或记录集中各种结果的概率。



# 假阳性

假阳性的概念是一个非常重要的统计学(数据科学)概念。假阳性是一个错误或错误的结果。也就是说，在这种情况下，过程或实验的结果表明条件已满足或为真，而实际上条件并不为真(未满足)。这种情况也被一些数据科学家称为假警报，通过考虑记录集或统计总体(我们在本节前面讨论过)的概念，最容易理解这种情况，记录集或统计总体不仅由处理的准确性决定，还由采样总体的特征决定。换句话说，数据科学家在统计过程中犯了错误，或者记录集是一个总体，对于所研究的内容，它没有合适的样本(或特征)。



# 统计推断

哪个开发人员在他或她的职业生涯中，不得不创建一个样本或测试数据？例如，我经常创建一个简单的脚本来生成一个随机数(基于可能的选项或选择的数量)，然后使用该数字作为选中的选项(在我的测试记录集中)。这对于数据开发可能很有效，但是对于统计学和数据科学来说，这是不够的。

为了创建样本数据(或样本总体)，数据科学家将使用一个称为**统计推断**的过程，这是通过分析您已经拥有或试图生成的数据来推断潜在分布选项的过程。这个过程有时被称为**推断统计分析**，包括测试各种假设和得出估计值。

当数据科学家确定某个记录集(或总体)应该比实际大时，就假定该记录集是来自更大总体的样本，然后数据科学家将利用统计推断来弥补差异。

数据科学家将使用中的数据或记录集称为观察数据。推理统计可以与描述统计形成对比，描述统计只关注观察数据的属性，不假设记录集来自更大的群体。



# 回归

回归是一种流程或方法(由数据科学家选择为最适合当前实验的技术)，用于确定变量之间的关系。如果你是一名程序员，你对变量有一定的理解，但是在统计学中，我们用不同的术语。变量被确定为非独立变量或独立变量。

自变量(也称为**预测值**)是由数据科学家在努力确定其与因变量的关系时操纵的变量。因变量是数据科学家正在测量的变量。

在数据科学进展或实验中，拥有多个独立变量并不罕见。

更准确地说，回归是帮助数据科学家理解当任何一个或多个自变量变化而其他自变量保持固定时，因变量(或标准变量)的典型值如何变化的过程。



# 适合的

拟合是衡量统计模型或过程描述数据科学家关于记录集或实验的观察的程度的过程。这些测量将试图指出观察值和可能值之间的差异。模型或过程的可能值称为分布或概率分布。

因此，概率分布拟合(或分布拟合)是指数据科学家将概率分布拟合到一系列关于变量现象重复测量的数据中。

数据科学家执行分布拟合的目的是预测某一时间间隔内某一现象发生的概率或频率。

拟合最常见的用途之一是检验两个样本是否来自相同的分布。

数据科学家可以从众多概率分布中进行选择。有些会比其他的更适合数据的观测频率。给出紧密拟合的分布被认为会导致好的预测；因此，数据科学家需要选择一个非常适合数据的分布。



# 分类数据

前面，我们解释了数据中的变量可以是独立的，也可以是相关的。变量定义的另一种类型是分类变量。这种类型的变量可以采用有限的、通常是固定数量的可能值中的一个，从而将每个人分配到特定的类别。

通常，收集的数据的含义不清楚。分类数据是一种数据科学家可以用来赋予数据意义的方法。

例如，如果收集一个数字变量(假设找到的值是 4、10 和 12)，如果对这些值进行分类，变量的含义就变得很清楚了。让我们假设基于对如何收集数据的分析，我们可以通过指示该数据描述了大学生来对数据进行分组(或分类),并且有以下数量的玩家:

*   4 名网球运动员
*   10 名足球运动员
*   12 名足球运动员

现在，因为我们将数据分类，所以含义变得清晰了。

分类数据的其他一些例子可能是个人宠物偏好(按宠物类型分组)，或车辆所有权(按所拥有的汽车类型分组)，等等。

因此，分类数据，顾名思义，就是归入某种类别或多个类别的数据。一些数据科学家将类别称为数据的子群体。

分类数据也可以是作为是或否答案收集的数据。例如，医院准入数据可以指示患者吸烟或不吸烟。



# 分类

数据的统计分类是确定数据点、观察值或变量应归入哪个类别(在上一节中讨论过)的过程。执行分类过程的数据科学过程被称为**分类器**。

确定一本书是小说还是非小说是一个简单的分类例子。对关于餐馆的数据的分析可能会导致将它们分类成几个类型。



# 使聚集

聚类是将数据事件划分成组或数据集的同类子集的过程，不是像在分类中那样预先确定一组组(如前一节所述),而是通过执行数据科学过程根据在事件中发现的相似性来识别组。

发现同一组(组也称为簇)中的对象比在其他组中(或在其他簇中)发现的那些对象彼此更相似(在某种意义上)。聚类过程在探索性数据挖掘中非常常见，也是统计数据分析的常用技术。



# 统计比较

简而言之，当您听到术语“统计比较”时，通常是指数据科学家执行分析过程来查看两个或更多组或人群(或记录集)的相似性或差异的行为。

作为数据开发人员，您可能熟悉各种实用程序，如 FC Compare、UltraCompare 或 WinDiff，它们旨在为开发人员提供两个或更多(甚至是二进制)文件内容的逐行比较。

在统计学(数据科学)中，这种比较过程是一种比较总体或记录集的统计技术。在这种方法中，数据科学家将进行所谓的**方差分析** ( **ANOVA** )，比较分类变量(在记录集中)，等等。

ANOVA 是一种统计方法，用于分析组均值之间的差异及其相关程序(如组、人群或记录集之间的差异)。这种方法最终演变成了六个适马数据集的比较。



# 编码

编码或统计编码也是数据科学家用来准备数据进行分析的过程。在此过程中，定量数据值(如收入或受教育年限)和定性数据(如种族或性别)都以一致的方式进行分类或编码。

数据科学家出于各种原因执行编码，例如:

*   更有效地运行统计模型
*   计算机理解变量
*   责任——因此数据科学家可以盲目地运行模型，或者不知道变量代表什么，以减少编程/作者偏见

您可以将编码过程想象成将数据转换成系统或应用程序所需形式的方法。



# 分布

统计记录集(或总体)的分布是显示数据的所有可能值(或有时称为区间)及其出现频率的可视化形式。当数据科学家创建分类数据的分布(我们在本章前面定义过)时，它试图显示每个组或类别中个体的数量或百分比。

将早期定义的术语与这个术语联系起来，用简单的术语表述的概率分布，可以被认为是显示实验中不同可能结果出现的概率的可视化。



# 数据挖掘技术

在[第 1 章](e74075ca-5c7e-4cb4-b1d9-df4b23809f1e.xhtml)、*从数据开发人员转变为数据科学家*中，我们说过，利用数据挖掘，人们通常更专注于数据关系(或数据点之间的潜在关系，有时称为变量)和认知分析。

为了进一步定义这一术语，我们可以提到，数据挖掘有时更简单地称为知识发现，甚至只是发现，它基于从新的或不同的角度处理或分析数据，并将其总结为有价值的见解，可用于增加收入、降低成本或两者兼而有之。

使用专用于数据挖掘的软件只是数据挖掘的几种分析方法之一。虽然有专门用于这个目的的工具(比如 IBM Cognos BI 和 Planning Analytics、Tableau、SAS 等等。)，数据挖掘就是在数据的许多领域中寻找相关性或模式的分析过程，使用 MS Excel 或任何数量的开源技术都可以有效地完成这一过程。

数据挖掘的一种常见技术是使用 R 或 Python 等工具创建定制脚本。通过这种方式，数据科学家能够定制逻辑和处理，以满足他们确切的项目需求。



# 决策树

统计决策树使用看起来像树的图表。这种结构试图表示可选的决策路径和每个选择路径的预测结果。数据科学家将使用决策树来支持、跟踪和模拟决策及其可能的后果，包括偶然事件结果、资源成本和效用。这是显示数据科学流程逻辑的常见方式。



# 机器学习

机器学习是数据科学中最有趣和最令人兴奋的领域之一。它唤起了围绕人工智能的所有形式的图像，包括神经网络、**支持向量机** ( **支持向量机**)等等。

从根本上说，我们可以将机器学习这个术语描述为一种训练计算机根据数据，特别是数据中的关系，做出或改进预测或行为的方法。此外，机器学习是一个基于数据中识别的模式进行预测的过程，此外，它是从数据模式中不断学习的能力，因此可以不断做出更好的预测。

经常有人把机器学习的过程误认为是数据挖掘，但数据挖掘更侧重于探索性的数据分析，被称为**无监督学习**。

机器学习可以用于学习和建立各种实体的基线行为简档，然后发现有意义的异常。

这里是令人兴奋的部分:机器学习(使用数据关系进行预测)的过程被称为**预测分析**。

预测分析使数据科学家能够做出可靠、可重复的决策和结果，并通过从数据的历史关系和趋势中学习来发现隐藏的见解。



# 争吵和争吵

术语 **munging** 和**wranging**是流行词汇或行话，意在描述一个人以某种方式影响数据、记录集或文件的格式，以努力为继续或其他处理和/或评估准备数据。

有了数据开发，你最有可能熟悉的就是**提取**、**转换**、**加载** ( **ETL** )。同样，数据开发人员可能会在 ETL 过程的转换步骤中争论数据。

常见的争吵可能包括删除标点符号或 HTML 标签、数据解析、过滤、各种转换、映射，以及将不是专门为互操作而设计的系统和界面捆绑在一起。Munging 还可以描述将原始数据处理或过滤成另一种形式，从而允许在其他地方更方便地使用数据。

在一个数据科学过程中和/或在发展过程的不同步骤中，可能会出现多次争吵和争论。有时，数据科学家使用 munging 来包括各种数据可视化、数据聚合、训练统计模型以及许多其他潜在的工作。就这一点而言，蒙骗和争论可能遵循这样的流程:从提取原始形式的数据开始，使用各种逻辑执行蒙骗，最后将结果内容放入结构中以供使用。

尽管有许多有效的选择来管理和争论数据、预处理和操作，但今天许多数据科学家都喜欢的一个工具是一个名为 **Trifecta** 的产品，它声称它是许多行业中头号(数据)争论解决方案。

Trifecta 可从[https://www.trifacta.com/](https://www.trifacta.com/)下载，进行个人评估。看看吧！



# 形象化

利用数据可视化技术的主要目的(尽管还有其他目标)是让复杂的事情变得简单。您可以将可视化视为创建图形(或类似图形)来传达信息的任何技术。

使用数据可视化的其他动机包括:

*   解释数据或将数据放在上下文中(突出人口统计学)
*   解决特定的问题(例如，识别特定业务模型中的问题区域)
*   探索数据以获得更好的理解或增加清晰度(例如，该数据跨越哪些时间段？)
*   突出显示或说明不可见的数据(例如隔离数据中的异常值)
*   预测，例如潜在的销售量(可能基于季节性销售统计)
*   以及其他等等

统计可视化几乎用于数据科学流程的每个步骤，包括探索和可视化、分析和学习等显而易见的步骤，但也可以在收集、处理和使用已识别见解的最终阶段加以利用。



# D3

D3 或`D3.js`，本质上是一个开源的 JavaScript 库，旨在使用当今的 web 标准可视化数据。D3 利用**可缩放矢量图形** ( **SVG** )、Canvas 和标准 HTML，帮助将生命注入数据。

D3 将强大的可视化和交互技术与数据驱动的 DOM 操作方法相结合，为数据科学家提供了现代浏览器的全部功能以及设计最佳描述目标或假设的正确可视化界面的自由。

与许多其他库相比，`D3.js`允许对数据的可视化进行过度控制。D3 嵌入在 HTML 网页中，使用预先构建的 JavaScript 函数来选择元素、创建 SVG 对象、设计它们的样式或者添加过渡、动态效果等等。



# 正规化

正则化是一种可能的方法，数据科学家可以使用它来改进统计模型或数据科学过程生成的结果，例如在处理统计和数据科学中的过度拟合情况时。

我们在本章前面定义了拟合(拟合描述了统计模型或过程描述数据科学家的观察的程度)。过度拟合是一种统计模型或过程似乎过于拟合或似乎过于接近实际数据的情况。

过度拟合通常发生在过于简单的模型中。这意味着你可能只有两个变量，并根据这两个变量得出结论。例如，使用我们之前提到的水仙花销售额的例子，可以生成一个以温度为自变量，销售额为因变量的模型。你可能会发现这个模型失败了，因为它并不像得出气温升高总会带来更多销售的结论那么简单。

在这个例子中，有一种趋势是将更多的数据添加到流程或模型中，希望获得更好的结果。这个想法听起来很合理。例如，你有平均降雨量、花粉计数、肥料销售等信息；这些数据点可以作为解释变量添加吗？

解释变量是一种有细微差别的自变量。当一个变量是独立的，它根本不受任何其他变量的影响。当一个变量不是确定的独立变量时，它就是一个解释变量。

继续向您的模型添加越来越多的数据会有效果，但可能会导致过度拟合，从而导致糟糕的预测，因为它与数据非常相似，而数据大多只是背景噪声。

为了克服这种情况，数据科学家可以使用正则化，将调整参数(数据点平均值或最小或最大限制等附加因素，使您能够更改模型的复杂性或平滑度)引入数据科学过程，以解决不适定问题或防止过度拟合。



# 评价

当数据科学家评估模型或数据科学流程的性能时，这被称为评估。可以通过多种方式来定义性能，包括模型的学习增长或模型通过额外的经验(例如，使用更多数据样本进行更多轮次的训练)来改进学习(以获得更好的分数)的能力或其结果的准确性。

评估模型或流程性能的一种流行方法叫做**自助抽样**。这种方法检查某些数据子集的性能，重复生成可用于计算精确度(性能)估计值的结果。

bootstrap 抽样方法随机抽取数据样本，将其分成三个文件——一个训练文件、一个测试文件和一个验证文件。模型或流程逻辑是基于培训文件中的数据开发的，然后使用测试文件进行评估(或测试)。这个调整然后测试的过程一直重复，直到数据科学家对测试结果满意为止。在这一点上，模型或者过程被再次测试，这一次使用验证文件，并且结果应该提供它将如何执行的真实指示。

您可以想象使用 bootstrap `sampling`方法来开发程序逻辑，通过分析测试数据来确定逻辑流，然后根据测试数据文件运行(或测试)您的逻辑。一旦您对您的逻辑处理了测试数据中发现的所有条件和异常感到满意，您就可以对一个新的、从未见过的数据文件运行最终测试，以进行最终验证测试。



# 交叉验证

交叉验证是一种评估数据科学流程绩效的方法。主要与预测建模一起使用，以估计模型在实践中的精确程度，人们可能会看到交叉验证用于检查模型可能如何进行归纳，换句话说，模型如何将它从样本推断的结果应用于整个总体(或记录集)。

通过交叉验证，您可以将一个(已知的)数据集确定为您的验证数据集，在该数据集上运行训练，同时还有一个未知数据(或第一次看到的数据)的数据集，模型将根据该数据集进行测试(这被称为您的**测试数据集**)。目标是确保诸如过度拟合(允许非包容性信息影响结果)之类的问题得到控制，并提供对模型将如何概括真实问题或真实数据文件的洞察。

交叉验证过程将包括将数据分成相似子集的样本，对一个子集(称为**训练集**)进行分析，并对另一个子集(称为**验证集**或**测试集**)的分析进行验证。为了减少可变性，交叉验证的多次迭代(也称为**折叠**或**回合**)使用不同的分区来执行，并且验证结果在回合上被平均。通常，数据科学家将使用模型稳定性来确定应该执行的交叉验证的实际次数。



# 神经网络

神经网络也被称为**人工神经网络** ( **神经网络**)，其目标是以与人脑相同的方式解决问题。

谷歌将提供人工智能专家 Maureen Caudill 于 1989 年 2 月在*神经网络初级读本:第一部分中对人工神经网络的如下解释:*

由几个简单、高度互联的处理元件组成的计算系统，这些元件通过对外部输入的动态响应来处理信息。

为了简化神经网络的概念，回想一下软件封装的概念，并考虑一个具有输入层、处理层和输出层的计算机程序。记住这一点，要理解神经网络也是由这些层组成的网络，通常有不止一个处理层。

模式通过输入层呈现给网络，然后输入层与一个(或多个)处理层通信(在处理层完成实际的处理)。然后，处理层链接到显示结果的输出层。

大多数神经网络还将包含某种形式的学习规则，该学习规则根据提供给它的输入模式来修改连接的权重(换句话说，网络学习哪些处理节点执行得更好并给予它们更重的权重)。以这种方式(在某种意义上)，神经网络通过示例进行学习，就像孩子通过接触猫的示例来学习识别猫一样。



# 助推

从某种意义上来说，boosting 是数据科学中普遍接受的用于提高弱学习数据科学过程的准确性的过程。

被定义为弱学习者的数据科学过程是那些产生的结果仅比你随机猜测的结果略好的过程。弱学习者基本都是阈值或者 1 级决策树。

具体来说，boosting 旨在减少监督学习中的偏差和差异。

我们所说的偏差和方差是什么意思？在进一步讨论提升之前，我们先来看看偏差和方差是什么意思。

数据科学家将偏见描述为数据收集过程中存在的偏袒水平，导致不均衡、不诚实的结果，并可能以各种不同的方式发生。一个`sampling`方法被称为**偏向**，如果它系统地偏向某些结果。

方差可以(由数据科学家)简单地定义为与变量均值的距离(或一个结果与平均值的距离)。

boosting 方法可以描述为数据科学家重复运行数据科学过程(已被确定为弱学习过程)，每次迭代对从原始群体记录集中采样的不同随机数据示例运行。然后将每次运行产生的所有结果(或分类器或残差)合并成一个合并的结果(即梯度)。

这种在每次迭代中使用原始记录集的随机子集的概念源于 bagging 中的 bootstrap 采样，并且对组合模型具有类似的方差减少效果。

此外，一些数据科学家认为 boosting 是一种将弱学习者转化为强学习者的方法；事实上，对一些人来说，提高的过程仅仅意味着把一个弱学习者变成一个强学习者。



# 电梯

在数据科学中，术语“提升”将记录集或总体中观察到的模式的频率与您预期在数据中偶然或随机出现的相同模式的频率进行比较。

如果提升非常低，那么数据科学家通常会认为所识别的模式很有可能是偶然出现的。升力越大，图案越有可能是真实的。



# 方式

在统计学和数据科学中，当数据科学家使用术语模式时，他或她指的是在数据样本中最常出现的值。模式不是计算出来的，而是手动或通过数据处理确定的。



# 局外人

异常值可以定义如下:

*   一个与其他数据相差甚远的数据点
*   那段不符合的数据
*   要么是非常高的值，要么是非常低的值
*   数据中的异常观察
*   远离所有其他观察点的观察点



# 预测建模

开发统计模型和/或数据科学流程来预测未来事件被称为**预测建模**。



# 大数据

同样，我们对大数据的定义有所不同。庞大的数据集合，如此庞大或复杂的数据集，以至于传统的数据处理应用程序无法胜任，以及有关我们生活方方面面的数据都被用来定义或指代大数据。2001 年，Gartner 分析师 Doug Laney 提出了 3V 的概念。

可以参考链接:[http://blogs . Gartner . com/Doug-Laney/files/2012/01/ad 949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-variety . pdf</span>](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf)

按照 Laney 的说法，3V 是数量、种类和速度。V 构成了大数据的维度:容量(或可测量的数据量)、多样性(指数据类型的数量)和速度(指处理或处理数据的速度)。



# 置信区间

置信区间是一个数值范围，数据科学家将围绕一个估计值指定该范围，以表明它们的误差幅度，并结合一个值落在该范围内的概率。换句话说，置信区间是未知总体参数的良好估计。



# 写作

尽管在展示数据科学流程或预测模型的输出或结果时，可视化吸引了更多的注意力，但写作技能仍然不仅是数据科学家沟通方式的重要组成部分，而且仍然被视为所有数据科学家取得成功的基本技能。



# 摘要

在本章中，我们说过，目前，如何定义数据科学是一个见仁见智的问题。一种实用的解释是，数据科学是一种进步，或者更好的说法是思想的进化，包括收集、处理、探索和可视化数据，分析(数据)和/或应用机器学习(对数据)，然后根据获得的洞察力做出决策(或规划)。

然后，为了像数据科学家一样思考，我们引入并定义了一些数据科学家应该熟悉的常用术语和概念。

在下一章中，我们将介绍和解释数据开发人员如何使用几种常见的统计方法来理解和处理数据清理的主题。