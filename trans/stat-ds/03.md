

# 开发人员的数据清理方法

本章讨论了开发人员如何使用几种常见的统计方法来理解和处理**数据清理**的主题。

在本章中，我们将事情分为以下几个主题:

*   了解基本数据清理
*   使用 R 来检测和诊断常见的数据问题，比如缺失值、特殊值、异常值、不一致和本地化
*   使用 R 处理高级统计情况，如转换、演绎校正和确定性插补



# 了解基本数据清理

在任何统计项目中，拥有干净(因此可靠)的数据的重要性怎么强调都不为过。即使有可靠的统计实践，脏数据也可能是不可靠的，并可能导致产生建议采取不正确行动的结果，甚至可能造成伤害或经济损失。有人说，数据科学家花了将近 90%的时间清理数据，只有 10%的时间用于数据的实际建模和从中得出结果。

那么，什么是数据清理呢？

数据清理也称为数据清理或数据清理，涉及检测和处理数据群体中的错误、遗漏和不一致的过程。

这可以用数据争论工具交互完成，或者通过脚本以批处理方式完成。我们将在本书中使用 R，因为它非常适合数据科学，因为它甚至可以处理非常复杂的数据集，允许通过各种建模函数处理数据，甚至提供生成可视化的能力，以仅用几行代码来表示数据并证明理论和假设。

在清理过程中，您首先使用逻辑来检查和评估您的数据池，以建立数据的质量水平。数据质量水平会受到数据输入、存储和管理方式的影响。清理可能涉及纠正、替换或删除数据点或整个实际记录。

清洗不应该与验证混淆，因为它们彼此不同。验证过程是一个“通过”或“失败”的过程，通常在数据被捕获时(输入时间)发生，而不是在为预期目的做准备时对数据执行的操作。

作为数据开发人员，应该对数据清理的概念或提高数据质量水平的重要性并不陌生。数据开发人员也会同意，处理数据质量的过程需要对数据进行例行和定期的审查和评估，事实上，大多数组织都有企业工具和/或过程(或者至少是策略)来例行地预处理和清理企业数据。

如果您感兴趣，有相当多的免费和付费工具可供选择，包括 iManageData、Data Manager、Data preparator(Trifecta)Wrangler 等等。从统计学的角度来看，首选应该是 R、Python 和 Julia。

在解决数据中的特定问题之前，您需要检测它们。检测它们需要您根据您的目标环境来确定什么是问题或错误(在本节的后面会有更多介绍)。



# 常见的数据问题

我们可以将数据困难分为几类。最普遍接受的(数据问题)分组包括:

*   **准确性**:数据不准确有很多种，最常见的例子包括糟糕的数学、超出范围的值、无效值、重复等等。
*   **完整性**:数据源可能缺少特定列的值，缺少整个列，甚至缺少完整的事务。
*   **更新状态**:作为质量保证的一部分，您需要确定数据刷新或更新的频率，并能够确定数据上次保存或更新的时间。这也称为延迟。
*   相关性:根据你的目标，识别和剔除你不需要或不关心的信息。例如，如果你打算研究个人美容产品，可以删除泡菜的销售交易。
*   **一致性**:经常需要交叉引用或翻译来自数据源的信息。例如，记录的对患者调查的响应可能需要转换成单个一致的指标，以便以后使处理或可视化更容易。
*   **可靠性**:这主要是为了确保数据收集的方法能够得到一致的结果。常见的数据保证流程包括建立基线和范围，然后定期验证数据结果是否符合既定的预期。例如，如果数据突然变成 100%单一党派，通常既有登记的民主党选民又有登记的共和党选民的地区将有权进行调查。
*   **适当性**:如果数据适合于预期目的，则认为数据是适当的；这可能是主观的。例如，假日交通影响购买习惯被认为是一个事实(美国国旗纪念日周的增加并不表明平均或预期的每周行为)。
*   **可访问性**:感兴趣的数据可能会在你不感兴趣的数据海洋中被冲淡，从而降低感兴趣的数据的质量，因为这些数据大多是不可访问的。这在大数据项目中尤为常见。此外，安全性也可能对数据质量产生影响。例如，特定的计算机可能被排除在捕获的日志文件之外，或者某些与健康相关的信息可能被隐藏而不是共享患者数据的一部分。



# 上下文数据问题

许多前面提到的数据问题可以被自动检测甚至纠正。这些问题最初可能是由用户输入错误、传输或存储损坏或不同数据源中类似实体的不同定义或理解引起的。在数据科学中，需要考虑的更多。

在数据清理期间，数据科学家将根据关于数据上下文及其预期目的的假设或设想，尝试识别数据中的模式。换句话说，数据科学家确定的任何数据，无论是明显与数据的假设或目标脱节，还是明显不准确，都将得到处理。这个过程依赖于数据科学家的判断和他或她的能力，以确定哪些点是有效的，哪些是无效的。

当依赖人类判断时，数据科学家的假设/假设中没有充分考虑的有效数据点总是有可能被忽略或被错误地处理。因此，维护已清理数据的适当标记版本是一种常见的做法。



# 清洁技术

通常，数据清理过程围绕识别那些异常值的数据点，或者那些不遵循数据科学家看到的或感兴趣的数据中的模式的数据点展开。

数据科学家使用各种方法或技术来识别数据中的异常值。一种方法是绘制数据点，然后目视检查结果图中那些远离总体分布的数据点。另一种技术是以编程方式消除不符合数据科学家的数学控制限制(基于统计项目的目标或意图的限制)的所有点。

其他数据清理技术包括:

*   **有效性检查**:有效性检查包括对数据应用规则以确定其是否有效。这些规则可以是全局的；例如，如果特定的唯一键是数据池的一部分(例如，社会保险号不能重复)，或者当字段值的组合必须是某个值时，数据科学家可以执行唯一性检查。验证可能是严格的(如删除缺少值的记录)或模糊的(如更正与现有已知值部分匹配的值)。
*   **增强**:这是一种通过添加相关信息使数据完整的技术。附加信息可以通过使用数据文件中的现有值来计算，也可以从其他来源添加。这些信息可用于参考、比较、对比或显示趋势。
*   **协调**:通过数据协调，数据值被转换，或者映射到其他更理想的值。
*   **标准化**:这包括将参考数据集更改为新标准。例如，使用标准代码。
*   **领域专业知识**:这涉及到根据数据科学家以前的经验或最佳判断删除或修改数据文件中的数据值。

在本章的下一节中，我们将逐一介绍这些技术的例子。



# r 和常见数据问题

让我们从 R 的一些背景知识开始这一部分。R 是一种易于学习的语言和环境，本质上非常灵活，并且非常专注于统计计算，这使它成为操纵、清理、汇总、生成概率统计等的一个很好的选择。

此外，这里还有一些使用 R 进行数据清理的理由:

*   它被大量的数据科学家使用，所以它不会很快消失
*   r 是独立于平台的，所以你创建的东西几乎可以在任何地方运行
*   r 有很棒的帮助资源——只要谷歌一下，你就知道了！



# 极端值

对异常值的最简单解释可能是，异常值是那些不适合您的其余数据的数据点。根据观察，任何非常高、非常低或不寻常的数据(在您的项目环境中)都是异常值。作为数据清理的一部分，数据科学家通常会识别异常值，然后使用普遍接受的方法处理异常值:

*   删除异常值，甚至删除存在异常值的实际变量
*   转换值或变量本身

让我们看一个使用 R 来识别并处理数据异常值的真实例子。

在游戏世界中，吃角子老虎机(一种通过将硬币投入投币口并拉动手柄来决定回报的赌博机器)非常受欢迎。今天大多数吃角子老虎机都是电子的，因此它们的所有活动都被连续跟踪。在我们的示例中，赌场的投资者希望使用这些数据(以及各种补充数据)来调整他们的盈利策略。换句话说，什么才是有利可图的老虎机？是机器的主题还是类型？新机器比旧机器或复古机器更赚钱吗？机器的物理位置呢？更低面额的机器更有利可图吗？我们试图用异常值来寻找答案。

我们得到一个游戏数据集合或池(格式为逗号分隔或 CSV 文本文件)，其中包括数据点，如老虎机的位置、面额、月、日、年、机器类型、机器年龄、促销、优惠券、天气和投币(即投入机器的总金额减去支出)。作为数据科学家，我们的第一步是审查(有时称为 **profile** )数据，我们将确定是否存在任何异常值。第二步是解决这些异常值。



# 步骤 1–分析数据

r 使得这一步非常简单。尽管有许多方法来编写解决方案，但让我们尽量减少实际程序代码或脚本的行数。我们可以首先在 R 会话(名为`MyFile`)中将 CSV 文件定义为一个变量，然后将文件读入 R `data.frame`(名为`MyData`):

```
MyFile <-"C:/GammingData/SlotsResults.csv" 
MyData <- read.csv(file=MyFile, header=TRUE, sep=",") 
```

在统计学中，`boxplot`是获取关于统计数据集的形状、可变性和中心(或中值)的信息的简单方法，因此我们将使用`boxplot`和我们的数据，看看我们是否可以识别中值`Coin-in`以及是否有任何异常值。为此，我们可以使用`boxplot`函数，让 R 为文件中的每台老虎机绘制`Coin-in`值:

```
boxplot(MyData[11],main='Gamming Data Review', ylab = "Coin-in") 
```

`Coin-in` is the 11th column in our file so I am referring to it explicitly as a parameter of the function `boxplot`. I've also added optional parameters (again, continuing the effort to stay minimal) which add headings to the visualization.

执行前面的脚本会产生下面的视觉效果。注意中间值(由穿过`boxplot`中方框的线显示)和四个异常值:

![](assets/0a587ec6-7715-4c0b-9ba0-69cf75ef54c5.png)

# 步骤 2–解决异常值

现在我们看到异常值确实存在于我们的数据中，我们可以解决它们，这样它们就不会对我们的研究产生负面影响。首先，我们知道负的`Coin-in`值是不合逻辑的，因为机器不能分发更多的硬币。根据这个规则，我们可以简单地从文件中删除任何具有负的`Coin-in`值的记录。同样，R 使它变得简单，因为我们将使用`subset`函数来创建我们的`data.frame`的新版本，它只包含具有非负`Coin-in`值的记录(或案例)。

我们称我们的`subset`数据帧为`noNegs`:

```
noNegs <- subset(MyData, MyData[11]>0) 
```

然后，我们将重新绘制以确保我们已经丢弃了负面的异常值:

```
boxplot(noNegs[11],main='Gamming Data Review', ylab = "Coin-in")
```

这产生了一个新的`boxplot`，如下面的截图所示:

![](assets/3612ac94-b612-443e-b700-7f16873846d0.png)

我们可以使用相同的方法，通过创建另一个`subset`然后重新绘制，来降低我们的极端正值`Coin-in`值(大于 1500 美元的值):

```
noOutliers <-subset(noNegs, noNegs[11]<1500) 
boxplot(noOutliers[11],main='Gamming Data Review', ylab = "Coin-in") 
```

当您对数据进行各种迭代时，最好保存大多数(如果不是最重要的)数据版本。您可以使用 R 功能`write.csv`:

```
write.csv(noOutliers, file = "C:/GammingData/MyData_lessOutliers.csv") 
```

大多数数据科学家采用一个通用的命名约定，在整个项目中使用(如果不是所有项目)。你的文件名应该尽可能的清晰，以节省你以后的时间。此外，尤其是在处理大数据时，您需要注意磁盘空间。

上述代码的输出如下:

![](assets/b6de9158-2f91-40e0-9715-3b1abd22039d.png)

# 领域专业知识

接下来，另一种数据清理技术被称为基于领域专业知识的数据清理。这不需要复杂。这种技术的要点是简单地使用数据中没有的信息。例如，以前我们排除了具有负的`Coin-in`值的情况，因为我们知道不可能有负的`Coin-in`量。另一个例子可能是飓风桑迪袭击美国东北部的时候。在那段时间里，大多数机器的案例都有很低的`Coin-in`(如果不是零的话)数量。数据科学家可能会根据该信息删除特定时间段内的所有数据案例。



# 有效性检查

正如我在本章前面提到的，交叉验证是指数据科学家将规则应用于数据池中的数据。

有效性检查是统计数据清理最常见的形式，也是数据开发人员和数据科学家最有可能(至少在某种程度上)熟悉的过程。

可以有任意数量的有效性规则用于清理数据，这些规则将取决于数据科学家的预期目的或目标。这些规则的示例包括:数据类型(例如，字段必须是数字)、范围限制(其中数字或日期必须在特定范围内)、必需(值不能为空或缺失)、唯一性(字段或字段组合在数据池中必须是唯一的)、集合成员(当值必须是离散列表的成员时)、外键(在案例中找到的某些值必须被定义为特定规则的成员或满足特定规则)， 正则表达式模式(简单地说就是验证一个值的格式是否符合规定的格式)和跨字段验证(一个例例中的字段组合必须满足一定的标准)。

让我们看看前面的几个例子，从数据类型开始(也称为**强制**)。r 提供了六个强制函数来简化它:

*   `as.numeric`
*   `as.integer`
*   `as.character`
*   `as.logical`
*   `as.factor`
*   `as.ordered`
*   `as.Date`

这些函数，加上一点 R 知识，可以使转换数据池中的值变得非常简单。例如，以以前的 GammingData 为例，我们可能会发现生成了一个新的 gamming 结果文件，并且年龄值被保存为一个字符串(或文本值)。为了清理它，我们需要将值转换为数字数据类型。我们可以使用下面一行 R 代码来快速转换文件中的值:

```
noOutliers["Age"]<-as.numeric(noOutliers["Age"]) 
```

一点:使用这个简单的方法，如果任何值不能被转换，它将被设置为一个 **NA** 值。在类型转换中，真正的工作是理解 a 值需要是什么类型，当然，还有什么数据类型是有效的；r 有各种各样的数据类型，包括标量、向量(数字、字符、逻辑)、矩阵、数据帧和列表。

我们在这里要看的另一个数据清理领域是正则表达式模式化的过程。在实践中，尤其是在处理从多个来源收集(或挖掘)的数据时，数据科学家肯定会遇到不符合所需格式的字段(对于手边的目标)或格式不一致的字段值(这可能会产生不正确的结果)。一些例子可以是日期、社会安全号码和电话号码。对于日期，根据来源的不同，您可能需要重新键入(如前所述)，但通常情况下，您还需要根据您的目标将值重新格式化为可用的格式。

重新键入日期很重要，这样 R 就知道使用该值作为实际日期，并且您可以正确地使用各种 R 数据函数。

一个常见的例子是，当数据包含日期可能被格式化为`YYYY/MM/DD`的情况，并且您想要执行显示周与周总和的时序分析，或者需要使用日期值但可能需要日期被重新格式化的一些其他操作，或者您只需要它是真正的 R date 对象类型。所以，让我们假设一个新的 Gamming 文件——这个文件只有两列数据:`Date`和`Coinin`。该文件是每天为单个老虎机收集的`Coinin`值的转储。

我们新文件中的记录(或案例)看起来像下面的屏幕截图:

![](assets/cd3da168-d9cf-497f-9191-62819faff066.png)

数据科学家可以使用各种清理示例。从验证每个数据点的数据类型开始。我们可以使用 R 函数类来验证文件的数据类型。首先(正如我们在前面的示例中所做的那样)，我们将 CSV 文件读入 R 数据框对象:

```
MyFile <-"C:/GammingData/SlotsByMachine.csv" 
MyData <- read.csv(file=MyFile, header=TRUE, sep=",")
```

接下来我们可以使用`class`函数，如下截图所示:

![](assets/2b4ebd5a-599e-47ed-9648-8f0a7cc8bf59.png)

您可以在前面的截图中看到，我们使用了`class`来显示我们的数据类型。

`MyData`是保存游戏数据的数据框，`Date`是`factor`类型，`Coinin`是`integer`。因此，数据帧和整数对您来说应该是有意义的，但是请注意，R 为我们的日期设置了它所谓的`factor`。因子是分类变量，有利于汇总统计、绘图和回归，但不如日期值有用。为了解决这个问题，我们可以使用 R 函数`substr`和`paste`，如下所示:

```
MyData$Date<-paste(substr(MyData$Date,6,7), substr(MyData$Date,9,10), substr(MyData$Date,1,4),sep="/") 
```

在所有情况下，这将在一行简单的脚本中重新格式化我们的`Data`字段的值，方法是将字段分成三个部分(月、日和年)，然后按照我们想要的顺序将这些部分重新粘贴在一起(用/作为分隔符(`sep`))，如下面的屏幕截图所示:

![](assets/d4fc1931-3a43-4145-89a3-4cee7e7fc7b3.png)

我们发现这一行脚本将我们的`Data`字段转换为类型`character`，最后，我们可以使用它们的`as.Date`函数将我们的值重新数据类型化为 R `Date`类型:

![](assets/9ecb376f-cd55-4b48-9eb4-cdfb14174244.png)

通过一点点尝试和错误，您可以完全按照您想要的方式重新格式化字符串或字符数据点。



# 增强数据

通过增强进行数据清理是另一种常见的技术，通过添加相关的信息、事实和/或数字来使数据变得完整(或许更有价值)。这些额外数据的来源可以是使用数据中已有信息的计算，也可以是从其他来源添加的信息。数据科学家花时间增强数据的原因有很多。

根据手头的目的或目标，数据科学家添加的信息可能用于参考、比较、对比或显示趋势。典型的使用案例包括:

*   派生事实计算
*   指示使用日历还是会计年度
*   转换时区
*   货币兑换
*   添加本期与前期指标
*   计算数值，例如每天运送的总单位数
*   保持缓慢变化的尺寸

作为一名数据科学家，您应该始终使用脚本来增强您的数据，因为这种方法比直接编辑数据文件好得多，因为它不容易出错，并且保持了原始文件的完整性。此外，创建脚本允许您将增强重新应用到多个文件和/或接收到的文件的新版本，而不必重复相同的工作。

对于一个工作示例，让我们再次回到我们的`GammingData`。假设我们正在通过吃角子老虎机接收`Coinin`金额的文件，并且我们的游戏公司现在在美国大陆之外经营赌场。这些地方向我们发送文件以纳入我们的统计分析，我们现在发现这些国际文件以当地货币提供`Coinin`金额。为了能够正确地对数据建模，我们需要将这些金额转换成美元。场景是这样的:

文件来源:英国

使用的货币:英镑或英镑

将英镑值转换成美元的公式就是将英镑值乘以汇率。所以，在 R 中:

```
MyData$Coinin<-MyData$Coinin * 1.4 
```

前面一行代码将完成我们想要的；然而，数据科学家需要确定需要转换哪种货币(英镑)以及要使用的汇率是多少。没什么大不了的，但是您可能想尝试创建一个用户定义的函数来确定要使用的速率，如下所示:

```
getRate <- function(arg){     
    if(arg=="GPB") { 
      myRate <- 1.4 
    } 
    if(arg=="CAD") { 
      myRate <- 1.34 
    } 
    return(myRate) 
}
```

尽管前面的代码片段相当简单，但它阐明了创建我们以后可以重用的逻辑的要点:

![](assets/f680be63-e282-49c2-8ae9-ecfd5b764a5f.png)

最后，为了让事情变得更好，保存你的函数(在一个 R 文件中),以便它可以一直被使用:

```
source("C:/GammingData/CurerncyLogic.R") 
```

然后:

```
MyFile <-"C:/GammingData/SlotsByMachine.csv" 
MyData <- read.csv(file=MyFile, header=TRUE, sep=",") 
MyData$Coin <- MyData$Coinin * getRate("CAD") 
```

当然，在最好的情况下，我们可以修改函数，根据国家代码在表或文件中查找费率，这样就可以更改费率，以反映最新的值，并从程序代码中分离数据。



# 和谐

通过数据协调，数据科学家根据要执行的分析的总体目标或目的，将数据值转换、翻译或映射为其他更理想的值。最常见的例子是性别或国家代码。例如，如果您的文件将性别编码为`1` s 和`0` s 或`M`和`F`，您可能希望将数据点转换为一致编码为`MALE`或`FEMALE`。

对于国家/地区代码，数据科学家可能希望绘制北美、南美和欧洲地区的总和，而不是分别绘制美国、ca、MX、BR、CH、GB、FR 和 DE。在这种情况下，他或她将创建聚合值:

北美=美国+加拿大+ MX

南美洲= BR + CH

欧洲= GB + FR + DE

为了说明这一点，也许数据科学家已经将多个调查文件缝合在一起，这些文件都包含性别，称为`gender.txt`，但使用不同的代码(`1`、`0`、`M`、`F`、`Male`和`Female`)。如果我们尝试使用 R 函数表，我们会看到以下不良结果:

![](assets/5a746257-5f93-402d-86ba-025d109b608a.png)

如果我们以最好的期望来想象这个:

```
lbs = c("Male", "Female") 
pie(table(MyData), main="Gambling by Gender")
```

我们看到下面的截图:

![](assets/80002012-cec3-4f09-ba71-1f94bb9cbc8e.png)

同样，为了解决数据点性别编码不一致的问题，我借用了上一节示例中的概念，并创建了一个简单的函数来帮助我们进行重新编码:

```
setGender <- function(arg){      
    if(substr(arg,1,1)=="0" | toupper(substr(arg,1,1))=="M") { Gender <- "MALE" } 
    if(substr(arg,1,1)=="1" | toupper(substr(arg,1,1))=="F") { Gender <- "FEMALE" } 
    return(Gender) 
} 
```

这一次，我添加了`toupper`函数，这样我们就不必担心大小写，还添加了`substr`来处理长度超过单个字符的值。

我假设参数值将是`0`、`1`、`m`、`M`、`f`、`F`、`Male`或`Female`，否则将会出现错误。

由于 R 将`Gender`值归类为数据类型`factor`，我发现很难轻松利用这个简单的函数，所以我决定创建一个新的 R 数据框对象来保存我们的协调数据。我还决定使用一个循环过程来通读我们文件中的每个例例(记录)，并将其转换为`Male`或`Female`:

```
MyFile <-"C:/GammingData/Gender.txt" 
MyData <- read.csv(file=MyFile, header=TRUE, sep=",") 
GenderData <-data.frame(nrow(MyData)) 
for(i in 2:nrow(MyData)) 
{ 
   x<-as.character(MyData[i,1])    
   GenderData[i,1] <-setGender(x) 
} 
```

现在我们可以通过书写来享受一个更合适的可视化:

```
lbls = c("Male", "Female") 
pie(table(GenderData), labels=lbls, main="Gambling by Gender") 
```

上述代码的输出如下:

![](assets/db65e0c3-1d12-491c-a69c-b6f6e4b0cdcb.png)

# 标准化

大多数主流数据科学家已经注意到在开始统计研究或分析项目之前，作为数据清理过程的一部分，标准化数据变量(将参考数据更改为标准)的重要性。这一点很重要，因为如果没有标准化，使用不同尺度测量的数据点很可能不会对分析做出同等贡献。

如果你考虑到 0 到 100 范围内的一个数据点会胜过 0 到 1 范围内的一个变量，你就可以理解数据标准化的重要性了。实际上，使用这些没有标准化的变量会使范围越大的变量在分析中的权重越大。为了解决这一问题并平衡变量，数据科学家试图将数据转换为可比较的尺度。

(数据点的)居中是数据标准化最常见的例子(尽管还有许多其他例子)。为了使数据点居中，数据科学家将从文件中的每个数据点减去所有数据点的平均值。

R 提供了`scale`函数，而不是进行数学运算。这个函数的默认方法是在一行代码中居中和/或缩放文件中的一列数字数据。让我们看一个简单的例子。

回到我们的老虎机！在我们的游戏文件中，您可能还记得有一个名为`Coinin`的字段，它包含一个数值，表示投入机器的总金额。这被认为是对机器盈利能力的一种衡量。在我们的盈利能力分析中，这似乎是一个明显的数据点。然而，这些金额可能会产生误导，因为存在不同面值的机器(换句话说，一些机器接受镍币，而其他机器接受一角或美元)。也许这种机器面额的差异造成了不平等的规模。我们可以使用`scale`函数来处理这种情况。首先，我们在下面的截图中看到，`Coin.in`的值:

![](assets/883fbbac-aedd-4d38-8b1a-67a080394fc2.png)

然后，我们可以编写下面一行代码来使我们的`Coin.in`数据点居中:

```
scale(MyData[11], center = TRUE, scale = TRUE) 
```

center 的值决定了如何执行列居中。使用 center is `TRUE`导致通过从相应的列中减去`Coin.in`的列平均值(省略 NAs)来进行定心。`scale`的值决定了如何进行列缩放(居中后)。如果刻度是`TRUE`，那么如果中心是`TRUE`，则通过将`Coin.in`的(中心)列除以它们的标准偏差来进行缩放，否则除以均方根。

我们在下面的截图中看到了不同之处:

![](assets/65eb0e76-4903-4db4-aac0-ab36c9324a8a.png)

# 转换

一种发人深省的数据清理类型是**数据转换**，这对于数据开发人员来说可能是一个新概念。数据转换是一个过程，在这个过程中，数据科学家通过一些数学运算实际上改变了您可能期望的有效数据值。

执行数据转换将数据从原始格式映射到适当应用程序所期望的格式，或者对于特定假设或目的更方便的格式。这包括值转换或翻译功能，以及标准化数值以符合最小值和最大值。

正如我们在本章前面使用 R，我们可以看到这个过程的一个非常简单的例子的语法是简单的。例如，数据科学家可能决定将给定值转换为该值的平方根:

```
data.dat$trans_Y <-sqrt(data.dat$Y) 
```

前面的代码示例通知 R 创建一个名为`trans_Y`的新变量(或`data.dat`数据集中的列),该变量等于原始响应变量`Y`的平方根。

虽然 R 可以支持您能想到或需要的任何数学运算，但语法并不总是直观的。r 甚至提供了通用函数`transform`**，但是在撰写本文时，它只适用于数据帧。`transform.default`将其第一个参数转换为数据帧，然后调用`transform.data.frame`。**

 **但你为什么要这么做？嗯，一个原因是这样的关系不会很好。例如，如果您正在试验数量级不同的值，将很难处理或使用它们。实际例子可以是物种之间的物理体重或各种声音频率及其对大气的影响的比较。在这些示例中，对数变换的使用使数据科学家能够以一种允许人们看到较低值的数据点之间的差异的方式来绘制值。

另一个例子是测试分数的转换(到分数与平均分数的距离)。

最后，各种广泛使用的统计方法假设正态性或正态分布形状。在数据科学家观察到非正常情况的情况下，可以使用数据转换。诸如对数或指数或幂变换之类的数据变换通常用于试图使形状非正态的数据分数的分布更正态。这些数据转换还可以帮助数据科学家将极端异常值与其余数据值更接近；这减少了异常值对汇总统计估计的影响，如样本均值或相关性。



# 演绎校正

在演绎推理中，人们使用已知的信息、假设或普遍接受的规则来得出结论。在统计学中，数据科学家使用这个概念(试图)来修复数据总体中的不一致和/或缺失值。

对于数据开发人员来说，演绎校正的例子包括将字符串或文本值转换为数字数据类型，或者将符号从负数转换为正数(反之亦然)。这些实例的实际例子是克服存储限制，例如当调查信息总是被捕获并存储为文本时，或者当会计需要将数字美元值表示为费用时。在这些情况下，可能会对数据进行审查(以推断需要进行哪些更正，也称为统计数据 **编辑**)，或者该过程可能会自动影响来自特定数据源的所有传入数据。

数据科学家例行执行的其他演绎校正包括输入键入错误、舍入错误、符号错误和值互换的校正。

有 R 包和库可用，如`deducorrect`包，它专注于舍入、键入和符号错误的纠正，包括三种数据清理算法(`correctRounding`、`correctTypos`和`correctSigns`)。然而，数据科学家大多希望为手头的项目专门定制一个解决方案。



# 确定性插补

我们一直在讨论数据科学家推断或确定如何解决或纠正脏数据问题的主题，如数据池中缺失、不正确、不完整或不一致的值。

当数据池中的数据丢失(或不正确、不完整或不一致)时，会使处理和分析变得困难，并会给对数据执行的分析结果带来偏差。这导致我们的归罪。

在数据统计中，插补是指数据科学家通过数据清理程序，用其他值替换缺失的(或指定的)数据。

由于缺失数据会给数据分析带来问题，插补被视为一种避免简单丢弃或完全删除缺失值案例所带来的危险的方法。事实上，一些统计软件包默认丢弃任何有缺失值的病例，这可能会引入偏差或影响结果的代表性。插补通过用基于其他可用信息的估计值替换缺失数据，保留了数据库中的所有病例。

确定性插补是数据科学家在为编辑失败的缺失、无效或不一致数据分配替换值的过程中使用的一种方法。换句话说，在特定情况下，当所有其他字段的特定值已知且有效时，如果只有一个(缺失)值会导致记录或案例满足或通过所有 data scientist 编辑，则该值将被估算。确定性插补是一种守恒插补理论，因为它针对的是简单识别的情况(如前所述)，并且可能是数据自动编辑和插补中首先考虑的情况。

目前，在数据科学领域，插补理论越来越受欢迎，并在继续发展，因此需要持续关注有关该主题的新信息。

其他一些试图处理缺失数据的著名插补理论包括热卡和冷卡插补；列表式和成对删除；均值插补；回归插补；最后一次观察结转；随机插补；和多重插补。



# 摘要

在本章中，我们概述了不同种类或类型的统计数据清理的基础知识。然后，使用 R 编程语言，我们举例说明了各种工作示例，展示了每一种最好的或常用的数据清理技术。

我们还介绍了数据转换、演绎校正和确定性插补的概念。

在下一章中，我们将深入探讨什么是数据挖掘以及它为什么重要的主题，并将 R 用于最常见的统计数据挖掘方法:降维、频繁模式和序列。**