

# 七、更进一步

在本章中，我们将介绍以下配方:

*   使用 Spark 流订阅 Twitter 流
*   使用 Spark 作为 ETL 工具(从 ElasticSearch 提取数据并发布到 Kafka)
*   使用 StreamingLogisticRegression 对使用 Kafka 作为训练流的 Twitter 流进行分类
*   使用 GraphX 分析 Twitter 数据
*   观看其他感兴趣的 Scala 库

# 简介

到目前为止，整本书都集中在 Breeze 和 Spark 上，特别是数据帧和机器学习。然而，在分析来自 Scala 的数据时，可以利用 Java 和 Scala 中的大量其他库。这一章会更深入地介绍 Spark 的其他组件，streaming 和 GraphX。请注意，本章中的每一个配方都会影响下一个配方。

### 注意

与本章相关的所有代码可以从[https://github . com/arun ma/scaladata analysis cookbook/tree/master/chapter 7-going further](https://github.com/arunma/ScalaDataAnalysisCookbook/tree/master/chapter7-goingfurther)下载。



# 使用 Spark Streaming 订阅 Twitter 流

就像 Spark 的所有其他组件一样，Spark 流也是可扩展和容错的，只是它管理的是数据流，而不是 Spark 通常管理的大量数据。Spark Streaming 处理流的方式是独特的，它将流累积成称为数据流的小批量，然后将它们作为小批量处理，这种方法通常称为 **微批量**。接收数据流并将其拆分成有时间限制的批处理窗口的组件称为接收器。

一旦接收到这些批次，Spark 就会接收这些批次，将它们转换成 rdd，并以与静态数据集相同的方式处理 rdd。驱动程序和执行器等常规框架组件保持不变。然而，就Spark流而言，数据流或离散流只是 rdd 的连续流。此外，就像`SQLContext`作为在 Spark 中使用 SQL 的入口点一样，还有`StreamingContext`作为 Spark 流的入口点。

在这个菜谱中，我们将订阅一个 Twitter 流，并将推文索引(存储)到elastic search([https://www.elastic.co/](https://www.elastic.co/))中。

## 怎么做...

运行这个菜谱的先决条件是在您的机器上有一个正在运行的 ElasticSearch 实例。

1.  **运行 ElasticSearch** :运行 ElasticSearch 的一个实例非常简单。只需从[https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)下载可安装程序并运行`bin/elasticsearch`。该配方使用最新版本 1.7.1。![How to do it...](graphics/InsertImage_B03812_07_01.jpg)
2.  **Creating a Twitter app**: In order to subscribe to tweets, Twitter requires us to create a Twitter app. Let's quickly set up a Twitter app in order to get the consumer key and the secret key. Visit [https://apps.twitter.com/](https://apps.twitter.com/) using your login and click **Create New App**.![How to do it...](graphics/InsertImage_B03812_07_02.jpg)

    我们将在应用程序中使用消费者密钥、消费者秘密密钥、访问令牌和访问秘密。

    ![How to do it...](graphics/InsertImage_B03812_07_03.jpg)
3.  **添加 Spark Streaming 和 Twitter 依赖项**:这里有两个需要添加，分别是`spark-streaming`和`spark-streaming-twitter`库:

    ```
    "org.apache.spark" %% "spark-streaming" % sparkVersion % "provided",

    "org.apache.spark" %% "spark-streaming-twitter" % sparkVersion
    ```

4.  **Creating a Twitter stream**: Creating a Twitter stream is super easy in Spark. We just need to use `TwitterUtils.createStream` for this. `TwitterUtils` wraps around the `twitter4j` library ([http://twitter4j.org/en/index.html](http://twitter4j.org/en/index.html)) to provide first-class support in Spark.

    `TwitterUtils.createStream`需要几个参数。让我们一个一个地构造它们。

    *   `StreamingContext` : `StreamingContext`可以通过传入`SparkContext`和批量的时间窗口:

        ```
          val streamingContext=new StreamingContext(sc, Seconds (5))
        ```

        来构造
    *   OAuthorization:为了订阅 Twitter 流，需要传递包含`OAuth`凭证的访问和消费者密钥:

        ```
          val builder = new ConfigurationBuilder()

                .setOAuthConsumerKey(consumerKey)

                  .setOAuthConsumerSecret(consumerSecret)

                  .setOAuthAccessToken(accessToken)

                  .setOAuthAccessTokenSecret(accessTokenSecret)

                  .setUseSSL(true)

          val twitterAuth = Some(new OAuthAuthorization(builder.build()))
        ```

    *   过滤标准:如果你打算订阅全部推文(的样本),你可以跳过这个参数。对于这个食谱，我们将添加一些过滤标准:

        ```
            val filter=List("fashion", "tech", "startup", "spark")
        ```

    *   这里是我们收到的成批到来的物品需要存放的地方。缺省值是能够溢出到磁盘的内存。一旦构建完成，让我们构建 Twitter 流本身:

        ```
          val stream=TwitterUtils.createStream(streamingContext, twitterAuth, filter, StorageLevel.MEMORY_AND_DISK)
        ```

5.  **将流保存到 ElasticSearch** :将 Tweets 写到 ElasticSearch 包含三个步骤:

    1.  添加`ElasticSearch-Spark`依赖项:让我们将 ElasticSearch Spark 的适当版本添加到我们的`build.sbt` :

        ```
        "org.elasticsearch" %% "elasticsearch-spark" % "2.1.0"
        ```

    2.  在 Spark 配置中配置 ElasticSearch 服务器位置:ElasticSearch 有一个名为`elasticsearch-spark`的子项目，它使 ElasticSearch 成为 Spark 世界中的一等公民。`org.elasticsearch.spark`包公开了一些方便的函数，将 case 类转换成 JSON(派生类型),将索引转换成 ElasticSearch。该软件包还提供了一些非常酷的隐含功能，这些功能可以将 RDD 保存到 ElasticSearch 中，并作为 RDD 从 ElasticSearch 加载数据。我们很快就会看到这些函数。

        弹性搜索目标节点统一资源定位器可以在Spark配置中指定。默认情况下,它指向本地主机和端口 9200。如果需要,我们可以定制它:

        ```
        //Default is localhost. Point to ES node when required

        val conf = new SparkConf()

            .setAppName("TwitterStreaming")

            .setMaster("local[2]")

            .set(ConfigurationOptions.ES_NODES, "localhost")

                      .set(ConfigurationOptions.ES_PORT, "9200")
        ```

    3.  将流转换为 case 类:如果我们对将数据推送到 ElasticSearch 不感兴趣，只对打印`twitter4j.Status`中的一些值感兴趣，`stream.foreach`将帮助我们迭代`RDD[Status]`。然而，在这个菜谱中，我们将从`twitter4j.Status`中提取一些数据，并将其推送到 ElasticSearch。为此，创建了一个 case 类`SimpleStatus`。我们提取数据作为案例类的原因是`twitter4j.Status`有太多我们不想索引的信息:

        ```
        case class SimpleStatus(id:String, content:String, date:Date, hashTags:Array[String]=Array[String](),

                                urls:Array[String]=Array[String](),

                                user:String, userName:String, userFollowerCount:Long)
        ```

使用`convertToSimple`函数将`twitter4j.Status`转换为`SimpleStatus`，该函数仅提取所需的信息:

```
 def convertToSimple(status: twitter4j.Status): SimpleStatus = {

    val hashTags: Array[String] = status.getHashtagEntities().map(eachHT => eachHT.getText())

    val urlArray = if (status.getURLEntities != null) status.getURLEntities().foldLeft((Array[String]()))((r, c) => (r :+ c.getExpandedURL())) else Array[String]()

    val user = status.getUser()

    val utcDate = new Date(dateTimeZone.convertLocalToUTC(status.getCreatedAt.getTime, false))

    SimpleStatus(id = status.getId.toString, content = status.getText(), utcDate,

      hashTags = hashTags, urls = urlArray,

      user = user.getScreenName(), userName = user.getName, userFollowerCount = user.getFollowersCount)

  }
```

一旦我们将`twitter4j.Status`映射到`SimpleStatus`，我们现在就有了一个`RDD[SimpleStatus]`。我们现在可以迭代`RDD[SimpleStatus]`并将每个 RDD 推送到 ElasticSearch 的`"spark"`索引。`"twstatus"`是指数类型。在 RDBMS 术语中，索引类似于数据库模式，索引类型类似于表:

```
stream.map(convertToSimple).foreachRDD { statusRdd =>

    println(statusRdd)

    statusRdd.saveToEs("spark/twstatus")

}
```

我们可以通过使用 Sense(ElasticSearch 的必备 Chrome 插件)指向 elastic search 的`spark`索引来确认索引，或者只需执行一个`curl`请求:

```
curl -XGET "http://localhost:9200/spark/_search" -d'

{

    "query": {

        "match_all": {}

    }

}'
```

### 注意

Chrome 的 Sense 插件可以从 Chrome 商店下载:[https://Chrome . Google . com/web store/detail/Sense-beta/lhjgkmllcaadmopgmanpapmpjgmfcfig？hl=en](https://chrome.google.com/webstore/detail/sense-beta/lhjgkmllcaadmopgmanpapmpjgmfcfig?hl=en) 。

![How to do it...](graphics/InsertImage_B03812_07_04.jpg)

# 使用 Spark 作为 ETL 工具

在前面的菜谱中，我们订阅了一个 Twitter 流，并将其存储在 ElasticSearch 中。另一个常见的流来源是 Kafka，一个分布式消息代理。事实上，它是一个分布式的消息日志，简单地说，这意味着可以有多个代理在它们之间划分消息。

在这个菜谱中，我们将订阅在上一个菜谱中摄取到 ElasticSearch 中的数据，并将消息发布到 Kafka 中。在我们将数据发布到 Kafka 后不久，我们将使用 Spark Stream API 订阅 Kafka。虽然这是一个演示将 ElasticSearch 数据作为 RDD 处理并使用 **KryoSerializer** 发布到 Kafka 的方法，但这个方法的真正意图是针对 Twitter 运行一个流分类算法，这是我们的下一个方法。

## 怎么做...

让我们看看这样做的各个步骤。

1.  **Setting up Kafka**: This recipe uses Kafka version 0.8.2.1 for Spark 2.10, which can be downloaded from [https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.2.1/kafka_2.10-0.8.2.1.tgz](https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.2.1/kafka_2.10-0.8.2.1.tgz).

    下载完成后，让我们解压、启动 Kafka 服务器，并通过 Kafka 主目录中的三个命令创建一个 Kafka 主题:

    1.  **启动 Zookeeper** : Kafka 使用Zookeeper([https://zookeeper.apache.org/](https://zookeeper.apache.org/))来保存 Kafka 服务器之间的协调信息。它还保存数据的提交偏移量信息，这样如果 Kafka 节点失败，它就知道从哪里恢复。Zookeeper `data`目录和客户端端口(默认`2181`)出现在`zookeeper.properties`中。`zookeeper-server-start.sh`希望将此作为参数传递，以便启动:

        ```

        bin/zookeeper-server-start.sh config/zookeeper.properties

        ```

    2.  **启动 Kafka 服务器**:同样，在启动 Kafka 的命令中，要传递给它的配置文件是`server.properties`。除了其他功能之外，`server.properties`还指定了 Kafka 服务器监听的端口(9092)和它需要连接的 Zookeeper 端口(2181)。这是传递给`kafka-server-start.sh`的启动脚本:

        ```

        bin/kafka-server-start.sh config/server.properties

        ```

    3.  **创建 Kafka 主题**:简单来说，一个主题可以比作一个 JMS 主题，不同之处在于 Kafka 中可以有多个发布者和一个订阅者。由于我们只使用一台 Kafka 服务器以非复制和非分区模式运行 Kafka，因此名为`twtopic` (Twitter 主题)的主题是用复制因子 1 创建的，分区数量也是 1:

        ```

        bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic twtopic

        ```

2.  **Pulling data from ElasticSearch**: The next step is to pull the data from ElasticSearch and treat it as a Spark DataFrame other than the optional setting in Spark configuration to point to the correct host and port. This is just a one-liner.

    配置更改(如果需要)是:

    ```
    //Default is localhost. Point to ES node when required

      val conf = new SparkConf()

        .setAppName("KafkaStreamProducerFromES")

        .setMaster("local[2]")

        .set(ConfigurationOptions.ES_NODES, "localhost")

        .set(ConfigurationOptions.ES_PORT, "9200")
    ```

    下面的行查询所有文档的`"spark/twstatus"` 索引(我们在上一个菜谱中发布到了该索引),并将数据提取到 DataFrame 中。或者，您可以将查询作为第二个参数传入(例如，`"?q=fashion"`):

    ```
      val twStatusDf=sqlContext.esDF("spark/twstatus")
    ```

    让我们尝试使用`show()`对数据帧进行采样:

    ```

    twStatusDf.show()

    ```

    输出是:

    ![How to do it...](graphics/InsertImage_B03812_07_05.jpg)
3.  **Preparing data to be published to Kafka**: Before we do this step, let's go over what we aim to achieve from this step. Like we discussed at the beginning of the recipe, we will be running a classification algorithm against streaming data in the next recipe. As you know, any supervised learning algorithm requires a training dataset. Instead of us manually curating the dataset, we will be doing that in a very primitive fashion by marking all the tweets that have the word `fashion` in them as belonging to the `fashion` class and the rest of the tweets as not belonging to the `fashion` class.

    我们将获取 tweet 的内容，并将转换成一个名为`LabeledContent`的 case 类(类似于 Spark MLlib 中的`LabeledPoint`):

    ```
    case class LabeledContent(label: Double, content: Array[String])
    ```

    `LabeledContent`只有两个字段:

    *   `label`:这表示推文是否与`fashion`有关(如果推文在`fashion`上，则为`1.0`，否则为`0.0`)
    *   `content`:这个保存了 tweet 本身的空间标记版本

        ```
        def convertToLabeledContentRdd(twStatusDf: DataFrame) = {

            //Convert the content alone to a (label, content) pair

            val labeledPointRdd = twStatusDf.map{row =>

                val content = row.getAs[String]("content").toLowerCase()

                val tokens = content.split(" ") //A very primitive space based tokenizer

                val labeledContent=if (content.contains("fashion")) LabeledContent(1, tokens)

                else LabeledContent(0, tokens)

                println (labeledContent.label, content)

                labeledContent

            }

            labeledPointRdd

        }
        ```

4.  **Publishing data to Kafka using KryoSerializer**: Now that we have the publish candidate (`LabeledContent`) ready, let's publish it to the Kafka topic. This involves just three lines.

    *   **构建连接和传输属性**:在`properties`中，我们配置 Kafka 服务器端口位置并注册我们用来序列化`LabeledContent` :

        ```
        val properties = Map[String, Object](ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> "localhost:9092").asJava
        ```

        的序列化器
    *   **使用连接属性和键和值序列化器**构建 Kafka 生产者:下一步是使用我们之前构建的`properties`构建 Kafka 生产者。`producer`还需要一个键和值序列化器。因为我们没有消息的密钥，所以我们退回到 Kafka 的缺省值，缺省情况下填充 hashcode，这是我们在接收时不感兴趣的。

        ```
        val producer = new KafkaProducer[String, Array[Byte]](properties, new StringSerializer, new ByteArraySerializer)
        ```

    *   **使用 send 方法**向 Kafka 主题发送数据:然后我们使用`KryoSerializer`序列化`LabeledContent`，并使用`producer.send`方法将其发送给 Kafka 主题`"twtopic"`(我们之前创建的那个)。这里使用`KryoSerializer` 的唯一目的是加快序列化过程:

        ```
        val serializedPoint = KryoSerializer.serialize(lContent)

                producer.send(new ProducerRecord[String, Array[Byte]]("twtopic", serializedPoint))
        ```

    对于 KryoSerializer，我们使用 Twitter 的 `chill`库(【https://github.com/twitter/chill】)为 Scala 的序列化提供了更简单的抽象。

    实际的 KryoSerializer 只有五行代码:

    ```
    object KryoSerializer {

      private val kryoPool = ScalaKryoInstantiator.defaultPool

      def serialize[T](anObject: T): Array[Byte] = kryoPool.toBytesWithClass(anObject)

      def deserialize[T](bytes: Array[Byte]): T = kryoPool.fromBytes(bytes).asInstanceOf[T]

    }
    ```

    需要添加到我们的`build.sbt`中的 Twitter `chill`的依赖项是:

    ```
    "com.twitter" %% "chill" % "0.7.0"
    ```

    整个发布方法如下所示:

    ```
      def publishToKafka(labeledPointRdd: RDD[LabeledContent]) {

        labeledPointRdd.foreachPartition { iterator =>

          val properties = Map[String, Object](ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> "localhost:9092", "serializer.class" -> "kafka.serializer.DefaultEncoder").asJava

          val producer = new KafkaProducer[String, Array[Byte]](properties, new StringSerializer, new ByteArraySerializer)

          iterator.foreach { lContent =>

            val serializedPoint = KryoSerializer.serialize(lContent)

            producer.send(new ProducerRecord[String, Array[Byte]]("twtopic", serializedPoint))

          }

        }

      }
    ```

5.  **在 Kafka** 中确认接收:我们可以使用 Kafka 暴露的JMX mbean 来确认数据是否在 Kafka 中。我们将使用 JConsole UI 来探索 MBeans。正如您所看到的，消息的数量是 **24849** ，这与 ElasticSearch 文档的数量相匹配(在之前的菜谱中已经公布)。![How to do it...](graphics/InsertImage_B03812_07_06.jpg)



# 使用 Kafka 作为训练流，使用 StreamingLogisticRegression 对 Twitter 流进行分类

在前一个配方中，我们将存储在 ElasticSearch 中的所有 tweets 发布到一个 Kafka 主题。在这个食谱中，我们将订阅 Kafka 流，并从中训练一个分类模型。我们稍后将使用这个训练好的模型来分类一个实时 Twitter 流。

## 怎么做...

这是一个非常小的食谱，由 3 个步骤组成:

1.  **Subscribing to a Kafka stream**: There are two ways to subscribe to a Kafka stream and we'll be using the `DirectStream` method, which is faster. Just like Twitter streaming, Spark has first-class support for subscribing to a Kafka stream. This is achieved by adding the `spark-streaming-kafka` dependency. Let's add it to our `build.sbt` file:

    ```
    "org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
    ```

    订阅过程或多或少与发布过程相反，甚至在我们传递给 Kafka 的属性方面也是如此:

    ```
    val topics = Set("twtopic")

    val kafkaParams = Map[String, String]("metadata.broker.list" -> "localhost:9092")
    ```

    一旦构建了属性，我们就使用`KafkaUtils.createDirectStream`订阅`twtopic`:

    ```
    val kafkaStream = KafkaUtils.createDirectStream[String, Array[Byte], StringDecoder, DefaultDecoder](streamingContext, kafkaParams, topics).repartition(2)
    ```

    有了这条河流，让我们重建它。我们可以通过 KryoSerializer 的`deserialize`函数来实现:

    ```
    val trainingStream = kafkaStream.map {

            case (key, value) =>

              val labeledContent = KryoSerializer.deserialize(value).asInstanceOf[LabeledContent]
    ```

2.  **Training the classification model**: Now that we are receiving the `LabeledContent` objects from the Kafka stream, let's train our classification model out of them. We will use `StreamingLogisiticRegressionWithSGD` for this, which as the name indicates, is a streaming version of the LogisticRegressionWithSGD algorithm we saw in [Chapter 5](ch19.html "Chapter 5. Learning from Data"), *Learning from Data*. In order to train the model, we have to construct a `LabeledPoint`, which is a pair of labels (represented as a double) and a feature vector. Since this is a text, we'll use the HashingTF's `transform` function to generate the feature vector for us:

    ```
    val hashingTf = new HashingTF(5000)

    val kafkaStream = KafkaUtils.createDirectStream[String, Array[Byte], StringDecoder, DefaultDecoder](streamingContext, kafkaParams, topics).repartition(2)

    val trainingStream = kafkaStream.map {

            case (key, value) =>

              val labeledContent = KryoSerializer.deserialize(value).asInstanceOf[LabeledContent]

              val vector = hashingTf.transform(labeledContent.content)

              LabeledPoint(labeledContent.label, vector)

    }
    ```

    `trainingStream`现在是一个`LabeledPoint`流，我们将用它来训练我们的模型:

    ```
    val model = new StreamingLogisticRegressionWithSGD()

          .setInitialWeights(Vectors.zeros(5000))

          .setNumIterations(25).setStepSize(0.1).setRegParam(0.001)

    model.trainOn(trainingStream)
    ```

    由于我们指定`HashingTF`中的最大特征数为`5000`，所以我们将所有 5000 个特征的初始权重设为`0`。其余参数与在静态数据集上训练的常规 LogisticRegressionWithSGD 算法相同。

3.  **Classifying a live Twitter stream**: Now that we have the model in hand, let's use it to predict whether the incoming stream of tweets is about `fashion` or not. The Twitter setup in this section is the same as the first recipe where we subscribed to a Twitter stream:

    ```
    val filter = List("fashion", "tech", "startup", "spark")

        val twitterStream = TwitterUtils.createStream(streamingContext, twitterAuth, filter, StorageLevel.MEMORY_AND_DISK)
    ```

    最关键的部分是对`model.predictOnValues`的调用，它给了我们预测的标签。预测完成后，我们将它们作为文本文件保存在本地目录中。这不是最好的方法，我们可能会将这些数据推送到某个*可追加的*数据源。

    ```
    val contentAndFeatureVector=twitterStream.map { status =>

          val tokens=status.getText().toLowerCase().split(" ")

          val vector=hashingTf.transform(tokens)

          (status.getText(), vector)

        }

        val contentAndPrediction=model.predictOnValues(contentAndFeatureVector)

    //Not the best way to store the results. Creates a whole lot of files

        contentAndPrediction.saveAsTextFiles("predictions", "txt")
    ```

    为了合并分布在多个文件中的预测，使用了一个非常简单的聚合命令:

    ```

    find predictions* -name "part*" |xargs cat >> output.txt

    ```

    ![How to do it...](graphics/InsertImage_B03812_07_07.jpg)

    这是预测的一个例子。考虑到训练数据集本身没有以非常科学的方式进行分类，结果还算不错。此外，令牌化只是基于空间的，数据没有缩放，也没有使用 IDF。

    ![How to do it...](graphics/InsertImage_B03812_07_08.jpg)



# 使用 GraphX 分析 Twitter 数据

GraphX 是 Spark 处理图形和针对图形进行计算的方法。在这个菜谱中，我们将看到 Spark 中 GraphX 组件的预览。

## 怎么做...

现在，我们已经将 Twitter 数据存储在 ElasticSearch 索引中，我们将使用图表对这些数据执行以下任务:

1.  将 ElasticSearch 数据转换成一个Spark图。
2.  图中的顶点、边和三元组示例。
3.  找到最上面的一组连接标签(连接组件)。
4.  列出该组件中的所有标签。

1.  **Converting the ElasticSearch data into a graph**: This involves two steps:

    1.  **将 ElasticSearch 数据转换成数据帧**:这一步，就像我们在前面的菜谱中看到的，只是一行程序:

        ```
        def convertElasticSearchDataToDataFrame(sqlContext: SQLContext) = {

            val twStatusDf = sqlContext.esDF("spark/twstatus")

            twStatusDf

        }
        ```

    2.  **Converting DataFrame to a graph**: Spark Graph construction requires an RDD for a vertex and an RDD of edges. Let's construct them one by one.

        顶点 RDD 需要表示一个`vertexId`和一个`vertex`属性的元组的 RDD。在我们的例子中，我们将对作为顶点 ID 的`hashTag`和作为属性的`hashTag`本身进行简单的哈希编码:

        ```
         val verticesRdd:RDD[(Long,String)] = df.flatMap { tweet =>

              val hashTags = tweet.getAs[Buffer[String]]("hashTags")

              hashTags.map { tag =>

                val lowercaseTag = tag.toLowerCase()

                val tagHashCode=lowercaseTag.hashCode().toLong

                (tagHashCode, lowercaseTag)

              }

         }
        ```

        对于边，我们构造了一个`RDD[Edge]`，它包装了一对顶点 id 和一个属性。在我们的案例中，我们使用第一个 URL(如果存在的话)作为 edge 的属性(我们没有在这个菜谱中使用它，所以空字符串也可以)。由于一条 tweet 可能有多个标签，我们使用`combinations`函数来选择标签对，然后将它们作为一条边连接在一起:

        ```
        val edgesRdd:RDD[Edge[String]] =df.flatMap { row =>

              val hashTags = row.getAs[Buffer[String]]("hashTags")

              val urls = row.getAs[Buffer[String]]("urls")

              val topUrl=if (urls.length>0) urls(0) else ""

              val combinations=hashTags.combinations(2)

              combinations.map{ combs=>

                val firstHash=combs(0).toLowerCase().hashCode.toLong

                val secondHash=combs(1).toLowerCase().hashCode.toLong

                Edge(firstHash, secondHash, topUrl)

              }

        }
        ```

    最后，我们使用两个 rdd 来构建图:

    ```
    val graph=Graph(verticesRdd, edgesRdd)
    ```

2.  **Sampling vertices, edges, and triplets in the graph**: Now that we have our graph constructed, let's sample and see what the vertices, edges, and triplets of the Graph look like. A triple is a representation of an edge and two vertices connected by that edge:

    ```

    graph.vertices.take(20).foreach(println)

    ```

    输出是:

    ![How to do it...](graphics/InsertImage_B03812_07_09.jpg)

    ```

    graph.edges.take(20).foreach(println)

    ```

    输出是:

    ![How to do it...](graphics/InsertImage_B03812_07_10.jpg)

    ```

    graph.triplets.take(20).foreach(println)

    ```

    输出是:

    ![How to do it...](graphics/InsertImage_B03812_07_11.jpg)
3.  **Finding the top group of connected hashtags (connected component)**: As you know, a graph is made of vertices and edges. A connected component of a graph is just a part of the graph (a subgraph) whose vertices are connected to each other by some edge. If there is a vertex that is not connected to another vertex directly or indirectly through another vertex, then they are not connected and therefore don't belong to the same connected component.

    GraphX 的`graph.connectedComponents`提供了所有顶点及其组件 id 的图形:

    ```
    val connectedComponents=graph.connectedComponents.cache()
    ```

    让我们获取具有最大顶点数的组件 ID，然后提取属于该组件的顶点(以及最终的标签):

    ```
    val ccCounts:Map[VertexId, Long]=connectedComponents.vertices.map{case (_, vertexId) => vertexId}.countByValue

        //Get the top component Id and count

        val topComponent:(VertexId, Long)=ccCounts.toSeq.sortBy{case (componentId, count) => count}.reverse.head
    ```

    因为`topComponent`只有组件 ID，为了获取顶层组件的`hashTags`,我们需要一个将`hashTag`映射到组件 ID 的表示。这是通过将图形的顶点连接到`connectedComponent`顶点来实现的:

    ```
    //RDD of HashTag-Component Id pair. Joins using vertexId

        val hashtagComponentRdd:VertexRDD[(String,VertexId)]=graph.vertices.innerJoin(connectedComponents.vertices){ case (vertexId, hashTag, componentId)=>

          (hashTag, componentId)

        }
    ```

    现在我们有了`componentId`和`hashTag`，让我们只过滤顶部组件 ID 的`hashTags`:

    ```
    val topComponentHashTags=hashtagComponentRdd

                .filter{ case (vertexId, (hashTag, componentId)) => (componentId==topComponent._1)}

                .map{case (vertexId, (hashTag,componentId)) => hashTag

        }

        topComponentHashTags
    ```

    整个方法看起来像这样:

    ```
    def getHashTagsOfTopConnectedComponent(graph:Graph[String,String]):RDD[String]={

        //Get all the connected components

        val connectedComponents=graph.connectedComponents.cache()

        import scala.collection._

        val ccCounts:Map[VertexId, Long]=connectedComponents.vertices.map{case (_, vertexId) => vertexId}.countByValue

        //Get the top component Id and count

        val topComponent:(VertexId, Long)=ccCounts.toSeq.sortBy{case (componentId, count) => count}.reverse.head

        //RDD of HashTag-Component Id pair. Joins using vertexId

        val hashtagComponentRdd:VertexRDD[(String,VertexId)]=graph.vertices.innerJoin(connectedComponents.vertices){ case (vertexId, hashTag, componentId)=>

          (hashTag, componentId)

        }

        //Filter the vertices that belong to the top component alone

        val topComponentHashTags=hashtagComponentRdd

                .filter{ case (vertexId, (hashTag, componentId)) => (componentId==topComponent._1)}

                .map{case (vertexId, (hashTag,componentId)) => hashTag

        }

        topComponentHashTags

      }
    ```

4.  **列出组件**中的所有标签:将`hashTags`保存到文件中就像调用`saveAsTextFile`一样简单。完成 `repartition(1)`只是为了让我们有一个单一的输出文件。或者，您可以使用`collect()`将所有数据带给驱动程序并检查它:

    ```
    def saveTopTags(topTags:RDD[String]){

        topTags.repartition(1).saveAsTextFile("topTags.txt")

    }
    ```

在我们的运行中，顶部连接组件中的标签数是 7，320。这表明，在我们的样本流中，大约有 7320 个与时尚相关的标签是相互关联的。它们可能是同义词，与时尚关系密切，或关系遥远。该文件的快照如下所示:

![How to do it...](graphics/InsertImage_B03812_07_12.jpg)

在本章中，我们简要介绍了 Spark 流、流 ML 和 GraphX。请注意，这绝不是这两个主题的详尽食谱列表，目的只是提供 Spark 中的流和 GraphX 可以做什么的体验。