

# 六、扩缩容

在本章中，我们将介绍以下配方:

*   建造优步罐子
*   向 Spark 集群提交作业(本地)
*   在 EC2 上运行 Spark 独立集群
*   在 Mesos 上运行 Spark 作业(本地)
*   在纱线上运行Spark作业(本地)

# 简介

在本章中，我们将研究如何捆绑我们的 Spark 应用程序，并将其部署在各种分布式环境中。

正如我们之前在[第 3 章](ch17.html "Chapter 3. Loading and Preparing Data – DataFrame")、*中讨论的，加载和准备数据-数据帧*Spark 的基础是 RDD。从程序员的角度来看，rdd 的可组合性，比如常规的 Scala 集合，是一个巨大的优势。RDD 包装了有助于数据重建的三条重要(和两条附属)信息。这实现了容错。另一个主要优点是，虽然 RDD 的处理可以使用 RDD 运算组成非常复杂的图形，但整个数据流本身并不十分难以推理。

除了可选的优化属性(如数据位置), RDD 的核心仅包含三条重要信息:

*   受抚养人/父 RDD(如果不可用，则为空)
*   分区的数量
*   需要应用于 RDD 每个元素的函数

Spark 为每个分区生成一个任务。因此，分区是 Spark 中并行性的基本单位。

分区数量可以是以下任意数量:

*   在读取文件的情况下由块数决定
*   由`spark.default.parallelism`参数设置的数字(在启动集群时设置)
*   通过呼叫 RDD 上的`repartition`或`coalesce`设置的号码

到目前为止，我们刚刚在独立的单 JVM 模式下运行了我们的 Spark 应用程序。虽然程序运行良好，但我们还没有利用 rdd 的分布式本质。

### 注意

和往常一样，本章的所有代码片段都可以从[https://github . com/arun ma/scaladata analysis cookbook/tree/master/chapter 6-scaling up](https://github.com/arunma/ScalaDataAnalysisCookbook/tree/master/chapter6-scalingup)下载。



# 建造优步罐

在集群上部署我们的 Spark 应用程序的第一步是将它捆绑到一个优步 JAR 中，也称为`assembly` JAR。在这个菜谱中，我们将看看如何使用 SBT 汇编插件来生成`assembly` JAR。当我们以分布式模式运行 Spark 时，我们将在后续的食谱中使用这个`assembly`罐子。我们也可以使用`spark.driver.extraClassPath`属性([https://spark . Apache . org/docs/1 . 3 . 1/configuration . html # runtime-environment](https://spark.apache.org/docs/1.3.1/configuration.html#runtime-environment))来设置依赖 jar。但是，对于大量依赖的 jar，这是不方便的。

## 怎么做...

构建`assembly` JAR 的目标是构建一个包含所有依赖项和我们的 Spark 应用程序的单一胖 JAR。参考下面的截图，它显示了一个`assembly`罐子的内部结构。您不仅可以看到 JAR 中的应用程序文件，还可以看到依赖库的所有包和文件:

![How to do it...](graphics/3812_06_01.jpg)

使用[https://github.com/sbt/sbt-assembly](https://github.com/sbt/sbt-assembly)的【SBT 组装】插件可以在 SBT 轻松构建`assembly`罐子。

为了安装`sbt-assembly`插件，让我们将下面一行添加到我们的`project/assembly.sbt`中:

```
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")
```

接下来，我们在尝试构建`assembly` JAR(或优步 JAR)时面临的最常见的问题是重复的问题——复制可传递的依赖 JAR，或简单地复制位于不同捆绑 JAR 中相同位置(如`MANIFEST.MF`)的文件。最简单的方法是安装【https://github.com/jrudolph/sbt-dependency-graph】`sbt-dependency-graph`插件，并检查哪两棵树带来了冲突的罐子。

为了添加`sbt-dependency-graph`插件，让我们将下面一行添加到我们的`project/plugins.sbt`中:

```
addSbtPlugin("net.virtual-void" % "sbt-dependency-graph" % "0.7.5")
```

让我们尝试使用`sbt assembly`来构建优步罐子。当我们从项目的根目录发出这个命令时，我们得到一个错误，告诉我们在我们的 JAR 中有重复的文件。

让我们看一个我们可能面临的重复错误消息的例子:

```

deduplicate: different file contents found in the following:

/Users/Gabriel/.ivy2/cache/org.apache.xmlbeans/xmlbeans/jars/xmlbeans-2.3.0.jar:org/w3c/dom/DOMStringList.class

/Users/Gabriel/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.4.01.jar:org/w3c/dom/DOMStringList.class

deduplicate: different file contents found in the following:

/Users/Gabriel/.ivy2/cache/org.apache.xmlbeans/xmlbeans/jars/xmlbeans-2.3.0.jar:org/w3c/dom/TypeInfo.class

/Users/Gabriel/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.4.01.jar:org/w3c/dom/TypeInfo.class

deduplicate: different file contents found in the following:

/Users/Gabriel/.ivy2/cache/org.apache.xmlbeans/xmlbeans/jars/xmlbeans-2.3.0.jar:org/w3c/dom/UserDataHandler.class

/Users/Gabriel/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.4.01.jar:org/w3c/dom/UserDataHandler.class

```

这种情况在以下情况下最常见:

*   我们的`sbt`依赖关系中的两个不同的库依赖于同一个外部库(或者将类与同一个包捆绑在一起的库)
*   在`sbt`中，我们已经明确地将传递性依赖描述为一个独立的依赖

无论是哪种情况，总是建议遍历整个依赖树来减少依赖。

### 传递依赖在 SBT 依赖中明确说明

一种更简单的方法是以 ASCII 树格式导出依赖树，并观察它来找到引用`xmlbeans` JAR 的两个实例。图形插件让我们可以做到这一点。一旦我们按照说明安装了插件，我们就可以导出并检查依赖关系树:

```
sbt dependency-tree > deptree.txt
```

该图也可以使用真实的图来可视化(但是，这缺乏文本搜索功能)。图表也有助于我们分析这一点。我们可以使用下面的代码将同一个树导出为一个`.dot`文件:

```
sbt dependency-dot > depdot.dot
```

它在我们的目标目录中输出一个`depdot.dot`文件，可以使用 Graphviz([http://www.graphviz.org/](http://www.graphviz.org/))打开这个。参考下面的截图，看看 Graphviz 中的`.dot`文件是什么样子的:

![Transitive dependency stated explicitly in the SBT dependency](graphics/3812_06_02.jpg)

正如我们在依赖树的第 96 行和第 573 行看到的(参考[https://github . com/arun ma/scaladataanalysiskook/blob/master/chapter 6-scaling up/depgraph _ xmlbeans _ duplicate . txt](https://github.com/arunma/ScalaDataAnalysisCookbook/blob/master/chapter6-scalingup/depgraph_xmlbeans_duplicate.txt)；给出了它的截图)`xmlbeans`的导入有两个实例:一个在通向`org.scalanlp:epic-parser-en-span_2.10:2015.2.19`的树中，一个在通向`org.scalanlp:epic_2.10:0.3.1`的树中。如果你注意到`epic-parser`图书馆的第二层，你会意识到它就是`epic`图书馆本身。

因此，我们可以通过从我们的`build.sbt`文件的依赖列表中删除`scalanlp:epic_2.10:0.3.1`来解决这个错误。

#### 两个不同的库依赖于同一个外部库

即使在我们移除了`epic`库之后，我们仍然会看到关于`xercesImpl`和`xmlapi`jar 的一些问题。当我们分析依赖树时，我们看到`epic` 的两个依赖库依赖于`xerces`、`xml` API 和`scala`库本身！

![Two different libraries depend on the same external library](graphics/3812_06_03.jpg)

我们注意到 Epic 库依赖于 Scala 库，但是我们也知道 Scala 库应该已经在主节点和工作节点上可用了。我们可以使用`assemblyOption`键将 Scala 库完全排除在捆绑之外:

```
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)
```

接下来，为了从`epic`库中排除`xml-apis`库，我们使用了`exclude`函数:

```
libraryDependencies  ++= Seq(

  "org.apache.spark" %% "spark-core" % sparkVersion % "provided",

  "org.apache.spark" %% "spark-sql" % sparkVersion % "provided",

  "org.apache.spark" %% "spark-mllib" % sparkVersion % "provided",

  "com.databricks" %% "spark-csv" % "1.0.3",

  ("org.scalanlp" % "epic-parser-en-span_2.10" % "2015.2.19").

    exclude("xml-apis", "xml-apis")

)
```

至于其余的冲突文件，我们可以使用`assembly`插件的合并策略来解决冲突。因为我们正在合并多个 jar 的内容，所以很有可能在同一个路径上有一个名称相似的文件，例如，`MANIFEST.MF`。如果同一位置的文件的内容不匹配，`sbt-assembly`插件提供了各种解决冲突的策略。默认策略是抛出一个错误，但是我们可以定制策略来满足我们的需要。

在合并策略中，如果 jar 中有多个`conf`文件，我们将追加`application.conf`的内容，按照`org.cyberneko.html`包的类路径的顺序使用第一个匹配的类/文件，并丢弃所有的`manifest`文件。对于所有其他情况，我们应用默认策略:

```
assemblyMergeStrategy in assembly := {

  case "application.conf"                            => MergeStrategy.concat

  case PathList("org", "cyberneko", "html", xs @ _*) => MergeStrategy.first

  case m if m.toLowerCase.endsWith("manifest.mf")    => MergeStrategy.discard

  case f                                             => (assemblyMergeStrategy in assembly).value(f)

}
```

整个`build.sbt`看起来是这样的:

```
organization := "com.packt"

name := "chapter6-scalingup"

scalaVersion := "2.10.4"

val sparkVersion="1.4.1"

libraryDependencies  ++= Seq(

  "org.apache.spark" %% "spark-core" % sparkVersion % "provided",

  "org.apache.spark" %% "spark-sql" % sparkVersion % "provided",

  "org.apache.spark" %% "spark-mllib" % sparkVersion % "provided",

  "com.databricks" %% "spark-csv" % "1.0.3",

  ("org.scalanlp" % "epic-parser-en-span_2.10" % "2015.2.19").

    exclude("xml-apis", "xml-apis")

)

assemblyJarName in assembly := "scalada-learning-assembly.jar"

assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)

assemblyMergeStrategy in assembly := {

  case "application.conf"                            => MergeStrategy.concat

  case PathList("org", "cyberneko", "html", xs @ _*) => MergeStrategy.first

  case m if m.toLowerCase.endsWith("manifest.mf")    => MergeStrategy.discard

  case f                                             => (assemblyMergeStrategy in assembly).value(f)

}
```

所以最后，当我们做一个`sbt assembly`，`scalada-learning-assembly.jar`就被创建了。如果您想从`build.sbt`文件的名称和版本中提取 JAR 名称，只需从`build.sbt`中删除`assemblyJarName`键:

```

> sbt clean assembly

```



# 向 Spark 集群提交作业(本地)

在分布式模式下运行 Spark 涉及多个组件。在自包含应用程序模式下(到目前为止，我们已经让在本书中运行的主程序)，所有这些组件都运行在单个 JVM 上。下图详细说明了在分布式模式下运行 Scala 程序的各种组件及其功能:

![Submitting jobs to the Spark cluster (local)](graphics/3812_06_04.jpg)

作为第一步，我们使用 RDD 上的各种操作(映射、过滤、连接等)构建的 RDD 图被传递给 **有向无环图** ( **DAG** )调度器。DAG 调度程序优化了流程，并将所有 RDD 操作转换为称为阶段的任务组。通常，洗牌之前的所有任务都被打包到一个阶段中。考虑任务之间存在一对一映射的操作；例如，映射或过滤运算符为每个输入生成一个输出。如果 RDD 上的一个元素上有一个映射，后面跟着一个过滤器，那么它们通常被流水线化(映射和过滤器)以形成一个可以由单个工作者执行的单个任务，更不用说数据局部性的好处了。将这与我们传统的 Hadoop MapReduce(数据在每个阶段都被写入磁盘)联系起来，将有助于我们真正理解 Spark 谱系图。

然后，这些随机分离的阶段被传递给任务调度器，任务调度器将它们分割成任务并提交给集群管理器。Spark 附带了一个简单的集群管理器，它可以接收任务并针对一组工作节点运行任务。但是，Spark 应用程序也可以在流行的集群管理器上运行，比如 Mesos 和 YARN。

使用 YARN/Mesos，我们可以在同一个 worker 节点上运行多个执行器。此外，YARN 和 Mesos 可以在集群中托管非 Spark 作业和 Spark 作业。

![Submitting jobs to the Spark cluster (local)](graphics/3812_06_05.jpg)

在 Spark standalone 集群中，在 Spark 1.4 之前，每个应用程序的每个工作节点的执行器数量被限制为 1。然而，我们可以使用`SPARK_WORKER_INSTANCES`参数增加每个 worker 节点的 worker 实例数量。有了 Spark 1.4([https://issues.apache.org/jira/browse/SPARK-1706](https://issues.apache.org/jira/browse/SPARK-1706)，我们能够在同一个节点上运行多个执行器，就像 Mesos/YARN 中的。

### 注意

如果我们打算在一台机器上运行多个 worker 实例，我们必须确保配置`SPARK_WORKER_CORES`属性来限制每个 worker 可以使用的内核数量。默认是*全部*！

![Submitting jobs to the Spark cluster (local)](graphics/3812_06_06.jpg)

在这个菜谱中，我们将在单台机器上运行的独立集群上部署 Spark 应用程序。对于本章中的所有食谱，我们将使用我们在上一章中构建的二进制分类应用程序作为部署候选。这个食谱假设你对 HDFS 的概念和基本操作有所了解。

## 怎么做...

向本地集群提交 Spark 作业包括以下步骤:

1.  下载Spark。
2.  在伪集群模式下运行 HDFS。
3.  在本地运行 Spark 主机和从机。
4.  将数据推送到 HDFS。
5.  在集群上提交 Spark 应用程序。

### 下载 Spark

在本书中，我们一直在使用 Spark 1 . 4 . 1 版本，正如我们在`build.sbt`中看到的。现在，让我们前往下载页面(【https://spark.apache.org/downloads.html】T4)并下载`spark-1.4.1-bin-hadoop2.6.tgz`包，如下所示:

![Downloading Spark](graphics/3812_06_07.jpg)

### 在伪集群模式下运行 HDFS

让我们将文件存储在 HDFS，而不是从本地文件系统为我们的 Spark 应用程序加载文件。为了做到这一点，我们来一个本地运行的 Hadoop 2.6.0 的伪分布式集群([https://Hadoop . Apache . org/docs/stable/Hadoop-project-dist/Hadoop-common/single cluster . html # Pseudo-Distributed _ Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation))。

在使用`bin/hdfs namenode -format`格式化我们的名称节点并使用`sbin/start-dfs.sh`调出我们的数据节点和名称节点之后，让我们确认我们需要的所有流程都正常运行。我们使用`Jps`来做这件事。下面的屏幕截图显示了启动`dfs`守护进程后您将会看到的内容:

![Running HDFS on Pseudo-clustered mode](graphics/3812_06_08.jpg)

### 本地运行 Spark 主机和从机

为了将我们的 `assembly` JAR 提交给 Spark 集群，我们必须首先启动 Spark 主节点和工作节点。

要在本地机器上运行 Spark，我们需要做的就是转到下载(并解压缩)的`spark`文件夹，并从`spark`主目录运行`sbin/start-all.sh`。这将启动 Spark 的主节点和工作节点。可以从端口 8080 访问主服务器的 web 用户界面。我们使用这个端口来检查作业的状态。主服务器的默认服务端口是 7077。我们将使用这个端口将我们的`assembly` JAR 作为一个作业提交给 Spark 集群。

![Running the Spark master and slave locally](graphics/3812_06_09.jpg)

让我们使用`Jps`来确认`Master`和`Worker`节点的运行:

![Running the Spark master and slave locally](graphics/3812_06_10.jpg)

### 将数据推送到 HDFS

这只需要在 HDFS 上运行`mkdir`和`put`命令:

```

bash-3.2$ hadoop fs -mkdir /scalada

bash-3.2$ hadoop fs -put /Users/Gabriel/Apps/SMSSpamCollection /scalada/

bash-3.2$ hadoop fs -ls /scalada

Found 1 items

-rw-r--r--   1 Gabriel supergroup     477907 2015-07-18 16:59 /scalada/SMSSpamCollection

```

我们也可以通过 50070 上的 HDFS web 界面，并转到**实用程序** | **浏览文件系统**来确认这一点，如下所示:

![Pushing data into HDFS](graphics/3812_06_11.jpg)

### 在集群上提交 Spark 应用

在我们提交要在本地集群上运行的 Spark 应用程序之前，让我们更改分类程序(`BinaryClassificationSpam`)以指向 HDFS 位置:

```
val docs = sc.textFile("hdfs://localhost:9000/scalada/SMSSpamCollection").map(line => {

    val words = line.split("\t")

    Document(words.head.trim(), words.tail.mkString(" "))

  })
```

默认情况下，Spark 1.4.1 使用 Hadoop 2.2.0。现在，我们正尝试在 Hadoop 2.6.0 上运行该作业，并使用针对 Hadoop 2.6 和更高版本预构建的 Spark 二进制文件，让我们更改`build.sbt`以反映这一点:

```
libraryDependencies  ++= Seq(

  "org.apache.spark" %% "spark-core" % sparkVersion % "provided",

  "org.apache.spark" %% "spark-sql" % sparkVersion % "provided",

  "org.apache.spark" %% "spark-mllib" % sparkVersion % "provided",

  "com.databricks" %% "spark-csv" % "1.0.3",

  "org.apache.hadoop"  % "hadoop-client" % "2.6.0",

  ("org.scalanlp" % "epic-parser-en-span_2.10" % "2015.2.19").

    exclude("xml-apis", "xml-apis")

)
```

运行`sbt clean assembly`来构建优步罐子，如下所示:

![Submitting the Spark application on the cluster](graphics/3812_06_12.jpg)

```

./bin/spark-submit \

 --class com.packt.scalada.learning.BinaryClassificationSpam \

 --master spark://localhost:7077 \

 --executor-memory 2G \

 --total-executor-cores 2 \

 <project root>/target/scala-2.10/scalada-learning-assembly.jar

```

以下是输出:

下面的屏幕截图显示，我们已经在 Spark 集群上成功运行了我们的分类作业，而不是我们在上一章中使用的独立应用程序:

![Submitting the Spark application on the cluster](graphics/3812_06_13.jpg)

# 在 EC2 上运行 Spark 独立集群

创建Spark 集群并以真正分布式的模式运行我们的 Spark 作业的最简单方法是 Amazon EC2 实例。Spark 安装目录中的`ec2`文件夹包含了我们创建集群所需的所有脚本和库。让我们快速浏览一下创建第一个分布式集群的步骤。

这个食谱假设你对 Amazon EC2 生态系统有基本的了解，特别是如何生成一个新的 EC2 实例。

![Running the Spark Standalone cluster on EC2](graphics/3812_06_14.jpg)

## 怎么做...

在继续这些步骤之前，我们必须确保拥有 AWS 的访问密钥和**隐私增强邮件** ( **PEM** )文件。事实上，如果我们想要登录到机器上，在启动任何 EC2 实例之前，我们需要拥有这些。

### 创建访问密钥和 pem 文件

关于创建密钥对和 pem 密钥的说明可从[http://docs . AWS . Amazon . com/AWS C2/latest/user guide/ec2-key-pairs . html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html)获得。总之，下面是相关截图。

从用户菜单中选择**安全凭证**，如下所示:

![Creating the AccessKey and pem file](graphics/3812_06_15.jpg)

点击 **Users** 菜单，创建一个访问密钥，如下图所示。下载凭据。我们将使用它为 Spark master 和 worker 节点创建 EC2 实例:

![Creating the AccessKey and pem file](graphics/3812_06_16.jpg)

可以使用**密钥对**菜单从 EC2 实例页面中创建密钥对，如下一个屏幕截图所示。创建配对后，您的浏览器将自动下载`pem`文件:

![Creating the AccessKey and pem file](graphics/3812_06_17.jpg)

一旦你有了`pem`文件，确保对`pem`文件的文件许可是`400`。否则，将显示一条错误消息，说明您的`pem`文件的权限过于开放:

```

chmod 400 spark.pem

```

启动和运行我们的 Spark 应用程序包括以下步骤:

1.  设置环境变量。
2.  运行启动脚本。
3.  验证安装。
4.  对代码进行更改。
5.  传输数据和作业文件。
6.  将数据集加载到 HDFS。
7.  运行作业。
8.  摧毁星团。

### 设置环境变量

作为第一步，让我们将访问和秘密访问密钥作为环境变量导出。启动我们实例的`ec2`脚本将使用这些命令:

```

export AWS_ACCESS_KEY_ID=AKIAI7H3OFQZ5W6H4IBA

export AWS_SECRET_ACCESS_KEY=[YOUR SECRET ACCESS KEY]

```

我还将`pem`文件复制到 spark 安装根目录，只是为了使启动命令更短(不指定`pem`文件的完整路径)，如下所示:

![Setting the environment variables](graphics/3812_06_18.jpg)

### 运行启动脚本

现在我们已经导出了访问密钥(和密钥),根文件夹中有了`pem`文件，让我们生成一个新的集群:

```

cd spark-1.4.1-bin-hadoop2.6

./ec2/spark-ec2 --key-pair=scalada --identity-file=scalada.pem --slaves=2 --instance-type=m3.medium --hadoop-major-version=2 launch scalada-cluster

```

显而易见，这些参数代表以下内容:

*   `key-pair`:这是您作为环境变量导出的访问密钥和秘密访问密钥所属的用户名。
*   `identity-file`:这是`pem`文件的位置。
*   `slaves`:这是工作者节点的数量。
*   `instance-type`:这是 AWS 实例类型之一(【http://aws.amazon.com/ec2/instance-types/】T2)。M3 媒体有一个核心和 3.75 GB 的内存。
*   `hadoop-major-version`:这是我们希望 Spark 捆绑的 Hadoop 版本。spark 版本本身源自我们的本地安装(也就是 1.4.1)。

我们也可以从 EC2 控制台确认这一点，如下图所示:

![Running the launch script](graphics/3812_06_19.jpg)

### 验证安装

让我们登录到主服务器，查看每个节点上运行的服务:

```

ssh -i scalada.pem root@ec2-54-161-176-58.compute-1.amazonaws.com

```

在主节点上执行`jps`操作显示，Spark 主节点、HDFS 名称节点和辅助名称节点正在 Spark 主节点上运行，如该屏幕截图所示:

![Verifying installation](graphics/3812_06_20.jpg)

同样，在工作节点上，我们看到 Spark 工作节点和 HDFS 数据节点正在运行，如下所示:

![Verifying installation](graphics/3812_06_21.jpg)

### 修改代码

我们的代码需要做一个小小的改变,以便在这个集群上运行——数据集位于 HDFS。但是，建议不要这样做，URL 应该来自外部配置文件:

```
 val conf = new SparkConf().setAppName("BinaryClassificationSpamEc2")

  val sc = new SparkContext(conf)

  val sqlContext = new SQLContext(sc)

  val docs = sc.textFile("hdfs://ec2-54-159-166-156.compute-1.amazonaws.com:9000/scalada/SMSSpamCollection").map(line => {

    val words = line.split("\t")

    Document(words.head.trim(), words.tail.mkString(" "))

  })
```

### 传输数据和作业文件

下一步，让我们将数据集和`assembly` JAR 复制到主节点，以便从保存`pem`文件的目录中执行:

```

scp -i scalada.pem <REPO_DIR>/chapter5-learning/SMSSpamCollection root@ec2-54-161-176-58.compute-1.amazonaws.com:~/.

scp -i scalada.pem  <REPO_DIR>/chapter6-scalingup/target/scala-2.10/scalada-learning-assembly.jar root@ec2-54-161-176-58.compute-1.amazonaws.com:~/.

```

主文件夹上的一个`ls`确认了这一点，如下图所示:

![Transferring the data and job files](graphics/3812_06_22.jpg)

### 将数据集加载到 HDFS

现在我们已经将数据集上传到主服务器的本地文件夹，让我们将它推送到 HDFS。正如我们前面在验证安装时看到的，Spark EC2 脚本为我们创建并运行了一个 HDFS 集群。让我们转到根目录下的`ephemeral-hdfs`文件夹并格式化文件系统。请注意，顾名思义，这个 HDFS 中的文件将在重新启动群集时被擦除。理想情况下，我们应该在这些节点上安装一个单独的 HDFS 集群，而不是依赖 Spark EC2 脚本创建的临时安装。

就像在我们之前的食谱中一样，让我们将`SMSSpamCollection`数据集推到 HDFS 的`/scalada`文件夹中:

```

root@ip-10-150-76-158 ephemeral-hdfs] $ ./bin/hdfs namenode -format

root@ip-10-150-76-158 ephemeral-hdfs] $ ./bin/hadoop fs -mkdir /scalada

root@ip-10-150-76-158 ephemeral-hdfs] $ ./bin/hadoop fs -put ../SMSSpamCollection /scalada/

root@ip-10-150-76-158 ephemeral-hdfs]$ ./bin/hadoop fs -ls /scalada

Found 1 items

-rw-r--r--   3 root supergroup     477907 2015-08-08 05:24 /scalada/SMSSpamCollection

```

### 运行作业

和前面的方法一样，我们将使用脚本向集群提交作业。让我们进入 spark 主目录(`/root/spark`)并执行下面几行:

```

./bin/spark-submit \

 --class com.packt.scalada.learning.BinaryClassificationSpamEc2 \

 --master spark://ec2-54-161-176-58.compute-1.amazonaws.com:7077 \

 --executor-memory 2G \

 --total-executor-cores 2 \

 ../scalada-learning-assembly.jar

```

我们可以看到该作业在群集的两个工作节点上运行，如该屏幕截图所示:

![Running the job](graphics/3812_06_23.jpg)

我们还可以从 **Stages** 选项卡中看到该作业的各个阶段，如下图所示:

![Running the job](graphics/3812_06_24.jpg)

不足为奇的是，准确性度量几乎是相同的，只是现在我们可以使用这个集群来处理更大的数据。

![Running the job](graphics/3812_06_25.jpg)

### 摧毁集群

最后，如果您想销毁集群，您可以使用相同的`ec2`脚本和`destroy`动作。从本地 Spark 安装目录中，执行以下代码行:

```

./ec2/spark-ec2 destroy scalada-cluster

```

![Destroying the cluster](graphics/3812_06_26.jpg)

# 在 Mesos 上运行 Spark 作业(本地)

与只能运行 Spark 应用程序的 Spark standalone 集群管理器不同，Mesos 是一个可以运行各种应用程序的集群管理器，包括 Python、Ruby 或 Java EE 应用程序。它还可以运行 Spark 作业。事实上，它是 Spark 最受欢迎的集群管理器之一。在这个菜谱中，我们将看到如何在 Mesos 集群上部署 Spark 应用程序。这个方法的先决条件是一个正在运行的 HDFS 集群。

## 怎么做...

在 Mesos 上运行 Spark 作业非常类似于在独立集群上运行它。它包括以下步骤:

1.  安装 Mesos。
2.  启动 Mesos 主机和从机。
3.  上传 Spark 二进制包和数据集到 HDFS。
4.  运行作业。

### 安装 Mesos

按照 http://mesos.apache.org/gettingstarted/[的](http://mesos.apache.org/gettingstarted/)指示，通过在本地机器上下载 Mesos。

在安装了构建 Mesos 所需的特定于操作系统的工具之后，您必须运行 configure 和 make 命令(具有 root 权限)来构建 Mesos(这将花费很长时间)，除非您将`-j <number of cores> V=0`传递给`make`命令，如下所示:

![Installing Mesos](graphics/3812_06_27.jpg)

顺便提一下，就像 Spark 一样，`mesos`安装目录中的`ec2`文件夹提供了生成新 EC2 `mesos`集群的脚本。

### 启动 Mesos 主从

现在我们已经安装了 Mesos，下一步是启动 Mesos 主服务器和从服务器:

```

bash-3.2$ pwd

/Users/Gabriel/Apps/mesos-0.22.1/build

bash-3.2$ sudo ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos

```

![Starting the Mesos master and slave](graphics/3812_06_28.jpg)

在另一个终端窗口中，让我们打开一个工作节点:

```

Gabriel@Gabriels-MacBook-Pro ~/A/m/build> pwd

/Users/Gabriel/Apps/mesos-0.22.1/build

Gabriel@Gabriels-MacBook-Pro ~/A/m/build> ./bin/mesos-slave.sh --master=127.0.0.1:5050

```

![Starting the Mesos master and slave](graphics/3812_06_29.jpg)

我们现在可以在`http://127.0.0.1:5050`查看 Mesos 状态页面，这是我们将看到的内容:

![Starting the Mesos master and slave](graphics/3812_06_30.jpg)

### 将 Spark 二进制包和数据集上传到 HDFS

Mesos 要求所有的 worker 节点都在机器上安装 Spark。我们可以通过在`spark`配置中配置`spark.mesos.executor.home`属性，或者简单地将整个 Spark `tar`包上传到 HDFS 并提供给 Mesos 工作人员来实现这一点:

```

./bin/hadoop fs -mkdir /scalada

./bin/hadoop fs -put /Users/Gabriel/Apps/spark-1.4.1-bin-hadoop2.6.tgz /scalada/spark-1.4.1-bin-hadoop2.6.tgz

```

让我们把Spark二进制设定为执行者 URI

```

export SPARK_EXECUTOR_URI=hdfs://localhost:9000/scalada/spark-1.4.1-bin-hadoop2.6.tgz

```

另外，让我们将数据集上传到 HDFS:

```

./bin/hadoop fs -mkdir /scalada

./bin/hadoop fs -put /Users/Gabriel/Apps/SMSSpamCollection /scalada/

```

### 运行作业

在运行程序本身之前，我们需要做一件事——配置`libmesos`本地库的位置。该文件可以在`/usr/local/lib`文件夹中以`libmesos.so`或`libmesos.dylib`的名称找到，这取决于您的操作系统:

```

export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libmesos-0.22.1.dylib

```

![Running the job](graphics/3812_06_31.jpg)

现在，让我们使用`cd`进入 Spark 安装目录，然后运行作业:

```

cd /Users/Gabriel/Apps/spark-1.4.1-bin-hadoop2.6

export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libmesos-0.22.1.dylib

./bin/spark-submit \

 --class com.packt.scalada.learning.BinaryClassificationSpamMesos \

 --master mesos://localhost:5050 \

 --executor-memory 2G \

 --total-executor-cores 2 \

 <REPO_FOLDER>/chapter6-scalingup/target/scala-2.10/scalada-learning-assembly.jar

```

正如您在下面的屏幕截图中看到的，任务在这个`single-worker-node`集群上运行良好:

![Running the job](graphics/3812_06_32.jpg)

下一个屏幕截图显示了已经完成的任务列表:

![Running the job](graphics/3812_06_33.jpg)

# 在纱线上运行Spark作业(本地)

Hadoop 有着悠久的历史，在大多数情况下，组织在将他们的 MR 工作转移到 Spark 之前，已经投资了 Hadoop 基础架构。与只能运行 Spark 作业的 Spark 独立集群管理器和可以运行各种应用的 Mesos 不同，YARN 可以一流地运行 Hadoop 作业。同时，它还可以运行 Spark 作业。这意味着当一个团队决定用 Spark 作业替换他们的一些 MR 作业时，他们可以使用同一个集群管理器来运行 Spark 作业。在这个菜谱中，我们将看到如何在 YARN cluster manager 上部署 Spark 应用程序。

## 怎么做...

在 YARN 上运行 Spark 作业与在 Spark 独立集群上运行非常相似。它包括以下步骤:

1.  安装 Hadoop 集群。
2.  开始 HDFS 和纱线。
3.  将 Spark 组件和数据集推送到 HDFS。
4.  在`yarn-client`模式下运行 Spark 作业。
5.  在`yarn-cluster`模式下运行Spark作业。

### 安装 Hadoop 集群

虽然集群本身的设置超出了本菜谱的范围，但是为了完整起见，让我们快速查看一下在本地机器上设置单节点伪分布式集群时所做的相关站点 XML 配置。参考[http://www . bogotobogo . com/Hadoop/BigData _ Hadoop _ Install _ on _ Ubuntu _ single _ node _ cluster . PHP](http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php)了解如何建立本地 YARN/HDFS 集群的完整细节:

`core-site.xml`文件:

```
<configuration>

  <property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value>

  </property>

</configuration>
```

`mapred-site.xml`文件:

```
<configuration>

   <property>

       <name>mapred.job.tracker</name>

       <value>localhost:54311</value>

   </property>

</configuration>
```

`hdfs-site.xml`文件:

```
<configuration>

  <property>

    <name>dfs.replication</name>

    <value>1</value>

  </property>

</configuration>
```

### 首发 HDFS 和纱

一旦集群的设置完成，让我们格式化 HDFS 并启动集群(`dfs`和`yarn`):

格式`namenode`:

```

hdfs namenode -format

```

启动 HDFS 和纱线:

```

sbin/start-all.sh

```

让我们确认服务正在通过`jps`运行，这是我们应该看到的:

![Starting HDFS and YARN](graphics/3812_06_34.jpg)

### 将 Spark 组件和数据集推送到 HDFS

理想情况下，当我们做一个 `spark-submit`，纱应该能够选择我们的`spark-assembly`罐(或优步罐)并上传到 HDFS。但是，这不会正确发生，并会导致以下错误:

```

Error: Could not find or load main class org.apache.spark.deploy.yarn.ExecutorLauncher

```

为了解决这个问题，让我们手动上传我们的`spark-assembly` JAR 到 HDFS，并修改我们的`conf/spark-env.sh`来反映位置。Hadoop 配置目录也应该在`spark-env.sh`中指定:

上传Spark装配到 HDFS。

```

hadoop fs -mkdir /sparkbinary

hadoop fs -put /Users/Gabriel/Apps/spark-1.4.1-bin-hadoop2.6/lib/spark-assembly-1.4.1-hadoop2.6.0.jar /sparkbinary/

hadoop fs -ls /sparkbinary

```

将垃圾邮件数据集上传到 HDFS:

```

hadoop fs -mkdir /scalada

hadoop fs -put ~/SMSSpamCollection /scalada/

hadoop fs -ls /scalada

```

`spark-env.sh`中的条目:

```

HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

SPARK_EXECUTOR_URI=hdfs://localhost:9000/sparkbinary/spark-assembly-1.4.1-hadoop2.6.0.jar

```

![Pushing Spark assembly and dataset to HDFS](graphics/3812_06_35.jpg)

在我们提交我们的 Spark 作业到纱线集群之前，让我们确认我们的设置使用 Spark shell 是好的。Spark shell 是 Scala REPL 的一个包装器，在类路径中设置了 Spark 库。配置`HADOOP_CONF_DIR`指向 Hadoop `config`目录确保 Spark 现在将使用 YARN 来运行它的任务。然而，在 YARN 中有两种模式可以运行 Spark 作业，即`yarn-client`和`yarn-cluster`。让我们在这个子主题中探索这两个主题。但在此之前，为了验证我们的配置，我们将启动 Spark shell，将`master`指向`yarn-client`。在大量日志之后，我们应该能够看到一个 Scala 提示符。这证实了我们的配置是好的:

```

bin/spark-shell --master yarn-client

```

![Pushing Spark assembly and dataset to HDFS](graphics/3812_06_36.jpg)

### 在 yarn-client 模式下运行 Spark 作业

现在，我们已经确认了外壳可以很好地加载到纱线主控件上，让我们开始部署我们在纱线上的 Spark 工作。

正如我们之前讨论的，有两种模式可以在 YARN 上运行 Spark 应用程序:`yarn-client`模式和`yarn-cluster`模式。在`yarn-client`模式下，驱动程序驻留在客户端，纱线工作节点仅用于执行作业。应用程序的所有大脑都驻留在客户机 JVM 中，它轮询应用程序主机的状态。应用程序主机除了监视执行器节点的故障，并相应地向资源管理器报告和请求资源之外，什么也不做。这也意味着只要应用程序执行，客户机(我们的驱动程序 JVM)就需要运行:

```

./bin/spark-submit \

 --class com.packt.scalada.learning.BinaryClassificationSpamYarn \

 --master yarn-client \

 --executor-memory 1G \

 ~/scalada-learning-assembly.jar

```

![Running a Spark job in yarn-client mode](graphics/3812_06_37.jpg)

正如我们从纱线控制台看到的，我们的工作运行良好。下面是显示这一点的截图:

![Running a Spark job in yarn-client mode](graphics/3812_06_38.jpg)

最后，我们可以在客户端 JVM(驱动程序)上看到输出:

![Running a Spark job in yarn-client mode](graphics/3812_06_39.jpg)

### 以纱簇模式运行Spark作业

在`yarn-cluster`模式中，客户端 JVM 根本不做任何事情。事实上，它只是提交和轮询应用程序主服务器的状态。驱动程序本身在应用主机上运行，应用主机现在拥有程序的所有大脑。与`yarn-client`模式不同，用户日志不会显示在客户机 JVM 上，因为合并结果的驱动程序是在 YARN 集群内部执行的:

```

./bin/spark-submit \

 --class com.packt.scalada.learning.BinaryClassificationSpamYarn \

 --master yarn-cluster \

 --executor-memory 1g \

 ~/scalada-learning-assembly.jar

```

![Running Spark job in yarn-cluster mode](graphics/3812_06_40.jpg)

正如所料，客户机 JVM 指示作业已经成功运行。但是，它不显示用户日志。

![Running Spark job in yarn-cluster mode](graphics/3812_06_41.jpg)

下面的屏幕截图显示了我们的客户端和集群模式运行的最终状态:

![Running Spark job in yarn-cluster mode](graphics/3812_06_42.jpg)

这个程序的实际输出在 Hadoop 用户日志中。当我们单击应用程序链接，然后单击控制台中的日志链接时，我们可以转到 Hadoop 的日志目录，或者从 Hadoop 控制台本身检查它。

正如你在下面的截图中看到的，`stdout`文件显示了我们尴尬的`println`命令:

![Running Spark job in yarn-cluster mode](graphics/3812_06_43.jpg)

在本章中，我们以 Spark 应用程序为例，将其部署在 Spark 独立集群管理器、YARN 和 Mesos 上。在这个过程中，我们触及了这些集群管理器的内部。