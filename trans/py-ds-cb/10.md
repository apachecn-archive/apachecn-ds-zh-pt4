

# 十、大规模机器学习——在线学习

在本章中，我们将看到以下食谱:

*   使用感知器作为在线线性算法
*   使用随机梯度下降进行回归
*   使用随机梯度下降进行分类



# 简介

在这一章中，我们将专注于大规模机器学习和适合处理这种大规模问题的算法。到目前为止，当我们训练所有的模型时，我们假设我们的训练集可以适合我们的计算机内存。在这一章中，我们将看到当这个假设不再满足时，如何着手建立模型。我们的训练记录非常庞大，所以我们无法将它们完全放入我们的记忆中。我们可能不得不分段加载它们，并且仍然产生具有良好精度的模型。训练集不适合我们的计算机内存的论点可以外推至流数据。对于流式数据，我们不会一次看到所有数据。我们应该能够基于我们所接触到的任何数据做出决策，并且还应该有一种随着新数据的到来而不断改进我们的模型的机制。

我们将介绍基于随机梯度下降算法的框架。这是一个通用的框架，可以处理无法完全放入我们的内存的大规模数据集。使用这个框架可以适应几种类型的线性算法，包括逻辑回归、线性回归和线性 SVM。为了处理具有非线性关系的数据集，我们在前一章介绍的内核技巧可以包含在这个框架中。

我们将从最古老的机器学习算法——感知器算法开始我们的食谱列表。感知器很容易理解和实现。然而，感知器仅限于解决线性问题。基于核的感知器可以用来解决非线性数据集。

在我们的第二个配方中，我们将正式介绍基于梯度下降的方法的框架，以及如何使用它来执行基于回归的任务。我们将研究不同的损失函数，看看如何使用这些函数构建不同类型的线性模型。我们还将看到感知器如何属于随机梯度下降的家族。

在我们的最终配方中，我们将看到如何使用随机梯度下降框架构建分类算法。

尽管我们没有流数据的直接示例，但通过我们现有的数据集，我们将看到如何解决流数据用例。在线学习算法不限于流数据，它们也可以应用于批量数据，只是它们一次只处理一个实例。



# 使用感知器作为在线学习算法

如前所述，感知器是最古老的机器学习算法之一。它在 1943 年的一篇论文中首次被提及:

神经活动中固有观念的逻辑演算。WARREN S. MCCULLOCH 和 WALTER PITTS 伊利诺伊大学医学院，伊利诺伊神经精神病学研究所精神病学系，美国芝加哥大学，芝加哥

让我们重温一下我们对分类问题的定义。每个记录或实例可以写成一个集合(X，y)，其中 X 是一组属性，y 是相应的类标签。

学习一个目标函数 F，它将每个记录的属性集映射到一个预定义的类标签 y，这是分类算法的工作。

我们案例的不同之处在于，我们有一个大规模的学习问题。我们所有的数据都放不进我们的主存。因此，我们需要将数据保存在磁盘上，并且每次只使用其中的一部分，以便构建我们的感知机模型。

让我们继续概述感知器算法:

1.  将模型的权重初始化为一个小的随机数。
2.  将输入数据`x`的平均值居中。
3.  在每个时间步 t(也称为历元):

    *   混洗数据集
    *   挑选记录的单个实例并进行预测
    *   观察预测与真实标签输出的偏差
    *   如果预测不同于真实标签

让我们考虑下面的场景。我们的磁盘上有完整的数据集。在单个时期中，即在步骤 3 中，对我们磁盘上的所有数据执行所有提到的步骤。在一个在线学习场景中，我们可以在任何时间点使用一系列基于窗口函数的实例。我们可以在单个时期内根据我们的窗口中的实例数量来更新权重。

让我们看看如何更新我们的权重。

假设我们的输入 X 如下:

![Using perceptron as an online learning algorithm](../images/00208.jpeg)

我们的`Y`如下:

![Using perceptron as an online learning algorithm](../images/00209.jpeg)

我们将我们的权重定义为以下等式:

![Using perceptron as an online learning algorithm](../images/00210.jpeg)

我们看到每条记录后的预测定义如下:

![Using perceptron as an online learning algorithm](../images/00211.jpeg)

如果权重和属性的乘积为正，则 sign 函数返回+1，如果乘积为负，则返回-1。

感知器继续将预测的 y 与实际的 y 进行比较。如果预测的 y 是正确的，它将继续下一条记录。如果预测不正确，有两种情况。如果预测的 y 是+1，而实际的 y 是-1，它用 x 值减少权重，反之亦然。如果实际的 y 是+1，而预测的 y 是-1，它将增加权重。为了更清楚起见，让我们把它看作一个等式:

![Using perceptron as an online learning algorithm](../images/00212.jpeg)

通常，提供学习速率α，以便以受控的方式更新权重。在数据中存在噪声的情况下，全增量递减将导致权重不收敛:

![Using perceptron as an online learning algorithm](../images/00213.jpeg)

Alpha 是一个很小的值，在 0.1 到 0.4 之间。

现在让我们开始我们的食谱吧。



## 准备就绪

让我们用生成器函数批量使用`make_classification`生成数据来模拟大规模数据和数据流，并着手编写感知器算法。



## 怎么做……

让我们加载必要的库。然后，将编写一个函数，`get_data`，它是一个生成器:

```
from sklearn.datasets import make_classification
from sklearn.metrics import  classification_report
from sklearn.preprocessing import scale
import numpy as np

def get_data(batch_size):
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    b_size = 0
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.8*no_features)
    repeated_features = int(0.1*no_features)

    while b_size < batch_size:
        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
                n_informative = informative_features, n_redundant = redundant_features \
                ,n_repeated = repeated_features, random_state=51)
        y_indx = y < 1
        y[y_indx] = -1
        x = scale(x,with_mean=True,with_std=True)

        yield x,y
        b_size+=1
```

我们将继续编写两个函数，一个构建我们的感知器模型，另一个测试我们模型的价值:

```
def build_model(x,y,weights,epochs,alpha=0.5):
    """
    Simple Perceptron
    """

    for i in range(epochs):

        # Shuffle the dataset
        shuff_index = np.random.shuffle(range(len(y)))
        x_train = x[shuff_index,:].reshape(x.shape)
        y_train = np.ravel(y[shuff_index,:])

        # Build weights one instance at a time
        for index in range(len(y)):
            prediction = np.sign( np.sum(x_train[index,:] * weights) ) 
            if prediction != y_train[index]:
                weights = weights + alpha * (y_train[index] * x_train[index,:])

    return weights

def model_worth(x,y,weights):
    prediction = np.sign(np.sum(x * weights,axis=1))
print classification_report(y,prediction)
```

最后，我们将编写我们的主函数来调用所有前面的函数，以演示感知器算法:

```
if __name__ == "__main__":
    data = get_data(10)    
    x,y = data.next()
    weights = np.zeros(x.shape[1])    
    for i in range(10):
        epochs = 100
        weights = build_model(x,y,weights,epochs)
        print
        print "Model worth after receiving dataset batch %d"%(i+1)    
        model_worth(x,y,weights)
        print
        if i < 9:
            x,y = data.next()
```



## 它是如何工作的……

先说我们的主函数。我们将要求我们的生成器向我们发送 10 组数据:

```
    data = get_data(10)    
```

这里，我们既要模拟大规模数据，也要模拟数据流。构建模型时，我们无法访问所有数据，只能访问其中的一部分:

```
    x,y = data.next()
```

我们将使用生成器中的`next()`函数来获取下一组数据。在`get_data`函数中，我们将使用 scikit-learn 中的`make_classification`函数:

```
        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
                n_informative = informative_features, n_redundant = redundant_features \
                ,n_repeated = repeated_features, random_state=51)
```

让我们看看将参数传递给的`make_classification`方法。第一个参数是所需实例的数量，在本例中，我们需要 1，000 个实例。第二个参数是关于每个实例需要多少属性。我们假设我们需要 30 个。第三个参数`flip_y`，随机交换 3%的实例。这样做是为了在我们的数据中引入一些噪声。下一个参数是关于这 30 个特征，以及它们中的多少个应该足够丰富以用于我们的分类。我们指定 60%的特征，也就是 30 个特征中的 18 个，应该是信息性的。下一个参数是关于冗余功能。这些是作为信息特征的线性组合产生的，以便在特征之间引入相关性。最后，重复要素是从信息要素和冗余要素中随机抽取的重复要素。

当我们调用`next()`时，我们将获得该数据的 1000 个实例。这个函数返回一个 y 标签为`{0,1}`；我们想要`{-1,+1}`，因此我们将把 y 中的所有零都改为`-1`:

```
        y_indx = y < 1
        y[y_indx] = -1
```

最后，我们将使用 scikit-learn 中的 scale 函数使我们的数据居中。

让我们继续用第一批数据构建我们的模型。我们将用零初始化权重矩阵:

```
    weights = np.zeros(x.shape[1])    
```

由于我们需要 10 批数据来模拟大规模学习和数据流，我们将在 for 循环中进行 10 次模型构建:

```
    for i in range(10):
        epochs = 100
        weights = build_model(x,y,weights,epochs)
```

我们的感知器算法内置于`build_model`。预测值 x、响应变量 y、权重矩阵和时间步长或时段数作为参数传递。在我们的例子中，我们已经将历元的数量设置为`100`。这个函数有一个额外的参数，alpha 值:

```
def build_model(x,y,weights,epochs,alpha=0.5)
```

默认情况下，我们将 alpha 值设置为`0.5`。

让我们看看我们的`build_model`。我们将从打乱数据开始:

```
        # Shuffle the dataset
        shuff_index = np.random.shuffle(range(len(y)))
        x_train = x[shuff_index,:].reshape(x.shape)
        y_train = np.ravel(y[shuff_index,:])
```

我们将检查数据集中的每条记录，并开始更新我们的权重:

```

 # Build weights one instance at a time
        for index in range(len(y)):
            prediction = np.sign( np.sum(x_train[index,:] * weights) ) 
            if prediction != y_train[index]:
                weights = weights + alpha * (y_train[index] * x_train[index,:])
```

在 for 循环中，您可以看到我们进行了预测:

```
            prediction = np.sign( np.sum(x_train[index,:] * weights) ) 
```

我们将训练数据乘以权重，然后相加。最后，我们将使用 np.sign 函数来得到我们的预测。现在，基于预测，我们将更新我们的权重:

```
                weights = weights + alpha * (y_train[index] * x_train[index,:])
```

仅此而已。我们将把权重返回给调用函数。

在我们的主函数中，我们将调用`model_worth`函数来打印模型的良好度。这里，我们将使用`classification_report`convenience 函数来打印模型的准确度分数:

```
        print
        print "Model worth after receiving dataset batch %d"%(i+1)    
        model_worth(x,y,weights)
```

然后我们将继续为的下一批输入数据更新我们的模型。注意，我们没有改变`weights`参数。它会随着每一批新数据的到来而更新。

让我们看看`model_worth`打印了什么:

![How it works…](../images/00214.jpeg)

## 还有更多……

Scikit-learn 为我们提供了感知器的实现。有关更多详细信息，请参考以下 URL:

[http://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。Perceptron.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)。

感知器算法的另一个改进是使用更多的特性。

记住预测方程，我们可以改写如下:

![There's more…](../images/00215.jpeg)

我们用一个函数替换了 x 值。在这里，我们可以发送一个特征生成器。例如，多项式特征生成器可以添加到我们的`get_data`函数中，如下所示:

```
def get_data(batch_size):
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    b_size = 0
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.8*no_features)
    repeated_features = int(0.1*no_features)
    poly = PolynomialFeatures(degree=2)

    while b_size < batch_size:
        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
                n_informative = informative_features, n_redundant = redundant_features \
                ,n_repeated = repeated_features, random_state=51)
        y_indx = y < 1
        y[y_indx] = -1
        x = poly.fit_transform(x)
        yield x,y
        b_size+=1
```

最后，基于内核的感知器算法可用于处理非线性数据集。关于基于内核的感知机的更多信息，请参考维基百科的文章:

[https://en.wikipedia.org/wiki/Kernel_perceptron](https://en.wikipedia.org/wiki/Kernel_perceptron)。



## 参见

*   *学习和使用内核*秘方[第五章](part0066_split_000.html#1UU541-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 5. Data Mining – Needle in a Haystack")、*数据挖掘——大海捞针*



# 使用随机梯度下降进行回归

在典型回归设置中，我们有一组预测值(实例)，如下所示:

![Using stochastic gradient descent for regression](../images/00216.jpeg)

每个实例有 m 个属性，如下所示:

![Using stochastic gradient descent for regression](../images/00217.jpeg)

响应变量 Y 是实值条目的向量。回归的工作是找到一个函数，使得当 x 作为输入提供给这个函数时，它应该返回 y:

![Using stochastic gradient descent for regression](../images/00218.jpeg)

前面的函数由一个权重向量参数化，即权重向量和输入向量的组合用于预测`Y`，因此用权重向量重写函数将得到如下结果:

![Using stochastic gradient descent for regression](../images/00219.jpeg)

现在的问题是，我们如何知道我们有正确的权重向量？我们将使用损失函数 L 来获得正确的权重向量。损失函数衡量做出一个错误预测的成本。当实际值为 y 时，它根据经验测量预测 y 的成本。回归问题现在变成了寻找将最小化损失函数的正确权重向量的问题。对于我们的整个数据集的`n`元素，总损失函数如下:

![Using stochastic gradient descent for regression](../images/00220.jpeg)

我们的权重向量应该是使前面的值最小化的向量。

梯度下降是一种优化技术，用于最小化前面的方程。对于这个方程，我们会找到梯度，也就是关于 w 的一阶导数。

与批量梯度下降等其他优化技术不同，随机梯度下降一次只对一个实例进行操作。随机梯度下降涉及的步骤如下:

1.  对于每个时期，混洗数据集。
2.  选择一个实例及其响应变量 y。
3.  计算损失函数及其导数，重量比。
4.  更新权重。

假设:

![Using stochastic gradient descent for regression](../images/00221.jpeg)

这表示导数 w . r . t . w。权重更新如下:

![Using stochastic gradient descent for regression](../images/00222.jpeg)

如您所见，权重在与梯度相反的方向上移动，从而强制下降，这将最终给出权重向量值，这可以减少目标成本函数。

平方损失是用于回归的典型损失函数。实例的平方损失定义如下:

![Using stochastic gradient descent for regression](../images/00223.jpeg)

将前面方程的导数代入权重更新方程。有了这些背景知识，让我们继续研究随机梯度下降回归的方法。

如感知器中所解释的，学习率η被添加到权重更新等式中，以避免噪声的影响:

![Using stochastic gradient descent for regression](../images/00224.jpeg)

## 准备就绪

我们将利用 scikit-learn 的 SGD 回归实现。与前面的一些配方一样，我们将使用 scikit-learn 的`make_regression`函数为我们的配方生成数据，以展示随机梯度下降回归。



## 怎么做……

让我们从一个非常简单的例子开始，演示如何构建随机梯度下降回归器。

我们将首先加载所需的库。然后，我们将编写一个函数来生成预测值和响应变量，以演示回归:

```
from sklearn.datasets import make_regression
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.cross_validation import train_test_split

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30

    x,y = make_regression(n_samples=1000,n_features=no_features,\
             random_state=51)
    return x,y
```

我们将继续编写函数来帮助我们构建、验证和检查我们的模型:

```
def build_model(x,y):
    estimator = SGDRegressor(n_iter = 10, shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='none')
    estimator.fit(x,y)

    return estimator

def model_worth(model,x,y):
    predicted_y = model.predict(x)
    print "\nMean absolute error = %0.2f"%mean_absolute_error(y,predicted_y)
    print "Mean squared error = %0.2f"%mean_squared_error(y,predicted_y)

def inspect_model(model):
    print "\nModel Itercept {0}".format(model.intercept_)
    print
    for i,coef in enumerate(model.coef_):
        print "Coefficient {0} = {1:.3f}".format(i+1,coef)
```

最后，我们将编写主函数来调用前面的所有函数:

```
if __name__ == "__main__":
    x,y = get_data()

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)

    model = build_model(x_train,y_train)

    inspect_model(model)

    print "Model worth on train data"
    model_worth(model,x_train,y_train)
    print "Model worth on dev data"
    model_worth(model,x_dev,y_dev)

    # Building model with l2 regularization
    model = build_model_regularized(x_train,y_train)
    inspect_model(model)
```



## 它是如何工作的……

让我们从我们的主函数开始。我们将调用`get_data`函数来生成我们的预测值 x 和响应变量 y:

```
    x,y = get_data()
```

在`get_data`函数中，我们将利用来自 scikit-learn 的方便的`make_regression`函数来为回归问题生成数据集:

```
    no_features = 30
    x,y = make_regression(n_samples=1000,n_features=no_features,\
             random_state=51)	
```

如您所见，我们将生成一个数据集，其中包含由一个`n_samples`参数指定的 1000 个实例，以及由一个`n_features`参数定义的 30 个特征。

让我们使用`train_test_split`将数据分成训练集和测试集。我们将保留 30%的数据进行测试:

```
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
```

我们将再次利用`train_test_split`将我们的测试数据分成开发和测试集:

```
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
```

将数据划分为构建、评估和测试模型，我们将继续构建我们的模型。

我们将使用我们的训练数据集调用`build_model`函数:

```
    model = build_model(x_train,y_train)
```

在`build_model`中，我们将利用 scikit-learn 的 SGD 回归器类来构建我们的随机梯度下降法:

```
    estimator = SGDRegressor(n_iter = 10, shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='none')
    estimator.fit(x,y)
```

SGD 回归器是一个庞大的方法，可用于拟合许多带有大量参数的线性模型。我们将首先解释随机梯度下降的基本方法，然后继续解释其他细节。

让我们看看我们使用的参数。第一个参数是为了更新权重，我们需要遍历数据集的次数。这里，我们会说我们想要 10 次迭代。就像在感知器中一样，在一次检查完所有记录后，当我们开始下一次迭代时，我们需要打乱输入记录。参数混洗用于相同的目的。shuffle 的缺省值是 true，我们将它包含在这里是为了便于解释。我们的损失函数是平方损失，我们想做一个线性回归；因此，我们将使用损耗参数来指定这一点。

我们的学习速率 eta 是一个常数，我们将使用`learning_rate`参数指定它。我们将使用 eta `0`参数为我们的学习率提供一个值。然后，我们会说，我们需要拟合截距，因为我们没有将我们的数据以其平均值为中心。最后，惩罚参数控制所需的收缩类型。在我们的例子中，我们不需要使用 none 字符串进行任何收缩。

我们将通过调用带有预测器和响应变量的拟合函数来构建我们的模型。最后，我们将把我们建立的模型返回给我们的调用函数。

现在让我们检查一下我们的模型，看看截距和系数的值:

```
    inspect_model(model)
```

在 inspect 模型中，我们将打印模型截距和系数的值:

![How it works…](../images/00225.jpeg)

现在让我们看看我们的模型在训练数据中的表现如何:

```
    print "Model worth on train data"
    model_worth(model,x_train,y_train)
```

我们将调用 model_worth 函数来查看我们的模型的性能。model_worth 函数打印平均绝对误差和均方误差值。

均方误差定义如下:

![How it works…](../images/00226.jpeg)

平均绝对误差定义如下:

![How it works…](../images/00227.jpeg)

平均平方误差对异常值敏感。因此，平均绝对误差是更稳健的度量。让我们使用训练数据来看看模型的性能:

![How it works…](../images/00228.jpeg)

现在，让我们使用开发数据来看看模型的性能:

![How it works…](../images/00229.jpeg)

## 还有更多……

我们可以在随机梯度下降框架中包括正则化。回想一下上一章中岭回归的以下成本函数:

![There's more…](../images/00230.jpeg)

我们在这里包括了平方损失函数的扩展版本，并添加了正则化项——权重平方的和。我们可以把它包括在我们的梯度下降程序中。假设我们把正则项记为 R(W)。我们的重量更新如下:

![There's more…](../images/00231.jpeg)

如您所见，现在我们有了损失函数相对于权重向量 w 的导数，正则化项相对于权重的导数被添加到我们的权重更新规则中。

让我们编写一个新函数来构建包含正则化的模型:

```
def build_model_regularized(x,y):
    estimator = SGDRegressor(n_iter = 10,shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='l2',alpha=0.01)
    estimator.fit(x,y)

    return estimator
```

我们可以从我们的 main 函数调用这个函数，如下所示:

```
model = build_model_regularized(x_train,y_train)
inspect_model(model)
```

让我们来看看与之前的构建模型方法相比，我们传递的新参数:

```
    estimator = SGDRegressor(n_iter = 10,shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='l2',alpha=0.01)
```

早些时候，我们提到我们的处罚为无。现在，你可以看到，我们提到，我们需要添加一个 L2 惩罚到我们的模型。同样，我们将使用`alpha`参数给出`0.01`的`alpha`值。让我们看看我们的系数:

![There's more…](../images/00232.jpeg)

你可以看到 L2 正则化的效果:许多系数已经达到零值。类似地，结合了 L1 正则化和 L2 正则化的 L1 正则化和弹性网可以使用罚参数来包含。

记得在我们的介绍中，我们提到过随机梯度下降更多的是一个框架，而不是单一的方法。通过改变损失函数，可以使用该框架生成其他线性模型。

可以使用ε不敏感损失函数建立 SVM 回归模型。该损失函数定义如下:

![There's more…](../images/00233.jpeg)

有关可传递给 scikit-learn 中的 SGD 回归器的各种参数，请参考下面的URL:

[http://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。SGDRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)。



## 参见

*   *在[第 7 章](part0078_split_000.html#2ACBS1-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 7. Machine Learning 2")、*机器学习 II* 中使用回归*公式预测实数值
*   *收缩使用岭回归*配方[第七章](part0078_split_000.html#2ACBS1-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 7. Machine Learning 2")，*机器学习 II*



# 使用随机梯度下降进行分类

除了响应变量之外，分类问题设置与回归设置非常相似。在分类设置中，响应是一个分类变量。由于其性质，我们有一个不同的损失函数来衡量错误预测的成本。让我们为我们的讨论和食谱假设一个二元分类器，我们的目标变量 Y 可以取值{ `0`，`1` }。

我们将在我们的权重更新规则中使用这个损失函数的导数来得到我们的权重向量。

scikit-learn 的 SGD 分类器类为我们提供了各种损失函数。然而，在这个配方中，我们将看到对数损失，这将为我们提供逻辑回归。

逻辑回归将线性模型拟合到以下形式的数据:

![Using stochastic gradient descent for classification](../images/00234.jpeg)

我们已经给出了一个通用的符号。截距被假定为我们的权重向量的第一维。对于二元分类问题，应用 logit 函数来获得预测。如下所示:

![Using stochastic gradient descent for classification](../images/00235.jpeg)

前面的函数也称为sigmoid 函数。对于 x_i 的非常大的正值，这个函数将返回一个接近 1 的值，对于接近 0 的大负值，反之亦然。据此，我们可以将对数损失函数定义如下:

![Using stochastic gradient descent for classification](../images/00236.jpeg)

通过将前面的损失函数拟合到梯度下降的权重更新规则中，我们可以得到适当的权重向量。

有关 scikit-learn 中定义的日志丢失函数，请参考以下 URL:

[http://sci kit-learn . org/stable/modules/generated/sk learn . metrics . log _ loss . html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)。

有了这些知识，让我们进入基于随机梯度下降的分类。



## 准备就绪

我们将利用 scikit-learn 的随机梯度下降分类器的实现。正如我们在之前的一些配方中所做的那样，我们将使用 scikit-learn 的`make_classification`函数为我们的配方生成数据，以展示随机梯度下降分类。



## 怎么做……

让我们从一个非常简单的例子开始，演示如何构建随机梯度下降回归器。

我们将首先加载所需的库。然后，我们将编写一个函数来生成预测值和响应变量:

```
from sklearn.datasets import make_classification
from sklearn.metrics import  accuracy_score
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import SGDClassifier

import numpy as np

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y
```

我们将继续编写有助于我们构建和验证模型的函数:

```
def build_model(x,y,x_dev,y_dev):
    estimator = SGDClassifier(n_iter=50,shuffle=True,loss="log", \
                learning_rate = "constant",eta0=0.0001,fit_intercept=True, penalty="none")
    estimator.fit(x,y)
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)

    print 
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)
```

最后，我们将编写我们的主函数来调用所有前面的函数:

```
if __name__ == "__main__":
    x,y = get_data()    

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)

    build_model(x_train,y_train,x_dev,y_dev)
```



## 它是如何工作的……

先说我们的主函数。我们将调用`get_data`来获得我们的`x`预测器属性和`y`响应属性。在`get_data`中，我们将利用`make_classification`数据集为随机森林方法生成训练数据:

```
def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y
```

让我们看看传递给`make_classification`方法的参数。第一个参数是所需的实例数量。在这种情况下，我们需要 500 个实例。第二个参数是关于每个实例需要多少属性。我们说我们需要 30 个。第三个参数`flip_y`，随机交换 3%的实例。这样做是为了在我们的数据中引入噪声。下一个参数是关于这 30 个特征中有多少应该足够信息丰富以用于我们的分类。我们指定 60%的特征，也就是 30 个特征中的 18 个，应该是信息性的。下一个参数是关于冗余功能。这些生成为信息性特征的线性组合，以便在特征间引入相关性。最后，重复要素是从信息要素和冗余要素中随机抽取的重复要素。

让我们使用`train_test_split`将数据分成训练集和测试集。我们将保留 30%的数据进行测试:

```
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
```

我们将再次利用`train_test_split`将我们的测试数据分成开发和测试集:

```
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
```

将数据划分为构建、评估和测试模型，我们将继续构建我们的模型:

```
build_model(x_train,y_train,x_dev,y_dev)
```

在`build_model`中，我们将利用 scikit-learn 的`SGDClassifier`类来构建我们的随机梯度下降方法:

```
    estimator = SGDClassifier(n_iter=50,shuffle=True,loss="log", \
                learning_rate = "constant",eta0=0.0001,fit_intercept=True, penalty="none")
```

让我们看看我们使用的参数。第一个参数是我们希望通过数据集更新权重的次数。这里，我们说我们想要 50 次迭代。就像在感知器中一样，在一次检查完所有记录后，当我们开始下一次迭代时，我们需要打乱输入记录。shuffle 参数用于相同的目的。shuffle 的缺省值是 true，我们将它包含在这里是为了便于解释。我们的损失函数是对数损失:我们想要做一个逻辑回归，我们将使用损失参数来指定它。我们的学习速率 eta 是一个常数，我们将使用`learning_rate`参数指定它。我们将使用 eta `0`参数提供学习率的值。然后，我们将继续说，我们需要拟合截距，因为我们没有根据它的平均值对我们的数据进行居中。最后，惩罚参数控制所需的收缩类型。在我们的例子中，我们会说，我们不需要任何收缩使用无字符串。

我们将通过使用我们的预测器和响应变量调用拟合函数来构建我们的模型，并使用我们的训练和开发数据集来评估我们的模型:

```
 estimator.fit(x,y)
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)

    print 
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)
```

让我们来看看我们的准确度分数:

![How it works…](../images/00237.jpeg)

## 还有更多……

正则化、L1、L2 或弹性网可用于 SGD 分类。该过程与回归过程相同，因此，我们在此不再赘述。请参考之前的配方。

在我们的例子中，学习速率 eta 是恒定的。情况不一定如此。随着每次迭代，eta 值可以减少。学习率参数`learning_rate`可设置为最佳字符串或无效比例。请参考以下 scikit 文档:

[http://scikit-learn.org/stable/modules/sgd.html](http://scikit-learn.org/stable/modules/sgd.html)。

该参数指定如下:

```
estimator = SGDClassifier(n_iter=50,shuffle=True,loss="log", \
learning_rate = "invscaling",eta0=0.001,fit_intercept=True, penalty="none")
```

我们用拟合的方法来建立我们的模型。如前所述，在大规模机器学习中，我们知道所有的数据不会立刻对我们可用。当我们批量接收数据时，我们需要使用`partial_fit`方法，而不是`fit`。使用`fit`方法将重新初始化权重，我们将丢失前一批数据中的所有训练信息。关于`partial_fit`的更多信息，请参考以下链接:

[http://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。SGD classifier . html # sk learn . linear _ model。SGDClassifier.partial_fit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.partial_fit) 。



## 参见

*   *收缩使用岭回归*配方[第七章](part0078_split_000.html#2ACBS1-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 7. Machine Learning 2")，*机器学习 II*
*   *使用随机梯度下降进行回归*第九章[中的配方](part0087_split_000.html#2IV0U1-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 9. Growing Trees")，*机器学习 III*