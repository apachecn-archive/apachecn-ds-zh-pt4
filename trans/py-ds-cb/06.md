<title>Chapter 6. Machine Learning 1</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 第六章。机器学习 1

在本章中，我们将讨论以下主题:

*   为模型构建准备数据
*   寻找最近的邻居
*   使用朴素贝叶斯分类文档
*   构建决策树解决多类问题

<title>Chapter 6. Machine Learning 1</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 简介

在这一章，我们将看看监督学习技术。在前一章中，我们看到了无监督技术，包括聚类和学习矢量量化。我们将从一个分类问题开始，然后进行回归。分类问题的输入是下一章中的一组记录或实例。

每个记录或实例可以写成一个集合(X，y)，其中 X 是一组属性，y 是相应的类标签。

学习将每个记录的属性集映射到一个预定义的类标签 y 的目标函数 F 是分类算法的工作。

分类算法的一般步骤如下:

1.  找到合适的算法
2.  使用训练集学习模型，并使用测试集验证模型
3.  应用模型来预测任何看不见的实例或记录

第一步是确定正确的分类算法。选择正确的算法没有规定的方法，它来自反复的试验和错误。在选择算法之后，创建训练和测试集，将其提供给算法以学习模型，即目标函数 F，如前所述。使用训练集创建模型后，将使用测试集来验证模型。通常，我们使用混淆矩阵来验证模型。我们将在菜谱中讨论更多关于混淆矩阵的内容:寻找最近邻。

我们将从一个菜谱开始，它将向我们展示如何将输入数据集划分为训练集和测试集。接下来我们将使用一种称为 K-最近邻的懒惰学习算法进行分类。然后我们将看看朴素贝叶斯分类器。我们将尝试使用决策树来处理多类问题。我们在这一章中对算法的选择不是随意的。除了二进制问题之外，我们将在这一章介绍的所有三种算法都能够处理多类问题。在多类问题中，我们有两个以上的实例所属的类标签。

<title>Preparing data for model building</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 为建模准备数据

在这个菜谱中，我们将看看如何从给定的数据集为分类问题创建一个训练和一个测试数据集。测试数据集永远不会显示给模型。在现实场景中，我们通常会构建另一个名为 dev 的数据集。Dev 代表开发数据集:我们可以使用这个数据集在连续的运行中不断地调整我们的模型。使用训练集对模型进行训练，并在 dev 中测量模型性能指标(如准确性)。基于这一结果，该模型被进一步调整，以防需要改进。在后面的章节中，我们将介绍能够进行比简单的训练测试分割更复杂的数据分割的方法。

<title>Preparing data for model building</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 准备就绪

我们将在这个食谱中使用 Iris 数据集。用这个数据集来演示这个概念很容易，因为我们对它很熟悉，因为我们在以前的许多食谱中都使用过它。

<title>Preparing data for model building</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 怎么做……

```
# Load the necesssary Library
from sklearn.cross_validation import train_test_split
from sklearn.datasets import load_iris
import numpy as np

def get_iris_data():
    """
    Returns Iris dataset
    """
    # Load iris dataset
    data = load_iris()

    # Extract the dependend and independent variables
    # y is our class label
    # x is our instances/records
    x    = data['data']
    y    = data['target']

    # For ease we merge them
    # column merge
    input_dataset = np.column_stack([x,y])

    # Let us shuffle the dataset
    # We want records distributed randomly
    # between our test and train set

    np.random.shuffle(input_dataset)

    return input_dataset

# We need  80/20 split.
# 80% of our records for Training
# 20% Remaining for our Test set
train_size = 0.8
test_size  = 1-train_size

# get the data
input_dataset = get_iris_data()
# Split the data
train,test = train_test_split(input_dataset,test_size=test_size)

# Print the size of original dataset
print "Dataset size ",input_dataset.shape
# Print the train/test split
print "Train size ",train.shape
print "Test  size",test.shape
```

这很简单。让我们看看类别标签是否按比例分布在训练集和测试集之间。这是一个典型的阶级不平衡问题:

def get_class_distribution(y):

```
"""
Given an array of class labels
Return the class distribution
"""
    distribution = {}
    set_y = set(y)
    for y_label in set_y:
        no_elements = len(np.where(y == y_label)[0])
        distribution[y_label] = no_elements
    dist_percentage = {class_label: count/(1.0*sum(distribution.values())) for class_label,count in distribution.items()}
    return dist_percentage

def print_class_label_split(train,test):
  """
  Print the class distribution
  in test and train dataset
  """  
    y_train = train[:,-1]

    train_distribution = get_class_distribution(y_train)
    print "\nTrain data set class label distribution"
    print "=========================================\n"
    for k,v in train_distribution.items():
        print "Class label =%d, percentage records =%.2f"%(k,v)

    y_test = test[:,-1]    

    test_distribution = get_class_distribution(y_test)

    print "\nTest data set class label distribution"
    print "=========================================\n"

    for k,v in test_distribution.items():
        print "Class label =%d, percentage records =%.2f"%(k,v)

print_class_label_split(train,test)
```

让我们看看我们如何在训练和测试集之间均匀地分配类别标签:

```
# Perform Split the data
stratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)

for train_indx,test_indx in stratified_split:
    train = input_dataset[train_indx]
    test =  input_dataset[test_indx]
    print_class_label_split(train,test)
```

<title>Preparing data for model building</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 它是如何工作的……

在我们导入必要的库模块后，我们必须编写一个方便的函数`get_iris_data()`，它将返回 Iris 数据集。然后我们将`x`和`y`列连接成一个名为`input_dataset`的数组。然后，我们打乱数据集，以便记录可以随机分布到测试和训练数据集。该函数返回实例和类标签的单个数组。

我们希望将 80%的记录包含在我们的训练数据集中，并将剩余部分用作我们的测试数据集。`train_size`和`test_size`变量包含一定百分比的值，这些值应该在训练和测试数据集中。

我们必须调用`get_iris_data()`函数来获取输入数据。然后，我们利用 scikit-learn 的`cross_validation`模型中的`train_test_split`函数将输入数据集一分为二。

最后，我们可以打印原始数据集的大小，然后是测试和训练数据集:

![How it works…](../images/00090.jpeg)

我们的原始数据集有 150 行和 5 列。记住只有四个属性；第五列是类标签。我们把 x 和 y 列连接起来。

如您所见，150 行中的 80%，即 120 条记录，已经分配给我们的训练集。我们已经展示了如何轻松地将输入数据分为训练集和测试集。

记住这是一个分类问题。该算法应该被训练来为给定的未知实例或记录预测正确的类标签。为此，我们需要在训练期间提供算法和所有类的平均分布。虹膜数据集是一个三类问题。这三个阶层的代表人数应该相等。让我们看看我们的方法是否处理了这个问题。

我们必须定义一个名为`get_class_distribution`的函数，它接受单个`y`参数的类标签数组。该函数返回一个字典，其中的键是分类标签，值是该分布记录数量的百分比。因此，这本字典给了我们分类标签的分布。我们必须在下面的函数中调用这个函数，以了解我们的类在训练和测试数据集中的分布情况。

`print_class_label_split`函数是不言自明的。我们必须将训练和测试数据集作为参数传递。因为我们已经连接了我们的`x`和`y`，最后一列是我们的类标签。然后我们提取`y_train`和`y_test`中的训练和测试类标签。我们将它们传递给`get_class_distribution`以获得分类标签及其分布的字典，最后，我们打印它。

然后我们可以最终调用`print_class_label_split`，我们的输出应该如下所示:

![How it works…](../images/00091.jpeg)

现在让我们检查输出。如您所见，与测试集相比，我们的训练集具有不同的分类标签分布。测试集中正好有 40%的实例属于`class label 1`。这不是分割的正确方法。我们应该在训练和测试数据集中有相等的分布。

在最后一段代码中，我们利用 scikit-learn 中的`StratifiedShuffleSplit`,以便在训练集和测试集中实现平等的类分布。让我们检查一下`StratifiedShuffleSplit`的参数:

```
stratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)
```

第一个参数是输入数据集。我们传递所有的行和最后一列。我们的测试规模是由最初声明的变量`test_size`定义的。我们可以假设我们只需要使用`n_iter`变量进行一次分割。然后我们继续调用`print_class_label_split`来打印类标签分布。让我们检查输出:

![How it works…](../images/00092.jpeg)

现在，我们有了在测试集和训练集之间均匀分布的类标签。

<title>Preparing data for model building</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 还有更多...

在将数据用于机器学习算法之前，我们需要仔细准备数据。为训练集和测试集提供统一的类分布是构建成功分类模型的关键。

在实际的机器学习场景中，除了训练和测试集之外，我们还创建了另一个称为 dev set 的数据集。我们可能不会在第一次迭代中得到正确的模型。我们不想向我们的模型显示我们的测试数据集，因为这可能会影响我们下一次的模型构建迭代。因此，我们创建了这个开发集，我们可以在迭代我们的模型构建练习时使用它。

我们在这个配方中指定的 80/20 经验法则是一个理想的场景。然而，在许多实际应用中，我们可能没有足够的数据来遗漏测试集的那么多实例。有一些实用的技术，比如交叉验证，在这种情况下会发挥作用。在下一章中，我们将研究各种交叉验证技术。

<title>Finding the nearest neighbors</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 寻找最近的邻居

在进入我们的食谱之前，让我们花一些时间了解如何检查我们的分类模型是否执行得令人满意。在引言部分，我们引入了一个术语，叫做混淆矩阵。

混淆矩阵是实际类别标签与预测类别标签的矩阵排列。假设我们有一个两类问题，即我们的`y`可以取值，`T`或`F`。假设我们训练了一个分类器来预测我们的`y`。在我们的测试数据中，我们知道了`y`的实际值。我们从模型中得到`y`的预测值。w0 值我们可以填充我们的混淆矩阵，如下:

![Finding the nearest neighbors](../images/00093.jpeg)

这里有一个表格，我们可以方便地列出测试集的结果。记住我们知道测试集中的类标签；因此，我们可以将我们的分类模型输出与实际的类标签进行比较。

*   在`TP`下，它是真阳性的缩写，我们有一个标签为`T`的测试集中所有那些记录的计数，其中模型也预测了`T`
*   在`FN`下，它是假阴性的缩写，我们有实际标签为`T`的所有记录的计数，但是算法预测 N
*   `FP`代表假阳性，实际标签为`F`，但算法预测为`T`
*   `TN`代表真阴性，其中算法将标签和实际类别标签都预测为`F`

有了这个混淆矩阵的知识，我们现在可以导出性能度量，用它我们可以测量我们的分类模型的质量。在以后的章节中，我们将探索更多的度量标准，但是现在，我们将介绍准确性和错误率。

准确度定义为正确预测与预测总数的比率。从混淆矩阵中，我们知道 TP 和 TN 之和就是正确预测的总数:

![Finding the nearest neighbors](../images/00094.jpeg)

来自训练集的准确性总是非常乐观。应该查看测试集的准确性值来确定模型的真实性能。

有了这些知识，让我们开始我们的食谱吧。我们将研究的第一个分类算法是 K-最近邻，简而言之，KNN。在深入 KNN 的细节之前，让我们看一个非常简单的分类算法，称为机械分类器算法。机械分类器记忆整个训练数据，也就是说，它将所有数据加载到内存中。我们需要对一个看不见的新训练实例执行分类:它将尝试将新训练实例与内存中的任何训练实例进行匹配。它将测试实例的每个属性与训练实例中的每个属性进行匹配。如果找到匹配，它预测测试实例的类标签作为匹配的训练实例的类标签。

现在你应该知道如果测试实例与加载到内存中的任何训练实例都不相似，那么这个分类器就会失败。

KNN 类似于机械分类器，除了它不是寻找精确的匹配，而是使用相似性度量。类似于机械分类器，KNN 加载所有的训练集到内存中。当它需要分类一个测试实例时，它测量测试实例和所有训练实例之间的距离。使用这个距离，它在训练集中选择 K 个最近的实例。现在，对测试集的预测基于 K 个最近邻的大多数类。

例如，如果我们有一个两个类的分类问题，我们选择 K 值为 3，如果给定测试记录的三个最近邻有类 1、1 和 0，它将测试实例分类为 1，这是大多数。

KNN 属于一个叫做基于实例学习的算法家族。此外，由于对测试实例进行分类的决定是最后做出的，这也被称为懒惰的学习者。

<title>Finding the nearest neighbors</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 准备就绪

对于这个菜谱，我们将使用 scikit 的 make_classification 方法生成一些数据。我们将生成一个由四列/属性/特征和 100 个实例组成的矩阵:

```
from sklearn.datasets import make_classification

import numpy as np
import matplotlib.pyplot as plt
import itertools

from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    x,y = make_classification(n_features=4)
    return x,y

def plot_data(x,y):
    """
    Plot a scatter plot fo all variable combinations
    """
    subplot_start = 321
    col_numbers = range(0,4)
    col_pairs = itertools.combinations(col_numbers,2)

    for col_pair in col_pairs:
        plt.subplot(subplot_start)
        plt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)
        title_string = str(col_pair[0]) + "-" + str(col_pair[1])
        plt.title(title_string)
        x_label = str(col_pair[0])
        y_label = str(col_pair[1])
        plt.xlabel(x_label)
        plt.xlabel(y_label)
        subplot_start+=1

    plt.show()

x,y = get_data()    
plot_data(x,y)
```

`get_data`函数内部调用`make_classification`为任何分类任务生成测试数据。

在开始将数据输入任何算法之前，最好先将数据可视化。我们的`plot_data`函数生成所有变量之间的散点图:

![Getting ready](../images/00095.jpeg)

我们绘制了所有的变量组合。上面的两个图表显示了第 0 和第 1 列之间的组合，接着是第 0 和第 2 列。这些点还通过它们的分类标签来着色。这就给出了一个概念，这些变量的组合需要多少信息来完成一个分类任务。

<title>Finding the nearest neighbors</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 怎么做……

我们将把我们的数据集准备和模型训练分成两种不同的方法:`get_train_test` 获取训练和测试数据，以及`build_model`构建我们的模型。最后，我们将使用`test_model`来验证我们模型的有效性:

```
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

def get_train_test(x,y):
    """
    Perpare a stratified train and test split
    """
    train_size = 0.8
    test_size = 1-train_size
    input_dataset = np.column_stack([x,y])
    stratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)

    for train_indx,test_indx in stratified_split:
        train_x = input_dataset[train_indx,:-1]
        train_y = input_dataset[train_indx,-1]
        test_x =  input_dataset[test_indx,:-1]
        test_y = input_dataset[test_indx,-1]
    return train_x,train_y,test_x,test_y

def build_model(x,y,k=2):
    """
    Fit a nearest neighbour model
    """
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x,y)
    return knn

def test_model(x,y,knn_model):
    y_predicted = knn_model.predict(x)
    print classification_report(y,y_predicted)

if __name__ == "__main__":

    # Load the data    
    x,y = get_data()

    # Scatter plot the data
    plot_data(x,y)

    # Split the data into train and test    
    train_x,train_y,test_x,test_y = get_train_test(x,y)

    # Build the model
    knn_model = build_model(train_x,train_y)

    # Test the model
    print "\nModel evaluation on training set"
    print "================================\n"
    test_model(train_x,train_y,knn_model)

    print "\nModel evaluation on test set"
    print "================================\n"
    test_model(test_x,test_y,knn_model)
```

<title>Finding the nearest neighbors</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 它是如何工作的……

让我们试着遵循 main 方法中的代码。我们必须从调用`get_data`开始，并使用`plot_data`绘制它，如前一节所述。

正如前面提到的，我们需要分离出一部分训练数据，用于评估我们的模型所需的测试。然后我们调用`get_train_test`方法来做同样的事情。

在`get_train_test`中，我们决定我们的列车测试分割尺寸，这是标准的 80/20。然后，我们使用 80%的数据来训练我们的模型。现在，我们使用 NumPy 的`column_stack`方法将 x 和 y 合并成一个矩阵。

然后，我们利用前面讨论的`StratifiedShuffleSplit`,以便在我们的训练集和测试集之间获得统一的类标签分布。

有了我们的训练集和测试集，我们现在准备构建我们的分类器。我们必须用我们的训练集、属性`x`和类标签`y`来调用构建模型。这个函数还将邻居的数量`K`作为参数，默认值为 2。

我们使用 scikit-learn 的 KNN 实现，`KNeighborsClassifier`。然后我们创建一个分类器的对象，并调用`fit`方法来构建我们的模型。

我们准备使用我们的训练数据来测试模型有多好。我们可以将我们的训练数据(x 和 y)和我们的模型传递给`test_model`函数。

我们知道我们实际的类标签(y)。然后我们用 x 调用预测函数来获得预测的标签。然后，我们打印一些模型评估指标。我们可以从打印模型的准确性开始，然后用一个混淆矩阵跟进，最后显示一个名为`classification_report`的函数的输出。scikit-learn 的度量模块提供了一个名为`classification_report`的函数，可以打印多个模型评估度量。

让我们看看我们的模型指标:

![How it works…](../images/00096.jpeg)

如你所见，我们的准确率是 91.25%。我们不会重复准确性的定义；可以参考入门部分。

现在让我们看看我们的混淆矩阵。左上角的单元格是真正的正单元格。我们可以看到没有假阴性，但是有七个假阳性(第二行的第一个单元格)。

最后，我们在分类报告中提供了精确度、召回率、F1 分数和支持。让我们看看它们的定义:

`Precision`是真阳性与真阳性和假阳性之和的比值

`Accuracy`是真阳性与真阳性和假阴性之和的比值

F1 分数是精确度和灵敏度的调和平均值

我们将在下一章的一个单独的菜谱中看到更多关于这个指标的内容。现在，假设我们将有高精度和召回值。

很高兴知道我们的模型有大约 91%的准确性，但真正的测试将在测试数据上运行时进行。让我们看看测试数据的指标:

![How it works…](../images/00097.jpeg)

很高兴知道我们的模型对测试数据有 95%的准确性，这表明在拟合模型方面做得很好。

<title>Finding the nearest neighbors</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 还有更多……

让我们更深入地看看我们构建的模型:

![There's more…](../images/00098.jpeg)

我们调用了一个名为`get_params`的函数。该函数返回传递给模型的所有参数。让我们检查每个参数。

第一个参数指的是 KNN 实现使用的底层数据结构。由于训练集中的每条记录都必须与其他每条记录进行比较，强力实施可能会占用大量计算资源。因此，我们可以选择`kd_tree`或`ball_tree`作为数据结构。一个蛮力将使用蛮力的方法，为每个记录遍历所有记录。

叶子大小是传递给`kd_tree`或`ball_tree`方法的参数。

度量是用于查找邻居的距离度量。p 值为 2 会将闵可夫斯基距离减少到欧几里德距离。

最后，我们有权重参数。KNN 根据其 K 个最近邻居的类别标签来决定测试实例的类别标签。多数投票决定测试实例的类标签。但是，如果我们将权重设置为距离，则每个邻居的权重与其距离成反比。因此，为了决定测试集的类别标签，执行加权投票，而不是简单投票。

<title>Finding the nearest neighbors</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 参见

*   *准备建模数据[第六章](part0073_split_000.html#25JP21-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 6. Machine Learning 1")、*机器学习一*中的*配方
*   *用距离度量工作*第五章[中的秘方](part0066_split_000.html#1UU541-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 5. Data Mining – Needle in a Haystack")，*数据挖掘——大海捞针*

<title>Classifying documents using Naïve Bayes</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 使用朴素贝叶斯分类文档

在这个菜谱中，我们将研究一个文档分类问题。我们将使用的算法是朴素贝叶斯分类器。贝叶斯规则是驱动朴素贝叶斯算法的引擎，如下所示:

![Classifying documents using Naïve Bayes](../images/00099.jpeg)

它显示了事件 X 发生的可能性，假设我们知道事件 Y 已经发生。现在，在我们的食谱中，我们将对文本进行分类。我们的问题是一个二元分类问题:给定一个电影评论，我们想要分类评论是正面的还是负面的。

在贝叶斯术语中，我们需要找到条件概率:给定评论，评论为正的概率，给定评论，评论为负的概率。让我们把它写成一个等式:

![Classifying documents using Naïve Bayes](../images/00100.jpeg)

对于任何评论，如果我们有前面两个概率值，我们可以通过比较这些值将评论分类为正面或负面。如果否定的条件概率大于肯定的条件概率，我们将评论归类为否定，反之亦然。

现在让我们用贝叶斯法则来讨论这些概率:

![Classifying documents using Naïve Bayes](../images/00101.jpeg)

因为我们要比较这两个方程来最终确定我们的预测，所以我们可以忽略分母，它是一个简单的比例因子。

前面方程的 LHS(左手边)称为后验概率。

让我们看看 RHS 的分子(右侧):

![Classifying documents using Naïve Bayes](../images/00102.jpeg)

`P(positive)`是称为先验的正类的概率。这是我们对基于我们的训练集的正类标签分布的信念。

我们将从我们的训练测试中估计它。计算方法如下:

![Classifying documents using Naïve Bayes](../images/00103.jpeg)

P(review|positive)是可能性。它回答了这个问题:假设课程是积极的，获得评论的可能性有多大。同样，我们将从我们的训练集中估计它。

在我们进一步扩展似然方程之前，让我们引入独立性假设的概念。由于这种假设，该算法被认为是幼稚的。与现实相反，我们假设单词彼此独立地出现在文档中。我们将使用这个假设来计算可能性。

评论是一个单词列表。让我们用数学符号来表示:

![Classifying documents using Naïve Bayes](../images/00104.jpeg)

根据独立性假设，我们可以说，这些单词中的每一个在评论中一起出现的概率是评论中组成单词的所有个体概率的乘积。

现在我们可以将似然方程写成如下形式:

![Classifying documents using Naïve Bayes](../images/00105.jpeg)

因此，给定一个新的评论，我们可以使用这两个等式，先验和似然，来计算评论是正面还是负面的。

希望，你已经跟踪到现在。这个难题还有最后一块:我们如何计算单个单词的概率？

![Classifying documents using Naïve Bayes](../images/00106.jpeg)

这一步指的是训练模型。

从我们的训练集中，我们将采取每个审查。我们也知道它的标签。对于这篇评论中的每一个单词，我们都会计算条件概率，并存储在一个表中。因此，我们可以使用这些值来预测任何未来的测试实例。

理论够了！让我们开始研究我们的食谱吧。

<title>Classifying documents using Naïve Bayes</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 准备就绪

对于这个菜谱，我们将对数据和算法都使用 NLTK 库。在 NLTK 的安装过程中，我们也可以下载数据集。一个这样的数据集是电影评论数据集。电影评论数据被分为正面和负面两类。对于每个类别，我们都有一个单词列表；评论被预先分成单词:

```
from nltk.corpus import movie_reviews

```

如此处所示，我们将通过从 NLTK 导入语料库模块来包含数据集。

我们将利用 NLTK 中定义的`NaiveBayesClassifier`类来构建模型。我们将把我们的训练数据传递给一个名为`train()`的函数来构建我们的模型。

<title>Classifying documents using Naïve Bayes</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 怎么做……

让我们从导入必要的函数开始。接下来我们将使用两个效用函数。第一个检索电影评论数据，第二个帮助我们将模型的数据分为训练和测试:

```
from nltk.corpus import movie_reviews
from sklearn.cross_validation import StratifiedShuffleSplit
import nltk
from nltk.corpus import stopwords
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures

def get_data():
    """
    Get movie review data
    """
    dataset = []
    y_labels = []
    # Extract categories
    for cat in movie_reviews.categories():
        # for files in each cateogry    
        for fileid in movie_reviews.fileids(cat):
            # Get the words in that category
            words = list(movie_reviews.words(fileid))
            dataset.append((words,cat))
            y_labels.append(cat)
    return dataset,y_labels

def get_train_test(input_dataset,ylabels):
    """
    Perpare a stratified train and test split
    """
    train_size = 0.7
    test_size = 1-train_size
    stratified_split = StratifiedShuffleSplit(ylabels,test_size=test_size,n_iter=1,random_state=77)

    for train_indx,test_indx in stratified_split:
        train   = [input_dataset[i] for i in train_indx]
        train_y = [ylabels[i] for i in train_indx]

        test    = [input_dataset[i] for i in test_indx]
        test_y  = [ylabels[i] for i in test_indx]
    return train,test,train_y,test_y
```

我们现在将介绍三个函数，它们是主要的特征生成函数。我们需要为我们的分类器提供特征或属性。给定一个检查，这些函数从检查中生成一组特征:

```
def build_word_features(instance):
    """
    Build feature dictionary
    Features are binary, name of the feature is word iteslf
    and value is 1\. Features are stored in a dictionary
    called feature_set
    """
    # Dictionary to store the features
    feature_set = {}
    # The first item in instance tuple the word list
    words = instance[0]
    # Populate feature dicitonary
    for word in words:
        feature_set[word] = 1
    # Second item in instance tuple is class label
    return (feature_set,instance[1])

def build_negate_features(instance):
    """
    If a word is preceeded by either 'not' or 'no'
    this function adds a prefix 'Not_' to that word
    It will also not insert the previous negation word
    'not' or 'no' in feature dictionary
    """
    # Retreive words, first item in instance tuple
    words = instance[0]
    final_words = []
    # A boolean variable to track if the 
    # previous word is a negation word
    negate = False
    # List of negation words
    negate_words = ['no','not']
    # On looping throught the words, on encountering
    # a negation word, variable negate is set to True
    # negation word is not added to feature dictionary
    # if negate variable is set to true
    # 'Not_' prefix is added to the word
    for word in words:
        if negate:
            word = 'Not_' + word
            negate = False
        if word not in negate_words:
            final_words.append(word)
        else:
            negate = True
    # Feature dictionary
    feature_set = {}
    for word in final_words:
        feature_set[word] = 1
    return (feature_set,instance[1])

def remove_stop_words(in_data):
    """
    Utility function to remove stop words
    from the given list of words
    """
    stopword_list = stopwords.words('english')
    negate_words = ['no','not']
    # We dont want to remove the negate words
    # Hence we create a new stop word list excluding
    # the negate words
    new_stopwords = [word for word in stopword_list if word not in negate_words]
    label = in_data[1]
    # Remove stopw words
    words = [word for word in in_data[0] if word not in new_stopwords]
    return (words,label)

def build_keyphrase_features(instance):
    """
    A function to extract key phrases
    from the given text.
    Key Phrases are words of importance according to a measure
    In this key our phrase of is our length 2, i.e two words or bigrams
    """
    feature_set = {}
    instance = remove_stop_words(instance)
    words = instance[0]

    bigram_finder  = BigramCollocationFinder.from_words(words)
    # We use the raw frequency count of bigrams, i.e. bigrams are
    # ordered by their frequency of occurence in descending order
    # and top 400 bigrams are selected.
    bigrams        = bigram_finder.nbest(BigramAssocMeasures.raw_freq,400)
    for bigram in bigrams:
        feature_set[bigram] = 1
    return (feature_set,instance[1])
```

现在让我们编写一个函数来构建我们的模型，稍后探测我们的模型以发现我们的模型的有用性:

```
def build_model(features):
    """
    Build a naive bayes model
    with the gvien feature set.
    """
    model = nltk.NaiveBayesClassifier.train(features)
    return model    

def probe_model(model,features,dataset_type = 'Train'):
    """
    A utility function to check the goodness
    of our model.
    """
    accuracy = nltk.classify.accuracy(model,features)
    print "\n" + dataset_type + " Accuracy = %0.2f"%(accuracy*100) + "%" 

def show_features(model,no_features=5):
    """
    A utility function to see how important
    various features are for our model.
    """
    print "\nFeature Importance"
    print "===================\n"
    print model.show_most_informative_features(no_features)        
```

很难在第一次中就把模型做好。我们需要尝试不同的功能和参数调整。这主要是一个试错的过程。在下一部分代码中，我们将通过改进我们的模型来展示我们的不同过程:

```
def build_model_cycle_1(train_data,dev_data):
    """
    First pass at trying out our model
    """
    # Build features for training set
    train_features =map(build_word_features,train_data)
    # Build features for test set
    dev_features = map(build_word_features,dev_data)
    # Build model
    model = build_model(train_features)    
    # Look at the model
    probe_model(model,train_features)
    probe_model(model,dev_features,'Dev')

    return model

def build_model_cycle_2(train_data,dev_data):
    """
    Second pass at trying out our model
    """

    # Build features for training set
    train_features =map(build_negate_features,train_data)
    # Build features for test set
    dev_features = map(build_negate_features,dev_data)
    # Build model
    model = build_model(train_features)    
    # Look at the model
    probe_model(model,train_features)
    probe_model(model,dev_features,'Dev')

    return model

def build_model_cycle_3(train_data,dev_data):
    """
    Third pass at trying out our model
    """

    # Build features for training set
    train_features =map(build_keyphrase_features,train_data)
    # Build features for test set
    dev_features = map(build_keyphrase_features,dev_data)
    # Build model
    model = build_model(train_features)    
    # Look at the model
    probe_model(model,train_features)
    probe_model(model,dev_features,'Dev')
    test_features = map(build_keyphrase_features,test_data)
    probe_model(model,test_features,'Test')

    return model
```

最后，我们将编写一段代码，用这段代码我们可以调用之前定义的所有函数:

```
if __name__ == "__main__":

    # Load data
    input_dataset, y_labels = get_data()
    # Train data    
    train_data,all_test_data,train_y,all_test_y = get_train_test(input_dataset,y_labels)
    # Dev data
    dev_data,test_data,dev_y,test_y = get_train_test(all_test_data,all_test_y)

    # Let us look at the data size in our different 
    # datasets
    print "\nOriginal  Data Size   =", len(input_dataset)
    print "\nTraining  Data Size   =", len(train_data)
    print "\nDev       Data Size   =", len(dev_data)
    print "\nTesting   Data Size   =", len(test_data)    

    # Different passes of our model building exercise    
    model_cycle_1 =  build_model_cycle_1(train_data,dev_data)
    # Print informative features
    show_features(model_cycle_1)    
    model_cycle_2 = build_model_cycle_2(train_data,dev_data)
    show_features(model_cycle_2)
    model_cycle_3 = build_model_cycle_3(train_data,dev_data)
    show_features(model_cycle_3)
```

<title>Classifying documents using Naïve Bayes</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 它是如何工作的……

让我们试着从主函数开始遵循这个配方。我们从调用`get_data`函数开始。如前所述，电影评论数据存储为两类，正面和负面。我们的第一个循环遍历这些类别。通过这些类别，我们在第二个循环中检索了这些类别的文件 id。使用这些文件 id，我们检索单词，如下所示:

```
            words = list(movie_reviews.words(fileid))
```

我们将把这些单词添加到一个名为`dataset`的列表中。类别标签被附加到另一个名为`y_labels`的列表中。

最后，我们返回单词和相应的类标签:

```
    return dataset,y_labels
```

有了数据集，我们需要将这个数据集分为测试数据集和训练数据集:

```
 # Train data    
    train_data,all_test_data,train_y,all_test_y = get_train_test(input_dataset,y_labels)
```

我们用一个输入数据集和类标签调用了`get_train_test`函数。这个函数为我们提供了一个分层样本。我们将 70%的数据用于训练集，其余的用于测试集。

我们再次调用`get_train_test`,使用从上一步返回的测试数据集:

```
    # Dev data
    dev_data,test_data,dev_y,test_y = get_train_test(all_test_data,all_test_y)
```

我们创建了一个单独的数据集，并将其命名为 dev 数据集。我们需要这个数据集来调整我们的模型。我们希望我们的测试集真正表现为一个测试集。我们不想在模型构建练习的不同阶段暴露我们的测试集。

让我们打印我们的培训、开发和测试数据集的大小:

![How it works…](../images/00107.jpeg)

正如你所看到的，70%的原始数据被分配给了我们的训练集。对于`Dev`和`Testing`，我们再次将剩余的 30%分成 70/30 的比例。

让我们开始模型制作活动。我们将用我们的培训和开发数据集打电话给`build_model_cycle_1`。在这个函数中，我们将首先通过在数据集中的所有实例上使用地图调用`build_word_feature`来创建我们的特征。`build_word_feature`是一个简单的特征生成函数。每一个字都是一个特色。这个函数的输出是一个特征字典，其中的键是单词本身，值是 1。这些类型的特征通常被称为单词袋(BOW)。使用训练和开发数据调用`build_word_features`:

```
    # Build features for training set
    train_features =map(build_negate_features,train_data)
    # Build features for test set
    dev_features = map(build_negate_features,dev_data)
```

我们现在将继续使用生成的特征来训练我们的模型:

```
    # Build model
    model = build_model(train_features)    
```

我们需要测试我们的模型有多好。我们使用`probe_model`函数来做这件事。`Probe_model`需要三个参数。第一个参数是感兴趣的模型，第二个参数是我们希望对照其来查看我们的模型有多好的特征，最后一个参数是用于显示目的的字符串。`probe_model`函数使用`nltk.classify`模块中的精度函数计算精度指标。

我们调用`probe_model`两次:一次使用训练数据来查看模型在我们的训练数据集上有多好，然后一次使用我们的开发数据集:

```
    # Look at the model
    probe_model(model,train_features)
    probe_model(model,dev_features,'Dev')
```

现在让我们来看看准确性数据:

![How it works…](../images/00108.jpeg)

我们的模型使用训练数据表现得非常好。这并不奇怪，因为模型已经在训练阶段看到了这一点。它在正确分类训练记录方面做得很好。然而，我们的开发准确性很差。我们的模型只能正确分类 60%的 dev 实例。当然，我们的特征信息不足以帮助我们的模型以良好的准确性对未知的实例进行分类。了解哪些特征更有助于将评论区分为正面和负面，这将是一件好事:

```
show_features(model_cycle_1) 
```

我们将调用`show_features`函数来查看特征对模型的贡献。`Show_features`函数利用 NLTK 分类器对象中的`show_most_informative_feature`函数。我们的第一个模型中最重要的特性如下:

![How it works…](../images/00109.jpeg)

阅读的方式是:特征`stupidity = 1`对于将评论分类为负面的有效性是 15 倍。

现在，让我们使用一组新的功能进行第二轮构建。我们将通过调用`build_model_cycle_2`来做到这一点。`build_model_cycle_2`与`build_model_cycle_1`非常相似，除了名为 inside map function 的特征生成功能。

特征生成函数称为`build_negate_features`。典型地，像 not 和 no 这样的词被称为否定词。假设我们的影评人说电影不好。如果我们使用前面的特征生成器，单词 good 在正面和负面评论中都将被同等对待。我们知道，好这个词应该用来区分正面的评论。为了避免这个问题，我们将在单词表中查找否定词 no 和 not。我们想修改我们的例句如下:

```
"movie is not good" to "movie is not_good"
```

这样，`no_good`可以用作区分负面评论和正面评论的良好特征。`build_negate_features`函数完成这项工作。

现在让我们来看看用这个否定特性构建的模型的探测输出:

![How it works…](../images/00110.jpeg)

我们将开发数据的模型准确性提高了近 2%。现在，让我们来看看该模型最具信息性的特性:

![How it works…](../images/00111.jpeg)

看最后一个特点。有趣的是，在将一篇评论区分为负面评论时，`Not_funny`功能的信息量是前者的 11.7 倍。

我们能在模型准确性上做得更好吗？目前，我们达到了 70%。让我们用一组新的特性进行第三次测试。我们将通过调用`build_model_cycle_3`来做到这一点。`build_model_cycle_3`与`build_model_cycle_2`非常相似，除了名为 inside map function 的特征生成功能。

`build_keyphrase_features`函数用作特征生成器。我们来详细看看函数。我们将从评论中生成关键短语，并将其用作特征，而不是使用单词作为特征。关键短语是我们认为重要的短语。关键短语可以由两个、三个或 n 个单词组成。在我们的例子中，我们将使用两个词(二元模型)来构建我们的关键短语。我们将使用的度量是这些短语的原始频率计数。我们将选择出现频率较高的短语。在生成关键短语之前，我们将做一些简单的预处理。我们将从单词列表中删除所有停用词和标点符号。调用`remove_stop_words`功能删除停用词和标点符号。NLTK 的语料库模块有一个英语停用词列表。我们可以按如下方式检索它:

```
stopword_list = stopwords.words('english')
```

类似地，Python 中的字符串模块维护一个标点符号列表。我们将删除停用词和标点，如下所示:

```
words = [word for word in in_data[0] if word not in new_stopwords \
and word not in punctuation]
```

但是，我们不会去掉 not 和`no`。我们将通过 not 创建一组新的停用词，包括上一步中的否定词:

```
new_stopwords = [word for word in stopword_list if word not in negate_words]
```

我们将利用 NLTK 中的`BigramCollocationFinder`类来生成我们的关键短语:

```
    bigram_finder  = BigramCollocationFinder.from_words(words)
    # We use the raw frequency count of bigrams, i.e. bigrams are
    # ordered by their frequency of occurence in descending order
    # and top 400 bigrams are selected.
    bigrams        = bigram_finder.nbest(BigramAssocMeasures.raw_freq,400) 
```

我们的衡量标准是频率计数。你可以看到我们在最后一行把它指定为`raw_freq`。我们将要求搭配查找器返回我们最多 400 个短语。

加载了我们的新特性后，我们将继续构建我们的模型，并测试我们模型的正确性。让我们看看我们模型的输出:

![How it works…](../images/00112.jpeg)

是啊！我们已经在开发集上取得了很大的改进。在我们第一次使用单词功能时，准确率为 68 %,而在使用关键短语功能时，准确率从 12%提高到了 80%。现在让我们将测试集暴露给这个模型，并检查其准确性:

```
    test_features = map(build_keyphrase_features,test_data)
    probe_model(model,test_features,'Test')
```

![How it works…](../images/00113.jpeg)

我们的测试集的精确度高于我们的开发集的精确度。我们在训练一个好模型方面做得很好，这个模型在一个看不见的数据集上工作得很好。在我们结束本食谱之前，让我们来看看最具启发性的关键短语:

![How it works…](../images/00114.jpeg)

关键词，奥斯卡提名，在鉴别一篇评论是积极的方面有 10 倍的帮助。你不能否认这一点。我们可以看到，我们的关键短语非常丰富，因此，我们的模型比前两次运行表现得更好。

<title>Classifying documents using Naïve Bayes</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 还有更多……

我们怎么知道 400 个关键短语和度量频率计数是生成二元模型的最佳参数？反复试验。虽然我们没有列出我们的试错过程，但我们几乎用各种组合运行了它，比如 200 个短语和逐点互信息，以及类似的其他方法。

这是现实世界中需要做的事情。然而，我们不是每次都在参数空间中盲目搜索，而是着眼于最具信息性的特征。这给了我们一个关于特征辨别能力的线索。

<title>Classifying documents using Naïve Bayes</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 参见

*   *准备建模数据[第六章](part0073_split_000.html#25JP21-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 6. Machine Learning 1")、*机器学习一*中的*配方

<title>Building decision trees to solve multiclass problems</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 构建决策树解决多类问题

在这份食谱中，我们将着眼于构建决策树来解决多类分类问题。直观地说，决策树可以定义为通过提出一系列问题来得出答案的过程:一系列按层次排列的 if-then 语句形成一棵决策树。由于这种性质，它们很容易理解和解释。

有关决策树的详细介绍，请参考下面的链接:

[https://en.wikipedia.org/wiki/Decision_tree](https://en.wikipedia.org/wiki/Decision_tree)

理论上，对于给定的数据集，可以构建许多决策树。有些树比其他树更准确。有有效的算法可以在有限的时间内开发出相当精确的树。一种这样的算法是 Hunt 算法。ID3、C4.5 和 CART(分类和回归树)等算法都是基于 Hunt 算法的。亨特的算法可以概括如下:

给定 n 个记录的数据集 D，并且每个记录具有 m 个属性/特征/列，并且每个记录被标记为 y1、y2 或 y3，算法如下进行:

*   如果 D 中的所有记录都属于同一个类标签，比如 y1，那么 y1 就是树的叶节点，被标记为 y1。
*   如果 D 具有属于一个以上类别标签的记录，则使用特征测试条件来将记录分成更小的子集。假设在初始运行中，我们对所有属性运行特性测试条件，并找到一个能够将数据集分成三个更小子集的属性。该属性成为根节点。我们对所有三个子集应用测试条件，以计算出下一级节点。这个过程是反复进行的。

注意，当我们定义我们的分类时，我们定义了三个类标签，y1、y2 和 y3。这与我们在前两个配方中解决的问题不同，在前两个配方中，我们只有两个标签。这是一个多类问题。我们在大多数食谱中使用的 Iris 数据集是一个三级问题。我们将我们的记录分布在三个类别标签上。我们可以将其推广为一个 n 类问题。数字识别是另一个例子，我们需要用 0 到 9 之间的一个数字对给定的图像进行分类。很多真题本来就是多类的。一些算法也天生能够处理多类问题。这些算法不需要任何改变。我们将在本章中讨论的算法都能够处理多类问题。决策树、朴素贝叶斯和 KNN 算法擅长处理多类问题。

让我们看看如何利用决策树来处理这个菜谱中的多类问题。这也有助于很好地理解决策树。我们将在下一章探讨的随机森林是一种更复杂的方法，在业界广泛使用，它基于决策树。

现在，让我们深入研究决策树方法。

<title>Building decision trees to solve multiclass problems</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 准备就绪

我们将使用Iris 数据集来演示如何构建决策树。决策树是一种非参数监督学习方法，可用于解决分类和回归问题。如前所述，使用决策树的优势是多方面的，如下所示:

*   它们很容易解释
*   它们只需要很少的数据准备和数据到特征的转换:还记得我们在前一个配方中的特征生成方法吗
*   他们自然支持多类问题

决策树并不是没有问题。它们带来的一些问题如下:

*   它们很容易过度拟合:在训练集中精度很高，而在测试数据中性能很差。
*   可能有数百万棵树适合给定的数据集。
*   类别不平衡问题可能会严重影响决策树。当我们的训练集不包含二进制分类问题中两个类别标签的相等数量的实例时，类别不平衡问题出现。这也适用于多类问题。

决策树的一个重要部分是特征测试条件。让我们花一些时间来理解特性测试条件。通常，我们实例中的每个属性都可以理解。

**二元属性**:这是一个属性可以取两个可能值的地方，例如，真或假。在这种情况下，功能测试条件应该返回两个值。

**名义属性**:一个属性可以取两个以上的值，例如 n 个值。特征测试条件应该输出 n 个输出或者将它们分组为二进制分割。

**顺序属性**:这是它们的值中存在的隐含顺序。例如，让我们假设一个名为 size 的属性，它可以取值为 small、medium 或 large。属性可以取三个值，它们有一个顺序:小、中、大。它们由类似于名义属性的特征属性测试条件来处理。

**连续属性**:可以取连续值的属性。它们被离散化成顺序属性，然后被处理。

特性测试条件是一种基于称为杂质的标准或度量将输入记录分割成子集的方法。这个杂质是根据实例中每个属性的类标签来计算的。构成最高杂质的属性被选择作为数据分割属性，或者换句话说，树中该级别的节点。

我们来看一个例子来解释一下。我们将使用熵来计算杂质。

熵的定义如下:

![Getting ready](../images/00115.jpeg)

其中:

![Getting ready](../images/00116.jpeg)

让我们考虑一个例子:

```
X = {2,2}
```

我们现在可以计算这个集合的熵，如下所示:

![Getting ready](../images/00117.jpeg)

这个集合的熵是 0。熵为 0 表示同质性。用 Python 编码熵是非常容易的。请看下面的代码列表:

```
import math

def prob(data,element):
    """Calculates the percentage count of a given element

    Given a list and an element, returns the elements percentage count

    """
    element_count =0
   	# Test conditoin to check if we have proper input
    if len(data) == 0 or element == None \
                or not isinstance(element,(int,long,float)):
        return None      
    element_count = data.count(element)
    return element_count / (1.0 * len(data))

def entropy(data):
    """"Calcuate entropy
    """
    entropy =0.0

    if len(data) == 0:
        return None

    if len(data) == 1:
        return 0
    try:
        for element in data:
            p = prob(data,element)
            entropy+=-1*p*math.log(p,2)
    except ValueError as e:
        print e.message

    return entropy
```

为了找到最佳分裂变量，我们将利用熵。首先，我们将基于类别标签计算熵，如下所示:

![Getting ready](../images/00118.jpeg)

让我们定义另一个术语叫做信息增益。信息增益是在给定的实例中找到哪个属性对类别标签之间的区分最有用的一种度量。

信息增益是父节点的熵和子节点的平均熵之差。在树的每一层，我们将使用信息增益来构建树:

[https://en . Wikipedia . org/wiki/Information _ gain _ decision _ trees](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)

我们将从训练集中的所有属性开始，并计算总熵。让我们看看下面的例子:

![Getting ready](../images/00119.jpeg)

前面的数据集是为用户收集的虚构数据，以确定他对哪种电影感兴趣。有四个属性:第一个属性是关于用户是否根据男主角来观看电影，第二个属性是关于用户是否根据电影是否获得奥斯卡奖来决定观看电影，第三个属性是关于用户是否根据电影是否票房成功来决定观看电影。

为了为前面的示例构建决策树，我们将从整个数据集的熵计算开始。这是一个两类问题，因此 c = 2。此外，总共有四条记录，因此，整个数据集的熵如下:

![Getting ready](../images/00120.jpeg)

数据集的总熵是 0.811。

现在，让我们看看第一个属性，即 lead 属性。对于主角 Y，有一个表示 Y 的实例类标签，另一个表示 N。对于主角 N，两个实例类标签都是 N。我们将按如下方式计算平均熵:

![Getting ready](../images/00121.jpeg)

这是一个平均熵。有两个主角为 Y 的记录和两个主角为 N 的记录；因此，我们将`2/4.0`乘以熵值。

在计算该数据子集的熵时，我们可以看到，在两条记录中，一条记录的类别标签为 Y，另一条记录的主要演员 Y 的类别标签为 N。类似地，对于主要演员 N，两条记录的类别标签都为 N。因此，我们得到了该属性的平均熵。

主角属性的平均熵值是 0.5。

信息增益现在是 0.811–0.5 = 0.311。

类似地，我们将发现所有属性的信息增益。具有最高信息增益的属性获胜，并成为决策树的根节点。

重复相同的过程，以便找到第二层节点，等等。

<title>Building decision trees to solve multiclass problems</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 怎么做……

让我们加载所需的库。接下来我们将使用两个函数，一个用于加载数据，第二个函数用于将数据拆分到训练集并对其进行测试:

```
from sklearn.datasets import load_iris
from sklearn.cross_validation import StratifiedShuffleSplit
import numpy as np
from sklearn import tree
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
import pprint

def get_data():
    """
    Get Iris data
    """
    data = load_iris()
    x = data['data']
    y = data['target']
    label_names = data['target_names']

    return x,y,label_names.tolist()

def get_train_test(x,y):
    """
    Perpare a stratified train and test split
    """
    train_size = 0.8
    test_size = 1-train_size
    input_dataset = np.column_stack([x,y])
    stratified_split = StratifiedShuffleSplit(input_dataset[:,-1], \
            test_size=test_size,n_iter=1,random_state = 77)

    for train_indx,test_indx in stratified_split:
        train_x = input_dataset[train_indx,:-1]
        train_y = input_dataset[train_indx,-1]
        test_x =  input_dataset[test_indx,:-1]
        test_y = input_dataset[test_indx,-1]
    return train_x,train_y,test_x,test_y
```

让我们编写一些函数来帮助我们构建和测试决策树模型:

```
def build_model(x,y):
    """
    Fit the model for the given attribute 
    class label pairs
    """
    model = tree.DecisionTreeClassifier(criterion="entropy")
    model = model.fit(x,y)
    return model

def test_model(x,y,model,label_names):
    """
    Inspect the model for accuracy
    """
    y_predicted = model.predict(x)
    print "Model accuracy = %0.2f"%(accuracy_score(y,y_predicted) * 100) + "%\n"
    print "\nConfusion Matrix"
    print "================="
    print pprint.pprint(confusion_matrix(y,y_predicted))

    print "\nClassification Report"
    print "================="

    print classification_report(y,y_predicted,target_names=label_names)
```

最后，主函数调用我们定义的所有其他函数如下:

```
if __name__ == "__main__":
    # Load the data
    x,y,label_names = get_data()
    # Split the data into train and test    
    train_x,train_y,test_x,test_y = get_train_test(x,y)
    # Build model    
    model = build_model(train_x,train_y)
    # Evaluate the model on train dataset    
    test_model(train_x,train_y,model,label_names)    
    # Evaluate the model on test dataset
    test_model(test_x,test_y,model,label_names)
```

<title>Building decision trees to solve multiclass problems</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 它是如何工作的……

先说主要功能。我们调用`x`、`y`和`label_names`变量中的`get_data`来检索虹膜数据集。我们采用标签名称，以便当我们看到我们的模型准确性时，我们可以通过单个标签来测量它。如前所述，虹膜数据提出了三类问题。我们将需要构建一个分类器，它可以对 setosa、versicolor 或 virginica 树类型中的任何新实例进行分类。

和前面的方法一样，`get_train_test`再次返回分层的训练和测试数据集。然后，我们利用 scikit-learn 的`StratifiedShuffleSplit`来获得具有相同类别标签分布的训练和测试数据集。

我们必须调用`build_model`方法在我们的训练集上诱导一个决策树。scikit-learn 的模型树中的`DecisionTreeClassifier`类实现了一个决策树:

```
model = tree.DecisionTreeClassifier(criterion="entropy")
```

正如你所看到的，我们使用`criterion`变量指定了我们的特性测试条件是一个熵。然后，我们通过调用`fit`函数来构建模型，并将模型返回给调用程序。

现在，让我们通过使用`test_model`函数来评估我们的模型。模型采用实例`x`，类标签`y`，决策树模型`model`，以及类标签的名称`label_names`。

scikit-learn 中的模块指标提供了三个评估标准:

```
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
```

我们在前面的配方和简介部分定义了精确度。

混淆矩阵打印在简介部分定义的混淆矩阵。混淆矩阵是评估模型性能的好方法。我们感兴趣的是具有真正值和假正值的单元格值。

最后，我们还有`classification_report`来打印精度、召回和 F1 分数。

我们必须首先根据训练数据评估模型:

![How it works…](../images/00122.jpeg)

我们在训练数据集上做得很好。我们有 100%的准确率。真正的测试是在橡胶接触路面的地方进行的。

让我们看看使用测试数据集的模型评估:

![How it works…](../images/00123.jpeg)

我们的分类器在测试集上也表现得非常好。

<title>Building decision trees to solve multiclass problems</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 还有更多……

让我们研究一下这个模型，看看的各种特征是如何有助于区分类别的。

```
def get_feature_names():
    data = load_iris()
    return data['feature_names']

def probe_model(x,y,model,label_names):

    feature_names = get_feature_names()
    feature_importance = model.feature_importances_
    print "\nFeature Importance\n"
    print "=====================\n"
    for i,feature_name in enumerate(feature_names):
        print "%s = %0.3f"%(feature_name,feature_importance[i])

    # Export the decison tree as a graph
    tree.export_graphviz(model,out_file='tree.dot')
```

树分类器对象提供了一个名为`feature_importances_`的属性，可以调用该属性来检索各种特性对于构建我们的模型的重要性。

我们编写了一个简单的函数`get_feature_names`，以便检索我们属性的名称。但是，这可以作为`get_data`的一部分添加到中。

让我们来看看打印语句的输出:

![There's more…](../images/00124.jpeg)

这看起来好像花瓣宽度和花瓣长度对我们的分类贡献更大。

有趣的是，我们还可以将分类器构建的树导出为一个点文件，并且可以使用 GraphViz 包可视化。在最后一行，我们将模型导出为一个点文件:

```
# Export the decison tree as a graph
tree.export_graphviz(model,out_file='tree.dot')
```

您可以下载并安装 Graphviz 包来实现这一点:

[http://www.graphviz.org/](http://www.graphviz.org/)

<title>Building decision trees to solve multiclass problems</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

## 参见

*   *准备建模数据[第六章](part0073_split_000.html#25JP21-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 6. Machine Learning 1")、*机器学习一*中的*配方
*   *寻找最近邻居*菜谱[第六章](part0073_split_000.html#25JP21-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 6. Machine Learning 1")，*机器学习 I*
*   *使用朴素贝叶斯*方法对文档进行分类[第六章](part0073_split_000.html#25JP21-6b04b7c0b98f44a0b8f82924fef317ec "Chapter 6. Machine Learning 1")，*机器学习 I*