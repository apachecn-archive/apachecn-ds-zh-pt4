

# 十三、个例研究

在这一章中，我们将看一些案例研究来帮助你更好地理解我们到目前为止看到的主题。

# 案例研究 1——基于社交媒体预测股票价格

我们的第一个例例研究将会非常激动人心！我们将尝试仅使用社交媒体情绪来预测上市公司的股票价格。虽然这个例子不会使用任何显式的统计/机器学习算法，但我们将利用 EDA(探索性数据分析)并使用视觉效果来实现我们的目标。

## 文本情感分析

当谈到情绪时，应该很清楚是什么意思。我所说的情绪是指介于-1 和 1 之间的一个数量值(在区间水平上)。如果一个文本片段的情感得分接近-1，就说它有负面情感。如果情感得分接近 1，那么该文本被认为具有积极的情感。如果情绪得分接近 0，我们说它有中性情绪。我们将使用一个名为`Textblob`的 Python 模块来测量我们的文本情感:

```
# use the textblob module to make a function called stringToSentiment that returns a sentences sentiment
def stringToSentiment(text):
    return TextBlob(text).sentiment.polarity
```

现在我们可以使用这个调用`Textblob`模块的函数来对现成的文本进行评分:

```
stringToSentiment('i hate you')
-0.8

stringToSentiment('i love you')
0.5

stringToSentiment('i see you')
0.0
```

现在让我们阅读我们的研究推文:

```
# read in tweets data into a dataframe
from textblob import TextBlob
import pandas as pd
%matplotlib inline
# these tweets are from last May and are about Apple (AAPL)
tweets = pd.read_csv('../data/so_many_tweets.csv')
tweets.head()
```

![Text sentiment analysis](graphics/B05260_013_f1.jpg)

## 探索性数据分析

所以我们有四列:

*   `Text`:名义层的非结构化文本
*   `Date` : Datetime(我们会以连续的方式考虑 Datetime)
*   `Status`:名义级别的状态唯一 ID
*   `Retweet`:tweet 的状态 ID，表明该 tweet 是名义级别的转发

所以我们有四列，但是有多少行呢？每一行代表什么？似乎每一行都代表了一条关于该公司的推文:

```
tweets.shape

(52512, 4)
```

所以我们有四列和`52512`条推文/行供我们使用！哦，天啊…我们的目标是最终使用推文的情感，所以我们可能需要在数据框中有一个情感栏。使用前一个例子中相当简单的函数，让我们添加这个列！

```
# create a new column in tweets called sentiment that maps stringToSentiment to the text column
tweets['sentiment'] = tweets['Text'].apply(stringToSentiment)

tweets.head()
```

前面的代码将把函数`stringToSentiment`应用于 tweets 数据帧的列`Text`中的每个元素:

```
tweets.head()
```

![Exploratory data analysis](graphics/B05260_013_f2.jpg)

所以现在我们对这个数据集中每条推文的情感分数有了一个感觉。让我们简化我们的问题，尝试使用一整天的推文来预测 AAPL 的价格是否会在 24 小时内上涨。如果是这样的话，我们还有另一个问题。`Date`栏显示我们每天有多条推文。只看前五条推文；他们都在同一天。我们将对这个数据集进行重新采样，以便了解每天 Twitter 上的股票情绪。

我们将分三步进行:

1.  我们将确保`Date`列是 Python `datetime`类型的。
2.  我们将用 datetime 列替换 Dataframe 的索引(这允许我们使用复杂的 datetime 函数)。
3.  We will resample the data so that each row, instead of representing a tweet, Will represent a single day with an aggregated sentinel score for each day:

    ### Note that the index of

    data frame is a special sequence used to identify the lines in our structure. By default, Dataframe will use incremental integers to represent rows (`0` for the first row, `1` for the second row, and so on).

    ```
    tweets.index
    RangeIndex(start=0, stop=52512, step=1)

    # As a list, we can splice it
    list(tweets.index)[:5]

    [0, 1, 2, 3, 4]
    ```

4.  让我们现在就解决这个日期问题吧！我们将确保`Date`列是 Python `datetime`类型:

    ```
    # cast the date column as a datetime
    tweets['Date'] = pd.to_datetime(tweets.Date)
    tweets['Date'].head()

    Date
    2015-05-24 03:46:08   2015-05-24 03:46:08
    2015-05-24 04:17:42   2015-05-24 04:17:42
    2015-05-24 04:13:22   2015-05-24 04:13:22
    2015-05-24 04:08:34   2015-05-24 04:08:34
    2015-05-24 04:04:42   2015-05-24 04:04:42
    Name: Date, dtype: datetime64[ns]
    ```

5.  我们将用 datetime 列替换 Dataframe 的索引(这允许我们使用复杂的 datetime 函数):

    ```
    tweets.index = tweets.Date
    tweets.index
    Index([u'2015-05-24 03:46:08', u'2015-05-24 04:17:42', u'2015-05-24 04:13:22',
           u'2015-05-24 04:08:34', u'2015-05-24 04:04:42', u'2015-05-24 04:00:01',
           u'2015-05-24 03:54:07', u'2015-05-24 04:25:29', u'2015-05-24 04:24:47',
           u'2015-05-24 04:06:42',
           ...
           u'2015-05-02 16:30:02', u'2015-05-02 16:29:35', u'2015-05-02 16:28:26',
           u'2015-05-02 16:27:53', u'2015-05-02 16:27:02', u'2015-05-02 16:26:39',
           u'2015-05-02 16:25:00', u'2015-05-02 16:23:39', u'2015-05-02 16:23:38',
           u'2015-05-02 16:23:21'],
          dtype='object', name=u'Date', length=52512)

    tweets.head()
    ```

    ![Exploratory data analysis](graphics/B05260_013_f11.jpg)

    ### 注意

    请注意,左边的黑色索引过去是数字,但现在是发送推文的确切日期时间
6.  对数据进行重新采样，以便每一行都代表一天，而不是一条推文，并汇总每天的情绪得分:

    ```
    # create a dataframe called daily_tweets which resamples tweets by D, averaging the columns
    daily_tweets = tweets[['sentiment']].resample('D', how='mean')
    # I only want the sentiment column in my new Dataframe.
    daily_tweets.head()
    ```

    ![Exploratory data analysis](graphics/B05260_013_f4.jpg)

现在看起来好多了！现在每一行代表一天，情绪得分栏显示了这一天的平均情绪。让我们看看我们有多少天的推文:

```
daily_tweets.shape

(23, 3)
```

好的，所以我们从超过 50，000 条推文到仅仅`23`天！让我们来看看情绪在几天内的发展:

```
# plot the sentiment as a line graph
daily_tweets.sentiment.plot(kind='line')
```

![Exploratory data analysis](graphics/B05260_13_5.jpg)

2015 年 5 月 23 天内特定公司的平均每日情绪

```
# get historical prices through the Yahoo Finance API
from yahoo_finance import Share
yahoo = Share("AAPL")
historical_prices = yahoo.get_historical('2015-05-2', '2015-05-25')
prices = pd.DataFrame(historical_prices)

prices.head()
```

![Exploratory data analysis](graphics/B05260_013_f5.jpg)

现在两件事:

*   我们实际上只对`Close`列感兴趣，这是交易日的最终价格
*   我们还需要将这个数据帧的索引设置为*日期时间*，这样我们就可以将情绪和价格数据帧合并在一起

    ```
    # Set the index of the price dataframe to also be datetimes
    prices.index = pd.to_datetime(prices['Date'])

    prices.info() #the columns aren't numbers!

    <class 'pandas.core.frame.DataFrame'>
    DatetimeIndex: 15 entries, 2015-05-22 to 2015-05-04
    Data columns (total 8 columns):
    Adj_Close    15 non-null object
    Close        15 non-null object        # NOT A NUMBER
    Date         15 non-null object
    High         15 non-null object
    Low          15 non-null object
    Open         15 non-null object
    Symbol       15 non-null object
    Volume       15 non-null object
    dtypes: object(8)
    ```

让我们解决这个问题。当我们这样做的时候，让我们也修正一下`Volume`，它代表当天交易的股票数量:

```
# cast the column as numbers
prices.Close = not_null_close.Close.astype('float')
prices.Volume = not_null_close.Volume.astype('float')
```

现在让我们试着在同一张图中绘制 AAPL 的交易量和价格:

```
# plot both volume and close as line graphs in the same graph, what do you notice is the problem?
prices[["Volume", 'Close']].plot()
```

![Exploratory data analysis](graphics/B05260_13_7.jpg)

哇，这里的怎么了？如果我们仔细观察，`Volume`和`Close`的比例非常不同！

```
prices[["Volume", 'Close']].describe()
```

![Exploratory data analysis](graphics/B05260_013_f6.jpg)

而且差了很多！ `Volume`列的均值以千万计，而平均收盘价只有 125！

```
# scale the columns by z scores using StandardScaler
# Then plot the scaled data
s = StandardScaler()
only_prices_and_volumes = prices[["Volume", 'Close']]
price_volume_scaled = s.fit_transform(only_prices_and_volumes)
pd.DataFrame(price_volume_scaled, columns=["Volume", 'Close']).plot()
```

![Exploratory data analysis](graphics/B05260_13_9.jpg)

那看起来好多了！你可以看到，随着 AAPL 价格在中间某个地方下跌，交易量也上升了！这实际上很常见:

```
# concatinate prices.Close, and daily_tweets.sentiment

merged = pd.concat([prices.Close, daily_tweets.sentiment], axis=1)
merged.head()
```

![Exploratory data analysis](graphics/B05260_013_f7.jpg)

嗯，为什么会有一些空值`Close`？如果你在日历上查找 2015 年 5 月 2 日，你会看到这是一个星期六，市场在星期六关闭，这意味着不可能有收盘价！因此，我们需要决定是否删除这些行，因为我们仍然有当天的情绪。最后，我们将尝试预测第二天的收盘价以及价格是否上涨，因此让我们继续并删除数据集中的任何空值:

```
# Delete any rows with missing values in any column
merged.dropna(inplace=True)
```

现在让我们尝试绘制我们的曲线图:

```
merged.plot()
# wow that looks awful
```

![Exploratory data analysis](graphics/B05260_13_11.jpg)

哇，太可怕了。再次重申，我们必须衡量我们的特征，才能看到任何有价值的见解:

```
# scale the columns by z scores using StandardScaler
from sklearn.preprocessing import StandardScaler
s = StandardScaler()
merged_scaled = s.fit_transform(merged)

pd.DataFrame(merged_scaled, columns=merged.columns).plot()
# notice how sentiment seems to follow the closing price
```

![Exploratory data analysis](graphics/B05260_13_12.jpg)

好多了！你可以开始看到股票的收盘价实际上是如何随着我们的情绪波动的。让我们更进一步，尝试应用监督学习模型。为了做到这一点，我们需要定义我们的特征和我们的反应。回想一下，我们的反应是我们希望预测的值，我们的特征是我们将用来预测反应的值。

如果我们查看数据的每一行，我们就有了当天的情绪和收盘价。然而，我们希望用今天的情绪来预测明天的股票价格，以及它是否上涨。想一想；这是一种欺骗，因为今天的情绪将包括收盘价最终确定后的推文。为了简化这一点，我们将忽略任何 tweet 作为预测今天价格的特征。

因此，对于每一行，我们的响应应该是今天的收盘价，而我们的特征应该是股票昨天的情绪。为此，我将使用 Pandas 中一个名为`shift`的内置函数，将我们的情绪栏向后移动一项:

```
# Shift the sentiment column backwards one item

merged['yesterday_sentiment'] = merged['sentiment'].shift(1)
merged.head()
```

![Exploratory data analysis](graphics/B05260_013_f8.jpg)

包含昨天情绪的数据框。

啊，很好，现在对于来说，每天我们都有真正的特色，那就是`yesterday_sentiment`。注意，在我们的头部(前五行)我们有一个新的空值！这是因为在第一天，我们没有昨天的值，所以我们必须删除它。但是在此之前，让我们定义一下我们的响应列。

我们有两个选择:

*   让我们的回答量化，并使用回归分析
*   将我们的回答转换为定性状态，并使用分类

选择哪条路线取决于数据科学家，取决于具体情况。如果你只是希望将情绪与价格运动联系起来，那么我推荐使用分类方法。如果你想把情绪和运动量联系起来，我推荐回归分析。我两样都会做！

### 回归路线

我们已经做好了在这条战线上前进的准备！我们有我们的回应和我们的特色。在继续之前，我们首先必须删除一个空值:

```
# Make a new dataframe for our regression and drop the null values

regression_df = merged[['yesterday_sentiment', 'Close']]
regression_df.dropna(inplace=True)
regression_df.head()
```

![Regression route](graphics/B05260_013_f9.jpg)

让我们同时使用随机森林和线性回归，并使用 RMSE 作为我们的衡量标准，看看哪种表现更好:

```
# Imports for our regression

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score
import numpy as np
```

我们将使用交叉验证的 RMSE 来比较我们的两个模型:

```
# Our RMSE as a result of cross validation linear regression

linreg = LinearRegression()
rmse_cv = np.sqrt(abs(cross_val_score(linreg, regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3, scoring='mean_squared_error').mean()))
rmse_cv

3.49837

# Our RMSE as a result of cross validation random forest

rf = RandomForestRegressor()
rmse_cv = np.sqrt(abs(cross_val_score(rf, regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3, scoring='mean_squared_error').mean()))
rmse_cv

3.30603
```

看看我们的 RMSE，两个模型都是大约 3.5，这意味着平均而言，我们的模型相差大约 3.5 美元，考虑到我们的股票价格可能不会像*和*那样大幅波动，这实际上是一个大问题:

```
regression_df['Close'].describe()

count     14.000000
mean     128.132858
std        2.471810    # Our standard deviation is less than our RMSE (bad sign)
min      125.010002
25%      125.905003
50%      128.195003
75%      130.067505
max      132.539993
```

测试我们模型有效性的另一种方法是将我们的 RMSE 与零模型的 RMSE 进行比较。回归模型的空模型预测每个值的平均值:

```
# null model for regression
mean_close = regression_df['Close'].mean()
preds = [mean_close]*regression_df.shape[0]
preds
from sklearn.metrics import mean_squared_error
null_rmse = np.sqrt(mean_squared_error(preds, regression_df['Close']))
null_rmse

2.381895
```

因为我们的模型没有击败空模型，也许回归不是最好的方法…

### 分类路线

对于分类，我们还有更多的工作要做，因为我们还没有一个明确的答案。要制作一个，我们需要将结束列转换成某种分类选项。我会选择做出如下回应。我将创建一个名为`change_close_big_deal`的新列，定义如下:

![Classification route](graphics/B05260_013_f10.jpg)

因此，如果我们的反应变化很大，我们的反应将是 *1* ，如果股票的变化可以忽略不计，我们的反应将是 *0* :

```
# Imports for our classification

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
import numpy as np

# Make a new dataframe for our classification and drop the null values

classification_df = merged[['yesterday_sentiment', 'Close']]

# variable to represent yesterday's closing price
classification_df['yesterday_close'] = classification_df['Close'].shift(1)

# column that represents the precent change in price since yesterday
classification_df['percent_change_in_price'] = (classification_df['Close']-classification_df['yesterday_close']) / classification_df['yesterday_close']

# drop any null values
classification_df.dropna(inplace=True)
classification_df.head()

# Our new classification response

classification_df['change_close_big_deal'] = abs(classification_df['percent_change_in_price'] ) > .01
classification_df.head()
```

![Classification route](graphics/B05260_013_f11.jpg)

我们的数据框架带有一个名为 change_close_big_deal 的新列，该列为真或假。

现在让我们执行与我们回归时相同的交叉验证，但这次我们将使用交叉验证模块的*准确性*功能，而不是回归模块，我们将使用两种分类机器学习算法:

```
# Our accuracy as a result of cross validation random forest

rf = RandomForestClassifier()
accuracy_cv = cross_val_score(rf, classification_df[['yesterday_sentiment']], classification_df['change_close_big_deal'], cv=3, scoring='accuracy').mean()

accuracy_cv

0.1777777
```

乌夫！不太好，让我们试试逻辑回归:

```
# Our accuracy as a result of cross validation logistic regression

logreg = LogisticRegression()
accuracy_cv = cross_val_score(logreg, classification_df[['yesterday_sentiment']], classification_df['change_close_big_deal'], cv=3, scoring='accuracy').mean()

accuracy_cv

0.5888
```

更好！但是当然我们应该用我们的零模型的准确性来检查它:

```
# null model for classification
null_accuracy = 1 - classification_df['change_close_big_deal'].mean()

null_accuracy

0.5833333
```

哇，我们的模型可以击败零准确性，这意味着我们的机器学习算法可以使用社交媒体情绪预测股票价格的变动，而不仅仅是随机猜测！

## 超越这个例子

我们有很多方法可以改进这个例子，以做出更可靠的预测。我们本可以包括更多的特征，包括情绪的移动平均值，而不是简单地看前一天的情绪。我们还可以引入更多的例子来增强我们对情感的理解。我们可以看看脸书、媒体等等，了解更多我们认为该股未来表现如何的信息。

我们实际上只有 14 个数据点，这对于制作一个生产就绪的算法来说是远远不够的。当然，就本书的目的而言，这已经足够了，但如果我们真的想开发一种能有效预测股价运动的金融算法，我们将不得不获得更多天的媒体和价格。

通过利用`sklearn`包中的`gridsearchCV`模块来充分利用我们的模型，我们可以花更多的时间来优化我们模型中的参数。还有其他专门处理时间序列数据(随时间变化的数据)的模型，包括一个名为**【ARIMA**的模型。ARIMA 等模型和类似的模型试图专注于特定的时间序列特征。



# 案例研究 2——为什么有些人会欺骗他们的配偶？

1978 年，对家庭主妇进行了一项调查，目的是找出导致她们追求婚外情的因素。这项研究成为未来许多男性和女性研究的基础，所有这些研究都试图聚焦于导致任何一方背着配偶在别处寻找伴侣的人和婚姻的特征。

监督学习并不总是关于预测。在这个例例研究中，我们将纯粹地试图确定许多因素中的几个，我们认为这些因素可能是导致某人追求婚外情的最重要的因素。

首先让我们读入数据:

```
# Using dataset of a 1978 survey conducted to measure likliehood of women to perform extramarital affairs
# http://statsmodels.sourceforge.net/stable/datasets/generated/fair.html

import statsmodels.api as sm
affairs_df = sm.datasets.fair.load_pandas().data
affairs_df.head()
```

![Case study 2 – why do some people cheat on their spouses?](graphics/B05260_013_f12.jpg)

`statsmodels`网站提供数据字典，如下:

*   `rate_marriage`:对婚姻的评价(妻子给的)， *1 =很差*， *2 =差*， *3 =一般*， *4 =好*， *5 =很好*；序数水平
*   `age`:妻子的年龄；比率水平
*   `yrs_married`:结婚年数:比率水平
*   `children`:夫妻生育子女数量:比例水平
*   `religious`:妻子有多虔诚， *1 =不*， *2 =温和*， *3 =相当*， *4 =强烈*；序数水平
*   `educ`:文化程度， *9 =初三学校*， *12 =高中*， *14 =某大专*， *16 =大专毕业*， *17 =某研究生院*， *20 =高级学位*；比率水平
*   `occupation` : *1 =学生*， *2 =务农，农业；半熟练或非熟练工人*； *3 =白领*； *4 =教师、辅导员、社工、护士；艺术家、作家；技术员、技术工人*、 *5 =管理、行政、商务*、 *6 =高级专业学位*；名义水平
*   `occupation_husb`:老公的职业。同职业；名义水平
*   `affairs`:婚外情时间的衡量；比率水平

好吧，所以我们有一个定量的反应，但我的问题只是什么因素导致某人有外遇。确切的分钟数或小时数并没有那么重要。为此，我们来做一个新的分类变量叫做`affair_binary`，要么为真(他们暧昧关系超过 0 分钟)，要么为假(他们暧昧关系 0 分钟):

```
# Create a categorical variable

affairs_df['affair_binary'] = (affairs_df['affairs'] > 0)
```

同样，该列要么有真值，要么有假值。如果此人婚外情超过 0 分钟，则该值为真。否则，该值为 false。从现在开始，让我们用这个二元反应作为我们的主要反应。现在我们试图找出这些变量中的哪些与我们的反应有关，所以让我们开始吧。

让我们从一个简单的相关矩阵开始。回想一下，这个矩阵向我们展示了定量变量和我们的反应之间的线性相关性。我将把相关矩阵显示为小数矩阵和热图。先来看看数字:

```
# find linear correlations between variables and affair_binary
affairs_df.corr()
```

![Case study 2 – why do some people cheat on their spouses?](graphics/B05260_013_f13.jpg)

1978 年李克特调查中婚姻事务数据的相关矩阵。

请记住，我们忽略了对角线上的一系列 1，因为它们只是告诉我们，每个定量变量都与自身相关。请注意其他相关变量，它们是最后一行或最后一列中最接近 1 和-1 的值(矩阵总是沿对角线对称)。

我们可以看到几个突出的变量:

*   `affairs`
*   `age`
*   `yrs_married`
*   `children`

这些是幅度最大的前四个变量(绝对值)。然而，这些变量之一是*作弊*。`Affairs`变量在数量上是最大的，但是与`affair_binary`密切相关，因为我们直接基于事件来制作变量`affair_binary`。所以我们忽略那个。让我们看一下我们的关联热图，看看我们的观点是否能在那里被看到:

```
import seaborn as sns
sns.heatmap(affairs_df.corr())
```

![Case study 2 – why do some people cheat on their spouses?](graphics/B05260_13_18.jpg)

同样的相关矩阵，但这次是热图。注意接近深红和深蓝的颜色(不包括对角线)。

我们正在寻找热图中的深红和深蓝区域。这些颜色与最相关的特征相关联。

记住，相关性并不是识别哪些特征与我们的反应相关的唯一方法。这种方法向我们展示了变量之间的线性相关性。通过评估决策树分类器的系数，我们可以找到另一个影响事务的变量。这些方法可能揭示与我们的变量相关的新变量，但不是以线性方式。

### 注意

还要注意，这里有两个变量实际上并不属于…你能找到它们吗？是`occupation`和`occupation_husb`变量。回想一下，我们之前认为它们是名义上的，因此无权包含在这个相关矩阵中。这是因为熊猫不知不觉地将它们转换成整数，现在认为它们是数量变量。别担心，我们很快会解决这个问题。

首先让我们把变成一个`X`和一个`y`数据框:

```
affairs_X = affairs_df.drop(['affairs', 'affair_binary'], axis=1) 
# data without the affairs or affair_binary column

affairs_y = affairs_df['affair_binary']
```

现在，我们将实例化一个决策树分类器，并交叉验证我们的模型，以确定模型在拟合我们的数据方面是否做得不错:

```
model = DecisionTreeClassifier() 
# instantiate the model

from sklearn.cross_validation import cross_val_score
# import our cross validation module

# check the accuracy on the training set
scores = cross_val_score(model, affairs_X, affairs_y, cv=10)

print scores.mean(), "average accuracy"
0.659756806845 average accuracy

print scores.std(), "standard deviation" # very low, meaning variance of the model is low
0.0204081732291 standard deviation

# Looks ok on the cross validation side
```

因为我们的标准差很低，我们可以假设我们模型的方差很低(因为方差是标准差的平方)。这很好，因为这意味着我们的模型在交叉验证的每一个折叠上都不会有很大的不同，它通常是一个可靠的模型。

因为我们同意我们的决策树分类器通常是一个可靠的模型，所以我们可以使树适合我们的整个数据集，并使用重要性度量来确定我们的树认为哪些变量最重要:

```
# Explore individual features that make the biggest impact
# rate_marriage, yrs_married, and occupation_husb. But one of these variables doesn't quite make sense right?
# Its the occupation variable, because they are nominal, their interpretations
model.fit(affairs_X, affairs_y)
pd.DataFrame({'feature':affairs_X.columns, 'importance':model.feature_importances_}).sort('importance').tail(3)
```

![Case study 2 – why do some people cheat on their spouses?](graphics/B05260_013_f14.jpg)

所以，`yrs_married`和`rate_marriage`都很重要，但最重要的变量是`occupation_husb`。但是这没有意义，因为这个变量是名义变量！因此，让我们应用我们的虚拟变量技术，其中我们创建新的列，代表`occupation_husb`和`occupation`的每个选项。

对于`occupation`栏:

```
# Dummy Variables:

# Encoding qualitiative (nominal) data using separate columns (see slides for linear regression for more)

occuptation_dummies = pd.get_dummies(affairs_df['occupation'], prefix='occ_').iloc[:, 1:]

# concatenate the dummy variable columns onto the original DataFrame (axis=0 means rows, axis=1 means columns)
affairs_df = pd.concat([affairs_df, occuptation_dummies], axis=1)
affairs_df.head()
```

这个新的数据框架有许多新列:

![Case study 2 – why do some people cheat on their spouses?](graphics/B05260_013_f15.jpg)

记住，这些新列`occ_2.0`、`occ_4.0`等等，代表一个二元变量，代表妻子是否拥有工作`2`或`4`，等等:

```
# Now for the husband's job

occuptation_dummies = pd.get_dummies(affairs_df['occupation_husb'], prefix='occ_husb_').iloc[:, 1:]

# concatenate the dummy variable columns onto the original DataFrame (axis=0 means rows, axis=1 means columns)
affairs_df = pd.concat([affairs_df, occuptation_dummies], axis=1)
affairs_df.head()

(6366, 15)
```

现在我们有 15 个新栏目了！让我们再次运行我们的树，找到最重要的变量:

```
# remove appropiate columns for feature set
affairs_X = affairs_df.drop(['affairs', 'affair_binary', 'occupation', 'occupation_husb'], axis=1)
affairs_y = affairs_df['affair_binary']

model = DecisionTreeClassifier()
from sklearn.cross_validation import cross_val_score
# check the accuracy on the training set
scores = cross_val_score(model, affairs_X, affairs_y, cv=10)
print scores.mean(), "average accuracy"
print scores.std(), "standard deviation" # very low, meaning variance of the model is low

# Still looks ok

# Explore individual features that make the biggest impact
model.fit(affairs_X, affairs_y)
pd.DataFrame({'feature':affairs_X.columns, 'importance':model.feature_importances_}).sort('importance').tail(10)
```

![Case study 2 – why do some people cheat on their spouses?](graphics/B05260_013_f16.jpg)

*   `age`
*   `yrs_married`
*   `children`

现在你知道了:

*   `rate_marriage`:婚姻的等级，由决策树告诉
*   根据决策树和我们的相关矩阵，他们有多少个孩子
*   根据决策树和我们的相关矩阵，他们结婚的年数
*   决策树显示的女性受教育程度
*   根据决策树和我们的相关矩阵，女性的年龄

在 1978 年的调查中，这些似乎是决定一个女人是否会卷入婚外情的前五个最重要的变量。



# 案例研究 3–使用 tensorflow

我想通过看一个更现代的模块来结束我们在一起的时间，这个模块是谷歌的机器学习部门最近才推出的，叫做 **tensorflow** 。

Tensorflow 是一个开源的机器学习模块，主要用于其简化的深度学习和神经网络能力。我想花点时间介绍一下这个模块，并使用 tensorflow 快速解决几个问题。tensorflow 的语法(就像第 12 章、*中的 py brain*一样)与我们通常的 *scikit-learn* 语法有点不同，所以我将一步一步地讲解它。先说一些进口:

```
from sklearn import datasets, metrics
import tensorflow as tf
import numpy as np
from sklearn.cross_validation import train_test_split
%matplotlib inline
```

我们从`sklearn`进口的有`train_test_split`、`datasets`、`metrics`。我们将利用我们的训练测试分割来减少过度拟合，我们将使用数据集来导入我们的`iris`分类数据，我们将使用度量模块来为我们的学习模型计算一些简单的度量。

Tensorflow 以不同的方式学习，因为它总是试图最小化误差函数。它通过迭代遍历我们的整个数据集来做到这一点，并不时更新我们的模型以更好地适应数据。

值得注意的是，tensorflow 不仅实现了神经网络，还可以实现更简单的模型。例如，让我们使用 tensorflow 实现一个经典的逻辑回归:

```
# Our data set of iris flowers
iris = datasets.load_iris()

# Load datasets and split them for training and testing
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)

####### TENSORFLOW #######

# Here is tensorflow's syntax for defining features.
# We must specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]
# notice the dimension is set to four because we have four columns

# We set our "learning rate" which is a decimal that tells the network 
# how quickly to learn
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)
# A learning rate closer to 0 means the network will learn slower

# Build a linear classifier (logistic regression)
# note we have to tell tensorflow the number of classes we are looking for
# which are 3 classes of iris
classifier = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns,
                                              optimizer=optimizer,
                                                      n_classes=3)

# Fit model. Uses error optimization techniques like stochastic gradient descent
classifier.fit(x=X_train,
               y=y_train,
               steps=1000)  # number of iterations
```

我将指出前面代码片段中的关键行代码，以真正巩固培训过程中发生的事情:

*   **feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]**

    这里我创建了四个输入*列*，我们知道它们与花的萼片长度、萼片宽度、花瓣长度和花瓣宽度相关。

*   **optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)**

    在这里，我告诉 tensorflow 使用称为**梯度下降**的东西进行优化，这意味着我们将定义一个误差函数(这将在下一步中发生),并逐渐地，我们将努力使这个误差函数最小化。

    我们的学习率应该徘徊在接近 0，因为我们希望我们的模型学习缓慢。如果我们的模型学习得太快，它可能会“跳过”正确的答案！

*   **classifier = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns, optimizer=optimizer, n_classes=3)**:

    当指定`LinearClassifier`时，我们表示逻辑回归最小化的相同误差函数，这意味着该分类器试图作为逻辑回归分类器工作。

    我们按照步骤 1 中的定义给模型赋予我们的`feature_columns`。

    `optimizer`是最小化我们的误差函数的方法；在这种情况下，我们选择梯度下降。

    我们还必须指定我们的类的数量为`3`。我们知道我们有三种不同的鸢尾花供模特选择。

*   **classifier.fit(x=X_train, y=y_train, steps=1000)**:

    这种训练看起来类似于 scikit-learn 模型，增加了一个名为`steps`的参数。步骤告诉我们想要查看数据集的次数。所以当我们指定`1000`时，我们是在数据集上迭代。我们走的步数越多，模型获得的学习机会就越多。

唷！当我们运行前面的代码时，一个线性分类器(逻辑回归)模型正在被拟合，当它完成时，它准备好被测试:

```
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=X_test,
                                     y=y_test)["accuracy"]

print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.973684
```

太棒了。值得注意的是，当使用 tensorflow 时，我们也可以利用一个类似的简单函数，`predict`:

```
# Classify two new flower samples.
new_samples = np.array(
    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)

y = classifier.predict(new_samples)
print('Predictions: {}'.format(str(y)))
Predictions: [1 2]
```

现在让我们将其与标准的 scikit-learn 逻辑回归进行比较，看看谁赢了:

```
from sklearn.linear_model import LogisticRegression
# compare our result above to a simple scikit-learn logistic regression

logreg = LogisticRegression()
# instantiate the model

logreg.fit(X_train, y_train)
# fit it to our training set

y_predicted = logreg.predict(X_test)
# predict on our test set, to avoid overfitting!

accuracy = metrics.accuracy_score(y_predicted, y_test)
# get our accuracy score

accuracy
# It's the same thing!
```

哇，所以看起来在 1000 步的情况下，梯度下降优化的 tensorflow 模型并不比简单的 sklearn logistic 回归好。好吧，那很好，但是如果我们允许模型在`iris`数据集上迭代更多会怎么样？

```
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]

optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)

classifier = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns,
                                               optimizer=optimizer,
                                            n_classes=3)

classifier.fit(x=X_train,
               y=y_train,
               steps=2000)  # number of iterations is 2000 now
```

我们的代码和以前完全一样，但是现在我们有了`2000`个步骤，而不是`1000`:

```
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=X_test,
                                     y=y_test)["accuracy"]

print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.973684
```

现在我们有了更高的精确度！

### Tip

请注意，您需要非常小心地选择步骤的数量。随着该数值的增加，模型反复看到相同精确训练点的次数也会增加。我们确实有可能变得过胖！为了补救这一点，我会建议选择多个训练测试分割，并在每个分割上运行模型( **k 倍交叉验证**)。

另外值得一提的是，tensorflow 实现了非常低偏差的高方差模型。这意味着对 tensorflow 再次运行前面的代码可能会得到不同的答案！这是深度学习的警告之一。他们可能会收敛到一个非常大的低偏差模型，但该模型将有很高的方差，因此，令人惊讶的是，可能不会推广到所有的样本数据。如前所述，交叉验证将有助于缓解这一问题。

## 张量流和神经网络

现在让我们将一个更强大的模型指向我们的`iris`数据集。让我们创建一个神经网络，它的目标是对鸢尾花进行分类(为什么不呢？):

```
# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]

optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                        hidden_units=[10, 20, 10],
                                        optimizer=optimizer,
                                        n_classes=3)

# Fit model.
classifier.fit(x=X_train,
               y=y_train,
               steps=2000)
```

请注意，我们的代码实际上与上一段没有什么变化。我们仍然有以前的`feature_columns`，但现在我们引入了一个 **DNNClassifier** ，代替线性分类器，它代表**深度神经网络分类器**。

这是 tensorflow 实现神经网络的语法。让我们仔细看看:

```
tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                     hidden_units=[10, 20, 10],
                                     optimizer=optimizer,
                                     n_classes=3)
```

我们看到我们输入的是同一个`feature_columns`、`n_classes`和`optimizer`，但是我们怎么会有一个新的参数叫做`hidden_units`？该列表表示输入层和输出层之间的每层中的节点数量。

总而言之，这个神经网络将有五层:

*   第一层有四个节点，每个节点对应一个虹膜特征变量。这一层是输入层。
*   10 个节点的隐藏层。
*   20 个节点的隐藏层。
*   10 个节点的隐藏层。
*   最后一层有三个节点，每个节点代表网络的一种可能结果。这被称为我们的输出层。

既然我们已经训练了我们的模型，让我们在我们的测试集上评估它:

```
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=X_test,
                                     y=y_test)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.921053
```

嗯，我们的神经网络在这个数据集上做得不太好，但也许这是因为网络对于这样一个简单的数据集来说有点太复杂了。让我们介绍一个新的数据集，它有更多的…

`MNIST`数据集由超过 50，000 个手写数字(0-9)组成，目标是识别手写数字并输出他们正在书写的字母。Tensorflow 内置了下载和加载这些图像的机制。我们以前见过这些图像，但在第 12 章[和超越本质的*中以小得多的比例:*](ch12.html "Chapter 12. Beyond the Essentials")

```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=False)

Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
```

注意，我们用于下载`mnist`的一个被称为`one_hot`。此参数要么将数据集的目标变量(即数字本身)作为单个数字引入，要么包含一个虚拟变量。

例如，如果第一个数字是 7，则目标要么是:

*   7:如果`one_hot`为假
*   0 0 0 0 0 0 0 0 1 0 0:如果`one_hot`为真(注意从 0 开始，第七个索引是 1)

我们将以前一种方式对我们的目标进行编码，因为这是我们的 tensorflow 神经网络和我们的 sklearn 逻辑回归所期望的。

数据集已经分为训练集和测试集，所以让我们创建新的变量来保存它们:

```
x_mnist = mnist.train.images
y_mnist = mnist.train.labels.astype(int)
```

对于`y_mnist`变量，我特意将每个目标都转换为整数(默认情况下它们以浮点形式出现)，因为否则 tensorflow 会向我们抛出一个错误。

出于好奇，我们来看一张图:

```
import matplotlib.pyplot as plt
plt.imshow(x_mnist[10].reshape(28, 28))
```

![Tensorflow and neural networks](graphics/B05260_13_22.jpg)

MNIST 数据集的数字 0

希望我们的目标变量也符合第 10 ^个指数:

```
y_mnist[10]
0
```

太棒了。现在让我们来看看我们的数据集有多大:

```
x_mnist.shape
(55000, 784)

y_mnist.shape
(55000,)
```

我们的训练规模是`55000`图像和目标变量。

让我们为我们的图像安装一个深度神经网络，看看它是否能够识别我们输入的模式:

```
# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=784)]
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                     hidden_units=[10, 20, 10],
                                           optimizer=optimizer,
                                                  n_classes=10)

# Fit model.
classifier.fit(x=x_mnist,
               y=y_mnist,
               steps=1000)
# Warning this is veryyyyyyyy slow
```

这段代码与我们之前使用`DNNClassifier`的片段非常相似；然而，看看在我们的第一行代码中，我是如何将列的数量改为`784`的，而在分类器本身中，我将输出类的数量改为`10`。这些是手动输入，tensorflow 必须提供这些输入才能工作。

前面的代码运行*非常*慢。它会一点一点地调整自己，以便从我们的训练集中获得尽可能好的表现。当然，我们知道这里的最终测试是在一个未知的测试集上测试我们的网络，这个测试集也是 tensorflow 给我们的:

```
x_mnist_test = mnist.test.images
y_mnist_test = mnist.test.labels.astype(int)

x_mnist_test.shape
(10000, 784)

y_mnist_test.shape
(10000,)
```

所以我们有 10，000 张图片来测试。让我们看看我们的网络是如何适应数据集的:

```
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=x_mnist_test,
                                     y=y_mnist_test)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.920600
```

```
Not bad, 92% accuracy on our dataset. Let's take a second and compare this performance to a standard sklearn logistic regression now:
logreg = LogisticRegression()
logreg.fit(x_mnist, y_mnist)
# Warning this is slow

y_predicted = logreg.predict(x_mnist_test)
from sklearn.metrics import accuracy_score
# predict on our test set, to avoid overfitting!

accuracy = accuracy_score(y_predicted, y_mnist_test)
# get our accuracy score

accuracy
0.91969
```

成功！我们的神经网络比标准的逻辑回归表现得更好。这可能是因为网络试图找到像素本身之间的关系，并使用这些关系将它们映射到我们正在写下的数字。在逻辑回归中，模型假设每个输入都是相互独立的，因此很难找到它们之间的关系。

有很多方法可以让我们的神经网络以不同的方式学习:

*   我们可以使我们的网络更宽，即增加隐藏层中的节点数，而不是有几个节点数较少的层:![Tensorflow and neural networks](graphics/B05260_13_23.jpg)

    来源:[http://电子设计。网站文件/电子设计。com/files/uploads/2015/02/0816 _ Development _ Tools _ F1 _ 0。gif](http://electronicdesign.com/site-files/electronicdesign.com/files/uploads/2015/02/0816_Development_Tools_F1_0.gif)

    ```
    # A wider network
    feature_columns = [tf.contrib.layers.real_valued_column("", dimension=784)]

    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)

    # Build 3 layer DNN with 10, 20, 10 units respectively.
    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                         hidden_units=[1500],
                                         optimizer=optimizer,
                                                n_classes=10)

    # Fit model.
    classifier.fit(x=x_mnist,
                   y=y_mnist,
                   steps=100)
    # Warning this is veryyyyyyyy slow
    # Evaluate accuracy.
    accuracy_score = classifier.evaluate(x=x_mnist_test,
                                         y=y_mnist_test)["accuracy"]
    print('Accuracy: {0:f}'.format(accuracy_score))
          Accuracy: 0.898400
    ```

    
*   我们可以提高学习速度，迫使网络更快地收敛到一个答案。正如我之前提到的，如果我们沿着这条路走下去，我们会冒模型完全跳过答案的风险。通常最好坚持较小的学习速率。
*   我们可以改变优化的方法。梯度下降很受欢迎；然而，还有其他算法可以做到这一点。一个例子叫做 **亚当优化器**。区别在于它们遍历误差函数的方式，以及它们接近优化点的方式。不同领域的不同问题需要不同的优化器。
*   除了尝试让网络为我们解决所有问题之外，没有什么可以替代传统的特征选择阶段。我们可以花时间去寻找相关的和有意义的特征，这些特征实际上会让我们的网络更快地找到答案！



# 总结

在这一章中，我们看到了来自三个不同领域的三个不同的案例研究，它们使用了许多不同的统计和机器学习方法。然而，所有这些问题的共同点是，为了正确解决它们，我们必须实现数据科学思维。我们必须以有趣的方式解决问题，获取数据，清理数据，可视化数据，最后，建模数据并评估我们的思维过程。

我真的希望你已经发现这本书的内容是有趣的，而不仅仅是最后一章！我让您继续探索数据科学的世界。继续学习 Python。坚持学习统计和概率。保持开放的心态。我希望这本书已经成为一种催化剂，让你走出去，在这个问题上找到更多。

对于这本书以外的进一步阅读，我强烈建议看看著名的数据科学书籍和博客，例如:

*   凯文·马卡姆的博客
*   *面向数据科学家的 Python*由 Packt

如果您出于任何原因想要联系我，请随时联系`<[sinan.u.ozdemir@gmail.com](mailto:sinan.u.ozdemir@gmail.com)>`。