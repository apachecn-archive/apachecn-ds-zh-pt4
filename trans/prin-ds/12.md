<title>Chapter 12. Beyond the Essentials</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 第十二章。超越本质

在这一章中，我们将讨论数据科学中一些更复杂的部分，这些部分可能会让一些人望而却步。这是因为数据科学并不全是有趣的机器学习。有时，我们必须讨论和考虑理论和数学范式，并评估我们的程序。

这一章将一步一步地探索这些程序，以便我们完全和彻底地理解这些主题。我们将讨论以下主题:

*   交叉验证
*   偏差方差权衡
*   过度拟合和欠拟合
*   组装技术
*   随机森林
*   神经网络

这些只是要涵盖的部分主题。我不想让你感到困惑。我将尽力用许多例子和图像来解释每一个程序/算法。

# 偏差方差权衡

我们已经在前面的章节中简要讨论了偏差和方差的概念。当我们在讨论这两个概念时，我们通常是指监督学习算法。我们具体讨论的是由偏差和方差导致的预测模型的误差。

## 偏差导致的误差

当谈到因偏差而产生的误差时，我们指的是我们的模型的预期预测值和我们试图预测的实际(正确)值之间的差异。实际上，偏差衡量的是，一般来说，我们的模型预测与正确值的差距。

将偏差简单地理解为预测值和实际值之间的差异。例如，考虑我们的模型，表示为 *F(x)* ，预测 *29* 的值如下:

![Error due to bias](graphics/B05260_012_f1.jpg)

这里， *29* 的值应该已经在 *79* 被预测，那么:

![Error due to bias](graphics/B05260_012_f2.jpg)

如果机器学习模型在其预测(回归或分类)方面往往非常准确，则它被认为是低偏差模型，而如果模型经常出错，则它被认为是高偏差模型。

偏差是一种基于*精确度*或模型平均正确程度来判断模型的方法。

## 由于差异造成的误差

由方差引起的误差取决于给定数据点的模型预测的可变性。想象一下，你一遍又一遍地重复机器学习模型构建过程。方差是通过查看不同最终结果之间对某一固定点的预测变化程度来衡量的。

为了想象你头脑中的变化，考虑一组数据点。如果你一次又一次地随机取样，你的机器学习模型每次会有多大程度的改变或不同程度的适应。如果模型在样本之间变化不大，则该模型将被视为低方差模型。如果您的模型在样本之间变化很大，那么该模型将被视为一个*高方差*模型。

方差是在概化的基础上判断我们的模型的一个很好的度量。如果我们的模型有一个低方差，我们可以预期它在被放入野外时会以某种方式表现，并在没有人类监督的情况下预测值。

我们的目标是优化偏差和方差。理想情况下，我们寻求尽可能低的方差和偏差。

我发现用一个例子可以很好地解释这一点。

**示例–比较哺乳动物的体重和大脑重量**

想象一下，我们正在考虑哺乳动物的大脑重量和它们相应的体重之间的关系。一种假设可能认为两者之间存在正相关关系(一个上升，另一个也上升)。但是这种关系有多牢固呢？它是线性的吗？也许，随着大脑重量的增加，体重会出现对数或二次增长。

让我们用 Python 来探索一下，如图所示:

```
# # Exploring the Bias-Variance Tradeoff

import pandas as pd
import numpy as np
import seaborn as sns
%matplotlib inline
```

我将使用一个名为`seaborn`的模块，将数据点可视化为散点图，并绘制线性(和高次多项式)回归模型:

```
# ## Brain and body weight

'''
This is a [dataset]) of the average 
weight of the body and the brain for 
62 mammal species. Let's read it into pandas and 
take a quick look:
'''

df = pd.read_table('http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt', sep='\s+', skiprows=33, names=['id','brain','body'], index_col='id')
df.head()
```

![Error due to variance](graphics/B05260_012_f3.jpg)

我们将获取小子集的样本，以加重偏差和方差的视觉表示，如下所示:

```
# We're going to focus on a smaller subset in which the body weight is less than 200:
df = df[df.body < 200]
df.shape
(51, 2)
```

我们实际上要假装只有`51`种哺乳动物存在。换句话说，我们假装这是所有已知哺乳动物的大脑和体重的全部数据。

```
# Let's create a scatterplot
sns.lmplot(x='body', y='brain', data=df, ci=None, fit_reg=False)
sns.plt.xlim(-10, 200)
sns.plt.ylim(-10, 250)
```

![Error due to variance](graphics/B05260_12_02.jpg)

哺乳动物大脑和体重散点图

对于哺乳动物来说，大脑和体重之间似乎有一种关系。到目前为止，我们可以假设这是一个正相关。

现在，让我们加入一个线性回归。让我们使用`seaborn`制作并绘制一次多项式(线性)回归。

```
sns.lmplot(x='body', y='brain', data=df, ci=None)
sns.plt.xlim(-10, 200)
sns.plt.ylim(-10, 250)
```

![Error due to variance](graphics/B05260_12_03.jpg)

与之前相同的散点图，加入了线性回归可视化

现在，让我们假设发现了一种新的哺乳动物物种。我们测量了我们能找到的这个物种的每个成员的体重，并计算出平均体重为 100。我们想预测这个物种的平均大脑重量(而不是直接测量)。使用这条线，我们可以预测大脑重量约为 45。

你可能会注意到，这条线离图表中的数据点不是很近，所以，也许它不是最好的模型！你可能会说*偏差太高了*。我同意！线性回归模型往往有很高的偏差，但线性回归也有锦囊妙计——它的方差非常低。然而，这到底意味着什么呢？

假设我们将所有哺乳动物随机分成两个样本，如下所示:

```
# set a random seed for reproducibility
np.random.seed(12345)

# randomly assign every row to either sample 1 or sample 2
df['sample'] = np.random.randint(1, 3, len(df))
df.head()
```

包括新的样品列:

![Error due to variance](graphics/B05260_012_f4.jpg)

```
# Compare the two samples, they are fairly different!
df.groupby('sample')[['brain', 'body']].mean()
```

![Error due to variance](graphics/B05260_012_f5.jpg)

我们现在可以告诉`seaborn`创建两个图，其中左边的图仅使用来自样本 1 的数据，右边的图仅使用来自样本 2 的数据:

```
# col='sample' subsets the data by sample and creates two 
# separate plots
sns.lmplot(x='body', y='brain', data=df, ci=None, col='sample')
sns.plt.xlim(-10, 200)
sns.plt.ylim(-10, 250)
```

![Error due to variance](graphics/B05260_12_06.jpg)

样品 1 和 2 的线性回归并排散点图

他们看起来没什么不同，对吗？如果你仔细观察，你会注意到样本之间没有一个数据点是相同的，然而这条线看起来几乎是一样的。为了进一步说明这一点，让我们将最佳拟合的两条线放在同一个图形中，并使用颜色来分隔样本，如图所示:

```
# hue='sample' subsets the data by sample and creates a 
# single plot
sns.lmplot(x='body', y='brain', data=df, ci=None, hue='sample')
sns.plt.xlim(-10, 200)
sns.plt.ylim(-10, 250)
```

![Error due to variance](graphics/B05260_12_07.jpg)

尽管事实上他们使用了不同的数据样本，但这两个图之间的线看起来非常相似。在这两种情况下，我们都预测大脑重量约为 45。

事实上，即使对从*相同人群*中提取的*完全不同的*数据集进行线性回归，也产生了非常相似的线，这表明该模型的方差很低。

如果我们增加模型的复杂性，让它*了解更多*会怎么样？与其拟合直线，不如让`seaborn`拟合一个四次多项式(四次多项式)。通过增加多项式的次数，图表将能够曲折变化，以便更好地拟合我们的数据，如下所示:

```
# What would a low bias, high variance model look like? Let's try polynomial regression, with an fourth order polynomial:
sns.lmplot(x='body', y='brain', data=df, ci=None, \
col='sample', order=4)
sns.plt.xlim(-10, 200)
sns.plt.ylim(-10, 250)
```

![Error due to variance](graphics/B05260_12_08.jpg)

使用四次多项式进行回归

请注意，对于来自同一总体的两个不同样本，四次多项式看起来有很大不同。这是高方差的标志。

这个模型是低偏差的，因为它与我们的数据吻合得很好！然而，这是一个很大的差异，因为模型根据样本中的点而有很大的不同。(对于体重为 100 的人，大脑重量的预测值要么是 40，要么是 0，这取决于样本中的数据。)

我们的多项式也不知道数据的一般关系。很明显，哺乳动物的大脑和体重之间存在着正相关关系。然而，在我们的四次多项式中，这种关系是无处可寻的，是不可靠的。在我们的第一个例子中(左边的图)，多项式最终向下，而在第二个图中，图向上接近终点。我们的模型是不可预测的，并且可以根据给定的训练集表现出非常不同的行为。

作为数据科学家，我们的工作就是找到一个中间地带。

也许我们可以创建一个比线性模型偏差更小，比四阶多项式方差更小的模型？

```
# Let's try a second order polynomial instead:
sns.lmplot(x='body', y='brain', data=df, ci=None, col='sample', order=2)
sns.plt.xlim(-10, 200)
sns.plt.ylim(-10, 250)
```

![Error due to variance](graphics/B05260_12_09.jpg)

使用二次多项式作为估计量的散点图

这张图似乎很好地平衡了偏差和方差。

## 偏倚/方差权衡的两种极端情况

我们刚刚看到的是模型拟合的两种极端情况:一种是欠拟合，另一种是过拟合。

### 欠拟合

当我们的模型很少或没有尝试去拟合我们的数据时，就会出现拟合不足。高偏差和低方差的模型容易出现欠拟合。在哺乳动物大脑/体重的例子中，线性回归不符合我们的数据。虽然我们对这种关系有一个大致的了解，但我们会有很大的偏见。

如果您的学习算法显示高偏差和/或不适合，以下建议可能会有所帮助:

*   **使用更多功能**:尝试在模型中加入新功能，如果这对我们的预测能力有帮助的话。
*   尝试更复杂的模型:增加模型的复杂性有助于改善偏差。过于复杂的模型也会造成伤害！

### 过度拟合

过度拟合是模型过于努力适应训练集的结果，导致较低的偏差但较高的方差。低偏差和高方差的模型容易过度拟合。在哺乳动物大脑/体重的例子中，四次多项式(四次)回归过度拟合了我们的数据。

如果您的学习算法显示高方差和/或过度拟合，以下建议可能会有所帮助:

*   使用更少的特征:使用更少的特征可以减少方差，防止过度拟合
*   **适应更多的训练样本**:在我们的交叉验证中使用更多的训练数据点可以减少过度适应的影响，并改进我们的高方差估计量

## 偏差/方差如何影响误差函数

误差函数(衡量我们的模型有多不正确)可以被认为是偏差、方差和不可约误差的函数。从数学上来说，使用我们的监督学习模型预测数据集的误差可能如下所示:

![How bias/variance play into error functions](graphics/B05260_012_f6.jpg)

在这里，*偏差 ²* 是我们的偏差项平方(在从更复杂的方程中简化上述陈述时出现)，*方差*是我们的模型拟合在随机样本之间变化多少的度量。

简单地说，偏差和方差都会导致误差。随着我们增加模型的复杂性(例如，从线性回归到八次多项式回归或使决策树更深)，我们发现**偏差²减少，方差增加，模型的总误差形成抛物线形状，如图所示:**

![How bias/variance play into error functions](graphics/B05260_12_10.jpg)

作为数据科学家，我们的目标是找到优化我们模型复杂性的*最佳时机*。我们的数据很容易过量。为了在实践中对抗过度拟合，我们应该总是使用交叉验证(迭代地分割数据集，重新训练模型并平均度量)来获得错误的最佳预测。

为了说明这一点，我将(快速)引入一个新的监督算法，并直观地演示偏差/方差权衡。

我们将使用**K-最近邻** ( **KNN** )算法，这是一种监督学习算法，使用相似范式，这意味着它根据过去看到的类似数据点进行预测。

KNN 有一个复杂性输入 K，它表示要比较的相似数据点的数量。如果 *K = 3* ，那么，对于给定的输入，我们查看最近的三个数据点，并使用它们进行预测。在这种情况下，K 代表我们的模型复杂度。

```
from sklearn.neighbors import KNeighborsClassifier
# read in the iris data
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target
```

所以，我们有我们的`X`和`y`。对模型进行过度拟合的一个好方法是根据完全相同的数据进行训练和预测。

```
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X, y)
knn.score(X, y)
1.0
```

哇，100%准确？！这太好了，不可能是真的。

通过对同一个数据进行训练和预测，我们本质上是在告诉我们的数据纯粹的死记硬背训练集，并把它吐回给我们(这叫做我们的训练错误)。这就是为什么我们在第 10 章[，*如何判断你的烤面包机是否在学习——机器学习基础*中介绍了我们的训练和测试集。](ch10.html "Chapter 10. How to Tell If Your Toaster Is Learning – Machine Learning Essentials")

<title>K folds cross-validation</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# K 倍交叉验证

k 倍交叉验证是我们模型性能的更好的估计，甚至比我们的训练测试分割更好。它是这样工作的:

1.  我们将对数据进行有限数量的等分(通常为 3、5 或 10)。假设这个数叫做 k。
2.  对于交叉验证的每个“折叠”,我们将 k-1 个部分作为训练集，其余部分作为测试集。
3.  对于剩余的折叠，k-1 个部分的不同排列被考虑用于我们的训练集，并且不同的部分是我们的训练集。
4.  我们为交叉验证的每个折叠计算一组指标。
5.  我们最后平均分数。

交叉验证有效地使用了在同一个数据集上进行的多个训练测试分割。这样做有几个原因，但主要是因为交叉验证是对我们模型的样本外误差最诚实的估计。

为了直观地解释这一点，让我们看一下哺乳动物大脑和体重的例子。下面的代码手动创建了一个五重交叉验证，其中五个不同的定型集和测试集来自同一总体:

```
from sklearn.cross_validation import KFold

df = pd.read_table('http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt', sep='\s+', skiprows=33, names=['id','brain','body'])
df = df[df.brain < 300][df.body < 500]
# limit points for visibility

nfolds = 5
fig, axes = plt.subplots(1, nfolds, figsize=(14,4))
for i, fold in enumerate(KFold(len(df), n_folds=nfolds, 
                               shuffle=True)):
    training, validation = fold
    x, y = df.iloc[training]['body'], df.iloc[training]['brain']
    axes[i].plot(x, y, 'ro')
    x, y = df.iloc[validation]['body'], df.iloc[validation]['brain']
    axes[i].plot(x, y, 'bo')
plt.tight_layout()
```

![K folds cross-validation](graphics/B05260_12_11.jpg)

五重交叉验证:红色=训练集，蓝色=测试集

在这里，每个图显示了完全相同的哺乳动物群体，但是如果它们属于该折叠的训练集，则这些点是红色的，如果它们属于测试集，则是蓝色的。通过这样做，我们获得了同一个机器学习模型的五个不同实例，以查看性能是否在所有褶皱中保持一致。

如果您盯着这些点看足够长的时间，您会注意到每个点在训练集中恰好出现四次(k–1)，而同一个点在测试集中恰好出现一次且只有一次。

K 倍交叉验证的一些特性包括:

*   这是比单次训练测试分割更准确的 OOS 预测误差估计，因为它采用了几次独立的训练测试分割，并将结果平均在一起。
*   这比单次训练测试分割更有效地利用数据，因为整个数据集用于多次训练测试分割，而不是仅用于一次。
*   我们数据集中的每条记录都用于训练和测试。
*   这种方法在效率和计算开销之间呈现出明显的折衷。10 倍 CV 比单次训练/测试分割的计算成本高 10 倍。
*   该方法可用于参数整定和型号选择。

基本上，每当我们希望在一组数据上测试一个模型时，无论我们刚刚完成了一些参数的调整还是特性工程，k-fold 交叉验证都是评估我们的模型性能的一种极好的方法。

当然，`sklearn`带有一个更容易使用的交叉验证模块，叫做`cross_val_score`，它自动为我们分割数据集，在每个折叠上运行模型，并给我们一个整洁的结果输出:

```
# Using a training set and test set is so important
# Just as important is cross validation. Remember cross validation 
# is using several different train test splits and  
# averaging your results!

## CROSS-VALIDATION

# check CV score for K=1
from sklearn.cross_validation import cross_val_score, train_test_split
tree = KNeighborsClassifier(n_neighbors=1)
scores = cross_val_score(tree, X, y, cv=5, scoring='accuracy')
scores.mean()
0.95999999999
```

这比我们之前的分数`1`要准确得多。请记住，我们不再获得 100%的准确性，因为我们有一个独特的训练和测试集。KNN 从未见过测试点的数据点，因此无法将它们与自己完全匹配。

让我们尝试用`K=5`交叉验证 KNN(增加我们模型的复杂性)，如下所示:

```
# check CV score for K=5
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
scores
np.mean(scores)
0.97333333
```

甚至更好！所以，现在我们要找到最好的 K？最好的 K 是最大化我们的准确性的 K。让我们试几个:

```
# search for an optimal value of K
k_range = range(1, 30, 2) # [1, 3, 5, 7, …, 27, 29]
errors = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
   # instantiate a KNN with k neighbors
   scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
 # get our five accuracy scores
    accuracy = np.mean(scores)
   # average them together
    error = 1 – accuracy
   # get our error, which is 1 minus the accuracy
    errors.append(error)
   # keep track of a list of errors
```

对于 K (1，3，5，7，9)的每个值，我们现在都有一个误差值(1-精度)..,.., 29):

```
# plot the K values (x-axis) versus the 5-fold CV score (y-axis)
plt.figure()
plt.plot(k_range, errors)
plt.xlabel('K')
plt.ylabel('Error')
```

![K folds cross-validation](graphics/B05260_12_12.jpg)

KNN 模型相对于 KNN 复杂度的误差图，用 K 值表示

将此图与之前的模型复杂性和偏差/方差图进行比较。向左，我们的图表有较高的偏差，并且拟合不足。随着我们增加模型的复杂性，误差项开始下降，但过了一段时间，我们的模型变得过于复杂，高方差开始出现，使我们的误差项回升。

K 的最佳值似乎在 6 到 10 之间。

<title>Grid searching</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 网格搜索

`sklearn`还有另一个叫做网格搜索的有用工具。网格搜索将强力尝试许多不同的模型参数，并根据我们选择的度量给出最佳参数。例如，我们可以选择通过以下方式优化 KNN 的准确性:

```
from sklearn.grid_search import GridSearchCV
# import our grid search module

knn = KNeighborsClassifier()
# instantiate a blank slate KNN, no neighbors

k_range = range(1, 30, 2)
param_grid = dict(n_neighbors=k_range)
# param_grid = {"n_ neighbors": [1, 3, 5, …]}

grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')

grid.fit(X, y)
```

在第`grid.fit()`行代码中，发生的事情是，对于每个特征组合，在这种情况下，K 有 15 种不同的可能性，我们对每种可能性进行了五次交叉验证。这意味着到这段代码结束时，我们将有 *15 * 5 = 75* 个不同的 KNN 型号！您可以看到，当将这种技术应用于更复杂的模型时，我们会遇到时间上的困难:

```
# check the results of the grid search
grid.grid_scores_
grid_mean_scores = [result[1] for result in grid.grid_scores_]
# this is a list of the average accuracies for each parameter 
# combination
plt.figure()
plt.ylim([0.9, 1])
plt.xlabel('Tuning Parameter: N nearest neighbors')
plt.ylabel('Classification Accuracy')
plt.plot(k_range, grid_mean_scores)
plt.plot(grid.best_params_['n_neighbors'], grid.best_score_, 'ro', markersize=12, markeredgewidth=1.5,
         markerfacecolor='None', markeredgecolor='r')
```

![Grid searching](graphics/B05260_12_13.jpg)

请注意，前面的图与我们之前用`for`循环得到的图基本相同，但要简单得多！

我们看到七个邻居(在上图中圈出的)似乎具有最好的准确性。但是，我们也可以非常容易地获得最佳参数和最佳模型，如下所示:

```
grid.best_params_
# {'n_neighbors': 7}

grid.best_score_
# 0.9799999999

grid.best_estimator_
# actually returns the unfit model with the best parameters
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=7, p=2,
           weights='uniform')
```

我会更进一步。也许你已经注意到 KNN 还有其他参数，比如`algorithm`、`p`和`weights`。快速浏览 scikit-learn 文档可以发现，我们对其中的每一个都有一些选项，如下所示:

*   `p`是一个整数，代表我们希望使用的距离类型。默认情况下，我们使用`p=2`，这是我们的标准距离公式。
*   默认情况下，权重是`uniform`，但也可以是`distance`，它根据点的距离对点进行加权，这意味着更近的邻居对预测的影响更大。
*   算法是模型查找最近邻的方式。我们可以试试`ball_tree`、`kd_tree`或者`brute`。默认为`auto`，尝试自动使用最好的一个。

    ```
    knn = KNeighborsClassifier()
    k_range = range(1, 30)
    algorithm_options = ['kd_tree', 'ball_tree', 'auto', 'brute']
    p_range = range(1, 8)
    weight_range = ['uniform', 'distance']
    param_grid = dict(n_neighbors=k_range, weights=weight_range, algorithm=algorithm_options, p=p_range)
    # trying many more options
    grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
    grid.fit(X, y)
    ```

前面的代码在我的笔记本电脑上运行大约需要一分钟，因为它尝试了 1648 种不同的参数组合，并对每种组合进行了五次交叉验证。总而言之，要得到最好的答案，就要装配 8400 种不同的 KNN 模型！

```
grid.best_score_
0.98666666

grid.best_params_
{'algorithm': 'kd_tree', 'n_neighbors': 6, 'p': 3, 'weights': 'uniform'}
```

网格搜索是一种简单(但效率低下)的参数调整方式，可以让我们的模型获得最佳结果。应该注意的是，为了获得最好的结果，数据科学家应该使用特征操作(归约和工程)来在实践中获得更好的结果。不应该仅仅依靠模型来实现最佳性能。

## 可视化训练误差与交叉验证误差

我认为再次检查并比较交叉验证误差和训练误差是很重要的。这一次，让我们把它们放在同一个图表上，比较当我们改变模型复杂性时，它们是如何变化的。

我将再次使用哺乳动物数据集来显示交叉验证误差和训练误差(预测训练集的误差)。回想一下，我们试图将哺乳动物的体重回归到哺乳动物的大脑重量。

```
# This function uses a numpy polynomial fit function to
# calculate the RMSE of given X and y
def rmse(x, y, coefs):
    yfit = np.polyval(coefs, x)
    rmse = np.sqrt(np.mean((y - yfit) ** 2))
    return rmse

xtrain, xtest, ytrain, ytest = train_test_split(df['body'], df['brain'])

train_err = []
validation_err = []
degrees = range(1, 8)

for i, d in enumerate(degrees):
    p = np.polyfit(xtrain, ytrain, d)
  # built in numpy polynomial fit function

    train_err.append(rmse(xtrain, ytrain, p))
    validation_err.append(rmse(xtest, ytest, p))

fig, ax = plt.subplots()
# begin to make our graph

ax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')
ax.plot(degrees, train_err, lw=2, label = 'training error')
# Our two curves, one for training error, the other for cross validation

ax.legend(loc=0)
ax.set_xlabel('degree of polynomial')
ax.set_ylabel('RMSE')
```

![Visualizing training error versus cross-validation error](graphics/B05260_12_14.jpg)

因此，我们看到，随着我们拟合度的增加，我们的训练误差顺利下降，但我们现在足够聪明地知道，随着我们增加模型复杂性，我们的模型过度拟合我们的数据，只是将我们的数据回流给我们，而我们的交叉验证误差线要诚实得多，在大约 2 或 3 度后开始表现不佳。

概括一下:

*   当交叉验证误差和训练误差都很高时，就会出现欠拟合
*   当交叉验证误差较高，而训练误差较低时，会发生过度拟合
*   当交叉验证误差较低，并且仅略高于训练误差时，我们有很好的拟合

欠拟合(高偏差)和过拟合(高方差)都会导致数据泛化能力差。

如果你面临高偏差或方差，这里有一些提示。

如果您的模型倾向于具有*高偏差*:

*   尝试向训练集和测试集添加更多功能
*   要么增加模型的复杂性，要么尝试更现代的复杂模型

如果您的模型倾向于具有*高方差*:

*   尝试包含更多的训练样本，这样可以减少过度拟合的影响

一般来说，偏差/方差权衡是在我们的学习算法中努力最小化偏差和方差。过去几十年里发明的许多更新的学习算法都是为了两全其美。

<title>Ensembling techniques</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 组装技术

集成学习(ensembling)是将多个预测模型结合起来产生一个超级模型的过程，该超级模型比任何单独的模型都更准确。

*   **回归**:我们将对每个模型的预测取平均值
*   **分类**:投票并使用最常见的预测，或者取预测概率的平均值

假设我们正在处理一个二元分类问题(预测 0 或 1)。

```
# ENSEMBLING

import numpy as np

# set a seed for reproducibility
np.random.seed(12345)

# generate 1000 random numbers (between 0 and 1) for each model, representing 1000 observations
mod1 = np.random.rand(1000)
mod2 = np.random.rand(1000)
mod3 = np.random.rand(1000)
mod4 = np.random.rand(1000)
mod5 = np.random.rand(1000)
```

现在，我们模拟五种不同的学习模型，每种模型都有大约 70%的准确率，如下所示:

```
# each model independently predicts 1 (the "correct response") if random number was at least 0.3
preds1 = np.where(mod1 > 0.3, 1, 0)
preds2 = np.where(mod2 > 0.3, 1, 0)
preds3 = np.where(mod3 > 0.3, 1, 0)
preds4 = np.where(mod4 > 0.3, 1, 0)
preds5 = np.where(mod5 > 0.3, 1, 0)

print preds1.mean()
0.699
print preds2.mean()
0.698
print preds3.mean()
0.71
print preds4.mean()
0.699
print preds5.mean()
0.685

# Each model has an "accuracy of around 70% on its own
```

现在，让我们应用我的魔法学位。呃抱歉，数学。

```
# average the predictions and then round to 0 or 1
ensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 + preds5)/5.0).astype(int)
ensemble_preds.mean()

0.83
```

随着你在投票过程中加入更多的模型，出错的概率将会降低；这就是众所周知的孔多塞陪审团定理。

很疯狂，对吧？

为了在实践中很好地进行组装，模型必须具有以下特征:

*   **准确性**:每个模型必须至少优于空模型
*   **独立性**:一个模型的预测不受另一个模型预测过程的影响

如果你有一堆单独的 *OK* 模型，一个模型所犯的边缘情况错误很可能不会被其他模型所犯，因此在组合模型时这些错误将被忽略。

有以下两种基本的组装方法:

*   通过编写大量代码来手动集成您的单个模型
*   使用适合你的模型

我们将会看到一个适合我们的模型。要做到这一点，让我们再来看看决策树。

决策树倾向于具有低偏差和高方差。给定任何数据集，该树可以不断提问(做出决策)，直到它能够挑剔并区分数据集中的每个示例。它可以一个问题接一个问题地问，直到每个叶(终端)节点中只有一个例子。这棵树太努力了，长得太深了，只是记住了我们训练集的每一个细节。然而，如果我们重新开始，树可能会问不同的问题，并且仍然会变得很深。这意味着有许多可能的树可以区分所有元素，这意味着更高的方差。它无法很好地概括。

为了减少单个树的方差，我们可以对树中提出的问题数量进行限制(参数`max_depth`),或者我们可以创建决策树的集合版本，称为随机森林。

## 随机森林

决策树的主要弱点是训练数据的不同分割会导致非常不同的树。Bagging 是一个通用程序，用于减少机器学习方法的差异，但对决策树特别有用。

Bagging 是 Bootstrap aggregation 的简称，意思是引导样本的聚合。什么是引导样本？这是一个带有替换的随机样本:

```
# set a seed for reproducibility
np.random.seed(1)

# create an array of 1 through 20
nums = np.arange(1, 21)
print nums
[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]

# sample that array 20 times with replacement
np.random.choice(a=nums, size=20, replace=True)
[ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]
# This is our bootstrapped sample notice it has repeat variables!
```

那么，装袋是如何为决策树工作的呢？

1.  使用来自训练数据的引导样本生长 B 树。
2.  根据引导样本训练每棵树，并进行预测。
3.  组合预测:

    *   对回归树的预测进行平均
    *   对分类树进行投票

以下是一些需要注意的事情:

*   每个引导样本应该与原始训练集的大小相同
*   b 应该是一个足够大的值，使得误差似乎已经*稳定*
*   这些树有意长得很深，以便它们具有低偏差/高方差

我们有意将树长得更深的原因是因为 bagging 通过减少方差内在地提高了预测准确性，类似于交叉验证如何减少与估计样本外误差相关的方差。

随机森林是袋装树的变种。

然而，当构建每棵树时，每次我们考虑特征之间的分割时，从完整的`p`特征集中选择一个随机的 *m* 特征样本作为分割候选。分割只允许是那些`m`特征之一:

*   在每次分割时，为每棵树的*选择一个新的随机特征样本*
*   为了分类， *m* 通常被选择为`p`的平方根
*   对于回归，`m`通常被选择在`p/3`和`p`之间

有什么意义？

假设数据集中有一个非常强的特征。当使用决策树(或袋装)时，大多数树将使用该特征作为顶部分裂，从而产生彼此高度相关的相似树的集合。

如果我们的树彼此高度相关，那么平均这些量不会显著减少方差(这是集成的整个目标)。此外，通过从每次分割中随机删除候选特征，随机森林降低了最终模型的方差。

随机森林可用于分类和回归问题，也可轻松用于 scikit-learn。让我们试着根据球员的统计数据来预测 MLB 的薪水，如下所示:

```
# read in the data
url = '../data/hitters.csv'

hitters = pd.read_csv(url)

# remove rows with missing values
hitters.dropna(inplace=True)

# encode categorical variables as integers
hitters['League'] = pd.factorize(hitters.League)[0]
hitters['Division'] = pd.factorize(hitters.Division)[0]
hitters['NewLeague'] = pd.factorize(hitters.NewLeague)[0]

# define features: exclude career statistics (which start with "C") and the response (Salary)
feature_cols = [h for h in hitters.columns if h[0] != 'C' and h != 'Salary']

# define X and y
X = hitters[feature_cols]
y = hitters.Salary
```

让我们先尝试使用一个决策树来预测工资，如图所示:

```
from sklearn.tree import DecisionTreeRegressor

# list of values to try for max_depth
max_depth_range = range(1, 21)

# list to store the average RMSE for each value of max_depth
RMSE_scores = []

# use 10-fold cross-validation with each value of max_depth
from sklearn.cross_validation import cross_val_score
for depth in max_depth_range:
    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)
    MSE_scores = cross_val_score(treereg, X, y, cv=10, scoring='mean_squared_error')
    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))

# plot max_depth (x-axis) versus RMSE (y-axis)
plt.plot(max_depth_range, RMSE_scores)
plt.xlabel('max_depth')
plt.ylabel('RMSE (lower is better)')
```

![Random forests](graphics/B05260_12_15.jpg)

针对树的最大深度(复杂性)的决策树模型的 RMSE

让我们做同样的事情，但这一次是随机森林:

```
from sklearn.ensemble import RandomForestRegressor

# list of values to try for n_estimators
estimator_range = range(10, 310, 10)

# list to store the average RMSE for each value of n_estimators
RMSE_scores = []

# use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)
for estimator in estimator_range:
    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)
    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='mean_squared_error')
    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))

# plot n_estimators (x-axis) versus RMSE (y-axis)
plt.plot(estimator_range, RMSE_scores)
plt.xlabel('n_estimators')
plt.ylabel('RMSE (lower is better)')
```

![Random forests](graphics/B05260_12_16.jpg)

针对树的最大深度(复杂性)的随机森林模型的 RMSE

请注意 y 轴，我们的 RMSE 平均要低得多！了解我们如何使用随机森林来大幅提高预测能力。

在随机森林中，我们仍然有重要特征的概念，就像我们在决策树中一样:

```
# n_estimators=150 is sufficiently good
rfreg = RandomForestRegressor(n_estimators=150, random_state=1)
rfreg.fit(X, y)
# compute feature importances
pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort('importance', ascending = False)
```

![Random forests](graphics/B05260_012_f7.jpg)

所以，看起来球员在联盟的年数仍然是决定球员薪水的最重要因素。

## 比较随机森林和决策树

重要的是认识到仅仅使用随机森林并不能解决数据科学的问题。虽然随机森林有很多优点，但也有很多缺点，如下所示。

随机森林的优点如下:

*   其性能可与最好的监督学习方法相媲美
*   它提供了对特征重要性的更可靠的估计
*   它允许您在不使用训练/测试分割或交叉验证的情况下估计样本外误差

随机森林的缺点如下:

*   它的可解释性较差(无法可视化整个决策树森林)
*   训练和预测速度较慢(对于生产或实时目的来说不是很好

<title>Neural networks</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 神经网络

神经网络可能是谈论最多的机器学习模型之一，是为模拟动物神经系统而建立的计算网络。在深入结构之前，我们先来看看神经网络的巨大优势。

神经网络的关键组成部分是，它不仅是一个复杂的结构，它是一个复杂和灵活的结构。这意味着以下两件事:

*   神经网络能够估计任何函数形状(这称为非参数化)
*   神经网络可以适应并根据环境改变自己的内部结构

## 基本结构

神经网络由个相互连接的节点(**个感知器**)组成，每个接收输入(量化值)，并输出其他量化值。信号在网络中传播，最终到达一个预测节点。

![Basic structure](graphics/B05260_12_18.jpg)

神经网络互连节点的可视化

神经网络的另一个巨大优势是它们可以用于监督学习、非监督学习和强化学习问题。如此灵活、预测多种功能形状以及适应其周围环境的能力使得神经网络在选定领域中非常受欢迎，如下所示:

*   **模式识别**:这可能是神经网络最常见的应用。一些例子是手写识别和图像处理(面部识别)。
*   **实体运动**:这方面的例子包括自动驾驶汽车、机器动物和无人机运动。
*   **异常检测**:由于神经网络擅长识别模式，当数据点不符合模式时，它们也可以用于识别。想象一个神经网络监控股票价格的运动；在学习股票价格的一般模式的一段时间后，网络可以在运动中出现异常时提醒你。

神经网络最简单的形式是一个单感知器。感知器，形象化如下，接受一些输入并输出一个信号:

![Basic structure](graphics/B05260_12_19.jpg)

该信号通过将输入与几个权重组合而获得，然后通过某个*激活函数*进行处理。对于简单的二进制输出，我们通常使用逻辑函数，如下所示:

![Basic structure](graphics/B05260_012_f8.jpg)

为了创建一个神经网络，我们需要以网络的方式将多个感知器相互连接起来，如下图所示。

一个**多层感知器**(**)是一个有限无环图。这些节点是具有逻辑激活的神经元。**

**![Basic structure](graphics/B05260_12_21.jpg)**

**当我们训练模型时，我们更新模型的权重(最初是随机的),以便获得可能的最佳预测。如果一个观察通过模型，并且在它应该是真的时候被输出为假，那么单个感知器中的逻辑函数会稍微改变。这叫做 **反向传播**。神经网络通常是分批训练的，这意味着网络会被一次性给定若干训练数据点若干次，而每一次，反向传播算法都会触发网络内部的权重变化。**

**不难看出，我们可以将网络发展得非常深入，并拥有许多隐藏层，这与神经网络的复杂性有关。当我们将我们的神经网络发展得非常深入时，我们正在尝试深度学习的想法。深度神经网络(具有许多层的网络)的主要优势在于，它们可以逼近几乎任何形状函数，并且它们可以(理论上)为我们学习最佳的特征组合，并使用这些组合来获得最佳的预测能力。**

**让我们看看它的实际效果。我将使用一个叫做 PyBrain 的模块来制作我的神经网络。然而，首先，让我们来看一个新的数据集，这是一个手写数字的数据集。我们将首先尝试使用随机森林来识别数字，如下所示:**

```
from sklearn.cross_validation import cross_val_score
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
%matplotlib inline

digits = datasets.load_digits()

plt.imshow(digits.images[100], cmap=plt.cm.gray_r, interpolation='nearest')
# a 4 digit
```

**![Basic structure](graphics/B05260_12_22.jpg)**

```
X, y = digits.data, digits.target

# 64 pixels per image
X[0].shape

# Try Random Forest
rfclf = RandomForestClassifier(n_estimators=100, random_state=1)
cross_val_score(rfclf, X, y, cv=5, scoring='accuracy').mean()
0.9382782
```

**相当不错！94%的准确率没什么可笑的，但是我们能做得更好吗？**

### **Tip**

**警告！PyBrain 的语法可能有点复杂。**

```
from pybrain.datasets            import ClassificationDataSet
from pybrain.utilities           import percentError
from pybrain.tools.shortcuts     import buildNetwork
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.structure.modules   import SoftmaxLayer
from numpy import ravel

# pybrain has its own data sample class that we must add
# our training and test set to
ds = ClassificationDataSet(64, 1 , nb_classes=10)
for k in xrange(len(X)): 
    ds.addSample(ravel(X[k]),y[k])

# their equivalent of train test split
test_data, training_data = ds.splitWithProportion( 0.25 )

# pybrain's version of dummy variables

test_data._convertToOneOfMany( )
training_data._convertToOneOfMany( )

print test_data.indim # number of pixels going in
# 64
print test_data.outdim # number of possible options (10 digits)
# 10

# instantiate the model with 64 hidden layers (standard params)
fnn = buildNetwork( training_data.indim, 64, training_data.outdim, outclass=SoftmaxLayer )
trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0.1, learningrate=0.01 , verbose=True, weightdecay=0.01) 

# change the number of epochs to try to get better results!
trainer.trainEpochs (10) # 10 batches
print 'Percent Error on Test dataset: ' , \
        percentError( trainer.testOnClassData (
           dataset=test_data )
           , test_data['class'] )
```

**该模型将在测试集上输出一个最终误差:**

```
Percent Error on Test dataset: 4.67706013363
accuracy = 1 - .0467706013363
accuracy
0.95322
```

**已经好多了！随机森林和神经网络在这个问题上都做得很好，因为它们都是非参数的，这意味着它们不依赖数据的基本形状来进行预测。他们能够估计任何形状的函数。**

**为了预测形状，我们可以使用下面的代码:**

```
plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')

fnn.activate(X[0])
array([ 0.92183643,  0.00126609,  0.00303146,  0.00387049,  0.01067609,
        0.00718017,  0.00825521,  0.00917995,  0.00696929,  0.02773482])
```

**![Basic structure](graphics/B05260_12_23.jpg)**

**该数组表示每个数字的概率，这意味着前面屏幕截图中的数字有 92%的可能性是 0(事实就是如此)。注意下一个最高的概率是 9，这是有意义的，因为 9 和 0 有相似的形状(卵形)。**

**神经网络确实有一个重大缺陷。如果不去管它，它们会有很高的方差。要了解这一点，让我们运行与上一个完全相同的代码，并根据完全相同的数据训练完全相同类型的神经网络，如下所示:**

```
# Do it again and see the difference in error
fnn = buildNetwork( training_data.indim, 64, training_data.outdim, outclass=SoftmaxLayer )
trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0.1, learningrate=0.01 , verbose=True, weightdecay=0.01) 

# change the number of eopchs to try to get better results!
trainer.trainEpochs (10)
print 'Percent Error on Test dataset: ' , \
        percentError( trainer.testOnClassData (
           dataset=test_data )
           , test_data['class'] )

accuracy = 1 - .0645879732739
accuracy
0.93541
```

**看看仅仅重新运行模型和实例化不同的权重是如何使网络变得与以前不同的？这是高方差模型的一个症状。此外，神经网络通常需要许多训练样本来对抗模型的高*可变性*，并且还需要大量的计算能力来在生产环境中良好工作。**

**<title>Summary</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 总结

我们对数据科学原理的漫长探索到此结束。在过去的 300 多页中，我们研究了概率、统计和机器学习中的不同技术，以回答最困难的问题。我想亲自祝贺你通过这本书。我希望它被证明是有用的，并激励你学习更多！

这不是我需要知道的一切吗？

没有。我只能在一本《原则》和《T1》级别的书中写这么多。还有很多东西要学。

从哪里可以了解更多信息？

我建议去寻找开源数据挑战([https://www.kaggle.com/](https://www.kaggle.com/)是一个很好的来源)。我也建议寻找，尝试和解决你自己的问题！

我什么时候可以称自己为数据科学家？

当你开始从公司和个人都可以使用的大大小小的数据集中培养可操作的洞察力时，你就有资格称自己为真正的数据科学家。**