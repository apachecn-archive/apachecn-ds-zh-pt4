

# 七、基本统计

本章将关注任何有抱负的数据科学家所需的统计数据。

我们将探索不受偏见影响的采样和获取数据的方法，然后使用统计测量来量化和可视化我们的数据。使用 z 分数和经验法则，我们将了解如何标准化数据，以达到图形化和可解释性的目的。

在本章中，我们将探讨以下主题:

*   如何获取数据并进行采样
*   中心、方差和相对位置的度量
*   使用 z 得分对数据进行归一化
*   经验法则

# 什么是统计？

这个问题可能看起来很奇怪，但是我经常惊讶于许多人不能回答这个简单而有力的问题:什么是统计？统计数字是你经常在新闻和报纸上看到的数字。当试图证明一个观点或试图吓唬你时，统计数据是有用的，但它们是什么呢？

要回答这个问题，我们需要花一分钟的时间来讨论一下为什么我们首先要测量它们。这个领域的目标是试图解释和模拟我们周围的世界。要做到这一点，我们必须看看人口。

我们可以将 **群体**定义为一个实验或模型的全部主体。

本质上，你的人口是你关心的人。你想说谁？如果你试图测试吸烟是否会导致心脏病，你的人口将会是全世界的吸烟者。如果你试图研究青少年饮酒问题，你的人群应该都是青少年。

现在，假设您想问一个关于您的人口的问题，例如，如果您的人口是您的所有员工(假设您有 1，000 多名员工)，您可能想知道他们中使用非法药物的百分比。这个问题叫做 **参数**。

我们可以将参数定义为描述人口特征的数值测量。

比如你问所有 1000 名员工，有 100 人在吸毒，吸毒率是 10%。这里的参数是 10%。

然而，让我们现实一点，你不可能问每一个员工他们是否在使用毒品。如果你有超过 10，000 名员工呢？很难找到每个人来得到你的答案。出现这种情况，就不可能算出这个参数。在这种情况下，我们可以*估计*这个参数。

首先，我们将采取一个 **样本人群的**。

我们可以将总体样本定义为总体的子集(不要求随机)。

因此，我们可能会询问 1000 名员工中的 200 人。这 200 人中，假设有 26 人吸毒，使得吸毒率为 13%。在这里，13%不是一个参数，因为我们没有机会问每个人。这个 13%是一个参数的估计值。你知道那叫什么吗？

没错，一个**统计**！

我们可以将统计定义为描述总体样本特征的数值度量。

统计只是对参数的估计。这是一个试图通过描述人口的子集来描述整个人口的数字。这是必要的，因为你永远不可能指望对世界上每一个青少年或每一个吸烟者进行调查。这就是统计学领域所要做的——获取人口样本并对这些样本进行测试。

所以，下一次你得到一个统计数据的时候，请记住，这个数字仅仅代表了人口中的一个样本，而不是整个受试者群体。



# 我们如何获取数据并进行采样？

如果统计学是关于获取总体样本的，那么知道我们如何获得这些样本一定是非常重要的，你是对的。让我们只关注获取和采样数据的许多方法中的几种。

## 获取数据

为我们的分析收集数据有两种主要方式:**观察**和**实验**。当然，这两种方式各有利弊。它们各自产生不同类型的行为，因此需要不同类型的分析。

### 观察

我们可以通过*观察*手段获得数据，这包括测量具体特征，但不试图修改被研究的对象。例如，你的网站上有一个跟踪软件，可以观察用户在网站上的行为，如在特定页面上花费的时间长度和广告的点击率，同时不影响用户的体验，那么这将是一个观察研究。

这是获取数据最常见的方法之一，因为它非常简单。你要做的就是观察和收集数据。观察性研究也受到你可能收集的数据类型的限制。这是因为观察者(你)无法控制环境。你只能观察和收集自然行为。如果你想诱导某种类型的行为，观察性研究是没有用的。

### 实验性的

一个**实验**包括一种治疗和对受试者效果的观察。实验中的对象称为实验单元。这通常是大多数科学实验室收集数据的方式。他们将人们分成两组或更多组(通常只有两组)，称他们为控制组和实验组。

对照组暴露在一定的环境中，然后进行观察。然后将实验组暴露在不同的环境中，然后进行观察。然后，实验者汇总两组的数据，并决定哪种环境更有利(有利是实验者要决定的品质)。

在一个营销示例中，假设我们将一半的用户暴露在具有特定图像和特定风格的特定登录页面(网站 A)中，我们测量他们是否注册了该服务。然后，我们让另一半接触不同的登陆页面、不同的图片和不同的风格(网站 B ),并再次衡量他们是否注册。然后，我们可以决定这两个网站中哪个表现更好，应该继续使用。具体来说，这被称为 **A/B 测试**。我们来看一个 Python 中的例子！让我们假设我们运行前面的测试，并以列表的形式获得以下结果:

```
results = [ ['A', 1], ['B', 1], ['A', 0], ['A', 0] … ]
```

这里，列表结果中的每个对象代表一个主题(人)。每个人都有以下两个属性:

*   他们接触了哪个网站，用一个字符表示
*   它们是否转换(0 表示否，1 表示是)

然后我们可以汇总并得出以下结果表:

```
users_exposed_to_A = []
users_exposed_to_B = []
# create two lists to hold the results of each individual website
```

一旦我们创建了这两个最终将保存每个转换布尔值(0 或 1)的列表，我们将迭代所有测试结果，并将它们添加到适当的列表中，如下所示:

```
for website, converted in results: # iterate through the results
  # will look something like website == 'A' and converted == 0
  if website == 'A':
    users_exposed_to_A.append(converted)
  elif website == 'B':
    users_exposed_to_B.append(converted)
```

现在，每个列表包含一系列 1 和 0。

### 注意

请记住，1 代表用户在看到网页后实际转换到该网站，0 代表用户在注册/转换前看到该页面并离开。

要获得访问网站`A`的总人数，我们可以使用 Python 中的`len()`功能，如图所示:

```
len(users_exposed_to_A) == 188 #number of people exposed to website A
len(users_exposed_to_B) == 158 #number of people exposed to website B
```

要统计皈依的人数，我们可以使用列表中的`sum()`，如图所示:

```
sum(users_exposed_to_A) == 54 # people converted from website A
sum(users_exposed_to_B) == 48 # people converted from website B
```

如果我们减去列表的长度和列表的总和，我们会得到每个站点中没有*转换的人数，如图所示:*

```
len(users_exposed_to_A) - sum(users_exposed_to_A) == 134 # did not convert from website A

len(users_exposed_to_B) - sum(users_exposed_to_B) == 110 # did not 
convert from website B
```

我们可以在下表中汇总和总结我们的结果，该表代表了我们的网站转换测试实验:

|   | 

没有报名

 | 

参加报名

 |
| --- | --- | --- |
| **网站 A** | One hundred and thirty-four | Fifty-four |
| **网站 B** | One hundred and ten | Forty-eight |

我们可以很快收集到一些描述性的统计数据。我们可以说这两个网站的网站转换率如下:

*   网站 A 的转换:![Experimental](graphics/B05260_07_f1.jpg)
*   网站 B 的转换:![Experimental](graphics/B05260_07_f2.jpg)

差别不大，但还是不同。即使 B 的转化率更高，我们真的可以说 B 版本的转化率更高吗？还没有。为了测试这种结果的统计显著性，应该使用假设检验。这些测试将在下一章深入讨论，我们将再次讨论这个完全相同的例子，并使用适当的统计测试来完成它。



# 采样数据

还记得统计是如何测量人口样本的结果吗？嗯，我们应该谈谈两种非常常见的方法来决定谁在我们测量的样本中获得荣誉。我们将讨论抽样的主要类型，称为随机抽样，这是决定样本大小和样本成员的最常见的方法。

## 概率抽样

概率抽样是从人群中抽样的一种方式，其中每个人被选中的概率是已知的，但是这个数字*可能与另一个用户的概率不同。最简单(可能也是最常见)的概率抽样方法是随机抽样。*

## 随机抽样

假设我们正在运行 A/B 测试，我们需要确定谁将在组 A 中，谁将在组 B 中。您的数据团队提供了以下三条建议:

*   **根据位置分离用户**:西海岸的用户被放在 A 组，东海岸的用户被放在 B 组
*   **根据一天中访问站点的时间来区分用户**:在晚上 7 点到凌晨 4 点之间访问的用户进入站点 A，而其余的用户被放在 B 组
*   **让它完全随机**:每个新用户都有 50%的机会被放入任何一组

前两个是选择样本的有效选项，并且实现起来相当简单，但是它们都有一个基本缺陷:它们都有引入采样偏差的风险。

当获取样本的方式系统性地偏向于某个结果而不是目标结果时，就会出现**采样偏差** 。

不难看出为什么选择选项 1 或选项 2 可能会引入偏见。如果我们根据他们居住的地方或登录的时间来选择我们的小组，我们就错误地启动了我们的实验，现在，我们对结果的控制要少得多。

具体来说，我们面临着将*混杂因素*引入我们的分析的风险，这是个坏消息。

一个**混杂因素** 是一个我们不直接测量的变量，但是连接了被测量的变量。

基本上，混杂因素就像我们分析中的缺失元素，它是看不见的，但会影响我们的结果。

在这种情况下，选项 1 没有考虑到*地理口味*的潜在混杂因素。例如，如果网站 A 对西海岸的用户没有吸引力，那么它会极大地影响你的搜索结果。

同样，备选方案 2 可能会引入一个时间(基于时间的)混杂因素。如果网站 B 在夜间环境下浏览效果更好(这是为 A 保留的)，而用户仅仅因为现在是什么时间而对这种风格不感兴趣，那该怎么办？这两个因素都是我们想要避免的，所以，我们应该选择选项 3，这是一个随机样本。

### 注意

虽然抽样偏倚会导致混杂，但它与混杂是不同的概念。选项 1 和 2 都是抽样偏差，因为我们错误地选择了样本，而且也是混杂因素的例子，因为在每种情况下都有第三个变量影响我们的决定。

选择随机的样本，使得群体中的每一个成员都有同等的机会被选为任何其他成员。

这可能是决定谁将成为您的样本的一部分的最简单和最方便的方法之一。每个人都有完全相同的机会属于任何特定的群体。随机抽样是减少混杂因素影响的有效方法。

## 不等概率抽样

回想一下，我之前说过一个概率抽样对于不同的潜在样本成员可能有不同的概率。但是如果这实际上引入了问题呢？假设我们对衡量员工的幸福水平感兴趣。我们已经知道，我们不能问每一个员工，因为这将是愚蠢和疲惫的。所以，我们需要取样。我们的数据团队建议随机抽样，一开始每个人都击掌，因为他们觉得自己很聪明，很有统计学意义。但接着有人问了一个看似无害的问题——有人知道在这里工作的男女比例吗？

击掌声停止了，房间变得寂静无声。

这个问题极其重要，因为性别很可能是一个混杂因素。团队对此进行了调查，发现公司中男女比例为 75%和 25%。

这意味着，如果我们引入一个随机样本，我们的样本将可能有类似的分裂，因此，有利于男性而不是女性的结果。为了解决这个问题，我们可以在我们的调查中包括更多的女性而不是男性，以使我们的样本分割对男性不利。

乍一看，在我们的随机抽样中引入偏好系统似乎是一个坏主意，然而，缓解不平等的抽样，并因此努力消除性别、种族、残疾等方面的系统性偏见更加相关。一个简单的随机样本，每个人都有和其他人一样的机会，很有可能淹没少数群体成员的声音和意见。因此，在你的取样技术中引入这样一个有利的系统是可以的。



# 我们如何衡量统计数据？

一旦我们有了样本，是时候量化我们的结果了。假设我们希望概括员工的快乐程度，或者我们想弄清楚公司的工资是否因人而异。

这些是衡量我们结果的一些常用方法。

## 中心的度量

中心的度量是我们如何定义数据集的中间或中心的。我们这样做是因为有时我们希望对数据值进行归纳。例如，也许我们很好奇西雅图的平均降雨量是多少，或者欧洲男性的平均身高是多少。这是一种概括大量数据的方式，以便更容易传达给某人。

中心的度量是数据集“中间”的值。

然而，这对不同的人有不同的意义。谁能说数据集的中间在哪里？定义数据中心有许多不同的方式。下面我们来看几个。

数据集的**算术平均值** 是通过将所有值相加，然后除以数据值的数量得到的。

这可能是定义数据中心最常见的方法，但可能有缺陷！假设我们希望找到下列数字的平均值:

```
import numpy as np

np.mean([11, 15, 17, 14]) == 14.25
```

很简单，我们的平均值是`14.25`，我们所有的值都非常接近它。但是如果我们引入一个新的值:`31`呢？

```
np.mean([11, 15, 17, 14, 31]) == 17.6
```

这极大地影响了平均值，因为算术平均值对异常值很敏感。新值`31`几乎是其余数字的两倍，因此，*扭曲了*平均值。

另一个有时更好的衡量中心的方法是中位数。

**中位数** 是数据集按顺序排序时位于中间的数字，如下图所示:

```
np.median([11, 15, 17, 14]) == 14.5
np.median([11, 15, 17, 14, 31]) == 15
```

注意使用中值的`31`的引入并没有对数据集的中值产生很大影响。这是因为中位数对异常值不太敏感。

当使用有许多异常值的数据集时，有时使用数据集的中间值更有用，而如果您的数据没有许多异常值，并且数据点彼此非常接近，那么平均值可能是更好的选择。

但是我们如何判断数据是否是分散的呢？嗯，我们将不得不引入一种新的统计方法。

## 变化的度量

中心测量值用于量化数据的中间值，但现在我们将探索测量我们收集的数据如何“扩散”的方法。这是识别我们的数据中是否潜伏着许多异常值的有用方法。先说个例子。

假设我们随机抽取了 24 位脸书的朋友，写下他们在脸书有多少朋友。列表如下:

```
friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125, 455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]

np.mean(friends) == 789.1
```

这份榜单的平均值刚刚超过 789。因此，我们可以说，根据这个样本，平均每个脸书朋友有 789 个朋友。但是只有 89 个朋友的人或者有超过 1600 个朋友的人呢？事实上，这些数字中没有多少是真正接近 789 的。

那么，我们使用中间值怎么样，如图所示，因为中间值通常不会受到异常值的影响:

```
np.median(friends) == 769.5
```

中位数是`769.5`，相当接近均值。嗯，想法不错，但是仍然没有真正解释这些数据点之间的巨大差异。这就是统计学家所说的测量数据的变化。让我们先介绍一下最基本的变化度量:范围。该范围就是最大值减去最小值，如图所示:

```
np.max(friends) – np.min(friends) == 1684
```

这个范围告诉我们两个最极端的值有多远。通常情况下，这个范围并没有被广泛使用，但它确实有其应用价值。有时我们希望知道异常值的分布有多广。这在科学测量或安全测量中非常有用。

假设一家汽车公司想要测量安全气囊展开需要多长时间。知道那个时间的平均值是很好的，但是他们也很想知道最慢的时间和最快的时间有多远。这可能是生与死的区别。

回到脸书的例子，1，684 是我们的范围，但我不太确定这是否说明了我们的数据。现在，让我们来看看最常用的变异度量，即 **标准差**。

我相信你们很多人都听过这个词，它甚至可能会引起一定程度的恐惧，但它到底是什么意思呢？本质上，当我们处理总体样本时，用 *s* 表示的标准偏差测量数据值偏离算术平均值的程度。

这基本上是一种查看数据分布情况的方法。有一个计算标准差的通用公式，如下所示:

![Measures of variation](graphics/B05260_07_f3.jpg)

这里:

*   s 是我们的样本标准偏差
*   ![Measures of variation](graphics/B05260_07_f4.jpg)是每个单独的数据点。
*   ![Measures of variation](graphics/1.jpg)是数据的平均值
*   ![Measures of variation](graphics/B05260_07_f5.jpg)是数据点的数量

在你抓狂之前，让我们来分析一下。对于样本中的每个值，我们将取该值，从中减去算术平均值，求差的平方，并且，一旦我们以这种方式将每个点加起来，我们将整体除以样本中的点数 *n* 。最后，我们对一切求平方根。

不深入分析公式，这么想:基本上是从距离公式推导出来的。本质上，标准差计算的是数据值与算术平均值的平均距离。

如果你仔细看看这个公式，你会发现它实际上是有道理的:

*   通过取![Measures of variation](graphics/B05260_07_f6.jpg)，你找到了样本值和平均值之间的字面差异。
*   通过对结果![Measures of variation](graphics/B05260_07_f7.jpg)求平方，我们对异常值施加了更大的惩罚，因为对大误差求平方只会使它变得更大。
*   通过除以样本中的项目数，我们得到(字面上)每个点和平均值之间的平均平方距离。
*   通过计算答案的平方根，我们用我们能理解的术语来表达这个数字。例如，通过朋友数量的平方减去平均值，我们将单位改为朋友平方，这没有意义。开平方使我们的单位回到“朋友”的状态。

让我们回到脸书的例子中来看一看，并进一步解释这一点。让我们开始计算标准差。所以，我们开始计算其中的几个。回想一下，数据的算术平均值大约是 789，因此，我们将使用 789 作为平均值。

我们首先取每个数据值与平均值之间的差值，对其求平方，将它们相加，再除以比数值少 1 的数，然后求平方根。这将如下所示:

![Measures of variation](graphics/B05260_07_f8.jpg)

另一方面，我们可以采用 Python 方法，以编程方式完成所有这些工作(这通常是首选)。

```
np.std(friends) # == 425.2
```

425 这个数字代表的是数据的传播。你可以说 425 是数据值与平均值的一种平均距离。简而言之，这意味着这些数据非常分散。

所以，我们的标准差大约是 425。这意味着这些人在脸书的朋友数量似乎不是一个数字，这一点在我们将数据绘制成条形图并绘制均值和标准差的可视化图形时非常明显。在下面的图中，每个人将由条形图中的单个条表示，条的高度表示每个人拥有的朋友数量:

```
import matplotlib.pyplot as plt
%matplotlib inline
y_pos = range(len(friends))

plt.bar(y_pos, friends)
plt.plot((0, 25), (789, 789), 'b-')
plt.plot((0, 25), (789+425, 789+425), 'g-')
plt.plot((0, 25), (789-425, 789-425), 'r-')
```

![Measures of variation](graphics/B05260_07_01.jpg)

中心的蓝线绘制在平均值(789)处，底部的红线绘制在平均值减去标准差(789-425 = 364)处，最后，顶部的绿线绘制在平均值加上标准差(789+425 = 1214)处。

请注意大多数数据如何位于绿线和红线之间，而离群值位于红线之外。也就是说，有三个人的好友数量低于红线，有三个人的好友数量高于绿线。

值得一提的是，标准偏差的单位实际上与数据的单位相同。因此，在这个例子中，我们会说，标准差是脸书上的 425 个朋友。

### 注意

变化的另一个衡量标准是方差，如前一章所述。方差就是标准差的平方。

所以，现在我们知道标准差和方差有利于检查我们的数据有多分散，并且我们可以使用它和平均值来创建一个我们的大量数据所在的范围。但是，如果我们想要比较两个不同数据集的分布，甚至可能使用完全不同的单位呢？这就是变异系数发挥作用的地方。

### 定义

**变异系数** 定义为数据的标准差与其均值的比值。

这个比率(顺便说一句，只有当我们在比率水平的测量中工作时才有帮助，在这种情况下，除法是允许的并且是有意义的)是一种标准化标准差的方法，这使得跨数据集的比较更容易。当试图比较平均值时，我们经常使用这种方法，它在不同规模的人群中传播。

### 示例-员工工资

如果我们看看同一家公司不同部门的员工工资的平均值和标准偏差，我们会发现，乍一看，可能很难比较差异。

![Example – employee salaries](graphics/B05260_07_f9.jpg)

当一个部门的平均工资为 25，000 美元，而另一个部门的平均工资在六位数范围内时，情况尤其如此。

然而，如果我们看最后一栏，也就是我们的变异系数，就可以清楚地看到，行政部门的人可能得到更多的报酬，但行政部门的员工得到的薪水却相差很大。这可能是因为首席执行官的收入远远高于办公室经理，后者仍在行政部门，这使得数据非常分散。

另一方面，收发室里的每个人，虽然没有赚那么多钱，但是和收发室里的其他人赚的差不多，这就是为什么他们的变异系数只有 8%。

有了变异的度量，我们就可以开始回答一些大问题，比如这些数据是如何分布的，或者我们如何得出大部分数据所处的一个好的范围。

## 相对地位的衡量

我们可以结合中心和变异的度量来创建相对排名的度量。

**变化测量** 测量特定数据值相对于整个数据集的位置。

让我们从学习统计学中一个非常重要的值开始，z 值。

**z 值** 是一种告诉我们单个数据值离平均值有多远的方式。

一个 *x* 数据值的 z 值如下:

![Measures of relative standing](graphics/B05260_07_f10.jpg)

其中:

*   ![Measures of relative standing](graphics/B05260_07_f4.jpg)是数据点
*   ![Measures of relative standing](graphics/1.jpg)是指
*   *s* 为标准差。

请记住，标准差是(某种程度上)数据与平均值的平均距离，现在，z 得分是每个特定数据点的个性化值。我们可以通过从平均值中减去某个数据值，然后除以标准差来计算该数据值的 z 得分。输出将是一个值与平均值的标准化距离。我们在所有的统计中使用 z 分数。这是一种非常有效的方法，可以将存在于不同尺度上的数据标准化，还可以将数据置于其含义的上下文中。

让我们来看看我们之前在脸书上的好友数量的数据，并将这些数据标准化为 z 分数。对于每个数据点，我们将通过应用前面的公式找到它的 z 得分。我们将选取每个人，从值中减去平均朋友数，然后除以标准差，如下所示:

```
z_scores = []

m = np.mean(friends)  # average friends on Facebook
s = np.std(friends)   # standard deviation friends on Facebook

for friend in friends:
    z = (friend - m)/s  # z-score
    z_scores.append(z)  # make a list of the scores for plotting
```

现在，让我们在条形图上绘制这些 z 分数。下面的图表显示了我们之前在脸书上使用朋友的示例中的相同个人，但是，现在每个条形图显示的不是朋友的原始数量，而是他们在脸书上拥有的朋友数量的 z 得分。如果我们绘制 z 分数的图表，我们会注意到一些事情:

```
plt.bar(y_pos, z_scores)
```

![Measures of relative standing](graphics/B05260_07_03.jpg)

*   我们有负值(意味着数据点低于平均值)
*   条形的长度不再代表朋友的原始数量，而是朋友数量与平均值的差异程度

这张图表让我们很容易挑出那些平均朋友水平低得多和高得多的人。例如，指数为 0 的人平均有较少的朋友(他们有 109 个朋友，而平均值是 789 个)。

如果我们想要绘制标准差的图表呢？回想一下，我们之前画了三条水平线:一条在均值处，一条在均值加标准差处(![Measures of relative standing](graphics/B05260_07_f11.jpg))，一条在均值减标准差处(![Measures of relative standing](graphics/B05260_07_f12.jpg))。

如果我们将这些值代入 z 分数的公式，我们将得到:

(![Measures of relative standing](graphics/2.jpg) ) = ![Measures of relative standing](graphics/B05260_07_f13.jpg)的 z 值

(![Measures of relative standing](graphics/B05260_07_f11.jpg) ) = ![Measures of relative standing](graphics/B05260_07_f14.jpg)的 z 值

(![Measures of relative standing](graphics/B05260_07_f12.jpg) ) ![Measures of relative standing](graphics/B05260_07_f15.jpg)的 z 值

这不是巧合！当我们使用 z 分数对数据进行标准化时，我们的标准差就成为了选择的度量标准。让我们看看标有标准偏差的新图表:

```
plt.bar(y_pos, z_scores)
plt.plot((0, 25), (1, 1), 'g-')
plt.plot((0, 25), (0, 0), 'b-')
plt.plot((0, 25), (-1, -1), 'r-')
```

前面的代码在下面的中添加了三行:

*   在 *y = 0* 处的蓝线代表远离平均值的零标准偏差(在 *x* 轴上)
*   代表平均值以上一个标准差的绿线
*   代表平均值以下一个标准偏差的红线

![Measures of relative standing](graphics/B05260_07_04.jpg)

线条的颜色与之前的原始朋友计数图中绘制的线条相匹配。如果你仔细观察，同样的人仍然落在绿线和红线之外。即同样的三个人仍然落在红色(下)线以下，同样的三个人落在绿色(上)线以上。

在这种标度下，我们也可以使用如下语句:

*   该数据点距离平均值超过一个标准偏差:![Measures of relative standing](graphics/B05260_07_05.jpg)
*   这个人有一个朋友计数在均值的一个标准差之内:![Measures of relative standing](graphics/B05260_07_06.jpg)

z 分数是*标准化*数据的有效方法。这意味着我们可以把整套放在同一个天平上。例如，如果我们还测量每个人的总体幸福度(在 0 到 1 之间)，我们可能会有一个类似于以下数据集的数据集:

```
friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125, 455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]

happiness = [.8, .6, .3, .6, .6, .4, .8, .5, .4, .3, .3, .6, .2, .8, 1, .6, .2, .7, .5, .3, .1, 0, .3, 1]

import pandas as pd

df = pd.DataFrame({'friends':friends, 'happiness':happiness})
df.head()
```

![Measures of relative standing](graphics/B05260_07_f16.jpg)

这些数据点在两个不同的维度上，每个维度具有非常不同的尺度。当我们的幸福指数停留在 0 到 1 之间时，朋友的数量可以达到数千。

为了补救这一点(对于一些统计/机器学习建模，这一概念将变得必不可少)，我们可以使用`scikit-learn`中预先构建的标准化包来简单地标准化数据集，如下所示:

```
from sklearn import preprocessing

df_scaled = pd.DataFrame(preprocessing.scale(df), columns = ['friends_scaled', 'happiness_scaled'])

df_scaled.head()
```

这段代码将同时缩放 friends 和 happy 列，从而显示每一列的 z 值。值得注意的是，通过这样做，`sklearn`中的预处理模块为每一列分别做以下事情:

*   求列的平均值
*   寻找列的标准偏差
*   对列中的每个元素应用 z 得分函数

结果是两个柱，如图所示，以相同的比例存在，即使它们以前不存在:

![Measures of relative standing](graphics/B05260_07_f17.jpg)

现在，我们可以用同样的比例绘制朋友和快乐，这个图表至少是可读的。

```
df_scaled.plot(kind='scatter', x = 'friends_scaled', y = 'happiness_scaled')
```

![Measures of relative standing](graphics/B05260_07_09.jpg)

现在我们的数据已经标准化为 z 分数，这个散点图很容易解释！在后面的章节中，这种标准化的思想不仅会使我们的数据更容易理解，而且在我们的模型优化中也是必不可少的。许多机器学习算法将要求我们有标准化的列，因为它们依赖于规模的概念。

### 洞察力部分——数据的相关性

在本书中，我们将讨论拥有数据和对数据有可操作的见解之间的区别。拥有数据只是成功的数据科学运营的一个步骤。能够获取、清理和绘制数据有助于讲述数据必须提供但不能揭示道德的故事。为了让这个例子更进一步，我们将看看在脸书有朋友和幸福之间的关系。

在后续章节中，我们将研究一种特定的机器学习算法，该算法试图找到定量特征之间的关系，称为线性回归，但我们不必等到那时才开始形成假设。我们有一个人的样本，一个衡量他们的在线社交存在和他们报告的快乐的标准。今天的问题是——我们能在脸书上找到朋友数量和总体幸福感之间的关系吗？

显然，这是一个大问题，应该受到尊重。回答这个问题的实验应该在实验室环境中进行，但是我们可以开始形成一个关于这个问题的假设。鉴于我们数据的性质，对于一个假设，我们实际上只有以下三种选择:

*   网上朋友的数量和幸福感之间存在正相关关系(一个上升，另一个也上升)
*   他们之间有一种负面的联系(随着朋友数量的增加，你的幸福感下降)
*   变量之间没有关联(当一个变量发生变化时，另一个变量实际上变化并不大)

关于这个问题，我们可以用基本统计学来形成一个假设吗？我说我们可以！但首先，我们必须引入一个概念，叫做相关性。

**相关系数** 是描述两个变量之间关联/关系强度的定量度量。

两组数据之间的相关性告诉我们它们是如何一起移动的。改变一个能帮助我们预测另一个吗？这个概念不仅在这种情况下很有趣，而且是许多机器学习模型对数据做出的核心假设之一。对于许多预测算法来说，它们依赖于这样一个事实，即我们所观察的变量之间存在某种关系。然后，学习算法利用这种关系来做出准确的预测。

关于相关系数，需要注意以下几点:

*   它将介于-1 和 1 之间
*   绝对值越大(越接近-1 或 1)，变量之间的相关性越强:

    *   相关性最强的是-1 或 1
    *   相关性最弱的是 0

*   正相关意味着当一个变量增加时，另一个变量也会增加
*   负相关意味着当一个变量增加时，另一个变量会减少

我们可以使用 Pandas 快速显示数据框中每个要素与其他要素之间的相关系数，如图所示:

```
# correlation between variables
df.corr()
```

![The insightful part – correlations in data](graphics/B05260_07_f18.jpg)

这张表显示了朋友和幸福之间的关系。注意前两件事，如下所示:

*   矩阵的对角线用正 1 填充。这是因为它们代表了变量和自身之间的相关性，当然，这形成了一条完美的线，使得相关性完全为正！
*   矩阵是对角线对称的。这对熊猫做的任何相关矩阵都是成立的。

相信相关系数有几个注意事项。一个是，一般来说，相关性将试图测量变量之间的线性关系。这意味着，如果这一衡量标准没有揭示出可见的相关性，这并不意味着变量之间没有关系，只是不存在容易通过这些线的最佳拟合线。可能有一个*非线性*关系来定义这两个变量。

重要的是要认识到因果关系并不隐含在相关性中。仅仅因为这两个变量之间存在微弱的负相关，并不一定意味着你的总体幸福感会随着你在脸书上的朋友数量增加而下降。这种因果关系必须进一步检验，在后面的章节中，我们将试图这样做。

总而言之，我们可以使用相关性来对变量之间的关系进行假设，但我们需要使用更复杂的统计方法和机器学习算法来巩固这些假设和假设。



# 经验法则

回想一下，正态分布被定义为具有类似钟形曲线的特定概率分布。在统计学中，我们喜欢我们的数据表现正常*。例如，如果我们有类似正态分布的数据，如下所示:*

*![The Empirical rule](graphics/B05260_07_10.jpg)*

***经验法则**指出，我们可以预期一定数量的数据存在于标准偏差集合之间。具体来说，经验规则针对正态分布的数据说明:*

*   *大约 68%的数据在 **1** 标准偏差范围内*
*   *大约 95%的数据在 **2** 标准偏差范围内*
*   *大约 99.7%的数据在 **3** 标准偏差范围内*

*例如，让我们看看我们的脸书朋友的数据是否支持这一点。让我们使用我们的数据框架来找出落在平均值的 1、2 和 3 个标准差范围内的人的百分比，如下所示:*

```
*# finding the percentage of people within one standard deviation of the mean
within_1_std = df_scaled[(df_scaled['friends_scaled'] <= 1) & (df_scaled['friends_scaled'] >= -1)].shape[0]
within_1_std / float(df_scaled.shape[0])
# 0.75

# finding the percentage of people within two standard deviations of the mean
within_2_std = df_scaled[(df_scaled['friends_scaled'] <= 2) & (df_scaled['friends_scaled'] >= -2)].shape[0]
within_2_std / float(df_scaled.shape[0])
# 0.916

# finding the percentage of people within three standard deviations of the mean
within_3_std = df_scaled[(df_scaled['friends_scaled'] <= 3) & (df_scaled['friends_scaled'] >= -3)].shape[0]
within_3_std / float(df_scaled.shape[0])
# 1.0*
```

*我们可以看到，我们的数据似乎确实遵循了经验法则。大约 75%的人在平均值的单个标准差范围内。92%左右的人在两个标准差以内，全部在三个标准差以内。*

***示例–考试成绩***

*假设我们正在测量一次考试的分数，分数通常呈钟形正态分布。考试的平均成绩是 84%，标准偏差是 6%。我们可以大致肯定地说:*

*   *班上大约 68%的人得分在 78%到 90%之间，因为 78 比 84 低 6 个单位，90 比 84 高 6 个单位*
*   *如果我们被问及班级中有多少百分比的学生得分在 72 和 96%之间，我们会注意到 72 比平均值低 2 个标准差，96 比平均值高 2 个标准差，因此，经验法则告诉我们，大约 95%的班级得分在该范围内。*

*然而，并不是所有的数据都是正态分布的，所以，我们不能总是使用经验法则。我们还有另一个定理，可以帮助我们分析任何一种分布。在下一章，我们将深入讨论什么时候我们可以假设正态分布。这是因为许多统计测试和假设要求基础数据来自正态分布的人群。*

### *注意*

*以前，当我们将数据标准化为 z 分数时，我们不需要正态分布假设。*

*

# 总结

在这一章中，我们讨论了大多数数据科学家需要的基本统计数据。涵盖了从如何获取/采样数据到如何根据 z 分数标准化数据以及经验法则应用的所有内容。

在下一章中，我们将研究统计学更高级的应用。我们要考虑的一件事是如何对我们可以假设为正态的数据进行假设检验。当我们使用这些测试时，我们还将量化我们的错误，并确定解决这些错误的最佳实践。*