<title>Chapter 6. Advanced Probability</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 第六章。高级概率

在前一章中，我们复习了概率的基础知识，以及如何将简单的定理应用到复杂的任务中。简而言之，概率是对可能发生或可能不发生的事件进行建模的数学。我们使用公式来描述这些事件，甚至研究多个事件如何一起发生。

在这一章中，我们将探索更复杂的概率定理，以及如何在预测能力中使用它们。

高级主题，如**贝叶斯定理** 和 **随机变量**，产生了常见的机器学习算法，如**朴素贝叶斯算法**(也涵盖在本书中)。本章将关注概率论中一些更高级的主题，包括以下主题:

*   详尽的事件
*   贝叶斯定理
*   基本预测规则
*   随机变量

在我们开始之前，我们还有一个定义要看(在有趣的东西之前的最后一个，我保证)。我们必须看一下*集体详尽的事件*。

# 统称穷尽事件

当给定一组两个或多个事件时，如果其中至少一个事件必须发生，那么这样一组事件就称为 **的统称**。

考虑下面的例子:

*   给定一组事件`{temperature < 60, temperature > 90}`，这些事件并不是全部，因为还有第三个选项在这组事件中没有给出:温度可能在`60`和`90`之间。然而，它们是**互斥** 的，因为两者不能同时发生。
*   在掷骰子时，掷出一个`{1, 2, 3, 4, 5, or 6}`的事件的集合是**集合，因为这些是唯一可能的事件，并且其中至少一个必须发生。**

<title>Bayesian ideas revisited</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 贝叶斯思想再探

在上一章中，我们非常简要地讨论了贝叶斯思维方式。简而言之，当谈到贝叶斯的时候，你是在谈论以下三件事以及它们是如何相互作用的:

*   先验分布
*   后验分布
*   可能性

基本上，我们关心的是找到后路。这是我们想知道的事情。

另一种表述贝叶斯思维方式的方式是，数据塑造并更新我们的信念。我们有一个先验概率，或者我们对一个假设的天真想法，然后我们有一个后验概率，这是我们对一个假设的想法，给定一些数据。

## 贝叶斯定理

贝叶斯定理是贝叶斯推理的大成果。让我们看看它是如何发生的。回想一下，我们之前定义了以下内容:

*   *P(A) =事件 A 发生的概率*
*   *P(A | B)= A 出现的概率，假设 B 出现*
*   *P(A，B)= A 和 B 出现的概率*
*   *P(A，B) = P(A) * P(B|A)*

最后一点可以读作*A 和 B 发生的概率是 A 发生的概率乘以 B 发生的概率，假设 A 已经发生了*。

贝叶斯定理就是从最后一点开始形成的。

我们知道:

P(A，B) = P(A) * P(B|A)

P(B，A) = P(B) * P(A|B)

P(A，B) = P(B，A)

所以:

P(B) * P(A|B) = P(A) * P(B|A)

两边除以 *P(B)* 得到贝叶斯定理，如下所示:

![Bayes theorem](graphics/B05260_06_f1.jpg)

你可以把贝叶斯定理想成这样:

*   是从 *P(A|B)* 到 *P(B|A)* 的一种方式(如果你只有一个的话)
*   如果你已经知道 *P(A)* (不知道 B)，这是一个得到 *P(A|B)* 的方法

让我们试着用术语*假设*和*数据*来思考贝叶斯。假设 *H =你对给定数据的假设*和 *D =你得到的数据*。

贝叶斯可以解释为试图计算出 *P(H|D)* ( *给定手头数据*，我们假设正确的概率)。

用我们以前的术语来说:

![Bayes theorem](graphics/B05260_06_f2.jpg)

*   *P(H)* 是我们观察到数据之前假设的概率，称为先验概率或恰好先验
*   *P(H|D)* 是我们要计算的，我们观察数据后的假设的概率，称为后验概率
*   *P(D|H)* 是给定假设下数据的概率，称为似然
*   *P(D)* 是数据在任何假设下的概率，称为归一化常数

这个概念与机器学习和预测分析的想法相差不远。在许多情况下，当考虑预测分析时，我们使用给定的数据来预测结果。使用当前的术语， *H* ( *我们的假设*)可以被认为是我们的结果， *P(H|D)* (给定我们的数据，我们的假设为真的概率)是另一种说法:*给定我面前的数据，我的假设正确的几率是多少？*。

让我们来看一个如何在工作中使用贝叶斯公式的例子。

假设你有两个人负责为你的公司写博客——露西和阿维纳什。从以往的表演来看，你喜欢露西 80%的作品，只喜欢阿维纳什 50%的作品。早上，一篇新的博客文章放在你的办公桌上，但是没有提到作者。你喜欢这篇文章。A+。它来自阿维纳什的概率有多大？每个博客作者写博客的速度非常相似。

在我们抓狂之前，让我们做任何有经验的数学家(现在还有你)都会做的事情。让我们写出我们所有的信息，如下所示:

*   假设=博客来自阿维纳什
*   *D* = data =你喜欢这篇博文

P(H|D) =它来自阿维纳什的可能性，因为你喜欢它

鉴于它来自阿维纳什，你喜欢它的机会

一篇文章来自阿维纳什的可能性

你喜欢一篇文章的机会

请注意，如果没有上下文，这些变量几乎没有任何意义。你喜欢放在你桌子上的任何一篇文章的概率是一个奇怪的概念，但是相信我，在贝叶斯公式的背景下，它很快就会成为现实。

另外，请注意，在最后两项中，他们没有假设任何其他内容。 *P(D)* 不承担博文的出处；把 *P(D)* 想象成*如果一篇不知名的文章被随手放在你的桌子上，你喜欢它的可能性有多大？*(再说一遍，我知道断章取义听起来很怪)。

所以，我们想知道 *P(H|D)* 。让我们尝试使用贝叶斯定理，如下所示:

![Bayes theorem](graphics/B05260_06_f2.jpg)

但是我们知道这个等式右边的数字吗？我说我们有！让我们看看这里:

*   P(H) 是任何给定的博客文章来自 Avinash 的概率。由于博客作者的写作速度非常相似，我们可以假设这是 0.5，因为我们有 50%的机会它来自任何一个博客作者(注意我没有假设 D，这个数据)。
*   *P(D|H)* 是你喜欢一个来自 Avinash 的帖子的概率，我们之前说是 50%，所以，. 5。
*   *P(D)* 有意思。这个就是你喜欢一篇文章*总体*的机会。这意味着，如果帖子来自露西或阿维纳什，我们必须考虑这种情况。现在，如果假设形成一套，那么我们可以使用我们的概率定律，就像上一章提到的那样。当一组假设全部穷尽且相互排斥时，就形成了一个集合。用外行人的话来说，在一系列事件中，有且只有一种假设可能发生。在我们的例子中，两个假设是文章来自露西，或者文章来自阿维纳什。这绝对是一个组曲因为以下原因:

    *   至少其中一个写的
    *   最多其中一个写的
    *   因此，*正好其中一个*写的

当我们有一个套件时，我们可以使用我们的乘法和加法规则，如下所示:

![Bayes theorem](graphics/B05260_06_f3.jpg)![Bayes theorem](graphics/B05260_06_f4.jpg)![Bayes theorem](graphics/B05260_06_f5.jpg)![Bayes theorem](graphics/B05260_06_f6.jpg)

咻！干得好。现在我们可以完成我们的方程，如下所示:

![Bayes theorem](graphics/B05260_06_f2.jpg)![Bayes theorem](graphics/B05260_06_f7.jpg)

这意味着这篇文章有 38%的可能性来自 Avinash。有趣的是 *P(H) = .5* 和 *P(H|D) = .38* 。这意味着在没有任何数据的情况下，一篇博客文章来自阿维纳什的可能性是抛硬币，或者 50/50。给定一些数据(你对这篇文章的想法)，我们更新了我们对假设的信念，这实际上降低了可能性。这就是贝叶斯思维的全部内容——在给定一些关于主题的新数据的情况下，从先前的假设中更新我们对某事的后验信念。

## 贝叶斯定理的更多应用

贝叶斯定理出现在很多应用中，通常是当我们需要根据数据和概率做出快速决策的时候。大多数推荐引擎，如网飞的，使用贝叶斯更新的一些元素。如果你仔细想想为什么会这样，这就说得通了。

让我们假设在我们这个简单的世界里，网飞只有 10 个类别可供选择。现在假设在没有数据的情况下，用户喜欢 10 类喜剧电影的几率是 10%(只有 1/10)。

好了，现在假设用户给了几部喜剧电影 5/5 星。现在，当网飞想知道用户喜欢另一部喜剧的可能性有多大时，他们可能喜欢一部喜剧的概率， *P(H|D)* ，将大于随机猜测的 10%！

让我们用更多的数据来尝试一些应用贝叶斯定理的例子。这一次，让我们变得更加坚韧不拔。

### 例如——泰坦尼克号

一个非常著名的数据集涉及到对 1912 年泰坦尼克号沉没幸存者的观察。我们将应用概率来计算是否有任何人口统计学特征显示出与乘客存活率的关系。主要是，我们很想看看是否能从我们的数据集中分离出一些特征，这些特征能告诉我们更多关于可能在这场灾难中幸存下来的人的类型。

首先，让我们读入数据，如下所示:

```
titanic = pd.read_csv(data/titanic.csv')#read in a csv
titanic = titanic[['Sex', 'Survived']]  #the Sex and Survived column
titanic.head()
```

![Example – Titanic](graphics/B05260_06_f8.jpg)

在上表中，每一行代表船上的一名乘客，现在，我们来看两个具体特征:个体的性别和他们是否在沉船中幸存。例如，第一行代表一个没有活下来的男人，而第四行(索引为 3，还记得 python 的索引列表)代表一个活下来的女人。

让我们从一些基本的开始。让我们从计算船上任何给定的人幸存的概率开始，不管他们的性别。为此，让我们计算幸存列中的“是”数，并将该数字除以总行数，如下所示:

```
num_rows = float(titanic.shape[0]) # == 891 rows
p_survived = (titanic.Survived=="yes").sum() / num_rows # == .38
p_notsurvived = 1 - p_survived                          # == .61
```

请注意，我只需要计算 *P(幸存)*，我使用共轭概率法则计算 *P(死亡)*，因为这两个事件是互补的。现在，让我们来计算任何一位乘客是男性还是女性的概率:

```
p_male = (titanic.Sex=="male").sum() / num_rows     # == .65
p_female = 1 - p_male # == .35
```

现在让我们问自己一个问题，特定的性别会影响存活率吗？为此，我们可以估计 P(幸存|女性)或者某个人幸存的几率，如果他们是女性的话。为此，我们需要将幸存的女性人数除以女性总人数，如下所示:

![Example – Titanic](graphics/B05260_06_f9.jpg)

```
number_of_women = titanic[titanic.Sex=='female'].shape[0] # == 314
women_who_lived = titanic[(titanic.Sex=='female') & (titanic.Survived=='yes')].shape[0]                       # == 233
p_survived_given_woman = women_who_lived / float(number_of_women)
p_survived_given_woman            # == .74
```

这是一个相当大的区别。似乎性别在这个数据集中起了很大的作用。

### 示例–医学研究

贝叶斯定理的一个经典应用是对医学试验的解释。在工作场所和学校，非法药物使用的常规检测越来越普遍。进行这些测试的公司坚持认为测试具有很高的灵敏度，这意味着如果他们的系统中有药物，他们很可能产生阳性结果*。他们声称，这些测试也是高度特异性的，这意味着如果没有药物*，它们很可能产生阴性结果*。*

平均来说，我们假设常见药物检测的灵敏度约为 60%，特异性约为 99%。这意味着，如果员工使用药物，测试有 60%的机会呈阳性，而如果员工没有使用药物，测试有 99%的机会呈阴性。现在，假设这些测试应用于实际药物使用率为 5%的劳动力。

这里真正的问题是，在测试呈阳性的人中，有多少人实际上使用毒品？

用贝叶斯的术语来说，我们希望在给定一个阳性测试的情况下，计算药物使用的概率。

让 *D* =使用药物的事件

设 *E* =测试结果为阳性的事件

让 *N* =药物未*使用的事件*

我们在找 *P(D|E)* 。

通过使用贝叶斯定理，我们可以推断如下:

![Example – medical studies](graphics/B05260_06_f10.jpg)

先验， *P(D)* 是在我们看到测试结果之前用药的概率，为 5%。可能性， *P(E|D)* 是假设药物使用的阳性测试的概率，这与测试的灵敏度是一回事。归一化常数 *P(E)* 有点复杂。

我们要考虑两件事: *P(E 和 D)* 以及 *P(E 和 N)* 。基本上，我们必须假设当使用者不使用药物时，测试可能是不正确的。查看以下等式:

![Example – medical studies](graphics/B05260_06_f11.jpg)![Example – medical studies](graphics/B05260_06_f12.jpg)![Example – medical studies](graphics/B05260_06_f13.jpg)![Example – medical studies](graphics/B05260_06_f14.jpg)

因此，我们的原始方程变成如下:

![Example – medical studies](graphics/B05260_06_f15.jpg)![Example – medical studies](graphics/B05260_06_f16.jpg)

这意味着在吸毒测试呈阳性的人中，大约有四分之一是无辜的！

<title>Random variables</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 随机变量

一个**随机变量**用真实的数值来描述一个概率事件。在我们之前关于变量的工作中(数学和编程)，我们习惯于变量取某个值的事实。例如，我们可能有一个三角形，它的斜边有一个变量 h，我们必须算出斜边的长度。在 Python 中，我们也可能有:

x = 5

这两个变量每次都等于一个值。在一个随机变量中，我们受制于随机性，这意味着我们的变量的值，嗯，就是可变的！根据环境的不同，它们可能具有多个值。

如前所示，随机变量仍然保存一个值。正如我们所见，变量和随机变量的主要区别在于，随机变量的值可能会根据情况而变化。

然而，如果一个随机变量可以有很多值，我们如何跟踪它们呢？随机变量可能取的每个值都与一个百分比相关联。对于一个随机变量可能取的每一个值，都有一个概率该变量就是这个值。

对于一个随机变量，我们也可以得到一个随机变量的概率分布，它给出了这个变量可能的值和它们的概率。

写出来，我们一般用单个大写字母(大多是特定字母 *X* 来表示随机变量。例如，我们可能有:

*   掷骰子的结果
*   Y =公司今年的收入
*   *Z* =申请人在面试编码测验中的分数(0-100%)

实际上，随机变量是一个函数，它将事件样本空间(所有可能结果的集合)中的值映射到一个概率值(介于 0 和 1 之间)。请将该事件想象成如下所示:

![Random variables](graphics/B05260_06_f17.jpg)

它会给每个选项分配一个概率。随机变量主要有两种类型:离散型和连续型。

## 离散随机变量

一个离散随机变量只具有可计数的可能值。例如，掷骰子的结果，如下所示:

![Discrete random variables](graphics/B05260_06_f18.jpg)

注意我是如何使用大写字母 *X* 来定义随机变量的。这是一种常见的做法。

此外，请注意随机变量如何将概率映射到每个单独的结果。

随机变量有很多属性，其中两个是它们的*期望值*和*方差*。

我们将用一个**概率质量函数**(**【PMF】**)来描述一个离散的随机变量。

它们呈现以下外观:

P(X = x) = PMF

所以，对于掷骰子来说， *P(X = 1) = 1/6* 和 *P(X = 5) = 1/6* 。

考虑以下离散变量的例子:

*   调查问题的可能结果(例如，1-10 分)
*   首席执行官是否会在年内辞职(对或错)

随机变量的期望值定义了随机变量的长期重复样本的平均值。这有时被称为变量的平均值。

例如，参考以下定义骰子滚动随机变量的 Python 代码:

```
import random
def random_variable_of_dice_roll():
    return random.randint(1, 7) # a range of (1,7) # includes 1, 2, 3, 4, 5, 6, but NOT 7
```

这个函数将调用一个随机变量并给出一个响应。让我们掷出`100`骰子并平均结果，如下所示:

```
trials = []
num_trials = 100
for trial in range(num_trials):
trials.append( random_variable_of_dice_roll() )
print sum(trials)/float(num_trials)  # == 3.77
```

因此，取`100`个掷骰子，并对它们进行平均，得到值`3.77`！让我们用各种各样的试验编号来尝试一下，如下图所示:

```
num_trials = range(100,10000, 10)
avgs = []
for num_trial in num_trials:
    trials = []
    for trial in range(1,num_trial):
        trials.append( random_variable_of_dice_roll() )
    avgs.append(sum(trials)/float(num_trial))

plt.plot(num_trials, avgs)
plt.xlabel('Number of Trials')
plt.ylabel("Average")
```

![Discrete random variables](graphics/B05260_06_03.jpg)

前面的图显示了我们看到的越来越多的掷骰子的平均掷骰子数。我们可以看到掷骰子的平均数正在迅速接近 3.5。如果我们看向图表的左侧，我们会看到，如果我们只掷骰子约 100 次，那么我们不能保证得到 3.5 次的平均掷骰子。然而，如果我们一个接一个地掷出 10，000 个骰子，我们会发现我们很可能期望掷出的平均骰子数是 3.5。

对于离散随机变量，我们也可以使用如下所示的简单公式来计算期望值:

![Discrete random variables](graphics/B05260_06_f19.jpg)

其中*x[I]是第 *i ^第个结果，而*p[I]是第 *i ^第个*个概率。***

因此，对于我们的掷骰子，我们可以找到确切的期望值如下:

![Discrete random variables](graphics/B05260_06_f20.jpg)

前面的结果告诉我们，对于任何给定的掷骰子，我们可以“期望”掷出 3.5 的骰子。很明显，这是没有意义的，因为我们不可能在掷骰子中得到 3.5 分，但当放在许多掷骰子的环境中时，这是有意义的。如果您掷出 10，000 个骰子，您的平均骰子点数应该接近 3.5，如前面的图形和代码所示。

一个随机变量的期望值的平均值一般不足以把握变量背后的全部思想。为此，我们引入一个新概念，叫做方差。

随机变量的方差代表变量的分布。它量化了期望值的可变性。

离散随机变量的方差公式表示如下:

![Discrete random variables](graphics/B05260_06_f21.jpg)

其中*x[I]T3和*p[I]表示与之前相同的值，而![Discrete random variables](graphics/chp6a.jpg)表示变量的预期值。在这个公式中，我还提到了 *X* 的 sigma。在这种情况下，适马是标准差，它被简单地定义为方差的平方根。让我们看一个更复杂的离散随机变量的例子。**

方差可以被认为是一个*或多或少的*度量。如果我说你可以*期望*从一手扑克中赢得 100 美元，你可能会非常高兴。如果我在这句话后面加上额外的细节，你可能会赢 100 美元，上下浮动 80 美元，那么你现在要处理的预期范围很广，这可能会令人沮丧，可能会让厌恶风险的玩家对加入游戏更加谨慎。我们通常可以说我们有一个期望值，给出或取标准差。

假设你的团队用李克特量表来衡量一个新产品的成功，也就是说，分为五类，0 代表完全失败，4 代表非常成功。他们根据用户测试和产品性能的初步结果，估计一个新项目有以下成功机会。

我们首先要定义我们的随机变量。

让 *X* 随机变量代表我们产品的成功。 *X* 确实是一个离散随机变量，因为 *X* 变量只能取五个选项中的一个: *0* 、 *1* 、 *2* 、 *3* 或 *4* 。

下面是我们的随机变量， *X* 的概率分布。请注意，我们如何为 *X* 的每个潜在结果设置了一个栏，在每个结果下面，我们有实现特定结果的概率:

![Discrete random variables](graphics/B05260_06_f22.jpg)

例如，项目有 2%的机会彻底失败，有 26%的机会获得巨大成功！我们可以如下计算我们的期望值:

e[X]= 0(0.02)+1(0.07)+2(0.25)+3(0.4)+4(0.26)= 2.81

这个数字意味着经理可以*期望*在这个项目中获得大约 2.81 的成功。现在，这个数字本身并没有多大用处。也许，如果有几种产品可供选择，期望值可能是比较几种产品潜在成功的一种方法。然而，在这种情况下，当我们只有一个产品要评估时，我们将需要更多。

现在，让我们检查差异，如下所示:

方差= V[X]=σX2 =(XiμX)2pi

= (0 − 2.81)2(0.02) + (1 − 2.81)2(0.07)+(2 − 2.81)2(0.25) + (3 − 2.81)2(0.4) + (4 − 2.81)2(0.26) = .93

现在我们已经有了项目得分的标准差和期望值，让我们试着总结一下我们的结果。我们可以说，我们的项目将有一个 2.81 正负 0.96 的预期分数，这意味着可以预期 1.85 和 3.77 之间的东西。

因此，我们可以解决这个项目的一种方法是，它可能会有一个 *2.81* 的成功率，给或拿大约一分。

你可能会想，*哇，思南，那么，这个项目最好的结果是 3.8，最坏的结果是 1.8？*。不完全是。

它可能比 4 好，也可能比 1.8 差。为了更进一步，让我们计算如下:

P(X >= 3)

首先，花一分钟时间说服自己，你可以自己读这个公式。当我要求 P(X > = 3) 时，我在要求什么？老实说，花点时间想清楚。

P(X > = 3) 是我们的随机变量取值至少为 3 的概率。换句话说，我们的产品获得 3 分或更高的成功率的可能性有多大？为了计算这一点，我们可以计算如下:

P(X >= 3) = P(X = 3) + P(X = 4) = .66 = 66%

这意味着我们有 66%的机会将我们的产品评为 3 级或 4 级。

另一种计算方法是共轭法，如下所示:

P(X > = 3)= 1–P(X < 3)

再次，花一点时间说服自己这个公式成立。我声称找到产品至少被评为 3 的概率等于 1 减去产品被评为低于 3 的概率。如果这是真的，那么这两个事件( *X > =3* 和 *X < 3* )一定是互补的。

这显然是真的！该产品可以是以下两种选项之一:

*   被评为 3 级或以上
*   被评为 3 级以下

让我们检查一下我们的数学:

P(X < 3)= P(X = 0)+P(X = 1)+P(X = 2)= 0.02+0.07+0.25 = . 034

1–P(X< 3) = 1 - .34 = .66 = P( x >= 3)

核实过了。

### 离散随机变量的类型

通过观察特定类型的随机变量，我们可以更好地了解随机变量在实践中是如何工作的。这些特定类型的随机变量模拟了不同类型的情况，并最终揭示了非常复杂的事件建模的简单得多的计算。

#### 二项随机变量

我们要研究的第一种离散型随机变量被称为**二项随机变量**。对于二项式随机变量，我们看一个设置，其中一个单一的事件发生一次又一次，我们试图计数的次数，结果是积极的。

在我们能够理解随机变量本身之前，我们必须看看它在什么条件下是合适的。

二项式设置有以下四个条件:

*   可能的结果不是成功就是失败
*   试验的结果不能影响另一个试验的结果
*   试验次数是固定的(固定的样本量)
*   每次试验的成功几率必须始终为 *p*

二项式随机变量是一个离散随机变量， *X* ，它计算二项式设置中的成功次数。参数是 *n =试验次数*和 *p =每次试验成功的几率*。

**例子-筹款会议:**

一家初创公司召开了 20 次风险投资会议来筹集资金，并统计他们收到的报价数量。

二项随机变量的**概率质量函数**(**【PMF】**)如下:

![Binomial random variables](graphics/B05260_06_f23.jpg)

这里，![Binomial random variables](graphics/B05260_06_f24.jpg)。

**示例–餐厅开业**

一个镇上的新餐馆有 20%的机会在第一年存活下来。如果今年有 14 家餐馆开业，找出正好有 4 家餐馆在对公众开放的第一年存活下来的概率。

首先，我们应该证明这是一个二项式设置:

*   可能的结果要么是成功，要么是失败(餐馆要么生存要么失败)
*   试验的结果不能影响另一个试验的结果(假设一家餐馆的开业不影响另一家餐馆的开业和生存)
*   试验次数已确定(14 家餐厅开业)
*   每次试验的成功几率必须总是 p(我们假设它总是 20%)

这里我们有我们的两个参数 *n = 14* 和 *p = .2* 。因此，我们现在可以将这些数字代入二项式公式，如下所示:

![Binomial random variables](graphics/B05260_06_f25.jpg)

因此，我们有 17%的机会在一年后有 4 家餐厅会开业。

**示例–血型**

一对夫妇有 25%的机会生出 O 型血的孩子。他们 5 个孩子中有 3 个是 O 型血的可能性有多大？

设*X = O 型血*的孩子数量*n = 5**p = 0.25*，如下图:

p(X = 3)= 5 0.253(0.75)53 = 10(0.25)3(0.75)2 = 0.087

我们可以计算 0、1、2、3、4 和 5 的值的概率，以了解概率分布:

![Binomial random variables](graphics/B05260_06_f26.jpg)

从这里，我们可以计算这个变量的期望值和方差:

![Binomial random variables](graphics/B05260_06_f27.jpg)

所以，这个家庭可能会有一两个 O 型血的孩子。

如果我们想知道他们的孩子中至少有 3 个是 O 型血的概率呢？要知道他们的孩子中至少有三个是 O 型血的概率，我们可以用下面的公式计算离散随机变量:

![Binomial random variables](graphics/B05260_06_f28.jpg)![Binomial random variables](graphics/B05260_06_f29.jpg)

所以，他们的孩子中有 10%的可能是 O 型血。

### Tip

**二项式期望值和方差的快捷方式**

二项式随机变量对期望值和方差的精确值有特殊的计算。如果 *X* 是二项随机变量，则:

*E(X) = np*

*V(X)= NP(1p)*

对于前面的示例，我们可以使用以下公式来计算精确的期望值和方差:

*   *E(X) = .25(5) = 1.25*
*   *V(X) = 1.25(.75) = .9375*

二项式随机变量是一种离散随机变量，用于计算二项式设置中的成功次数。它被用在各种各样的数据驱动的实验中，如计算有转化机会的网站注册人数，甚至在简单的层面上，预测有下跌机会的股票价格走势(别担心；我们稍后将应用更复杂的模型来预测股票市场。

**几何随机变量；**

我们要看的第二个离散随机变量叫做几何随机变量。它实际上与二项式随机变量非常相似，因为我们关注的是一个事件反复发生的环境。然而，在几何设置的情况下，主要的区别是我们没有固定样本大小。

作为一家初创公司，我们不会参加 20 次风投会议，也不会有 5 个孩子。相反，在一个几何环境中，我们正在模拟在我们获得哪怕一次成功之前我们需要看到的试验次数。具体来说，几何设置有以下四个条件:

*   可能的结果不是成功就是失败
*   试验的结果不能影响另一个试验的结果
*   试验次数没有确定
*   每次试验成功的机会必须总是 p

请注意，除了第三个条件之外，这些条件与二项式变量完全相同。

一个**几何随机变量** 是一个离散随机变量， *X* ，它计算获得一次成功所需的试验次数。参数为 *p =每次试验成功的几率*和*(1p)=每次试验失败的几率*。

要将前面的二项式示例转换为几何示例，我们可以执行以下操作:

*   数一数一家初创企业为了获得第一个*肯定*必须参加的风险投资会议
*   数一数需要抛多少硬币才能得到正面(是的，我知道这很无聊，但这是一个很好的例子！)

PMF 的公式如下:

p(X = X)=(1p)[x1]p

二项式和几何式设置都包含成功或失败的结果。最大的区别是二项随机变量有固定的试验次数，记为 *n* 。几何随机变量没有固定的试验次数。相反，几何随机变量模拟了获得第一次成功试验所需的样本数量，无论在那些实验条件下成功意味着什么。

**示例-天气**

四月的任何一天都有 34%的可能性会下雨。求 4 月第一天下雨发生在 4 月 4 日的概率。

设 *X =下雨前的天数*(成功)*p = 0.34**(1p)= 0.66*

所以，*P(X = 8)=(0.66)81(0.34)*

*= (0.66)7(0.34)*

*= 0.01855*

四月四日下雨的可能性如下:

![Binomial random variables](graphics/B05260_06_f30.jpg)![Binomial random variables](graphics/B05260_06_f31.jpg)

因此，本月的第一场雨有 80%的可能性会在头四天内下。

### Tip

**几何期望值和方差的快捷方式**

几何随机变量对于期望值和方差的精确值也有特殊的计算。如果 *X* 是几何随机变量，那么，

*E(X) = 1/p*

*V(X) = (1-p)/p2*

#### 泊松随机变量，

离散随机变量的第三个也是最后一个具体的例子是泊松随机变量。

为了理解我们为什么需要这个随机变量，假设我们希望建模的事件发生的概率很小，并且我们希望计算该事件在特定时间范围内发生的次数。如果我们从过去的实例中了解到特定时间段内的平均发生次数，那么由 *X = Poi( )* 表示的泊松随机变量将计算该事件在该给定时间段内的总发生次数。

换句话说，泊松分布是一种离散的概率分布，它计算在给定的时间间隔内发生的事件的数量。

**考虑以下泊松随机变量** **:** 的例子

*   了解网站过去的表现，找出网站在一小时内有一定数量访客的概率
*   根据过去的警方报告估计十字路口的车祸数量

如果我们让 *X =给定区间*内的事件数，每个区间的平均事件数为 *λ* 数，那么在给定区间内观测到 *x* 事件的概率由下式给出:

![Poisson random variable,](graphics/B05260_06_f32.jpg)

这里， *e =欧拉常数(2.718…)*。

**示例-呼叫中心:**

到达呼叫中心的呼叫数量遵循泊松分布，每小时 5 个呼叫。晚上 10 点到 11 点之间正好有 6 个电话打进来的概率是多少？

为了建立这个例子，让我们写出给定的信息。设 *X* 为晚上 10 点到 11 点之间到达的电话数量。这是我们的泊松随机变量，均值 *λ = 5* 。

平均值为 5，因为我们使用 5 作为我们之前对此时来电数量的期望值。这个数字可能来自估计每小时或特别是晚上 10 点以后来电数量的宝贵工作。主要想法是，我们确实知道应该有多少来电，然后我们使用该信息创建泊松随机变量，并使用它进行预测。

继续我们的示例，我们有以下内容:

P(X = 6) = = 0.146

这意味着大约有 14.6%的几率在晚上 10 点到 11 点之间会有 6 个电话打来

### Tip

**泊松期望值和方差的快捷方式**

泊松随机变量对于期望值和方差的精确值也有特殊的计算。如果 *X* 是带均值的泊松随机变量，则:

*E(X) =λ*

*V(X) =λ*

这很有趣，因为期望值和方差是同一个数字，而这个数字就是给定的参数！现在我们已经看到了三个离散随机变量的例子，我们必须看看另一种类型的随机变量，称为连续随机变量。

#### 连续随机变量

完全转换档位，不像离散随机变量，连续随机变量可以有*无限*个可能值，而不仅仅是几个可数的值。我们称描述分布密度曲线的函数为概率质量函数。

考虑以下连续变量的例子:

*   销售代表的电话通话时长(不是通话次数)
*   标记为 20 加仑的油桶中的实际油量(不是油桶的数量)

如果 *X* 是连续的随机变量，那么有一个函数 *f(x)* ，使得对于任意常数 *a* 和 *b* :

![Continuous random variables](graphics/B05260_06_f34.jpg)

前面的 *f(x)* 函数称为 **概率密度函数** ( **PDF** )。PDF 是离散随机变量的 PMF 的连续随机变量版本。

最重要的连续分布是 **标准正态分布**。毫无疑问，你要么听说过正态分布，要么处理过它。背后的想法很简单。该发行版的 PDF 如下:

![Continuous random variables](graphics/B05260_06_f35.jpg)

这里，![Continuous random variables](graphics/chp6b.jpg)是变量的平均值，![Continuous random variables](graphics/chp6c.jpg)是标准差。这可能看起来令人困惑，但让我们用 Python 绘制它，平均值为 0，标准差为 1，如下所示:

```
def normal_pdf(x, mu = 0, sigma = 1):
    return (1./np.sqrt(2*3.14 * sigma**2)) * 2.718**(-(x-mu)**2 / (2\. * sigma**2))

x_values = np.linspace(-5,5,100)
y_values = [normal_pdf(x) for x in x_values]
plt.plot(x_values, y_values)
```

![Continuous random variables](graphics/B05260_06_10.jpg)

哪一个引出了大家再熟悉不过的钟形曲线。注意图是围绕 x = 0 线对称的。让我们试着改变一些参数。首先，让我们试试![Continuous random variables](graphics/B05260_06_f36.jpg):

![Continuous random variables](graphics/B05260_06_11.jpg)

接下来，让我们用的值![Continuous random variables](graphics/B05260_06_f37.jpg)来试试:

![Continuous random variables](graphics/B05260_06_12.jpg)

最后，我们将尝试使用值![Continuous random variables](graphics/B05260_06_f38.jpg):

![Continuous random variables](graphics/B05260_06_13.jpg)

在所有的图中，我们都有熟悉的标准钟形，但当我们改变参数时，我们会看到钟形可能变得更细、更粗，或者从左向右移动。

在下面关注统计的章节中，我们将更多地使用正态分布，因为它适用于统计思维。

<title>Summary</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 总结

概率作为一个领域来解释我们的随机和混乱的世界。利用概率的基本定律，我们可以模拟包含随机性的现实生活事件。我们可以使用随机变量来表示可能取几个值的值，我们可以使用概率质量或密度函数来比较产品线或查看测试结果。

我们已经看到概率在预测中的一些更复杂的应用。使用随机变量和贝叶斯定理是将概率分配给现实生活情况的极好方法。在后面的章节中，我们将重温贝叶斯定理，并使用它来创建一个非常强大和快速的机器学习算法，称为朴素贝叶斯算法。这种算法抓住了贝叶斯思维的力量，并将其直接应用于预测学习的问题。

接下来的两章集中在统计思维上。像概率一样，这些章节将使用数学公式来模拟现实世界的事件。然而，主要的区别将是我们用来描述世界的术语和我们对不同类型事件建模的方式。在接下来的章节中，我们将尝试仅基于一个样本来模拟整个数据点群体。

我们将重温概率中的许多概念，以理解统计定理，因为它们是紧密联系的，并且都是数据科学领域中重要的数学概念。