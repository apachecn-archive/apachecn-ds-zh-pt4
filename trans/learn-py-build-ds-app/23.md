<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Best Practices and Python Performance

After going through the preceding chapters and learning various things about Python, we have come to the last chapter. Here, we want to discuss some general strategies that you can implement and how to write code that works faster, is cleaner, and is easier to maintain. These approaches can be used for data-oriented code—or any other type of code, for that matter.

This chapter is split into three parts. The first section will discuss how you can analyze and speed up your code, the second section will cover best practices for maintaining your code so that you'll code faster and cleaner, and in the third and final section, we'll go through a brief overview of the non-Python technologies that you might find ...

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Technical requirements

The code for this chapter can be found in this book's GitHub repository ([https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications)), which is stored in the `Chapter20` folder. The code requires an array of packages to be installed, including the following:

*   `numpy`
*   `scipy`
*   `numba`
*   `dask`
*   `black`
*   `wily`
*   `hypothesis`
*   `line_profiler`
*   `python-graphviz` and `graphviz`

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Speeding up your Python code

In the previous chapter, we talked about different best practices, approaches, and ways to boost code performance. As a toy example for performance, we'll build our own KNN model, which we used in [Chapter 13](c6bd4bea-7b67-46bf-bdf9-761f8b400f75.xhtml), *Training a Machine Learning Model*. As a reminder, KNN is a simple ML model that predicts the target variable by identifying *K* closest records in the training set, then taking a mode (for classification) or weighted average (for regression) of the target variable. Obviously, there are quite a few implementations of KNN already, and so we will use one as an example.

For starters, let's write a naive implementation; it has already been fairly optimized through the use of NumPy commands. First, let's import all ...

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Rewriting the code with NumPy

NumPy is a library that's used for fast numeric computation and serves as a foundation for Python's scientific ecosystem. It's also the backbone for SciPy and Pandas. Since we have slow, numeric code, NumPy is a great place to start with your optimization attempts. 

The algorithm is mostly written in NumPy already—we couldn't perform a true closest-*N* search in `pandas` since it doesn't support multidimensional indexing. However, there is one low-hanging fruit: our naive model uses `argsort` to pick the *N* closest records, which does sort the whole dataset. We don't need sorting, even for those *N* closest ones—let alone any other element. Here, we can swap the `np.argsort` method with `np.argpartition`. This function does exactly what we want—it puts the *N* smallest distances first (no matter the order) and keeps all the rest to the right:

```
def _closest_N2(X1, X2, N=1):
    matrix = euclidean_distances(X1, X2)
    return np.argpartition(matrix, kth=N, axis=1)[:, :N]
```

To ensure that the functions are interchangeable, let's write a simple test function:

```
def _test_closest(f):
    x1 = pd.DataFrame({'a':[1,2], 'b':[20,10]})
    x2 = pd.DataFrame({'a':[2,1, 0], 'b':[10,20, 25]})

    answer = np.array([[1,0, 0]]).T
    assert np.all(f(x2, x1, N=1) == answer)

_test_closest(_closest_N2)
```

Feel free to add more test cases (this is where you can leverage PyTest suites)!

Now, we can create a new version of the KNN by using this new function:

```
class numpyNearestNeighbour(NearestNeighbor):

    def predict(self, X):
            closest = _closest_N2(X, self.X, N=self.N)
            return np.mean(np.take(ytrain.values, closest), axis=1)
```

Note that we also got rid of `pd.Series`. This will speed up the algorithm, but you'll probably have to wrap values to the series outside. Let's get our customers to decide on that.

Now, let's see how that version performs on the same dataset:

```
>>> numpyKNN = numpyNearestNeighbour(N=5)
>>> numpyKNN.fit(Xtrain.values, ytrain.values)

>>> %%timeit
>>> _ = numpyKNN.predict(Xtv)

448 ms ± 14.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

We went from 1.43 seconds to 448 ms—that's a boost of 69%! Let's look at the distribution by line:

```
>>> %lprun -f _closest_N2 numpyKNN.predict(Xtv)

Timer unit: 1e-06 s

Total time: 0.440021 s
File: <ipython-input-134-29fa1851d880>
Function: _closest_N2 at line 1

Line # Hits  Time        Per Hit   % Time    Line Contents
==============================================================
 1                                      def _closest_N2(X1, X2, N=1):
 2 1     212103.0    212103.0  48.2         matrix = euclidean_distances(X1, X2)
 3 1     227918.0    227918.0  51.8         return np.argpartition(matrix, kth=N, axis=1)[:, :N]
```

This time, it seems that the matrix and partition take approximately the same time (this will change for larger datasets, though). To summarize, vectorizing the code with NumPy allowed us to boost our computations by 68%—all while making our code cleaner and more expressive. For most tasks, NumPy remains the first solution to try out—and often, the result is good enough already. 

NumPy is essentially a foundation and industry standard for Python numeric computations. Many libraries are based on NumPy or interact with it. In fact, modern NumPy does a great deal of work defining the interface, allowing other libraries to plug in the actual computations and be interchangeable. One example of that is CuPy—a GPU-based alternative for NumPy with a near-identical interface.

If you want to dive deeper into NumPy-based computations, take a look at these resources:

*   *The "NumPy" Approach*, by James Powell: [https://www.youtube.com/watch?v=8jixaYxo6kA](https://www.youtube.com/watch?v=8jixaYxo6kA) 
*   *NumPy Essentials*, by Leo (Liang-Huan) Chin and Tanmay Dutta: [https://www.packtpub.com/big-data-and-business-intelligence/numpy-essentials](https://www.packtpub.com/big-data-and-business-intelligence/numpy-essentials)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Specialized data structures and algorithms

Another (arguably the best one, in general) way to make things more performant is to make use of the right data structures and algorithms—in other words, we need to design our code better and use the right tools for the job in the first place. In our case, any spatial query, especially for a large dataset, will gain from the use of a spatial index. Essentially, this creates a hierarchical index, based on the spatial distribution itself. It allows it to measure the distances within a small subset of records. Let's try to make use of it in our model:

```
from scipy.spatial import cKDTreeclass kdNearestNeighbor:    _kd = None    y = None        def __init__(self, N=3):        self.N=N        def fit(self, X, y): self._kd = cKDTree(X, ...
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Dask

So far, everything we've run was run on one CPU, sequentially—with the exception of some ML models and transformations, which support the number of jobs (parallel executors); for example, `cKDTree` supports multiprocessing, if needed.

The caveat here is the overhead—in order to run a multicore process, a lot of additional memory needs to be allocated and data needs to be copied; it is essentially a fixed cost. Because of that, most of the tasks we ran wouldn't benefit from multiple cores, except for cases where data is very large and computations are fairly parallelized. On the flip side, once we run a task on multiple cores, spreading it across multiple machines is simple.

While the most typical task for Dask to deal with is heavy computation on multiple cores or machines, it also allows you to run computations nicely on data that wouldn't fit in a computer's memory (by loading and operating chunks of data, one per core, at a time). Thus, in theory, it can be used to run some analysis on small IoT devices—especially given that it also supports streaming.

Most of the time, using multiple cores—or multiple machines—will not boost the computations you're able to run on the local machine, loading all the data in memory. However, if your data is big enough, you have to use chunks, and computation will take hours to run. Due to this, using distributed computation could be your only choice (obviously, assuming the bottleneck is not bad code).

For that, we need to introduce Dask—a system that allows you to run heavy computations with big datasets on multiple cores of one machine, or on a cluster of machines. The best part (for us) of Dask is that it emulates the behavior of Pandas or NumPy on the surface. In many cases, Dask's dataframe can be used as if it was a Pandas dataframe—except that it is spread across cores and machines. One big difference in using Dask is that no computation is executed until you ask it to compute.

Let's try pulling the same data we were using for 311 predictions. Since Dask is meant to be used with large datasets and multiple files, it can handle path patterns—we don't need to glob explicitly (also, it can glob on the S3 bucket and read from there). To do so, we'll import the `dask` dataframe, specify a path, `pattern`, using an asterisk (wildcard) to identify parts of the path that vary. Finally, we will use the `read_csv` method to read those, just like we'd do with `pandas` (we do this because Dask runs Pandas' `read_csv` method under the hood here). Setting `blocksize` to `None` here explicitly makes Dask use one worker (core) per file. We also explicitly set Dask to use processes (multicore) scheduler. Here's what this looks like in code:

```
from dask import dataframe as dd
import dask
dask.config.set(scheduler='processes')
```

As you will notice, the code won't take long to execute—this is because it didn't actually run anything. For now, `df` is just a schedule object that will execute once we call a `compute()` method. Let's continue coding as if it was a dataframe:

```
df = df[df.complaint_type.str.lower().str.contains('noise')]
cols = ['x_coordinate_state_plane', 'y_coordinate_state_plane', 'created_date', 'closed_date', 'complaint_type', 'open_data_channel_type']

df = dg.dropna(subset=cols)

X = df[['x_coordinate_state_plane', 'y_coordinate_state_plane']]
X['dow'] = df['created_date'].dt.dayofweek
X['hour'] = df['created_date'].dt.dayofweek
X['doy'] = df['created_date'].dt.dayofyear
```

Like before, the code didn't take long to execute—for the same reason. The tasks are combining, though, and are forming a directed graph. We can cross-check that graph as follows:

```
X.visualize(filename='chart.png')
```

This is what we'll get:

![](Images/f08f882d-1826-4293-8a2b-2be5a053dd35.png)

Here, each separate graph represents a chunk that could live on a separate CPU, while each node represents an operation. It is very useful to cross-check those graphs, especially for complex operations (think `groupby` and `similar`). Once you're ready, hit `compute`:

```
data = X.compute()
```

The best part is that while the preceding code will execute on the local machine, it is easy to deploy a cluster of machines on the cloud. Once that is done, Dask can be configured to spread your computation to those machines, with no changes needing to be made to the code on Dask's side (obviously, it will benefit from having data stored in storage that's accessible to all the machines in the cluster).

Dask is a Python-based framework for big data computation. Its more famous alternative is Spark, and the PySpark package for Python. Spark is a great tool and can scale easily. At the same time, the core code of this technology is written in Java, and so you'll have to be prepared to debug Java code. Dask, on the other hand, is 100% Python and has familiar APIs, so you won't need to change that much code.

For more information on Dask, take a look at *Scalable Data Analysis with Dask*, by Mohammed Kashif: [https://www.packtpub.com/web-development/scalable-data-analysis-python-dask-video](https://www.packtpub.com/web-development/scalable-data-analysis-python-dask-video)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Dask-ML

Dask is not necessarily a good way to scale up your model training—most models require interaction, and therefore should stay within one machine. At the same time, most `sklearn` models can work on multiple CPUs on their own, and so Dask isn't required.

With that being said, there are plenty of cases when using Dask could be beneficial. For that, there is an additional layer over Dask—Dask-ML. Dask-ML helps connect Dask to `sklearn` and other ML libraries (for example, XGBoost and TensorFlow), thereby allowing you to run some parallelizable models (linear models, for example, or some clustering algorithms), execute hyperparameter searches with different hyperparameters being executed on different servers, or connect distributed datasets ...

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Numba

In this final subsection, we want to talk about Numba. It is probably one of the hottest ways to speed up your Python code with almost no changes. Numba compiles Python code—vanilla Python or NumPy-based—into C code using LLVM. By doing so—and by leveraging a suite of optimizations along the way—it drastically increases the speed of the code, especially if you use a lot of loops and NumPy arrays.

The great thing about Numba is that, in the best-case scenario, it will improve your code by adding a simple decorator over your function or class—that is, if you're lucky. If you're not, you'll have to work through the documentation and somewhat obscure error messages and experiment with datatype annotations. In some cases, Numba could be more performant than NumPy! As if that isn't enough, Numba can also compile your code for CUDA, leveraging the heavy performance of GPUs—which are often an order of magnitude faster than CPUs! 

Here is a simple example. The `compute_distances` function resembles the behavior of `euclidean_distances` and performs fairly well:

```
def distance(p1, p2):
    distance = 0
    for c1, c2, in zip(p1,p2):
        distance += (c2-c1)**2

    return np.sqrt(distance)

def compute_distances(points1, points2):
    A = np.zeros(shape=(len(points1), len(points2)))

    for i, p1 in enumerate(points1):
        for j, p2 in enumerate(points2):
                 A[i, j] = distance(p1, p2)

    return A

%timeit compute_distances([(0, 0)]*100, [(1,1)]*200)
```

The performance (output) of the preceding code snippet is as follows:

```
>>> 43.8 ms ± 1.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

However, once we add a decorator to each function, performance increases more than tenfold: 

```
@jit()
def distance(p1, p2):

    distance = 0
    for c1, c2, in zip(p1,p2):
        distance += (c2-c1)**2

    return np.sqrt(distance)

@jit()
def compute_distances(points1, points2):
    A = np.zeros(shape=(len(points1), len(points2)))

    for i, p1 in enumerate(points1):
        for j, p2 in enumerate(points2):
                 A[i, j] = distance(p1, p2)

    return A

%timeit compute_distances([(0, 0)]*100, [(1,1)]*200)
```

The performance (output) of the preceding code snippet is as follows:

```
>>> 3.02 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 1 loop each) 
```

On that run Numba shows a deprecation warning—future versions will require to specify a list type; in the current version it works as it is.

In our experience, Numba is great for non-trivial, multi-nested computations, where it is easier to write in pure Python (and optimize with Numba) than in NumPy. At the same time, it isn't very mature code (as NumPy is), and different changes occurring in the API happen fairly often.

In this section, we covered a few ways to improve the performance of Python code. Starting from a naive, slow, but easy algorithm implementation, we took on different angles in order to make it faster, such as using vectorized C-based loops, specific data structures that are efficient for the task, running operations on multiple cores or multiple machines, and using modern compilers. Some of those solutions can and should be combined. All of them have their own benefits, limitations, and requirements—larger memory, more CPUs and computers, specific knowledge, and so on. Don't rush to implement any optimization before you're sure you need it. Once you are sure, though, a wide range of possibilities is available.

Numba is not the only way to compile Python into a more performant C version. In fact, there are quite a few other ways to do this. Among the most popular ones is Cython. The idea behind this package is somewhat similar to Numba, but there is no LLVM involved, and code is compiled to C directly—by doing this, you can store and use the compiled code. In addition, Numba can be compiled to CUDA and run on a GPU!

For more information on Numba, check out the following resources:

*   *Numba—Tell Those C++ Bullies to Get Lost, SciPy 2017 Tutorial, Gil Forsyth and Lorena Barba*: [https://www.youtube.com/watch?v=1AwG0T4gaO0](https://www.youtube.com/watch?v=1AwG0T4gaO0)
*   *Accelerating Python with the Numba JIT Compiler, SciPy 2015, Stanley Seibert*: [https://www.youtube.com/watch?v=eYIPEDnp5C4](https://www.youtube.com/watch?v=eYIPEDnp5C4)

Now, let's talk about an important topic we've ignored so far—concurrency.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Concurrency and parallelism

Concurrency is the simultaneous execution of multiple pieces of code. Theoretically, concurrency can significantly increase the speed of code execution and it is widely used in software. For example, tasks that require some sort of big loop that does exactly the same operation many times with no interaction between those operations (for example, vectorized operations on dataset columns) are often called **embarrassingly parallel** and present a good target for concurrent execution. That being said, it has its limitations and suits some tasks (for example, where a number of tasks are independent of each other) better than others—read about *Amdahl's law* for some theoretical background. 

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Different types of concurrency

There are various ways to achieve concurrency in Python, including threads, tasks, processes, and so on. First, while we *say* that a concurrent task occurs simultaneously, this is not always the case. In fact, threads and tasks don't really run concurrently—instead, the CPU can switch between different threads really fast so that they seem to be running in parallel, but it always executes one thread at a time. This is ensured by part of the Python interpreter called **Global Interpreter Lock**, or **GIL**. Threading can still boost your code execution, which it does by switching to other threads when the CPU is waiting for data to be loaded from the network—we'll talk about that in a minute.

Even then, there are multiple ways to execute code on one CPU. Python's built-in `threading` library allows the operating system to stop threads and switch between them—the code itself doesn't need to do anything. The problem with threading, in general, is that the OS can stop threads at any moment—even in the middle of writing or computing data—so you should be extremely careful when sharing any data between threads and not use it anywhere until all of the computations are complete. The problem of shared data is often referred to as **thread safety**.

Another built-in library, `asyncio` (one that allows asynchronous functions, which we touched on in [Chapter 18](e1d8b121-e8db-45da-9dbf-a034664926fa.xhtml), *Serving Models with a REST API*), works slightly differently—synchronous tasks declare that they are done or blocked, in which case another task will start (or continue) running. Thus, tasks cannot be switched while the process occurs until you allow that from within the task.

However, you can run parts of your code truly in parallel (this feature is usually called **parallelism***).* There are two ways to do this. First, we can leverage other CPUs on your machine—many modern computers have at least two or four CPUs. In order to do that, you can use the built-in `multiprocessing` library, or any code/library built on it (for example, Dask can run on multiple CPUs of one machine). While this approach allows you to actually run in parallel, it has a large overhead of copying data and orchestrating the process. Because of that fixed cost, multiprocessing generally does not make sense, except for computationally heavy operations.

Lastly, yet another option is to run code simultaneously on many machines. This option was rarely feasible for ordinary developers, even a few years ago, but with the modern cloud-based infrastructure that we have and software tools such as Kubernetes (which we'll discuss later) that are quite accessible and relatively cheap, this is possible. There is no built-in library for that, but frameworks such as Dask and PySpark can help. Running on multiple machines has the same issues as multiprocessing, to the power of ten—deploying machines, loading data, orchestrating tasks, then pulling results together is a huge overhead! But, for better or for worse, there is simply no alternative for huge computations with large datasets that wouldn't fit into one machine's memory. The good news is that, once running a cluster, you can easily add more and more machines when needed—there is virtually no limit (except for the price, of course).

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Two types of problems

Now, let's get back to the task at hand. There are two general types of problems concurrency can solve—CPU-bound and I/O-bound tasks. As you can guess from their names, CPU-bound tasks require more computing power than one CPU can provide. For obvious reasons, this kind of problem can't be solved by threading or asynchronous execution, and so multiprocessing and cluster computing are our only options.

The second type, I/O-bound, is limited by the input/output (for example, it has to wait for the database or network). Network resources are *usually* way slower than the CPU, so in this case, our computer just waits for data to come. This is where threading and asynchronous execution shines.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Before you start rewriting your code

Don't rush into rewriting your code in a concurrent fashion just yet. There are plenty of reasons *not* to write concurrent code. Let's look at a few reasons here:

*   First of all, don't do it if you don't *need* that boost—any type of concurrency adds code complexity and makes debugging exponentially harder.
*   Second, the code for many specific computation-demanding tasks is already written. For example, multiple `sklearn` models support multicore execution—you just need to specify the number of CPUs to use. Some solutions, such as Numba, can release the GIL for specific operations, without large code changes being made. 
*   Some important packages do not support concurrent operations—asynchronous execution in particular—such as `sqlalchemy` and most database access tools in general.

All in all, make sure you really need your code to run concurrently or in parallel before investing your time and effort. As cool as it sounds, concurrent code is notoriously difficult and takes significantly more time to develop, optimize, and maintain.

If you want to get a deeper understanding of concurrency in Python (which is a very wide topic), we can recommend the following resources:

*   *Curious Course on Coroutines and Concurrency*, by David Beazley: [https://www.youtube.com/watch?v=Z_OAlIhXziw](https://www.youtube.com/watch?v=Z_OAlIhXziw)
*   *Concurrent Execution *([https://docs.python.org/3/library/concurrency.html](https://docs.python.org/3/library/concurrency.html))

PEP 554 (currently in draft status) proposes to use sub interpreters (isolated instances, controlled by the main interpreter process) to allow better multiprocessing without GIL getting in the way. To learn more about this proposal, read the PEP: [https://www.python.org/dev/peps/pep-0554/#about-subinterpreters](https://www.python.org/dev/peps/pep-0554/#about-subinterpreters).

Speaking of maintenance, let's talk about the other side of the same performance coin — apart from code performance, there is *coding* performance. On many occasions, the ability to write code faster, add changes quickly, and introduce fewer bugs could be even more valuable than the speed of code itself. Thus, let's talk about best practices when it comes to coding and tools that will help you be a better developer.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Using best practices for coding in your project

In this section, we'll switch to another, although adjacent, topic—best practices for maintaining good quality code. Here, we will define "good" in a broad way—as dry, concise, expressive, easy to read, change, and build upon. To illustrate this topic, we will review the `wikiwwii` package we built in [Chapter 15](eb2254cc-485d-4603-9d17-7a442aa5e3b8.xhtml), *Packaging and Testing with Poetry and PyTest*.

All the changes we make to the package throughout this chapter are stored on the `best-practices` branch in this book's GitHub repository.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Code formatting with black

First of all, let's talk about formatting. It may sound like a minor issue—and it generally is—but formatting won't affect your code performance. In a team, however, formatting matters. It improves readability and allows quicker reading through the code; good formatting highlights both typical and non-trivial areas of the code, helping to skim through trivial parts and focus on what's important. At the same time, formatting, if not automated, takes time and can cause arguments within a team, given that PEP8 does not have strict rules on any single aspect, and there are always matters of taste.

Now, there are quite a few tools that help with formatting—and statically finding potential issues in code—including wrong syntax, non-used variables, and so on. These tools are called **linters**. Arguably the most popular linter for Python is **flake8**. Under the hood, it combines three linters:

*   PyFlake 8
*   `pycodestyle` (formerly PEP8)
*   McCabe

Another popular one is `pylama`, which combines seven linters, including the preceding ones, under the hood (it helps with linting docstrings, too!). Among others, there is **Bandit**, **Radon**, and **MyPy**, which specifically check code versus the given type hints. The good news is that many IDEs and code editors support running linters in the background, highlighting potential errors while you code. In order to use one in VS Code, just go to the command palette and type `select linter`—VS Code will offer you a list of supported ones and will install and start running the chosen one all by itself.

You should definitely use linters! However, they were designed to *inform* you, and can be configurable (for example, to ignore specific errors). To automate the process further—and make everyone on the team follow the same set of formatting rules—we will introduce `black`.

`black` is designed to be a deterministic, automated formatter. It is easy to set up as a pre-commit hook (in other words, it will run automatically before every Git commit). Therefore, you don't need to change your personal formatting habits (or lack thereof)—once the code is ready to be committed, `black` will take over and process everything. The best part is that `black` is not configurable, so there is no room for debates in the team regarding which formatting style is the best.

Let's check whether we can improve the readability of our `wikiwwii` package. `black` has a `diff` option and will show which files will be changed without changing them. Let's run this first:

1.  In the repository root folder, type the following in the Terminal:

```
black ./wikiwwii --diff
```

Quite a few lines were affected—`black` replaces all the single quotation marks with doubles, makes sure that the comment symbol is separated from the code by two whitespaces, and so on and so forth. Where possible, it keeps elements on the same line—if not, it will keep every argument on the same indentation level.

2.  Let's run that without `--diff` to reformat our code. Feel free to revise all the changes via VS Code:

![](Images/71948300-d45f-46b8-9ddd-da5bd16d78e8.png)

The preceding is a `diff` visualization (available via the GIT tab) of the file before and after black formatting (on the left, red lines and characters with a minus sign near the line number were removed/modified, while green ones with the plus sign on the right were added or changed).

I think you'll agree that those changes make sense—some of them are more important than others, but still, it definitely looks better than it did before.

3.  Now, how could we set that to run automatically? The easiest way is to leverage another package that deals with GitHub hooks, called `pre-commit`. In order to use it, we'll create a new file in the repository's root and name it `.pre-commit-config.yaml`. Inside, type the following settings:

```
repos:
-   repo: https://github.com/python/black
    rev: stable
    hooks:
    - id: black
      language_version: python3.7
```

With that setting in place, we can run `pre-commit install`, which will "deploy" the preceding settings into a hook.

4.  Finally, we can set a few settings that `black` accepts. As per the developers' recommendation, it is better to set that up via the `pyproject.toml` file:

```
[tool.black]
line-length = 88
target-version = ['py37', 'py38']
exclude = '''
/(
    \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | \.dvc
  | _build
  | buck-out
  | build
  | dist
)/
'''
```

Now, everything should be in place. Let's try committing the changes.

For the first run, the black hook will take a few seconds to download and run. From now on, if the code is not formatted on a Git commit, it will be reformatted, and the commit process will halt (so that you can check the commit results). Once you feel safe to proceed, commit one more time, and you're all good. The best part is that once this code is on GitHub, every collaborator will have to format with those exact settings!

Lastly, we want to add `black` to our development dependencies in the `pyproject.toml` file so that our fellow developers get `black` as part of their development environment automatically:

```
[tool.poetry.dev-dependencies]
pytest = "^3.0"
pytest-cov = "^2.7"
pytest-azurepipelines = "^0.6.0"
black = "^19.3"
```

Don’t forget to run `poetry add black` and `poetry update`. For more on `black` (or, rather, the motivation behind it), please check out this video from PyCon 2019 by Łukasz Langa, the creator of `black`: [https://www.youtube.com/watch?v=ia19n_yK4Qs](https://www.youtube.com/watch?v=ia19n_yK4Qs).

Good code formatting is important, and settling on one style within the team is even more so. But what are the other dimensions of good code? And, more importantly, how can we measure them? That's what we'll talk about in the next section.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Measuring code quality with Wily

So far, we've figured out how to keep code formatted, but is this the only factor when it comes to code quality? Of course not; in fact, there are plenty of abstract metrics to take into consideration when it comes to the quality of code, such as the following:

*   Lines of code (the fewer lines there are, the fewer bugs there will be).
*   Cyclomatic complexity, which counts the number of logical branches in the code; for each `if`/`else` loop, or another indentation block, complexity grows by one. 
*   Maintainability index—a measurement that mixes cyclomatic complexity, lines of code, and the number of variables.

But how are those metrics useful? Every task is different, and there are problems that require code that's ...

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Writing tests with hypothesis

Finally, we're going to go back to a topic we've already covered—unit tests. Unit tests are very important; they will give you peace of mind during development—you really don't want to play a whack-a-mole game with your bugs.

Now, testing a data-heavy application is hard. Depending on complex datasets, data structures expose us to dozens of rare, but possible, quirks and edge cases. Often, we don't even need to think of those possibilities, instead focusing on the datasets we have at hand. For example, any function that operates on a dataframe should deal (one way or another) with an empty dataframe, the dataframe of a wrong datatype, a NumPy array, a dataframe of null values, and so on.

One approach to mitigate this problem is to use pre-generated suites of tests that are focused on quirks and possible issues of specific data structures.

To illustrate this idea with an example, let's use `hypothesis`, as follows:

1.  Let's play in a sandbox environment of a Jupyter Notebook. We'll start by importing all the necessary pieces:

```
from hypothesis.strategies import integers, randoms, composite
from hypothesis.extra.pandas import series
from hypothesis import given, strategies as st 
```

2.  Now, we will define a custom strategy (a sample generator). Consider the following code. Here, we are synthetically creating a series of strings that resemble the ones from the Wikipedia entry—they do have numbers and keywords to parse:

```
units = [
    ' men',
    ' guns',
    ' tanks',
    ' airplanes',
    ' captured'
]

def generate_text(values, r):
    r.shuffle(units)
    result = ''
    for i, el in enumerate(values):
        result += str(el)
        result += (units[i] + ' ')

    return (values, result.strip())

StrSintetic = st.builds(generate_text, 
                 st.lists(st.integers(min_value=1, max_value=2000), 
                          min_size=1, max_size=5), 
                          st.randoms()) 

SyntSeries = series(StrSintetic)
```

3.  Now, we can pass `SyntSeries` as a sample value for our tests:

```
@given(SyntSeries)
def test_parse_casualties_h(s):
    from wikiwwii.parse.casualties import _parse_casualties

    values = _parse_casualties(s)
    assert ( values.sum(1) > 0).all(), values
```

A new sample will be generated every time. It won't be completely random, however — strategies memorize previous examples—and failed tests—and will start with the values that failed on the previous runs, and new examples if everything prior passed. This particular test has passed.

4.  Just to illustrate, let's add an explicit case of an empty string—it will be raised. Parsing the empty strings will result in a zero value sum:

```
@given(SyntSeries)
@example(pd.Series(["", ""]))
def test_parse_casualties_h(s):
    from wikiwwii.parse.casualties import _parse_casualties
    values = _parse_casualties(s)
```

The output on adding an explicit case of an empty string is as follows:

```
> assert (values.sum(1) > 0).all(), values.to_string()
E AssertionError: killed wounded captured tanks airplane guns ships submarines
E 0 0.0 0 0 0 0 0 0 0
E 1 0.0 0 0 0 0 0 0 0
E assert False
```

As we can see, this failed. Note that if you run the code for a second time, it will fail faster—Hypothesis will run the same failed sample first. Those generators, called strategies, are the main superpower of the package. Due to this, Hypothesis ensures that the code behaves well not only on a few hand-picked cases but also in the wild when fed with synthesized datasets. The test we added may seem not-so-useful (we tested that function before), but it will be quick to catch if we break parsing by mistake—and will start with the failed case on the next run to check whether the code was fixed. It also has a set of smart strategies that have been built for the most popular datatypes.

Hypothesis is a great tool for data-driven testing, as it will automatically generate most of edge cases and make us cover edge cases we haven't even thought about. Because of that, it proves to be a valuable asset for any data-heavy application.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Beyond this book – packages and technologies to look out for

Throughout this book, we've shared a wide range of Python frameworks and libraries for data-driven development. However, there are some tools we couldn't fit in, but that you need to be aware of. We'll discuss some of them here. In particular, we want to cover three somewhat connected topics—Python flavors, Docker containers, and Kubernetes.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Different Python flavors

In the *Numba* section, we showed you how to use Numba to speed Python code up. To do so, Numba uses a modern compilation engine. It does so by exploiting the C nature of Python. Another project, Cython, does the same—it compiles Python code into C using a somewhat different approach.

A third (or, chronologically, the first) option is PYPY (not to be confused with PYPI)—a totally separate interpreter for the Python language. Compared to Numba and Cython, PYPY does not need any changes to be made in the code itself—all the optimization is done under the hood in the interpreter. While this is convenient, the problem is that PYPY requires some work since it needs a proper installation of Numpa, `sklearn`, and basically any other beyond-simple-Python package, so it is rarely used on data-heavy applications.

But there is a whole slew of other options as well! For example, Jython (as you can guess from the name) is a Java-based Python interpreter, which can come in handy if you want to integrate your Python code as part of broader Java code or applications. Another, known as Brython, is a JavaScript-based interpreter that you can use to write both the backend and frontend of your website in Python. In fact, there is a package called `vue.py` ([https://stefanhoelzl.github.io/vue.py/](https://stefanhoelzl.github.io/vue.py/)), based on the Brython and Vue frontend framework, that attempts to cover both backend and frontend web development at the same time. Of course, we should note that while Numba and Cython try to make Python faster, Brython's goal is to run JavaScript via a Python interface. Due to this, performance is a lot slower.

Something that sits aside from other projects is the PyIodide project. It does not mimic Python in any other language. Instead, it compiles it into WebAssembly format—a special type of binary format that can be executed in a browser, and so anywhere you can open a browser—whether it be a mobile phone, a tablet, or a smart fridge. It can also interact with web pages similar to JavaScript, on any major browser! While being somewhat slower, this approach works and is very promising.

As an example, Pyodide offers a notebook-style application to try out ([https://github.com/iodide-project/pyodide](https://github.com/iodide-project/pyodide))—it almost looks like Jupyter—except that this time, there is no Python server—everything (for example, `matplotlib`, `sklearn`, and so on) runs on your machine. Here's what it looks like:

![](Images/f8ed274c-56b2-4fde-8b28-2b8d87e74e52.png)

Similarly, the PyIodide package can run other languages—Rust, Go, and more—all in a browser, with no installation required. For more information on PyIodide, check out this video by Michael Droettboom for PyData New York, 2018: [https://www.youtube.com/watch?v=iUqVgykaF-k](https://www.youtube.com/watch?v=iUqVgykaF-k).

Another far-fetched but very interesting application for PyIodide (and WebAssembly in general) is as a lingua franca for packaging so that you don't need to worry about adding dependencies, Python, and so on—just download the file and run it. But this is all perspective—for now, we have Docker containers to do that.

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Docker containers

Docker containers allow developers to isolate and package certain code and some parts of the surrounding operating system as a binary file. This file can then be run on any other machine with a similar OS, with no changes needed. Because Docker images do not include the whole OS, they are relatively small (a few gigabytes in size) and can be pulled over the internet. At the same time, they are fairly isolated and can be run with little exposure. Multiple containers can run on the same machine at once. Using Docker software, an image can be compiled into a set of layers, similar to how classes inherit from each other. For this compilation, you should use a short text file, commonly called a `Dockerfile`, that's convenient to ...

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Kubernetes

Kubernetes (also known as K8S) is an orchestrating engine that operates over a pool of machines—physical or virtual—and can spawn and wind off containers dynamically. Technically, any container technology can be used, but Docker is by far the most popular one. 

For example, it is typical to use Dask on Kubernetes—in this case, Kubernetes will spawn more worker machines when you need them, and can either shut them down or switch to other users, once you're done—all without your intervention. Similarly, it can preserve a composition of containers performing different roles—for example, a load balancer for a web API, which then will redirect requests to different workers, who might operate on one or many database servers—all under the control and orchestration of a Kubernetes server.

This may seem like an approach that's too large and complex for a beginner developer and small services, but it is way easier to operate your services that way: you won't need to log and manually set up specific containers. Many technologies, including load balancers, databases, and so on, have pre-generated images already. In fact, you can even find and reuse predefined instructions on how to start running systems as a whole—including multiple servers—by default.

For more context on this technology, check out this video on *Using Kubernetes for Machine Learning Model Deployment*, by Niels Zeilemaker, PyData Amsterdam, 2017: [https://www.youtube.com/watch?v=f3I0izerPvc](https://www.youtube.com/watch?v=f3I0izerPvc).

Being a developer means constantly learning new things. I can guarantee that, next year, there will be at least a couple of new, cool packages that every developer should learn about. New technologies usually boost productivity and streamline the development process. The secret to productive learning is understanding the scope and requirements of what you actually need—not jumping on a new cool thing just because it is cool. 

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Summary

In this final chapter, we covered multiple topics on code performance and quality and discussed a few important technologies beyond Python. In particular, we discussed how the combination of efficient code, a better understanding of requirements, and smart usage of appropriate data structures can significantly speed up the performance of code—in our case, a hundred times more performant! Then, we discussed how we can deal with big data by computing in parallel on multiple CPUs—or multiple machines in the cluster.

In the second part of this chapter, we discussed a few ways to keep code quality under control—by running sophisticated non-deterministic test suits, automating code formatting, and tracking code maintainability. 

Both code ...

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Questions

1.  How can we measure which line in the code took the most time to complete?
2.  Does NumPy run faster than Pandas?
3.  When should we use Numba? What are the challenges and benefits of using Numba?
4.  When should we use Dask?
5.  Does code formatting matter? Why is Black better than linters?
6.  How does Hypothesis help you test your code?

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Further reading

*Architectural Patterns and Best Practices with Python*, by Anand Balachandran Pillai: [https://www.packtpub.com/application-development/architectural-patterns-and-best-practices-python-video](https://www.packtpub.com/application-development/architectural-patterns-and-best-practices-python-video)