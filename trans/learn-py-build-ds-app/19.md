        

# Luigi 的数据管道

到目前为止，我们一直将代码作为单独的笔记本和脚本来编写。在前一章中，我们学习了如何将这些脚本分组到一个包中，以便它可以被正确地分发和测试。然而，在许多情况下，我们需要按照严格的时间表来执行某些任务。通常，需要它来处理某些数据——完成分析、从外部来源收集信息或重新训练 ML 模型。所有这些都容易出错:任务可能依赖于其他任务，一些任务不应该在其他任务之前运行。重要的是，任务应该易于编排、监控和重新运行，以便于使用。

在本章中，我们将学习构建和编排我们自己的数据管道。建设良好的管道是一项重要的技能，对于掌握这项技能的人来说，它可以节省大量的时间和压力。

特别是，我们将涵盖以下主题:

*   ETL 管道简介
*   Building the first task in Luigi
*   Scheduling with cron
*   Time-based task scheduling
*   Structuring with Luigi

        

# 技术要求

对于这一章，我们将使用一个名为`luigi`的包。最后几个任务将需要另外两个包— `boto3`和`sqlalchemy`。我们还将使用我们在[第 15 章](eb2254cc-485d-4603-9d17-7a442aa5e3b8.xhtml)、*中构建的`wikiwwii`包，用 poem 和 PyTest* 进行封装和测试。您可以按照一章的说明自己构建它，或者运行以下命令来安装它:

```jl
pip install git+https://github.com/Casyfill/wikiwwii.git
```

所有的代码都在资源库中，在`Chapter16`文件夹中([https://github . com/packt publishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications))。

        

# ETL 管道简介

数据管道非常重要，而且无处不在。即使只有少量在线业务的组织也运行着自己的工作:数以千计的研究机构、气象中心、天文台、医院、军事基地和银行都运行着它们的内部数据处理。

数据管道的另一个名称是 **ETL** ，代表**提取**、**转换**、**加载**——每条管道的三个概念部分。乍一看，这项任务可能听起来微不足道。在某种程度上，我们的大多数笔记本都是 ETL 工作——我们加载一些数据，使用它，然后将它存储在某个地方。然而，建立和维护一个好的管道需要一个彻底和一致的方法。流程应该是可靠的、易于重新运行的和可重用的。特定的任务不应该运行超过一次，或者如果它们的依赖关系没有得到满足(比如说，其他任务还没有完成)。

然而，这并不是什么新事物 ETL 和相应的编程解决方案的市场已经建立起来了。市场上有相当多的框架，既有企业级的，也有开源的。举几个例子，有气流，弹球，阿兹卡班，泡沫，以及其他十几个。还有一个新来的孩子，级长。气流可以说是目前最受欢迎的。Airbnb 开发的 Airflow 允许使用任意任务(不一定用 Python 编写)在集群上运行任务，并从 web 仪表板协调它们——用户不需要在他们的机器上安装任何东西。Airflow 是一个很好的工具，但是部署和维护起来需要更多的麻烦，这使得它成为这本书或任何一次性简单管道的艰难选择。

在这一章中，我们将使用`luigi`，一个相对轻量且灵活的框架，它允许管道在本地运行，不需要远程服务器，并且开销相对较小。正因为如此，`luigi`很容易使用和试验，对于构建任何过程，甚至是一个小的过程，都可能非常有用——例如，训练一个机器学习模型。

让我们从概念概述开始介绍。`luigi`基于几个核心原则:

*   Every project should be represented as a **Directed Acyclic Graph** (**DAG**), where each node represents one logical step in the process, usually called **tasks**. Edges define the sequence and dependency of each task. Some tasks will depend on the completion of one or a few other tasks.
*   Tasks can be parameterized (say, run for a specific date) and can send information (including those parameters) to each other.
*   For every task, there is a simple and unambiguous way to check whether the task is complete. The scheduler will not run the same task twice.

在`luigi`中，dag 不是预定义的——您对任务进行操作，为每个任务指定其结果(比如它将存储其数据的文件或表)和依赖关系，以及哪些任务需要在此任务之前运行。下面是一个`luigi`任务的基本形式。如您所见，它继承自一个模板类，有一个参数`date`，和两个方法`output`和`run`:

```jl

class MyTask(luigi.Task):
    date = luigi.DateParameter(default='2019-06-01')

    def output(self):
        return luigi.LocalTarget(f'./data/data/{self.date:%Y/%m-
        %d}.csv')

    def run(self):
        # do stuff
        # ...
        data.to_csv(self.output().path)

```

Luigi 应该通过类继承来使用。在执行时，`luigi`将执行以下操作:

1.  Check whether `MyTask` is complete, using its `complete` method, built into `luigi.Task`. By default, it returns true if the output file exists.
2.  If not, `luigi` will check whether `MyTask` has any dependencies, defined in the `requires` method. As we didn't override the method, `luigi` uses the one from the template, which returns no dependencies. If a task does have dependencies, and they are not complete (as shown in the preceding code), the scheduler will look at them first (go to the start of this list). Once they are executed successfully, the scheduler will eventually switch back to this task.
3.  As `MyTask` doesn't have any dependencies, it will be executed immediately, by running the `run` function.
4.  The parameter (`date`) will be encoded in the output path.

对于我们添加的每一个任务，关键是定义(覆盖)`run`、`requires`、`output`，如果需要的话，还有`complete`函数(例如，如果任务生成了多个文件，并且您想要通过特定文件的存在来检查完整性)。

        

# 将您的代码重新设计为管道

但是如何定义管道呢？为了保持代码的成本效率和低维护性，拆分代码的最佳步骤是什么？根据我们的经验，这主要取决于两个因素的组合:

*   How reliable is the data source?
*   How critical is this information, or will the data be available to re-pull after a while?

根据经验，如果数据集是外部的，因此不可靠(例如，维基百科页面或开放数据门户)，我们建议将注入管道分成不同的步骤。在第一步中，您将按照提供的方式收集所有数据——比方说，存储整个 HTML 页面。对于数据，使用 JSON 或 CSV——没有严格的模式。存储原始数据后，您可以...

        

# 在 Luigi 建立我们的第一个任务

幸运的是，`luigi`允许我们从小处着手。我们将使用我们的`wikiwwii`包中的代码，从构建一个任务开始，这个任务将拉动战斗中的所有链接。首先，我们将在一个单独的文件中导入所有我们需要的东西，`luigi_fronts.py`:

```jl
# luigi_fronts.py
from pathlib import Path
import json

import luigi
from wikiwwii.collect.battles import collect_fronts
URL = 'https://en.wikipedia.org/wiki/List_of_World_War_II_battles'
folder = Path(__file__).parents[1] / 'data' 
```

在这里，我们为战斗声明了一个链接，导入了我们的`collect_fronts`函数，并指定了一个相对文件夹来存储数据。现在，让我们来编写任务本身。在下面，我们将创建一个任务类，将 URL 定义为一个带有默认值的`luigi`参数(稍后将详细介绍)，并添加(或者更确切地说，覆盖)两个方法— `output`，它返回一个带有`path`数据的本地目标，以及`run`，它描述要运行的实际代码—它收集数据并将其写入在`output`中定义的文件。实际上，这里的实际业务逻辑只有一行——这要感谢我们在前一章中制作的`wikiwwii`包:

```jl
class ScrapeFronts(luigi.Task):
    url = luigi.Parameter(default=URL, description='page url')

    def output(self):
        name = self.link.split('/')[-1]
        path = str(folder / f'{name}.json')
        return luigi.LocalTarget(path)

    def run(self):
        data = collect_fronts(self.url)
        with open(self,output().path, 'w') as f:
            json.dump(data, f)
```

让我们运行它！Luigi 为此提供了一个方便的命令行界面:

```jl
$ python -m luigi --module luigi_fronts ScrapeFronts --local-scheduler
```

这里，我们指定要使用的文件`luigi_fronts`和一个特定的任务`ScrapeFronts`。因此，您应该会得到以下摘要:

```jl
===== Luigi Execution Summary =====

Scheduled 1 tasks of which:
* 1 ran successfully:
 - 1 ScrapeFronts(url=https://en.wikipedia.org/wiki/List_of_World_War_II_battles)

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====
```

这里最重要的指示器是笑脸——这意味着一切运行顺利——正如您所看到的，数据文件已经创建。为了实验起见，请尝试再运行一次。这将导致以下结果:

```jl
===== Luigi Execution Summary =====

Scheduled 1 tasks of which:
* 1 complete ones were encountered:
 - 1 ScrapeFronts(url=https://en.wikipedia.org/wiki/List_of_World_War_II_battles)

Did not run any tasks
This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====
```

这意味着*我检查过了，结果文件已经在那里了*。如果我们仍然需要重新运行任务，我们必须删除或重命名该文件。

        

# 将这些点连接起来

到目前为止，我们只创建了一个任务。即使就其本身而言，它也有一定的价值，因为它将工作和产出形式化了。现在，让我们添加任务来收集战斗数据。它看起来与前一个非常相似——我们创建了一个任务，继承自`Task`类:

```jl
# luigi_battles.pyfrom misc import _parse_in_depthfrom luigi_fronts import ParseFrontsclass ParseFront(luigi.Task):    front = luigi.Parameter()    def requires(self):        return ScrapeFronts()         def output(self):        path = str(folder / 'fronts' / (self.front + '.json'))        return luigi.LocalTarget(path)     def run(self):        with open(self.input().path, 'r') as f:            fronts = json.load(f)        front = fronts[self.front]        result = {}        for cp_name, campaign in front.items(): result[cp_name] = _parse_in_depth(campaign, ...
```

        

# 了解基于时间的任务

管道对于安排数据收集特别有用，例如，每晚下载新数据。

假设我们想要收集纽约市前一天每天早上 311 电话的新数据。首先，我们来写拉取函数本身。代码相当简单。你可以通过这个链接看一下**索克拉塔**(纽约使用的数据共享平台)API 文档，[https://dev.socrata.com/consumers/getting-started.html](https://dev.socrata.com/consumers/getting-started.html)。唯一棘手的部分是数据集可能很大，但 Socrata 不会一次给我们超过 50，000 行。因此，如果输入的长度等于 50，000，很有可能数据被封顶，我们将需要使用偏移量进行另一次拉取，一次又一次，直到行数变少。参数中的`resource`代表数据集的唯一 ID——可以从数据集的网页中获取:

```jl
def _get_data(resource, time_col, date, offset=0):          
    Q = f"where=created_date between '{date}' AND '{date}T23:59:59.000'"
    url = f'https://data.cityofnewyork.us/resource/{resource}.json?$limit=50000&$offset={offset}&${Q}'

    r = rq.get(url, headers=headers)
    r.raise_for_status()

    data = r.json()
    if len(data) == 50_000:
        data2 = _get_data(resource, time_col, date, offset=(offset + 
                50000)
        data.extend(data2)

    return data
```

现在，让我们来编写任务本身。它实际上相当短——我们所做的就是定义一个资源，`time_column`(我们将在其上查询 API)，以及要获取的日期:

```jl
class Collect311(luigi.Task):
    time_col = 'Created Date'
    date = luigi.DateParameter()
    resource = 'fhrw-4uyv'

    def output(self):
        path = f'{folder}/311/{self.date:%Y/%m/%d}.csv'
        return luigi.LocalTarget(path)

    def run(self):
        data = _get_data(self.resource, self.time_col, self.date, 
               offset=0)
        df = pd.DataFrame(data)

        self.output().makedirs()
        df.to_csv(self.output().path)
```

该任务基本上可以在 Socrata 提供的任何数据集上使用——您所需要的就是指定`resource`和要查询的列。数据参数还定义了文件的结果路径。现在，我们可以从命令行运行该任务:

```jl
$ python -m luigi --module luigi_311 Collect311 --date 2019-06-07 --local-scheduler
```

我们现在甚至可以使用`DateRange`特性批量运行这个任务:

```jl
$ python -m luigi --module luigi_311 RangeDaily --of Collect311 --start 2019-06-01 --days-back 10  --local-scheduler
```

前面的代码将生成多达 10 个任务，10 天中的每一天都有一个任务—今天和 10 天前，但不会超过 2019 年 6 月 1 日。这个技巧对于计划来说特别好——即使有些事情会阻止任务在当天运行，它也会在第二天或第三天重新运行，以此类推。

        

# 使用 cron 进行调度

我们已经多次提到日程安排——但是我们怎么做呢？不管是好是坏，这不是 Luigi 自己能够做到的。但是不用担心——对于您的操作系统来说，这是一项合适的任务。在 Windows 上，可以使用`Schtasks`实用程序来完成。在 macOS 和 Linux 上，调度由`cron`工具通过所谓的`crontabs`来管理。

Crontab 有自己的调度迷你语言:每个命令都以五个符号开始，代表特定的分钟、小时、日、月和星期几(当我们没有特定值时可以使用星号)。例如，如果我们想在每天 05:00 运行一个任务，我们将使用以下命令:

```jl
0 5 * * * <our command>
```

Crontab 也支持多个值。...

        

# 探索不同的输出格式

在用 cron 调度*部分的代码中，我们使用了本地目标，写入我们计算机的文件系统。在现实世界的场景中，这很少能满足需求——您可能会写入存储在云中的数据库或文件。事实上，如果没有理由不写的话，我们强烈建议你从一开始就把任务写到云上(例如，S3 桶)。Luigi 支持 FTP、S3、Azure Blobs、Google Cloud、Spark、MongoDB、SQL 数据库等等。唯一的问题是创建这些资源并设置访问它们的凭证。对他们中的许多人来说，最好的部分是界面非常相似，所以很容易将*目标*替换为现有的任务，只需更改几行代码。*

        

# 写给一个 S3 桶

S3 桶和类似的 blob 存储服务已经被证明是一个很好的工具。鉴于其低廉的价格和易用性，它们无疑是云中共享数据交换的最佳解决方案。在这里，我们不会深入讨论 S3 和亚马逊网络服务。相反，我们将展示如何修改您现有的管道，将它们重定向到 S3。除了数据在云中，这还有共享状态的好处——如果另一台计算机或用户试图运行管道，他们会发现数据已经在那里了。

让我们假设您已经注册为 AWS 客户，并且拥有一个 S3 桶。为了使用 buckets，`luigi`使用了`boto3`包，这是一个由 Amazon 构建的用于 AWS 相关操作的官方库。这意味着...

        

# 写入 SQL

在许多情况下，写入数据库比写入平面文件更好或更方便。让我们用 311 数据管道来说明这种情况。

将数据写入数据库是非常相似的，我们不需要做太多的改变。一个主要的区别是任务完成检测——因为一个明显的原因，没有*文件*来检查是否存在。作为一种变通方法，`luigi`创建了一个实用程序表来存储完整任务的唯一记录。这个过程是框架的组成部分，所以大多数时候，我们没有理由去想它。也就是说，基于 SQL 的管道有两个非常重要的警告:

*   As the task does not result in an isolated task, there is no simple way to pull data from this specific task. A new task or your external code will need to query for the right slice of data.
*   For the same reason, if something goes wrong, you will need to remove both a specific subset of data (as mentioned previously) and a marker record in the utility database.

另一方面，Luigi 有几个助手任务可以利用。例如，我们可以使用一个`CopyToTable`任务对象来代替默认的任务对象。顾名思义，这个任务预先设计了将特定数据上传到特定 SQL 表的一切。为了让它运行，我们只需要添加一些信息。

在我们的例子中，为了简单起见，我们将使用 SQLite 数据库。Luigi 有一套针对 PostgreSQL、MySQL、MSSQL、Hive 等定制的解决方案。它没有针对 SQLite 的特定代码，所以我们必须使用通用的 SQLAlchemy 解决方案。它是这样工作的:

1.  We inherit from the `luigi.contrib.sqla.CopyToTable` task.
2.  The task has to contain certain attributes, including `table` (the table to write to), `connection_string` for the SQLAlchemy connection, and the `columns` iterable, which contains the `sqlalchemy` data types for each column.
3.  Finally, we don't need to override the `run` function, which contains an implementation of data inset. Instead, we will override the `rows` method. This method should return tuples of values, one for each row of data, and is compatible with the columns we mentioned.

让我们看看下面的例子。首先，我们必须定义一个表模式。出于简单和安全的考虑，我们决定将所有内容都保存为字符串，除了`unique_key`，我们将使用它作为主键。一些列具有相当长的值，我们将它们定义为`Text`数据类型:

```jl
from sqlalchemy import String, Integer, Text

COLUMNS_RAW = [
    (["address_type", String(64)], {}),
    (["agency", String(64)], {}),
    (["bbl", String(64)], {}),
    (["borough", String(64)], {}),
    ...
    (["y_coordinate_state_plane", String(64)], {}),
    (["resolution_action_updated_date", Text()], {}),
    (["resolution_description", Text()], {}),
    (["location", Text()], {}),
    (["unique_key", String(64)], {"primary_key": True})
]
```

注意，或者，我们可以单独创建数据库，然后在每次运行时从表中提取数据类型— `sqlalchemy`支持这一点。让我们现在重写我们的任务。我们只需要添加一些属性，将`run`重命名为`rows`，并使其产生行。似乎最简单的方法是通过`df.values.tolist()`。考虑以下代码:

```jl
class Collect311_SQLITE(sqla.CopyToTable):
    time_col = "Created Date"
    date = luigi.DateParameter(default=date.today())
    resource = "fhrw-4uyv"

    columns = COLUMNS_RAW               
    connection_string = SQLITE_STRING   
    table = "raw"                                                            

    def rows(self):
        data = _get_data(self.resource, self.time_col, self.date, 
        offset=0)

        df = pd.DataFrame(data).astype(str)
        df['unique_key'] = df['unique_key'].astype(int)

        for row in df.values.tolist():
            yield row
```

如你所见，我们的任务几乎没有改变——这就是`luigi`的力量！

        

# 用自定义模板类扩展 Luigi

在上一节中，我们使用了`CopyToTable`类作为模板，而不是`luigi.Task`。事实上，这是一个很好的模式！如果有任何自定义配置或代码可以在任务之间使用，请随意创建自己的自定义任务类。例如，在我们的实践中，我们使用一个定制的`S3Task`类，类似于下面的:

```jl
from luigi.contrib.s3 import S3Client, S3Targetimport pandas as pdfrom io import StringIO, BytesIOclass S3Task(luigi.Task):    client = S3Client()    def _upload_csv(df, path):        content = df.to_csv(float_format="%.3f", index=None)        self.client.put_string(            content=content, destination_s3_path=path,             ContentType="text/csv"        ) def _upload_binary(self, ...
```

        

# 摘要

在这一章中，我们学习了如何将我们的代码组成生产级的数据管道，这些管道可以根据需要进行调度和重新运行。构建良好的管道是一项重要的技能，因为它使您能够拥有最新的数据并处理您的业务逻辑(例如，解析信息)，而不是反复运行管道脚本或构建您自己的*自行车*解决方案。这种可靠且健壮的解决方案是将您的代码作为可交付成果进行部署和调度的好方法。在本章的后半部分，我们学习了`luigi`中不同的输出格式和定制模板。

在下一章中，我们将在我们建立的管道的基础上进行构建。我们将使用我们收集的数据来构建几个交互式仪表板，使我们能够监控流程并分析数据的持续趋势。

        

# 问题

1.  What are the benefits of writing tasks rather than using simple scripts?
2.  What is the base element of Luigi jobs?
3.  How are DAGs defined in Luigi? What are the benefits of that architecture?
4.  How can we parametrize a task?
5.  What is the best way to run time-based tasks in bulk?
6.  How can we schedule a job with Luigi?

        

# 进一步阅读

*数据管道*(【https://hub.packtpub.com/data-pipelines/】T2)。