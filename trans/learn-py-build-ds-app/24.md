        

# Assessments

        

# Chapter 1

**What version of Python do we use?**

Throughout this book, we are using the Anaconda distribution, along with Python version 3.7.3.

**Will it work on a Windows PC?**

Absolutely! Python is a cross-platform language and will run on any Windows, Mac, or Linux device. In fact, it can even run on Raspberry Pi, Lego Mindstorms, and Arduino boards!

**Do I need to install any additional packages?**

Not if you've installed them in bulk using the `environment.yaml` file from the repository, or using a Docker image. Otherwise, you need to install packages using PIP or Conda.

**What is a Jupyter Notebook?**

Jupyter Notebook is a special file format, based on JSON, and used by Project Jupyter; in a nutshell, it represents code in an interactive and descriptive manner ...

        

# Chapter 2

**Why do we need to use variables in code?**

Variables work as aliases or symbols in mathematic equations. With variables, we can write business logic, or *how*, without knowing specific values, or *what*, beforehand – we don't have to repeat doing so over and over again.

**What is the recommended way of naming variables? Why does it matter?**

There are a few simple requirements when it comes to naming variables that are mandatory—they can't start with a number, contain whitespaces, or special characters. Finally, none of the keywords that are reserved by Python can be used.

That being said, there is some guidance on to better naming; first of all – PEP8\. According to PEP, it is recommended to name variables meaningfully and consistently so that they are easy to understand. It is also suggested to use "snakecase" (lowercase whitespace represented by an underscore) for functions and variables and "camelCase" (words joined together, with second and further words beginning with an uppercase letter). None of this execution, but it will help you navigate the code.

**What do data types mean and why do they matter for computation?**

Data types define core properties of values—how much memory to allocate for them, what the corresponding methods are, and how other objects interact with them.

**What are the four most popular data types in Python?**

The most popular (common) data types are string, integer, float, and Boolean.

**What does the @ operator stand for? Why doesn't it work?**

Starting with Python 3.5, it is reserved for matrix multiplication, but it is not implemented (neither are matrices) in core Python.

**What are the two operators that will work with strings?**

Addition (`+`) will concatenate two strings together; for example:

```jl
>>> 'Good ' + 'Morning'
'Good Morning'
```

Multiplication by an integer (`*`) will repeat a string a corresponding number of times; for example:

```jl
>>> 'Repeat'*3
'RepeatRepeatRepeat'
```

**How would you combine the results of two tests if we need to return a True value, but only when both of them return True? What about when at least one returns True? What about if only one (but not both) returns True?**

This can be done using the `AND`, `OR`, and `XOR` operators. Take a look:

```jl
>>> (1 > 0) and (0 > -1)
True
>>> (1 > 0) or (0 > 1)
True
>>> (1 > 0) ^ (0 > -1)
False
```

Simple!

        

# Chapter 3

**What are functions, and when should we use them?**

In programming, a function is the named section of the code that encapsulates a specific task and can be used relatively independently from the surrounding code. 

**How can data be provided to functions?**

Conceptually, code in a function can access data from outside. The best way to pass the data, however, is via arguments—special temporary variables used exactly for that.

**What does indentation mean? Is it required?**

Yes; in Python, indentation is required and defines the grouping of code. 

**What should be covered in the docstring function? How can I read the docstring function?**

Ideally, every module, function, and class should have a docstring. In all those cases, a docstring can be shown ...

        

# Chapter 4

**How do we retrieve one element from a list? How do we retrieve the last element of the list without computing its length explicitly?**

To retrieve any element from a list, we can pass its index (order, starting with zero) in square brackets: `mylist[0]` will get the first element. Similarly, negative indices will return elements in reverse order—`mylist[-1]` will get the last element, no matter how many of them are stored.

**How do we get all the elements of a list – except the first one and the last one – ****in reverse order?**

For that, we can use *slicing*. In a slice, the first number represents the start, the second number represents the end, and the third one represents the step. A negative number will lead to the reverse order. Since we're using all three values and the step is negative, we need to swap the start and end values. Since the start is already included but the end isn't, we'll have to shift indices accordingly:

```jl
>>> mylist = [1,2,3,4,5]
>>> mylist[-2:0:-1]
[4,3,2]
```

Alternatively, we can do that in two steps:

```jl
>>> mylist[1: -1][::-1]
[4,3,2]
```

**How do we merge two dictionaries, and what happens if some of the keys are the same in both of them?** 

The best way to merge two dictionaries is to update one with another. In this case, overlapping values will be overwritten by the values of the second dictionary:

```jl
>>> D1, D2 = {'a':1, 'b':2} {'b':3, 'c':4}
>>> D1.update(D2)
>>> D1{'a':1, 'b':3, 'c':4}
```

**What is the best data structure to check for membership?**

The best structure to check for membership is a set.

**Can we get the last element of the generator without getting all the others?**

No, it is not feasible by definition—to get the last value in a generator, you have to go over all previous ones—and besides, generators can be infinite.

**How do we combine elements from N triplets into three arrays of N, one by one?**

For that, there is the `zip` function. Consider the following example (*N* is equal to 4):

```jl
>>> arrays = ((1,2,3), (4,5,6), (7,8,9), (10,11,12))
>>> list(zip(*arrays)) # list is needed for printing
[(1, 4, 7, 10), (2, 5, 8, 11), (3, 6, 9, 12)]
```

**What is the short way of generating a list of specific dictionary properties – which are retrieved from a list of dictionaries – if a certain other property of each dictionary is in the set?**

Here, we need to execute multiple operations at once – filter dictionaries and get a specific value out of them. The shortest way to do that is via one-liners:

```jl
>>> dicts = [{'a':1, 'keep':False},
 {'a':2, 'keep':True},
 {'a':3, 'keep':True}]

>>> [D['a'] for D in dicts if D['keep']]
[2,3]
```

That's it!

        

# Chapter 5

**Can the if clause work with multiple (more than two) logical branches?**

Yes! For that, you can use an additional keyword—`elif`. This way, you can have an unlimited number of logical branches, though it's recommended to use no more than four to five at a time.

**What is the difference between for and while loops?**

`for` loops are explicitly finite—they run for every element in a given iterable (although you can pass an infinite iterable if you need to). They are also meant to use that iterable. 

`while` loops are explicitly infinite until certain criteria are met—so they are good if you don't know the number of iterations it would require to meet them (or want an explicitly infinite loop, which would be stopped from within the loop itself). ...

        

# Chapter 6

**What is an API? Why would we use it?**

An API is a programmatic interface; for example, a way to interact with a given tool or service using code. Generally speaking, any tool can (and many do) have an API; for example, every Python package has some, but usually, it is used in the context of a Web API—in other words, an interface for a certain service that's accessible programmatically via the internet. You use Web APIs all of the time—most applications on your phone communicate with the corresponding servers via their APIs. For us, a Web API is a way to leverage the power and information of web services from within our Python code.

**What do the various HTTP(S) response status codes mean?**

HTTP response statuses are integers that define the status of interactions and are defined by a server. For example, if routing servers can't find a URL you're passing, they will return a famous status, 404\. Similarly, if an interaction with the server is successful, the server will return a code 200, along with the other data. Checking for good status codes is the simplest way to check an interaction status programmatically.

**Is there a built-in library for dealing with HTTP? Why do we use requests instead?**

Indeed, there is the built-in `urllib` package. It is a good option if you don't want to add extra dependencies. For most use cases, however, it is too low-level and requires some boilerplate—it is way nicer to use the `requests` package, with its clean and beautiful interface.

**How do you define command-line interface parameters for Python scripts?**

There are quite a few options—such as the lightweight `docopt` and the production-grade `click`. There is also the built-in `argparse`. To use it, you need to create an instance of a parser and add the parameters you need.

**What does if __name__ == '__main__' mean and why do we need it at the end of a script?**

This is a standard Pythonic way to ensure certain code is executed only when a file is called directly, as a script. On execution, Python assigns a few special variables to the root namespace, including `__name__`. If the script is called directly, its value is equal to `__main__`. Of course, this code can be located anywhere in the code, as long as everything within is defined—but it is easier and cleaner to put it at the end.

        

# Chapter 7

**What does the term web scraping mean?**

Web scraping is the process of collecting information directly from HTML web pages. Just like mining, we have to first collect ore of the HTML, from which we can then refine the valuable data points.

**What are the main differences between scraping and using a web API? What are the challenges?**

The main difference is the lack of any guarantees – there is no promise that the web page won't change in terms of its structure, or will be shown at all. In fact, many services actively attempt to prevent web scraping. Another challenge is processing raw HTML into valuable information, as it often requires some custom code. 

**What exactly does Beautiful Soup do? Can we scrape without it?**

In our stack (

        

# Chapter 8

**What are classes? When should we use them?**

Classes represent a way in which we can create complex objects, with the corresponding data (attributes) and functions (methods). Classes are a useful concept to represent any entity, such as a database connection, file object, algorithm, and so on. There's also a set of special methods and variables that's used by Python to change the behavior of certain instances.

**Can we compare two instances of a class or use arithmetic operations with them?**

Yes—this is one of the use cases for special methods. For example, in order for us to check instances for equality, we need to set the `__eq__` method of the class. Here, we are checking whether the instance is greater, smaller, and so on—there is a corresponding special method for each operation.

**When should we use inheritance?**

Inheritance is an important property of classes. By inheriting from another class, you let your class acquire all the methods and attributes of the class you're acquiring from. If there is an overlap in the names, the values of your class are preserved. Inheritance is useful to avoid repetition; for example, if multiple classes have shared attributes or methods, it makes sense for all of them to inherit from the base class. Another frequent case for inheritance is to use a base class with some non-trivial behavior, for example, that's provided by an external framework, and override special attributes and methods that will be executed.

**What is the use case for data classes?**

Data classes are merely syntactic sugar—they simplify the code that's required to create a class and provide some properties, such as equality, initialization, and hashing, by default. They are extremely useful if your class is mostly required to store data.

        

# Chapter 9

**What is a shell? Why and when are command-line interfaces advantageous compared to graphical interfaces?**

A shell is a user interface that you use to interact with the operating system of a computer. Usually, people use this term to refer to a command-line shell that allows you to control the OS with a set of textual commands. There are three main advantages of command-line interfaces over GUIs. First, textual commands can be combined and stored and thus form scripts. Second, they require a minimal amount of memory and thus are way more suitable for interacting with remote machines via the internet. Third, command-line interfaces are quite unified across different operating systems—commands on Linux and macOS are identical, and even ...

        

# Chapter 10

**Why should we use a special stack of packages for data analysis?**

Data analysis requires a fast and easy way to operate on multiple elements at once—a so-called vectorized approach. Python's scientific stack allows this by using `numpy`—a package for fast array operations.

**Why are NumPy computations so fast compared to normal Python?**

NumPy is drastically faster than vanilla Python on numerical operations. This is all thanks to a different data representation—NumPy arrays, in contrast to standard Python collections, require all the elements to be of the same data type. Because of that, an array can be passed to a CPU as one entity and computed more effectively.

**What is the use case and benefit of using Pandas over NumPy?**

NumPy only supports numeric arrays. Pandas, on the other hand, supports datetime, string, and categorical arrays. In addition, it has tons of helpful functions and operations that are useful for everyday data processing, such as groupby aggregation, resampling, and plotting.

**What does sklearn stand for?**

`sklearn` stands for **SciPy kit for machine learning** and has this name due to its origin as a SciPy subpackage.

        

# Chapter 11

**Why, if there is an empty cell in the Pandas column, are integer values in this column converted into floats?**

This happens since NumPy (and based on it, Pandas) does not support null integers—every null is a special case of a float. Thus, to keep the datatype consistent across the column, NumPy has to convert all integers into floats.

**What is the benefit of plotting missing values?**

Often, missing values in a dataset can have a certain pattern—for example, records with a missing value in one column also miss values in others. Having a bird's-eye view allows you to find those patterns and define an appropriate imputation strategy.

**What is RegEx? Is it a separate language?**

Indeed, Regular Expressions, or regex, is a distinct mini-language ...

        

# Chapter 12

**How can we understand some general properties of a dataset with pandas?**

Using either specific statistics, such as mean, median, or standard deviation, on specific columns. Alternatively, you can use the `describe` method—it will compute descriptive statistics (the ones above it, plus the minimum/maximum, quartiles, and a few more) for all the columns in a dataframe.

**What does the resample function do in pandas? How is it different from aggregation?**

This method is meant to be used on a dataframe of time-based records. `resample` works similar to aggregation, except that it groups by a time period and returns rows (with empty values) for missing periods as well.

**How does visualization work in pandas?**

Pandas has an extensive and simple interface for visualization, but it doesn't create charts on its own; all the actual visual stuff is done by `matplotlib`. Starting with version 0.25, `pandas` allows other visualization engines to be used instead.

**What are the benefits of declarative data visualization (for example, with Altair)?**

There are multiple benefits to this approach. First, declaration (also known as a specification) is decoupled from the engine – so, in theory, it can be used with different engines. Next, specification is also decoupled from the data, and so it can be used on different datasets with no change. Third, it is decoupled from the aesthetical parts, so colors, fonts, and margins can be defined externally and easily adjusted outside of the specification. As a result, the declarative approach allows for a very flexible and effective workflow, allowing ease of change, iteration, and reuse.

**In which cases can big data visualization be useful?**

Big data visualization can be extremely useful if you wish to understand the overall distribution of the dataset. This is especially true if you're working with spatial data, networks, or embeddings.

        

# Chapter 13

**What is machine learning?**

Machine learning is a discipline (a branch of artificial intelligence) that focuses on automatic model building. Machine learning algorithms allow us to automatically find patterns or a hierarchy in data (unsupervised learning), or even predict the property of a given sample after training on the prepared "training" dataset (supervised learning).

**What is the difference between supervised and non-supervised learning?**

Unsupervised learning algorithms operate on any given dataset with no special preparation required and aim to find patterns or structures without any prior knowledge. Supervised learning models are trained on a properly labeled "training set," which they do by building a generalized model, ...

        

# Chapter 14

**What is overfitting?**

Many ML models (for example, decision trees) actively fit to perform well on the training set at hand, but at some point, this process goes beyond generalizable knowledge that's valuable for the task, with some parts being irrelevant to the test set. This is not only meaningless but will also affect the model's performance on other data. This phenomenon is known as overfitting, and there are ways to overcome it.

**Why should we use cross-validation?**

Cross-validation is a technique that's aimed at overcoming the issue of overfitting. In its basic form, it splits a training set into multiple folds, trains multiple models with the same settings on different combinations of those folds, and measures their performance on other folds—and then averages the performance across all models. As a result, this sampling and prediction on the data each model never saw prevents it from reporting "better" results on a dataset by addressing its specifics. Thus, using cross-validation allows for safe feature-engineering and model adjustments.

**Why can it be bad if our metrics are improving on the test set? Which features are useful for improving model performance on cross-validation?**

Improvements on the test set can represent an actual increase in model performance—or overfitting. To improve a model's "actual" performance, you need to either tune the model's parameters, add new features or process existing features to better represent underlying dependencies. One usual trick, for example, is to convert date features into a set of features representing cycles, such as day of the week, time of the day, month/day of the year, and so on.

**Why do some features decrease the performance of a decision tree on test data or in cross-validation?**

Certain features with little to no value for prediction but a high enough variance "appear" to be useful for decision trees (and other algorithms) on their training sets – and thus lead to a decrease in out-of-sample performance. To prevent that, you need to either filter features thoroughly or use algorithms that have fewer issues with overfitting.

**What is the difference between the random search and grid search algorithms for parameter optimization?**

Both algorithms are designed to find the best combination of the model's hyperparameters—a set of parameters that can only be optimized by running the model on a specific dataset. GridSearch is the most simple, brute-force solution – all it does is run the model over a finite number of combinations ("grid") of those parameters. Due to dimensionality reduction, even a small, finite number of choices for each quickly leads to huge computations. Random search, on the other hand, is similar, but does not require a finite set of choices, instead deriving from distributions, and attempts to pick each other combination based on the results of the previous runs. As a result, it is a faster and better-resulting solution to use than GridSearch in most cases.

**Why is Git not sufficient for data version control?**

Git, at its core, is an immutable file-based system – which means that, on each commit, it stores a copy of each file that was changed since the last commit. Thus, any kind of change in any dataset beyond basic metrics will result in a copy of the whole file, which will quickly lead to huge memory consumption. In fact, GitHub prevents uploading files above a certain threshold, so using Git to control data is not an option for data version control.

**What are the alternatives to DVC for data version control and experimentation logging?**

Currently, there are plenty of alternative solutions to data version control. All of them have different flavors and focus on different aspects. Among the most popular alternatives are MLflow and Sacred, but both are language-specific and require some custom code.

        

# Chapter 15

**What are the benefits of packaging code?**

Packaging code is a great way to do the following:

*   Make certain code available to use from multiple other packages
*   Share code with colleagues or make it easy to install for yourself
*   Set a project to collaborate on with others
*   Add reliability to your code by constantly running tests
*   Structure code better and isolate it from your day-to-day work

**What is the main difference between Conda and pip as package managers?**

At this moment, the difference is not as great as it was before. Historically, pip didn't support adding non-Python code as a binary for various reasons. This is a problem for data analysis projects since many data-related packages, namely NumPy, SciPy, and `sklearn`, use C and ...

        

# Chapter 16

**What are the benefits of writing tasks rather than using simple scripts?**

Scripts are great for simple and one-off jobs. If you have a repetitive task to do – or even more so if there is a set of tasks that depend on each other, and you need to ensure that they don't run without a dependency missing, or that they won't override (or append to) existing data—then ETL pipelines and tasks are for you. As a free bonus, frameworks such as Luigi have a lot of utility code that helps to build pipelines – you won't need to write a solution for writing to S3 or a database, or parse a command-line command.

**What is the base element of Luigi jobs?**

The base element of Luigi jobs (pipelines) is the `Task` class. All the business logic of a task needs to be wrapped in the `run` method. Its output and dependencies are defined within the `output` and `requires` methods.

**How are DAGs defined in Luigi? What are the benefits of that architecture?**

Luigi forms DAGs (pipelines) automatically; there is no need to set them up explicitly. To define a DAG, you need to run the last task in the pipe—Luigi will check for its dependencies if they are not met, will check for theirs, and so on. Once the queue of tasks to computing is ready, Luigi starts to compute them, one by one, starting with the earliest dependency—and adding others once their requirements are met. 

This allows the pipeline to be flexible and easy to build, one step at a time. If something "external" to the pipeline task needs to be dependent, all it needs is to refer to a task.

**How can we parameterize a task?**

To parameterize a task, all we need to do is set a task attribute to be of the `luigi.Parameter` type, or its derivative. Once set, the parameter can be used as an argument that's passed on class initiation, or passed on the command line. Parameters can be used to run the task on a specific subset of data, or with a specific mode – for example, you can pass a production flag that will direct the dataflow to the production database or staging if the flag is not raised. 

**What is the best way to run time-based tasks in bulk?**

For time-based jobs, Luigi provides built-in functionality for bulk execution – with the main focus on backfill. By using the `DailyRange` (or other ranges) built-in utility, you can pass either the start and end date, or one of those and a number of days to fill. The program will automatically spawn and execute the given task for each day in this range. However, this has one caveat—a task can only have one `DateTime` parameter, which will be used.

**How can we schedule a job with Luigi?**

Luigi itself does not provide a scheduling mechanism. To schedule a task, an external tool such as cron should be used. Cron is a tool that's used for scheduling arbitrary tasks and is built into all Mac and Linux OS systems. Windows has its own similar tool such as `schtasks` or PyCron.

        

# Chapter 17

**What are the main differences between visualizing data in the notebook and on a dashboard?**

The main differences are as follows:

1.  The audience for the dashboard is meant to be wide—so the dashboard should be easily accessible, for example, via an internet browser, and well-explained. One-off visualizations, on the other hand, are often made for self-consumption, and thus don't need to be self-explanatory.
2.  Dashboards are meant to be frequently updated and exploratory. Visualizations are often static and show a specific aspect of data.

**Why do we call some dashboards "static"? What are the pros and cons of a static dashboard?**

In common terms, *static* web pages are ones that are provided "as-is," as flat files, and there is no active ...

        

# Chapter 18

**What is the REST API?**

REST, or REpresentational State Transfer, is a general architecture for APIs interaction that uses the HTTP protocol. The main features of REST-compliant systems are being stateless and their separation of concerns between the client and the server.

**What Python packages can be used to build a REST API?**

At this point, there are quite a lot of frameworks that can be used to build a REST API in Python. The most popular ones are Flask, Django REST, Hug, Falcon, CherryPy, Quart, and many others. In this book, we're using the FastAPI framework.

**What are the key features of the FastAPI framework?**

FastAPI has a few unique characteristics. First, it is designed specifically with API in mind, which is different to many others. Second, it fully supports asynchronous execution and can work with a Uvicorn-Gunicorn inspired asynchronous server. Third, it makes use of type annotations, using them to generate interactive documentation (OpenAPI) and, most importantly, to validate passed data automatically. Finally, it provides simple tools for the validation and conversion of data—all that's left is to write the business logic.

**Why OpenAPI (Swagger)?**

OpenAPI, previously known as Swagger, is an API description specification that allows you to define an API with a simple file and then generate web-based interactive documentation from that file. In fact, while the documentation is autogenerated by FastAPI, this specification can also be used to do the following:

*   Autogenerate client libraries in multiple languages, including Python
*   Autogenerate API stubs (API-serving server code templates) from the file, also in multiple languages

**Why do we need Uvicorn or Gunicorn servers?**

Both Uvicorn and Gunicorn are **Web Server Gateway Interface** (**WSGI**) tools. Their job is to deploy multiple instances of an application, restart them if needed, and pass requests to them and responses back to the client. WSGI servers usually run under a web server, such as nginx, which takes care of requests, returns files, and directs only correct requests to the WSGI (which then passes them to the application). Uvicorn is specifically focused on asynchronous execution and has a slightly different implementation of asynchronous work.

**What metrics does the Locust package measure?**

Locust is a package that's designed to test the traffic loads of a service (for example, an API). Its main metrics are requests per second, average response time, and the number of errors.

        

# Chapter 19

**What does a serverless application mean?**

Serverless applications still run on normal servers, but control over the server's behavior and the stack are completely handled by the cloud provider—all that's required from the developer is to write a function that describes the business logic. This function can be set to trigger on a request to a certain API endpoint, on a certain event (for example, a file addition to the S3 bucket), or on a scheduler so that it runs every day.

**What are the limitations of the serverless approach?**

Serverless applications are mainly bound by the memory they can use and, therefore, the packages that can be installed. For AWS Lambda, the limit is 50 MB.

**What are the benefits of serverless APIs?**

Serverless ...

        

# Chapter 20

**How can we measure which line in the code took the most time to complete?**

The simplest way to do that is via a utility called `line__profiler`. This utility will show each line of the given code and show how much time was spent on each line. Knowing the distribution of the time that was required helps us focus on the right parts of the code.

**Does NumPy run faster than Pandas?**

In most cases with numeric computations, Pandas uses NumPy under the hood, so the difference is minimal. It does, however, spend certain additional time on building series and dataframes, when needed. So, for a well-scoped and purely numeric task, it makes sense to switch to pure NumPy.

**When should we use Numba? What are the challenges and benefits of using Numba?**

Numba uses a modern C compiler with some modern techniques to significantly improve performance. It can also be run on a GPU. Its "superpower" is that it's arbitrary Python code with only a few lines of alterations. This makes Numba a great tool of choice if you have a large set of pure Python code that needs to run faster. The challenges of Numba are twofold—first, it requires an LLVM compiler that is relatively large in size. Second, it is not trivial, and in some cases it's impossible to house with existing C code, which means it has problems with SciPy and `sklearn`.

**When should we use Dask?**

Dask is a powerful and nicely designed library for parallel computations—it can work on multiple cores of a single machine, or on many machines at the same time. Best of all, it has a few different interfaces that "resemble" (actually, just use under the hood) popular libraries, such as NumPy and pandas. As a result, on many occasions, you only need to change a few lines to run the same code in a distributed fashion.

**Does code formatting matter? Why is Black better than linters?**

It does. Good, standardized formatting helps improve the readability of code, decreases cognitive loads, and helps to avoid syntactic errors and typos. In addition, a unified approach to formatting decreases the number of pointless formatting changes that complicate the use of Git.

Black is an automated formatter—not a linter. Compared to linters, it not only finds code that needs to be edited but also edits it itself. Black is perfect to use on Git pre-commit hooks—it will automatically format the code on every commit.

**How does Hypothesis help you test your code?**

Standard unit tests provide one of a few cases for code to be tested against. While this is fine most of the time, there are usually plenty of options you wouldn't have thought of beforehand. Hypothesis tries to address that—it allows you to create a probabilistic dataset or set of arguments that follow certain rules—and then will test your code against different data. In doing so, it will use a few known edge cases, such as empty strings or data frames, and some random data. If a certain test fails, Hypothesis will start a new test from the data that led to a failure previously.