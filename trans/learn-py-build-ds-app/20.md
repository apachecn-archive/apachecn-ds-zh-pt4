        

# Let's Build a Dashboard

In the previous chapter, we learned how to create robust pipelines and schedule them so that we have metrics updated every day. With that, our valuable data product is automated! What should we do? Perhaps it is a good moment to discuss dashboards. Dashboards are essentially the entry point for you to monitor the behavior of the system (your service, markets, users, or anything else) via a set of data visualizations. Dashboards help teams and companies to ensure the business is running smoothly or to detect—and adjust to—changes or anomalies. So, to help us understand them better, let's see how they work.

The following topics will be covered in this chapter:

*   Different ways to build a dashboard
*   Building a static dashboard ...

        

# Technical requirements

The following packages are required for the code in this chapter to run:

*   `matplotlib`
*   The `altair` visualization package, version 3 or above
*   `panel`

As usual, all of the code for this chapter is in the GitHub repository, in the `Chapter17` folder: [https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications).

        

# Building a dashboard – three types of dashboard

In [Chapter 12](3ba037c3-3249-45f2-8f8a-0fa8af176e64.xhtml), *Data Exploration and Visualization*, we explored a dataset by visualizing different features, using two packages—`matplotlib` and `altair`. The differences between those visualizations and a dashboard are twofold:

*   The audience for the dashboard is meant to be wide, so it should be easily accessible via an internet browser. Visualizations are often made for self-consumption.
*   Dashboards are meant to be frequently updated and, to some extent, interactive. Visualizations are often done on-the-spot, are static, and show only specific aspects of the data.

To a large extent, dashboards are full-blown projects requiring regular improvement and maintenance! However, as the demand for this ...

        

# Static dashboards

Despite the name, static dashboards are not static per se—they are not just still images. Here, static refers to the fact that the dashboard is served as a flat HTML file; all of the interaction happens in the client's browser. As a result, the dashboard can be uploaded anywhere on the web (say, an S3 bucket or similar service) and stay there almost for free, with little maintenance required. It is also easy to update the dashboard or data, with essentially no downtime. And of course, this approach means you won't need to think about the scalability and performance of the dashboard.

Obviously, that approach has its downsides, as well. First of all, it is limited to a specific amount of data it can use, and the dataset will be basically available for everyone, directly. If your dashboard requires complex queries and real-time aggregation, this approach will not work. It would be hard to create authentication or to customize the dashboard for a specific user. In a nutshell, this type of dashboard is perfect for the following:

*   Serving a wide audience.
*   It uses a relatively small dataset that is fine to share with everyone.
*   This data is updated occasionally—definitely not in real-time (the computation may take a lot of resources).

One obstacle in going down that path for many backend developers and data scientists is JavaScript itself. This is virtually the only option with which to write interactive web applications. At the same time, most data scientists and Python developers don't know JavaScript well enough to use it in production, and, often, don't even want to write JavaScript. There are a few ways to dodge that, for example, compiling your code to `WebAssembly` (which browsers can also run), but that, at least for now, is a hard task in its own right and is a huge overkill.

Another, arguably better, alternative is to use one of the existing Python tools and packages that will generate both HTML and JavaScript code for us. Earlier, we mentioned the difference between visualizing in the notebook and on the dashboard, but this kind of tool can generate charts for both cases.

In the previous chapter, we built a pipeline that collects data on 311 calls every day and then generates a report. Now, let's built a static dashboard of this data, using the `altair` library we used to plot interactive visualizations in the notebook. We will start in the same way: in the Notebook then store it as HTML; finally, we will redirect the visualization to use an external dataset—the one we're scheduled to update.

Let's start preparing our notebook and loading the dataset:

```
import pandas as pd
import altair as alt

alt.data_transformers.disable_max_rows()

data = pd.read_csv('./data/top5.csv', parse_dates=['date',]).fillna(0)
```

Now, what would we want to have on a dashboard? Usually, a primary goal is to highlight any temporal abnormalities—say, a day that was skipped in data collection or whether the number of complaints deviated significantly. One way to do that is to show a line chart of the total number of complaints—say, split by boroughs:

```
timeline = alt.Chart(data, width=800).mark_line().encode(
    x='date',
    y='value',
    color='boro'
).transform_filter(
    (alt.datum.metric == 'complaints')
)
```

The code results in this diagram:

![](Images/cff8ea09-8a7e-4f5d-9136-e42112b9dae6.png)

And already, we see some interesting stuff: missing values for June 7 and some peaks in January and February. This is a good example of the type of insight quick graphical overviews can give. We can also see different levels of complaints for different boroughs—Brooklyn has, for some reason, more than the others.

Now, it would be great to see what this is all about—which types of complaint are the most popular within a given interval of time. Let's first build a bar chart of the top five complaint types for the entire period:

```
barchart = alt.Chart(data, width=800).mark_bar().encode(
    x='svalue:Q',
    y=alt.Y(
        'metric:N',
        sort=alt.EncodingSortField(
            field="svalue", # The field to use for the sort
            order="descending" # The order to sort in
        )        
    ),
    color=alt.value('purple'),
    tooltip=['metric', 'svalue:Q']).transform_filter(
    "datum.metric != 'complaints'").transform_filter(
    "datum.boro == 'NYC'").transform_aggregate(
    svalue='sum(value)',
    groupby=["metric" ]).transform_window(
    rank='rank(svalue)',
    sort=[alt.SortField('svalue', order='descending')])
    .transform_filter('datum.rank <= 10')
```

Here, we have to filter for NYC (to avoid counting metrics twice) and for the complaints metric, for the same reason. As we want to drop the long tail, we have to generate a rank for each row and then filter by its value. The following is the result:

![](Images/8c87d33d-95f4-4867-aac2-4b7fe1c60d2a.png)

Finally, we want to combine the two: selecting the time period and seeing a distribution of complaint types for that period. It is just a combination of the two, with a `brush` element added:

```
brush = alt.selection_interval(encodings=['x'], empty='all')

T = timeline.add_selection(brush)

B = barchart.transform_filter(brush)

dash = alt.vconcat(A, B, data=data)
```

Here, the `dash` variable represents a combined chart that knows how to filter bars based on the interval on the timeline. Feel free to play around and see how top complaints change over time! Of course, there are plenty of features to add (for example, see different complaint types for a particular time of the day), but those features and transformations will quickly grow too exponentially complex for rapid design—that's the downside of using a Vega stack and computing everything in the browser, in general.

On the following diagram, you can see a screenshot of the resulting dashboard:

![](Images/d2726442-0296-484b-a0b0-14083867c496.png)

The gray area on the timeline represents the selected range. The bar chart then shows the overall number of complaints for the top 10 complaint types within the period. This interactivity allows us to dive deeper into the data, exploring more nuanced trends of a particular time period.

We could imagine linking our dashboard to an API as an alternative to serving flat files. This way, the dashboard will show the data upto the current moment; it is also possible to connect Altair/Vega to a data stream, so that the dashboard will be updating in real time.

Working with Altair is great, as it is easy to create a beautiful visualization with advanced interaction—except for the cases when it won't work. In the next section, let's talk about the ways we can debug your plots and understand what is going wrong.

        

# Debugging Altair

The preceding example works, but in the real world, any development is a process of trial and error. Debugging might be daunting with the Vega stack due to the layered nature of the product (Altair converts charts into Vega-Lite, Vega-Lite converts them into Vega, and the Vega engine works in JavaScript) and because we're working in this non-Pythonic world.

As with all code in general, the process is to isolate different parts and layers of the application to identify the root of the problem. Identification will give you ideas on how to solve the problem and work around it. Unfortunately, we can't just split parts of the specification. So, what can we do if your chart does not work as intended?

Despite all of the issues, ...

        

# Connecting your app to the Luigi pipeline

Once the dashboard is working and looks good, we can discuss its deployment details. For now, all of the data is internalized in the dashboard itself, which makes the specification large and a little hard to update. Let's link the chart to the external CSV file we generated with Luigi and stored on the S3 bucket. We could use the URL (path to the file) from the beginning; it's just easier to be able to open and investigate the dataset. Copy the URL to the dataset and override the attribute:

```
url = 'https:/path/to/your/dataset.csv'
dash.data = url
```

Make sure it is still working! Now, we can write the dashboard to the HTML, as follows:

```
dash.save('chart.html')
```

As we discussed in [Chapter 12](3ba037c3-3249-45f2-8f8a-0fa8af176e64.xhtml), *Data Exploration and Visualization*, this will store a standalone HTML page with a working dashboard. The last step is to publish the chart itself (for example, on the same S3 bucket). Published, the chart will reflect the changes whenever we update the CSV. We can further automate that update by scheduling a Luigi pipeline to run every day: with this, we'll get a "live" dashboard for monitoring 311 situations in the city. 

This dashboard costs virtually no money (only whatever we spend on S3 buckets) and requires no time to maintain. It could be easily customized and developed as an Altair object, a Vega-Lite/Vega application, or a standard HTML/JS-based app. Lastly, this dashboard can be easily styled and restyled according to your design guidelines. Being cheap and simple, this approach has its limitations: as with most client-based visualization solutions, Vega is limited in the amounts of data it can reflect, and the whole dataset needs to be publicly exposed, which is not always an option.

All of that makes Altair a great solution for public-facing charts and dashboards. Often, though, we need an internal dashboard, with access to large amounts of data and the ability to drill-down to specific records. For that, a different type of dashboard should be used: dynamic, server-dependent dashboards. Let's discuss them in the next section. 

        

# Understanding dynamic dashboards

An alternative approach to building a dashboard of your own is to make an actual web application, with a live server running Python on a backend; this will, upon request, show you a dashboard. This approach is, essentially, the exact opposite of a static dashboard in terms of pros and cons: it requires maintenance, needs to be scaled if the traffic is heavy, and could be slower. It also allows you to configure access, customize dashboards for any user or group of users, and compute the results live, even for a comparatively large dataset, without the need to share this dataset as a whole with the audience.

Of course, we could build an entire web application, controlling each and every feature (we won't do ...

        

# First try with panel

The idea behind `panel` (and Voila) is very simple and appealing: given that we essentially build web pages with our code and charts—the notebooks—we simply convert them into dashboards. The best part of that is that all of the code and every visualization library that can be used in Jupyter can be used in the dashboards; we could even use our existing Altair charts if we wanted. Let's try building something from the same dataset! As with Altair, we'll start with a Jupyter Notebook:

```
import sqlite3

import param
import panel as pn
import datetime as dt
pn.extension()
```

Now, the `panel` package is designed to make it extremely easy to build interactive widgets as part of your exploration process. To build an interaction, you just need a function with default values—`panel` will use them to understand the value types and generate input widgets accordingly. Here is an extremely naive example:

```
def interact_example(a=2, b=3):

    plot = plt.figure()
    ax = plot.add_subplot(111)

    pd.Series({'a':a, 'b':b}).plot(kind='bar',ax=ax)

    plt.tight_layout()
    plt.close(plot)
    return plot

pn.interact(interact_example)
```

The following screenshot is a result of the preceding code. The bar chart is interactive and responds to changes in input:

![](Images/5f0c214d-01dc-4dc2-9c5a-cc0be23af9e1.png)

Here, we use `matplotlib` as a base visualization tool. Let's now try it on with a more complex task—showing the aggregate statistics on the 311 data we collected in [Chapter 16](beaaddb0-c3fe-481f-8530-a843a42afac3.xhtml), *Data Pipelines with Luigi*, live.

        

# Reading data from the database

Before we dive into the nitty-gritty of visualization, let's get our data. Here, we will use the database connection to the SQLite file created. First, we'll create a connection to the file:

```
import sqlitecon = sqlite3.connect('../Chapter16/data/311.db')
```

Next, we will define a simple query to aggregate raw records into statistics:

```
Q = '''SELECT date(created_date) as date, lower(borough) as boro, complaint_type, COUNT(*) as complaintsFROM raw WHERE borough != 'Unspecified' GROUP BY 1,2,3;'''
```

Finally,  we will pull the data using the `pandas` SQL command. As we're dealing with SQLite, we'll have to re-parse date-times in Python:

```
DATA = pd.read_sql_query(Q, con)DATA['date'] = pd.to_datetime(DATA['date'])
```

Alternatively, ...

        

# Creating an interactive dashboard in Jupyter

The functional approach we used in the previous section is convenient for exploration within the notebook. For a complex dashboard, however, it is better to use a somewhat declarative approach for more complex dashboards. In order to do that, we need to inherit from the Panel's `param.Parameterized` object and declare the parameters as it's attributed. For each view, we will create a separate method, using the `@param.depends('param1', 'param2')` decorator to bind the view refresh with the corresponding parameter updates. Let's give it a try:

1.  First, we'll define the `DateRange` parameter, using a simple tuple of date-time values:

```
bounds = (dt.datetime(2019,1,1),dt.datetime(2019,5,30))
dr = param.DateRange(bounds=bounds, default=bounds)
```

2.  Another parameter we want to use is boroughs. As we want to be able to select multiple boroughs at the same time, we'll have to explicitly pass them:

```
boros_list = ['Manhattan', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island']
boros = param.ListSelector(default=boros_list, objects=boros_list)
```

3.  Once the parameters are defined, we can create our view object, based on the dummy `param.Parametrized` class, as follows:

```
class Timeline(param.Parameterized):
    dr = dr
    boros = boros
```

4.  Next, we need to draw visualizations. As everything here runs in Python, all we need is to access the input parameters, filter data by them, and make a chart, returning the plot. The decorator will tell the Panel which parameters we want the chart to be updated on. We'll start with the timeline:

```
    # method for Timeline
    @param.depends('dr', 'boros')
    def view_tl(self):
        start, end = pd.to_datetime(self.dr[0]), 
                     pd.to_datetime(self.dr[1])

        tl_data = boro_total.loc[(boro_total.index >= start) & 
                  (boro_total.index <= end), 
                  [el.lower() for el in self.boros]]

        plot = plt.figure(figsize=(10,5))    
        ax = plot.add_subplot(111)
        tl_data.plot(ax=ax, linewidth=1)

        ax.legend(loc=4)
        plt.tight_layout()
        plt.close(plot)
        return plot
```

Similarly, we will create a chart for the top five complaint types. It also consists of a decorator, filtering, and visualization parts. Consider the following code:

```
    @param.depends('dr', 'boros')
    def view_top(self, N=5):
        start, end = pd.to_datetime(self.dr[0]), 
                     pd.to_datetime(self.dr[1])

        boro_mask = DATA.boro.isin([el.lower() 
                    for el in self.boros])
        time_mask = (DATA.date >= start) 
                     & (DATA.date <= end)

        top = DATA[boro_mask & time_mask]

        S = top.groupby(['complaint_type', 'boro'])
            ['complaint_type'].count().unstack()
        topN = S.iloc[S.sum(1).argsort()].tail(N)

        plot = plt.figure()
        ax = plot.add_subplot(111)

        topN.plot(kind='barh',stacked=True, ax=ax)
        plt.tight_layout()
        plt.close(plot)

        return plot
```

This concludes the logic behind the dashboard. Now, we need to define its layout. For that, we will create a simple grid, using Panel's `Row` and `Column` objects. For more advanced layouts, `panel` also has a `Grid` object, but we will not use it here:

```
panel = pn.Column( '<h1>NYC 311 dashboard</h1>',
                  T.view_tl, 
                  pn.Row(T.param, T.view_top,), sizing_mode='stretch_width')
```

Now, all we need is to start serving `panel`:

```
panel.servable()
```

As a result, we'll get the following dashboard:

![](Images/024df13e-ef0f-49b6-b563-7dfa6aaf9433.png)

Dashboards can be represented as separate windows, using two object methods of an—either `panel.show()` or `panel.servable()`. Both will result in a new browser tab, serving the dashboard. The difference between them is that, with the second method, we can also run the dashboard with no Jupyter Notebook attached, using a bash command:

```
 panel serve --show 2_panel.ipynb 
```

Using `panel serve`, we can deploy our dashboard as an independent web application. All we need is to deploy the environment (all of the packages we need and Python itself) on a dedicated machine and make it run this command. In this case, we could swap the SQLite connection with one to the external database, so that the data will be shared between the dashboard and any other applications. The power of dynamic dashboards lies in their large capacity and flexibility. Here, we ignored the intermediary step of computing top complaint types per day and were able to run our analytics on the raw data. If needed, we could always drill-down and check the properties of a specific record, as well.

As everything is running on the server, in Python, we can use any package and are very flexible in designing the dashboard. As we mentioned, Panel supports any visualization that can be rendered in the notebook, so you can even reuse some visualizations you already have, including those we built in Altair.

One limitation to this approach is that we can only control the appearance of the visualizations to the extent that all of the libraries we use allow us to do so; for example, while we can use Altair, there is no way, at least currently, to pull back its parameters and use it to interact with other packages.

Overall, dynamic dashboards allow a wide range of possibilities for exploring and communicating your data. Compared to the static ones, they are easy to write, support any Python package, and can do the heavy lifting of data consumption and aggregation, pulling from raw data. This, of course, requires a dedicated server and may require maintenance and governance, especially if available to the public.

        

# Summary

In this chapter, we learned to build two similar dashboards—a static one, with no server needed and using Altair, and a dynamic one, built from an ordinary Jupyter Notebook with arbitrary code and visualization packages, using the `panel` package. We discussed the pros and cons of each approach and when to select one over the other.

Either way, the dashboard is a great way to communicate your data product to your colleagues and clients. Dashboards allow us to get insights into business processes and spot issues early on. In many cases, that would make a perfect deliverable. In some cases, though, you might need to create a programmatic access point for your code, for example, a machine learning algorithm for an external application  ...

        

# Questions

1.  What are the main differences between visualizing data in a notebook and on a dashboard?
2.  Why do we call some dashboards "static"? What are the pros and cons of a static dashboard?
3.  What are the benefits of using a dynamic dashboard?
4.  What are the features of the `panel` package?

        

# Further reading

*   *Apache Superset Quick Start Guide* by Shashank Shekhar, published by Packt ([https://www.packtpub.com/big-data-and-business-intelligence/apache-superset-quick-start-guide](https://www.packtpub.com/big-data-and-business-intelligence/apache-superset-quick-start-guide))
*   *Visualization Dashboard Design* ([https://hub.packtpub.com/visualization-dashboard-design/](https://hub.packtpub.com/visualization-dashboard-design/))