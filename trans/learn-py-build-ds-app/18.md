        

# Packaging and Testing with Poetry and PyTest

Until now, all our code has lived in either notebooks or Python files. While that is totally fine, with the growth in volume and complexity of our code, it is increasingly becoming a good idea to form one or more go-to sources for the code we use most frequently, as well as sources for the complex code that we don't want to risk adding mistakes to. 

In this chapter, we will learn how to build our own packages for use in multiple projects or to be easily shared with others, using the `poetry` package. A package can work as a deliverable—something you can pass to or share with your client! Building and testing packages is a vital skill that increases your productivity and allows you to save time and reduce stress by enabling you to reuse the same properly tested body of code again and again.

Building packages also likely to improve your overall coding skills, as packages require code to be abstract and flexible and allow you to spend time on efficient implementation.

The following topics will be covered in this chapter:

*   The benefits of having a custom package
*   A few ways to develop a package
*   Defining dependencies and resources with Poetry
*   Workflow with the editable package installation
*   Testing your code with PyTest
*   Documentation with sphinx
*   Testing with CI

        

# Technical requirements

For this chapter, we will need the following packages (as always, they are included in our base environment):

*   `poetry`
*   `pytest`
*   `sphinx`

As we're creating an independent package, all the code is stored on GitHub at [https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications](https://github.com/PacktPublishing/Learn-Python-by-Building-Data-Science-Applications).

        

# Building a package

So far in this book, we have been either using third-party packages, such as `requests` and `pandas`, or writing raw code as `.py` scripts or notebooks. While using Python files directly is absolutely fine for certain projects, it makes it hard for code to be reused and built upon; it is not sustainable for complex algorithms and tools that can be used over and over again. Such code is also hard to share as it has no overall structure, tends to decay over time, and doesn't have a robust dependency system; the code may not work on other systems with other packages (or other versions of packages) installed. Last but not least, this kind of practice affects the quality of our code, as we tend to write and use the code as a one-time solution. The best way to mitigate all those issues at once is to form your code into a package.

But what is a package? In Python, packages are defined as specific bodies of code that are registered in the system (via a system path) and thus can be imported and used in any specific code base. Packages are stored on the dedicated system paths and are not meant to be changed or deployed manually. Instead, it is preferable to use dedicated tools, such as the built-in `setuptool` package or package managers (for example, `pip` or `conda`). We'll learn about these in detail in the coming sections.

        

# Bringing your own package

Most of the packages that we've taken a look at so far are very generic—for instance, `requests` works with HTTP on the client side, while `pandas` is focused on data analysis. Why should you write your own package with a very specific task in mind that's not necessarily useful or applicable to another person's use case? Simply put, because you'll be the first to benefit!

The main goal that packages solve is to create a deliverable; any new user on a new machine will be able to install the package, as well as all the packages it requires to work properly, at once. As a byproduct, packages remain isolated from the code you write every day. This isolation is a good thing—it makes you, as a package designer, build your ...

        

# Using a package manager – pip and conda

All the packages we've introduced so far are publicly accessible for everyone via one of two package managers—`pip` and `conda`. The role of the package manager is to provide a unified interface to securely install, upgrade, and uninstall packages in a system. As most packages depend on other packages (usually specific versions), it is also the job of the package manager to resolve those dependencies—that is, to install packages that fit the version criteria for all installed packages that will depend on them. 

`pip` and `conda` are by far the two most popular package managers for Python. The first one is officially supported by the Python Software Foundation. It is the main Python package management, period.

Why do we use `conda` at all, then? Historically, `pip` and PyPI (a corresponding online service) did not support binaries as part of packages. Binaries have to be compiled for each OS separately, and they are (obviously) written in languages other than Python itself. Thus, providing support for binaries was considered too demanding. 

It turned out, however, that this is exactly what is required for fast numeric operations—as we've mentioned before, `numpy`, `pandas`, and `sklearn` all run C and even Fortran under the hood. For a long time, the lack of support meant that everyone had to compile binaries locally every time they installed those new packages. As such, the Python leadership offered to build a package manager that would support binaries—and `conda` was born. As an additional bonus, `conda` can also create separate virtual environments, so you can safely replicate a full Python environment from a shared `.yaml` file or have multiple versions of Python and packages at the same time.

Unfortunately, `conda` requires a bit more effort to work with, compared to `pip`. Most critically for us, it does not support the installation of packages from a Git repository (even a private one), while `pip` does. Another feature of `pip` is that it allows the installation of packages in editable mode—that is, with the code stored on a non-system path and ready to be edited. In this chapter, we will build a package targeted at `pip`.

        

# Creating a package scaffolding

So, how do we start? First of all, we need to create a scaffolding—a default package structure with all the necessary folders, files with metadata and configs, and so on. Our code will be stored on GitHub, so let's start by creating a new repository:

1.  This step is easy. Go to [https://github.com/](https://github.com/), log in, hit the New button, and fill in all the information. In our case, we'll name the repository the same as the package—`wikiwwii`. We'll also add a standard Python `.gitignore` file and a `README.MD` file. You can also add a license of your choice for your repositories.
2.  Next, we create the repository and copy the path to the clipboard. With that, we can open a terminal, go to the proper location on the hard drive, and ...

        

# A few ways to build your package

The structure of a Python package is defined by a few specifications ([https://packaging.python.org/specifications/](https://packaging.python.org/specifications/)) and **PEPs** (short for **Python Enhancement Proposals**, such as PEP517—[https://www.python.org/dev/peps/pep-0517/](https://www.python.org/dev/peps/pep-0517/), PEP518—[https://www.python.org/dev/peps/pep-0518/](https://www.python.org/dev/peps/pep-0518/), and PEP427—[https://www.python.org/dev/peps/pep-0427/](https://www.python.org/dev/peps/pep-0427/)), and the overall definition comes from the **Python Packaging Authority** (**PyPA**). In essence, a package is required to have, in addition to the actual code, a special file with metadata, including the package name, the description version, the tags, Python version support details, the authors, and the dependencies. This file could be a Python `setup.py` file—which was the standard solution for a long time—or a `pyproject.toml` file. The latter is a new, safer approach, but does not have as well-designed a specification.

It is entirely possible to build a package manually. All it takes is a little structure and a file with the metadata. It is, however, a tedious task, so there are quite a few packages designed to help with packaging. A standard bundle is `distutuls`. There is also `setuptools`. Both of them expect a `setup.py` file.

One of the challenges with package building is managing dependencies between packages. One package may depend on another, which may depend on several more—so don't be surprised if your package requires the installation of dozens of other packages. There is even a fair chance that some of those packages will depend upon different versions of the same package, so *someone* will have to figure out a way to install versions that suit all the requirements or somehow install two versions of the same package. This is a challenge that the previous generation of package builders didn't fully solve.

Recently, a few new tools arrived—namely, `flit` and `poetry`—and both of them
support `pyptoject.toml` files and work well to resolve dependencies. In this chapter, we will use `poetry` to build our package. This package has a slick and expressive interface and supports `.toml` format. It also builds a dedicated virtual environment for developers and has two levels of dependency description: a flexible list of packages required for the user, and one explicit and precise package, for developers, which contains a list of exact versions of all dependency packages installed on the developer's machine. It also has tools for dependency diagnostics and package publishing.

        

# Trying out code with Poetry

Poetry, like `flit`, is one of the most recent packages aimed at helping with package development in Python. Among its features are the ability to write a `pyproject.toml` file, which is more secure and easy than the older approach with `setup.py`, and the ability to create a dedicated virtual environment for a project, with all dependencies pinned. Even more importantly, it has a thorough dependency resolution engine that makes sure all dependency versions fit with each other, and an interface to monitor and bump your dependency tree.

But first, we will start by creating a project template. First, type this:

```
poetry new --name=wikiwwii my-package
```

This will generate a new folder, `my-package`, with default files and folders ...

        

# Adding actual code

Now that the package structure is in place, we can start adding the actual code. For starters, we copy and paste the code from [Chapter 7](232fe2da-7fa8-4d76-b5fc-d4bf80535e86.xhtml), *Scraping Data from the Web with Beautiful Soup 4*, for the `wiki.py` package. As we want to have code for both collecting and cleaning in the same package, it sounds smart to create two sub-folders—`collect` and `parse`. The code from [Chapter 7](232fe2da-7fa8-4d76-b5fc-d4bf80535e86.xhtml), *Scraping Data from the Web with Beautiful Soup 4*, will go to the latter one. For now, we will create two files—`battles.py` and `fronts.py`—in the `parse` folder. In Python, upon import, they will be mapped to a path such as `wikiwwii.parse.battles`, enabling access to all the functions and variables in them.

Next, we add the code for cleaning in a similar fashion. However, most of the cleaning code here is stored in the `1_data_cleaning.ipynb` notebook. Of course, we could run a Jupyter server and copy and paste all the files into **Visual Studio** (**VS**) Code, but there is an even better option. Instead, open the command palette (*Shift* + command/*Ctrl* + *P* by default), select Python: Import Jupyter Notebook, and pick our notebook. As you'll see, VS Code will convert the file into a normal script, marking cells with the comments.

VS Code actually even allows the running of converted cells interactively, step by step. Moreover, it supports converting this file back into a notebook. This is useful when you need to tweak something in a notebook from within VS Code.

Here is the folder tree after we moved the actual code:

```
wikiwwii
├── README.md
├── pyproject.toml
├── tests
│ ├── __init__.py
│ └── test_wikiwwii.py
└── wikiwwii
 ├── __init__.py
 ├── collect
 │ ├── __init__.py
 │ ├── battles.py
 │ └── fronts.py
 └── parse
 ├── __init__.py
 ├── bellengerets.py
 ├── casualties.py
 ├── dates.py
 ├── geocode.py
 └── qa.py
```

The code in packages is not generally meant to be run directly on import; thus, it (usually) consists only of functions, variables, and objects. Until it is clearly needed, consider it a bad practice to actually execute code directly in a package—it will then be executed every time someone import from this file. Similarly, where possible, try not to import packages you don't need, or generate big structures, until you actually need them. It is a good practice to import dependency packages only where they are necessary so that even if the package is missing, some code will still be executable.

        

# Defining dependencies

Now, as you may have noticed, our code from [Chapter 7](232fe2da-7fa8-4d76-b5fc-d4bf80535e86.xhtml), *Scraping Data from the Web with Beautiful Soup 4*, relies on two libraries—`requests` and `BeautifulSoup4`—to work. Parsing requires another package, `pandas`. For our imaginary user, it would be preferable to install those packages and, even better, make sure that we install versions we think will support what we need. This is where `poetry` thrives. In your Terminal, type the following:

```
poetry add requests beautifulsoup4 pandas
```

Poetry will scan our current environment, detect the version of those packages we use, and add their characteristics to both `pyproject.tamp` and a new file, `poetry.lock`. The former is a (recently added) standard specification—it will be used, upon ...

        

# Non-code resources

If your package needs to include something besides the Python files—for example, a small dataset or a query template—you'll need to add it explicitly as part of the package in `pyproject.toml`, as here:

```
[tool.poetry]
include = ["*.sql"]
```

In some cases, you may not want to include some Python scripts in the actual package (for example, some support scripts). For that, you need to add a similar `exclude` line in the same section:

```
[tool.poetry]
include = ["*.sql"]
exclude = ["wikiwwii/uploader.py"]
```

We don't have any files to add or exclude in the package we're building—at least right now—so we won't have this section in the `.toml` file.

        

# Publishing the package

Now, assuming everything looks good, we can try installing dependencies with `poetry install`. This won't (despite the somewhat misleading name) install that package in your current environment—instead, `poetry` creates its own virtual environment for testing purposes.

Once that's done, we can build and publish our package to PyPI (using `poetry build` and `poetry publish`) so that it will be available for everyone. Let's not hurry—our package is in its infancy and is not yet tested and secured.

Instead, let's use GitHub as a sharing platform. Once your current version is pushed to the repository, the package can be installed straight from GitHub itself—`pip` supports that as well:

```
pip install git+https://github.com/Casyfill/wikiwwii.git ...
```

        

# Development workflow

Now, with great power comes great responsibility. For now, we are committing to the master branch, which is the default one for installation, so we probably don't want to add some unreliable code there. So, let's adhere to the following, rather simple, practice—we should never push code directly to the master branch. Instead, we should work on a separate development branch, push it to GitHub, and then—once we're satisfied with results—merge the branch into the master. For that, GitHub even supports code review and discussions. Let's switch to a development branch now:

```
git checkout -b tests
```

As you may notice, VS Code will mark the current branch in the lower-left corner of the window. In fact, you can click on it and switch to a different branch or even create a new one manually. Next comes testing the code and checking how it works.

        

# Testing the code so far

How would we know whether the code is good, anyway? The only good way is to rigorously test your code. While it may sound like a lot of somewhat unnecessary work, it is a practice that will repay you many times over in the future—once you're sure your code behaves as intended, it is much easier to add new features and be sure that they didn't break any of the existing ones. Furthermore, you can upgrade dependencies or compare different implementations, all being sure that your code behaves as intended.

As for many other things, Python has a standard library for testing—`unittest`. In contrast to most of the standard libraries, however, `unittest` is fairly unpopular. Instead, another library, `pytest`, is considered the ...

        

# Testing with PyTest

In fact, `poetry` even generates a test function for us, though it doesn't test our code; instead, it checks the version. Take a look at the code here:

```
from wikiwwii import __version__

def test_version():
    assert __version__ == '0.1.0'
```

Here, two things are worth discussing. First, as you can see, the test is just a function with the word "test" in its name. Having this word is necessary—this is the way `pytest` finds all the tests. Second, each test results in one or a few `assert` statements. To pass the test, `assert` should not raise any issues. That's all the basics of test development. 

Now let's run this existing test; generally speaking, all we need is to type `pytest tests` on the command line. With `poetry`, however, we have a hidden virtual environment intended for development, so that's where we should run our tests; for that, type `poetry run pytest tests`. If everything is okay, `pytest` should print out a small report with the version of Python, `pytest`, the package, and a description of the tests, as follows:

```
>>> poetry run pytest tests
========================= test session starts =========================================
platform darwin -- Python 3.7.1, pytest-3.10.1, py-1.8.0, pluggy-0.12.0
rootdir: /Users/philippk/Dropbox/personal_projects/wikiwwii, inifile:
collected 1 item 

tests/test_wikiwwii.py .                                                             [100%]

============================ 1 passed in 0.02 seconds =====================================
```

Yay! This test has passed.

        

# Writing our own tests

But seriously, let's write a test of our own now. First, let's start with something relatively basic, such as finding out how our functions extract data from a given page and its elements. 

There is not much value in testing the ability of the `requests` library to collect pages from the web—it is safe to assume it does, and it has its own tests. Because of that, we moved all the parsing code from the `parse_battle_page` function into a `private _parse_page` function. This way, we can focus on testing the parsing, not the internet.

First, we'll store an HTML page of a few battles in the `tests/data/pages` folder. Now we will create a file called `test_collect_battles.py`—here we'll store all our tests for this particular module. ...

        

# Automating the process with CI services

Now, as you may recall, we are working on a `tests` branch of our repository. If you go to GitHub, it may offer to create a pull request—a procedure meant to merge your branch into the master branch or any other branch, as in the yellow section of the following screenshot. Even if the interface does not offer this (it won't if there was already a pull request a few minutes before), you can create a pull request yourself, via the New pull request button. See the following screenshot:

![](Images/92c644bc-1d96-4bfe-8dbd-df1abdad604b.png)

Using GitHub, you can request other people to review your changes, comment on them, and more; GitHub will also confirm whether merging is possible or whether you'll need to resolve conflicts first.

While, in our case, we did run our tests locally and we know it is safe to merge, there is no way for others to check that easily. In order to make life simpler for everyone, and save some time for you (for large projects, proper testing might take a while), **continuous integration** (**CI**) services are used. Most of the time, all CI services do is trigger on a new commit, pull code to a virtual machine, run your tests, then report back whatever the tests succeeded in determining. Because of CI services' automatic nature, it is easy to run even multiple machines with different environments—say, one service could test your code on Python 3 and one on Python 2.

Note that CI services can do more than this. For example, they can automatically re-generate documentation from a repository and publish it, push your package to the registry, and upload any other artifact objects somewhere else. Explore these options!

Generally speaking, CI services do cost money. Most of them, though, have free tiers for open source projects. As our package is open source and open to anyone, let's leverage the free tier of a CI service. There are plenty of great services around, and all of them are more or less the same. We will use Azure pipelines, but you can pick something else if you want.

To get started, we need to go through a few simple steps:

1.  First, we need to go to the Azure DevOps website ([https://azure.microsoft.com/en-us/services/devops/](https://azure.microsoft.com/en-us/services/devops/)) and register. We'll give it access to our GitHub account and create a build pipeline for the `wikiwwii` repository. In a moment, Azure will offer you a few scenarios, starting with Python—this is what we need.
2.  Next, it will show a simple pipeline as a YAML file. It will, by default, offer to run multiple instances with different Python versions. We can drop all but 3.7.
3.  As we are using `poetry`, we can replace the `pip install` line with the command for `poetry` installation as per Poetry's installation guide. On the next line, the `pytest-azurepipelines` package is installed. We can't use that, because we need to install it via `poetry`, so we'll have to add it into the `poetry` development dependency list. At the same time, there is no sense in having this package locally, so we can mark it as an extra:

```
poetry add --dev --extras azure pytest-azurepipelines
```

4.  Here is the first step of our pipeline:

```

script: |
          curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python
          source $HOME/.poetry/env
          poetry --version
          poetry install -E azure
          displayName: 'Install dependencies'

```

5.  Now, the second step is simple—just run `pytest` from within Poetry:

```
- script: |
           poetry run pytest --cov=wikiwwii tests
           displayName: 'pytest'
```

6.  Last but not least, the default pipeline works on any activity on the master branch. Instead, let's trigger it on a pull request to the master. To do that, just replace the trigger with `pr` in the YAML:

```
pr:
- master
```

And we're done! Now allow Azure to submit that code to the master branch; don't forget to commit the new version of the package with `pytest-azurepipelines` added. Let's try a pull request.

If everything worked as it is supposed to, GitHub will show a small yellow circle near the last commit in a list in the pull request section. If you hover on it, it will show the current CI status as either queued or running. Once the pipeline runs, the circle will turn either green or red, depending on the results. The following screenshot shows this:

![](Images/4f851b13-1c19-4e23-9cc2-92f39dd254fd.png)

And now we're done with CI. All other CI systems are very similar; most of them use YAML as a declaration of processes, so it is easy to switch between different CI systems if needed.

Up next, let's generate some documentation for our pet project.

        

# Generating documentation generation with sphinx

Documentation is king when it comes to supporting consumers of your code and convincing newcomers that it actually makes sense to buy in and use your package. For most people, a documentation website is the first place they go to learn about the package. It is, by definition, assumed to be the single source of truth on the code in its current version. 

The role of documentation is usually threefold:

*   Explain how to install your package and what the general requirements are (for example, which Python versions are supported)
*   Show how to use the package (preferably with a quick example showing its immediate value)
*   Express the general idea and philosophy of the package

A documentation website does ...

        

# Installing a package in editable mode

As we have mentioned, you can install a package from GitHub and it will behave the same as any other installed package—it can be upgraded or uninstalled.

Often, however, you will want to use a package while developing it. It would be hard to do both in the normal installation routine; you'd have to either update or re-install the package every time you made any developmental changes, just to reflect those changes. To get around this, there is a great feature that keeps the advantages of both worlds—your code is treated as a package but can be easily modified in place. This feature is called editable mode. Essentially, it means the folder on your filesystem is registered as a package, and so the imported package will always reflect all the changes that you've made.

In order to reap these benefits, you have to have a repository of the package in question on your local machine. We obviously have our package, but it is also easy to pull the raw code via `git clone my/package/url`. Next, you open the terminal, and while being one step above the `package` folder, you type the following:

```
pip install -e wikiwwii
```

Boom! You have an editable package. You can test it in your Jupyter Notebook:

```
>>> import wikiwwii  
```

See? It behaves as if it is a properly installed package but you can make changes to your code and they will be immediately applied upon your next import.

At the time of writing this book, editable mode is not supported for the TOML-based packages. This should change soon.

This section helped you to install a package in editable mode, so now we should be good to try out some new code files!

        

# Summary

In this chapter, we went over all the processes of packaging code. In particular, we created a GitHub repository, generated a template via `poetry`, and added all the dependencies, meaning everyone can now install the package from GitHub using `pip`. We then went further, adding a few tests to make sure our package works as expected throughout future development. To simplify the process and make it transparent, we integrated a CI service, Azure pipelines, to run tests on each pull request in order to prevent us from merging failing code into production.

In the next chapter, we will review another case, building a robust, secure, production-ready data pipeline using `luigi`.

        

# Questions

1.  What are the benefits of packaging code?
2.  What is the main difference between `conda` and `pip` as package managers?
3.  What is dependency resolution, and why is it hard?
4.  What are the benefits of `poetry` over standard `setuptools`?
5.  Why do we need tests?
6.  What is the purpose of CI?

        

# Further reading

*   *Getting Started with Python Packages* ([https://hub.packtpub.com/getting-started-python-packages/](https://hub.packtpub.com/getting-started-python-packages/))
*   *Writing a Package in Python* ([https://hub.packtpub.com/writing-package-python/](https://hub.packtpub.com/writing-package-python/))
*   *Testing Tools and Techniques in Python* ([https://hub.packtpub.com/testing-tools-and-techniques-python/](https://hub.packtpub.com/testing-tools-and-techniques-python/))