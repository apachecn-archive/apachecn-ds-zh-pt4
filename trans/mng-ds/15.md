  

# 十五、实现ModelOps

在这一章中，我们将看看 ModelOps 和它的近亲 DevOps。我们将探索如何为数据科学构建开发管道，并使项目可靠、实验可重复、部署快速。为此，我们将熟悉一般的模型培训流程，并从开发基础设施的角度来看数据科学项目与软件项目有何不同。我们将看到哪些工具可以帮助版本化数据、跟踪实验、自动化测试和管理 Python 环境。使用这些工具，您将能够创建一个完整的 ModelOps 管道，它将自动交付新的模型版本，同时考虑可再现性和代码质量。

在本章中，我们将讨论以下主题:

*   了解 modelos
*   调查 DevOps
*   管理代码版本和质量
*   将数据与代码一起存储
*   管理环境
*   跟踪实验
*   自动化测试的重要性
*   连续模型训练
*   为您的项目提供电源包

  

# 了解 modelos

ModelOps 是一组实践，用于自动化数据科学项目中出现的一组常见操作，包括以下内容:

*   模型培训渠道
*   数据管理
*   版本控制
*   实验跟踪
*   测试
*   部署

没有 ModelOps，团队被迫在那些重复的任务上浪费时间。每个任务本身都很容易处理，但是项目可能会在这些步骤中出错。ModelOps 帮助我们创建项目交付管道，它像一个精确的传送带一样工作，带有自动测试程序，试图捕捉编码错误。

先说 ModelOps 的近亲——devo PS。

  

# 调查 DevOps

**DevOps** 代表**开发运营**。软件开发过程包括许多重复和容易出错的任务，每次软件从源代码到工作产品的旅程中都应该执行这些任务。

让我们检查一组组成软件开发管道的活动:

1.  对错误、打字错误、不良编码习惯和格式错误进行检查。
2.  为一个或几个目标平台构建代码。许多应用程序应该在不同的操作系统上运行。
3.  根据需求，运行一组测试来检查代码是否按预期工作。
4.  包装代码。
5.  部署打包软件。

**持续集成和持续部署** ( **CI/CD** )指出，所有这些步骤都可以而且应该自动化，并且尽可能频繁地运行。经过彻底测试的较小更新更可靠。如果一切都出错了，恢复这样的更新会容易得多。在 CI/CD 之前，手动执行软件交付管道的软件工程师的吞吐量限制了部署周期的速度。

现在，高度可定制的 CI/CD 服务器使我们摆脱了手工劳动，并完全自动化了所有必要的活动。它们运行在源代码版本控制系统之上，监视新的代码变化。一旦出现新的代码变更，CI/CD 服务器就可以启动交付管道。为了实现 DevOps，你需要花时间编写自动化测试和定义软件管道，但之后，管道就可以在你需要的时候工作了。

DevOps 席卷了软件开发界，产生了许多让软件工程师更有生产力的技术。像任何技术生态系统一样，专家需要花时间学习并将所有工具集成在一起。随着时间的推移，CI/CD 服务器变得越来越复杂，功能越来越丰富，许多公司感到需要一名全职专家来管理他们项目的交付管道。于是，他们想出了 DevOps 工程师这个角色。

DevOps 世界的许多工具变得越来越容易使用，只需要在用户界面上点击几次。一些 CI/CD 解决方案(如 GitLab)可以帮助您自动创建简单的 CI/CD 管道。

CI/CD 基础设施的许多优势适用于数据科学项目；然而，许多地区仍未被覆盖。在本章的下一节中，我们将了解数据科学项目如何使用 CI/CD 基础架构，以及您可以使用哪些工具来使数据科学项目交付的自动化更加完整。

  

# 探索数据科学项目基础设施的特殊需求

现代软件项目可能会使用以下基础设施来实现 CI/CD:

*   版本控制—Git
*   代码协作平台—GitHub、GitLab
*   自动化测试框架——取决于实现语言
*   CI/CD 服务器—Jenkins、Travis CI、Circle CI 或 GitLab CI

所有这些技术都缺少对数据科学项目至关重要的几个核心功能:

*   数据管理—用于解决大量数据文件的存储和版本控制问题的工具
*   实验跟踪—跟踪实验结果的工具
*   自动化测试——测试数据密集型应用的工具和方法

在介绍上述问题的解决方案之前，我们将熟悉数据科学交付渠道。

  

# 数据科学交付渠道

数据科学项目由多个相互依赖的数据处理管道组成。下图显示了数据科学项目的一般流程:

![](Images/939b8432-da8e-45bc-9227-01a32fe4253b.png)

让我们快速总结一下上图中的所有阶段:

1.  每个模型管道都从存储在某种数据源中的原始数据开始。
2.  然后，数据科学家执行**探索性数据分析**(**EDA**)**并创建 **EDA 报告**以加深对数据集的理解并发现数据可能存在的问题。**
3.  **数据处理管道** 将原始数据转换成一种中间格式，这种格式更适合于为训练、验证和测试模型创建数据集。
4.  **模型数据集管道**为训练和测试模型创建现成的数据集。
5.  **模型训练管道**使用准备好的数据集来训练模型，通过执行离线测试来评估它们的质量，并生成包含关于模型测试结果的详细信息的**模型质量报告**。
6.  在管道的末端，您得到最终的工件——一个保存在硬盘或数据库中的经过**训练的模型**。

现在，我们准备讨论 ModelOps 的实现策略和示例工具。

  

# 管理代码版本和质量

数据科学项目处理大量代码，因此数据科学家需要使用 Git 等**源代码版本控制** ( **SVC** )系统作为强制组件。使用 Git 最明显的方式是使用代码协作平台，比如 GitLab 或 GitHub。这些平台提供了现成的 Git 服务器，以及用于代码审查和问题管理的有用的协作工具，使得共享项目的工作更加容易。此类平台还提供了与 CI/CD 解决方案的集成，创建了一个完整且易于配置的软件交付管道。GitHub 和 GitLab 可以免费使用，GitLab 可用于内部安装，因此您的团队没有理由错过使用这些平台的好处。

许多团队将 Git 与一个流行的平台产品同义，但是知道它不是您唯一的选择有时是有用的。有时，您无法访问互联网，也无法在服务器上安装额外的软件，但仍然希望获得将代码存储在共享存储库中的好处。您仍然可以在那些受限的环境中使用 Git。Git 有一个叫做**文件远程**的有用特性，它允许你把你的代码推到任何地方。

例如，您可以使用 u 盘或共享文件夹作为远程存储库:

```
git clone --bare /project/location/my-code /remote-location/my-code #copy your code history from a local git repo
git remote add usb file:///remote/location/my-code 
# add your remote as a file location
git remote add usb file:///remote/location/my-code 
# add your remote as a file location
git push usb master 
# push the code

# Done! Other developers can set up your remote and pull updates:
git remote add usb file:///remote/location/my-code # add your remote as a file location
git pull usb mater # pull the code
```

通过将`file:///`路径改为`ssh:///`路径，您还可以将代码推送到本地网络上的远程 SSH 机器。

大多数数据科学项目都是用 Python 编写的，在 Python 中，静态代码分析和代码构建系统不像在其他编程语言中那样普遍。这些工具允许您自动整理代码，并在每次尝试构建项目时检查代码中的关键错误和可能的 bug。Python 也有这样的工具——看看预提交([https://pre-commit.com](https://pre-commit.com))。

以下屏幕截图演示了在 Python 代码存储库上运行预提交:

![](Images/07a4c0aa-e54b-40ab-9d85-eb90b5dc742f.png)

已经介绍了处理代码的主要建议，现在让我们看看如何对数据实现相同的结果，这是任何数据科学项目不可或缺的一部分。

  

# 将数据与代码一起存储

正如您之前看到的，我们可以将数据科学项目中的代码构建成一组管道，这些管道产生各种工件:报告、模型和数据。不同版本的代码产生不断变化的输出，数据科学家经常需要重现结果或使用来自过去版本的管道的工件。

这将数据科学项目与软件项目区分开来，并产生了管理数据版本和代码的需求:**数据版本控制**(**)。一般来说，不同的软件版本可以通过单独使用源代码来重构，但是对于数据科学项目来说，这是不够的。让我们看看当您试图使用 Git 跟踪数据集时会出现什么问题。**

**  

# 跟踪和版本控制数据

为了在数据科学管道的每个版本之间进行训练和切换，您应该跟踪数据和代码的变化。有时，一个完整的项目管道可能需要几天来计算。为了节省时间，您不仅应该存储和记录传入的数据集，还应该存储和记录项目的中间数据集。从单个数据集创建几个模型训练管道非常方便，无需在每次需要时都等待数据集管道完成。

结构化管道和中间结果是一个有趣的话题，值得特别关注。项目的管道结构决定了哪些中间结果可供使用。每个中间结果都创建了一个分支点，其他几个管道可以从这个分支点开始。这创造了重用中间结果的灵活性，但是以存储和时间为代价。包含许多中间步骤的项目会消耗大量磁盘空间，并且会花费更多的时间来计算，因为磁盘输入/输出需要大量时间。

请注意，模型培训管道和生产管道应该不同。为了研究的灵活性，模型训练管道可能有许多中间步骤，但是生产管道应该在性能和可靠性方面进行高度优化。只需要执行执行最终产品流水线严格必需的中间步骤。

存储数据文件对于再现结果是必要的，但是对于理解它们是不够的。通过记录数据描述，以及包含您的团队从数据中得出的摘要和结论的所有报告，您可以节省大量时间。如果可以的话，以简单的文本格式存储这些文档，这样就可以在版本控制系统中很容易地跟踪它们以及相应的代码。

您可以使用以下文件夹结构来存储项目中的数据:

*   项目根目录:
    *   数据:
        *   原始数据—来自客户的原始数据
        *   中间数据—由处理管道生成的中间数据
        *   预处理-模型数据集或输出文件
    *   报告-针对 EDA、模型质量等的项目报告
    *   参考-数据字典和数据源文档

  

# 在实践中存储数据

我们已经探索了存储和管理数据工件以及代码的重要性，但是没有研究我们如何在实践中做到这一点。像 Git 这样的代码版本控制系统不适合这个用例。Git 是专门为存储源代码变更而开发的。在内部，Git 中的每个更改都存储为一个`diff`文件，该文件表示源代码文件中的更改行。

您可以在下面的截图中看到一个简单的`diff`文件示例:

![](Images/186ed2d9-af3c-402d-a2be-b9a513b47f2e.png)

标有+的突出显示的行代表添加的行，标有–的突出显示的行代表删除的行。在 Git 中添加大的二进制或文本文件被认为是不好的做法，因为这会导致大量冗余的`diff`计算，这使得存储库处理起来很慢并且很大。

文件服务于一个非常特殊的问题:它们允许开发人员浏览、讨论和在变更集之间切换。`diff`是一种基于行的格式，面向文本文件。相反，二进制数据文件中的微小变化将导致完全不同的数据文件。在这种情况下，Git 将为每个小的数据修改生成一个巨大的`diff`。

一般来说，您不需要浏览或讨论对基于行的格式的数据文件的更改，因此计算和存储每个新数据版本的`diff`文件是不必要的:每次更改时存储整个数据文件要简单得多。

对数据版本控制系统日益增长的需求产生了几个解决这个问题的技术方案，最流行的是 GitLFS 和 DVC。GitLFS 允许您在 Git 中存储大文件，而不会产生大的差异，而 DVC 更进一步，允许您在各种远程位置存储数据，如亚马逊 S3 存储或远程 SSH 服务器。DVC 不仅实现了数据版本控制，还允许您通过捕获代码及其输入数据、输出文件和指标来创建自动化的可再现管道。DVC 还处理管道依赖图，因此它可以自动查找和执行管道的任何先前步骤，以生成您需要作为代码输入的文件。

现在我们已经配备了处理数据存储和版本控制的工具，让我们看看如何管理 Python 环境，这样您的团队就不会在服务器上的包冲突上浪费时间。

  

# 管理环境

数据科学项目依赖大量开源库和工具来进行数据分析。这些工具中有许多会不断更新新特性，有时会破坏 API。以一种可共享的格式修复所有依赖关系是很重要的，这种格式允许每个团队成员使用相同的版本和构建库。

Python 生态系统有多种环境管理工具来处理不同的问题。工具在使用案例上有所重叠，并且常常难以选择，因此我们将简要介绍每一种工具:

*   pyenv(【https://github.com/pyenv/pyenv】T2)是一个在单台机器上管理 Python 发行版的工具。不同的项目可能使用不同的 Python 版本，pyenv 允许您在项目之间切换不同的 Python 版本。
*   **virtualenv**([https://virtualenv . pypa . io](https://virtualenv.pypa.io))是一个创建包含不同 Python 包集合的虚拟环境的工具。虚拟环境对于在不同项目之间切换上下文非常有用，因为它们可能需要使用 Python 包的冲突版本。
*   **pipenv**([https://pipenv-searchable . readthedocs . io](https://pipenv-searchable.readthedocs.io))比 virtualenv 更上一层楼。Pipenv 关心的是为一个项目自动创建一个可共享的虚拟环境，其他开发人员可以很容易地使用它。
*   **康达**([https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/))是 pipenv 一样的另一个环境管理者。Conda 在数据科学界很受欢迎，原因有几个:
    *   它允许通过`environment.yml`文件与其他开发者共享环境。
    *   它提供了 Anaconda Python 发行版，其中包含千兆字节的预装流行数据科学包。
    *   它提供了流行的数据分析和机器学习库的高度优化版本。科学 Python 包通常需要从源代码构建依赖关系。
    *   Conda 可以把 CUDA 框架和你喜欢的深度学习框架一起安装。CUDA 是在 GPU 上优化深度神经网络所需的专用计算库。

如果您还没有使用 conda 来管理数据科学项目环境，请考虑使用 conda。它不仅能解决你的环境管理问题，还能通过加速计算节省时间。下面的图展示了使用 **pip** 和 **conda** 安装的 TensorFlow 库的性能差异(点击此链接可以找到原文:【https://www.anaconda.com/tensorflow-in-anaconda/】T4):

![](Images/a1f3b1e7-4599-4dcb-97ae-7c7486879d5d.png)

接下来，我们将讨论实验跟踪的主题。实验是每个数据科学项目的自然组成部分。一个项目可能包含数百甚至数千次实验的结果。做记录很重要，这样你才能对实验结果做出正确的结论。

  

# 跟踪实验

实验是数据科学的核心。数据科学家执行许多实验来寻找解决手头任务的最佳方法。一般来说，实验存在于与数据处理流水线步骤相关的集合中。

例如，您的项目可能包含以下实验集:

*   特征生成实验
*   不同机器学习算法的实验
*   超参数优化实验

每个实验都会影响其他实验的结果，因此能够独立地重现每个实验是至关重要的。跟踪所有结果也很重要，这样您的团队可以比较管道变量，并根据指标值为您的项目选择最佳的一个。

带有数据文件和代码版本链接的简单电子表格文件可用于跟踪所有实验，但再现实验将需要大量手工工作，并且不能保证按预期工作。尽管在一个文件中跟踪实验需要手工操作，但是这种方法有它的好处:它非常容易开始，而且版本化也很好。例如，您可以将实验结果存储在一个简单的 CSV 文件中，该文件与您的代码一起在 Git 中进行版本控制。

指标跟踪文件的推荐最小列集如下:

*   实验日期
*   代码版本(Git 提交哈希)
*   型号名称
*   模型参数
*   训练数据集大小
*   训练数据集链接
*   验证数据集大小(交叉验证的折叠数)
*   验证数据集链接(无交叉验证)
*   测试数据集大小
*   测试数据集链接
*   指标结果(每个指标一列；每个数据集一列)
*   输出文件链接
*   实验描述

如果您有适量的实验，文件很容易处理，但是如果您的项目使用多个模型，并且每个模型都需要大量的实验，那么使用文件就变得很麻烦。如果一个数据科学家团队同时进行实验，跟踪每个团队成员的文件将需要手动合并，数据科学家最好花时间进行更多的实验，而不是合并其他队友的结果。对于更复杂的研究项目，存在跟踪实验结果的特殊框架。这些工具集成到模型培训管道中，并允许您在共享数据库中自动跟踪实验结果，以便每个团队成员都可以专注于实验，而所有簿记都自动发生。这些工具提供了一个丰富的用户界面，用于搜索实验结果、浏览度量图，甚至存储和下载实验工件。使用实验跟踪工具的另一个好处是，它们可以跟踪大量技术信息，这些信息可能会变得很方便，但是手工收集起来太繁琐了:服务器资源、服务器主机名、脚本路径，甚至实验运行中出现的环境变量。

数据科学社区使用三种主要的开源解决方案来跟踪实验结果。这些工具包含的功能比实验跟踪多得多，我们将简要介绍每个工具:

*   **神圣的**:这是一个模块化架构的高级实验追踪服务器。它有一个用于管理和跟踪实验的 Python 框架，可以很容易地集成到现有的代码库中。神圣也有几个用户界面，你的团队可以用来浏览实验结果。在所有其他的解决方案中，只有神圣专注于实验跟踪。它捕获最广泛的信息，包括服务器信息、指标、工件、日志，甚至实验源代码。神圣提出了最完整的实验跟踪体验，但很难管理，因为它需要您设置一个单独的跟踪服务器，应该始终在线。如果无法访问跟踪服务器，您的团队将无法跟踪实验结果。
*   这是一个实验框架，允许跟踪实验、服务模型和管理数据科学项目。MLflow 易于集成，既可以在客户端-服务器设置中使用，也可以在本地使用。它的跟踪功能落后于神圣的发电站，但对于大多数数据科学项目来说已经足够了。MLflow 还提供了从模板快速启动项目和将训练好的模型作为 API 的工具，提供了一种将实验结果作为生产就绪服务发布的快速方法。
*   DVC:这是一个用于数据版本控制和管道管理的工具包。它还提供了基本的基于文件的实验跟踪功能，但与 MLflow 和 Sacred 相比，它的可用性较差。DVC 的力量在于实验管理:它允许你创建完全版本化和可复制的模型训练管道。有了 DVC，每个团队成员都能够从服务器上获取代码、数据和管道，并通过一条命令再现结果。DVC 的学习曲线相当陡峭，但值得学习，因为它解决了在数据科学项目合作中出现的许多技术问题。如果你的度量跟踪要求很简单，你可以依赖 DVC 的内置解决方案，但如果你需要更丰富和可视化的东西，请将 DVC 与 MLflow 或神圣跟踪结合起来——这些工具并不相互排斥。

现在，您应该已经完全理解了什么工具可以用来在您的项目中将代码、数据和实验作为一个单独的实体进行跟踪。接下来，我们将讨论数据科学项目中自动化测试的主题。

  

# 自动化测试的重要性

自动化测试在软件工程项目中被认为是强制性的。软件代码中的微小变化会在其他部分引入意想不到的错误，因此尽可能频繁地检查一切是否正常工作是很重要的。用编程语言编写的自动化测试允许你测试系统任意多次。CI 的原则建议每次将代码变更推送到版本控制系统时都要运行测试。对于所有主要的编程语言，都存在大量的测试框架。使用它们，开发人员可以为他们产品的后端和前端部分创建自动化测试。大型软件项目可能包括数以千计的自动化测试，每次有人更改代码时都会运行这些测试。测试会消耗大量资源，并且需要大量时间才能完成。为了解决这个问题，CI 服务器可以在多台机器上并行运行测试。

在软件工程中，我们可以将所有的测试分成一个层次:

1.  端到端测试对系统的主要功能进行全面检查。在数据科学项目中，端到端测试可以在完整数据集上训练模型，并检查指标值是否满足最低模型质量要求。

2.  集成测试检查系统的每个组件是否按预期的那样协同工作。在数据科学系统中，集成测试可能会检查模型测试管道的所有步骤是否成功完成，并提供所需的结果。
3.  单元测试检查单个的类和函数。在数据科学项目中，单元测试可以检查数据处理管道步骤中单个方法的正确性。

如果软件测试世界在技术上如此发达，那么数据科学项目能从自动化测试中受益吗？数据科学代码和软件测试代码的主要区别在于数据的可靠性。对于大多数软件项目来说，在测试运行之前生成的一组固定的测试数据就足够了。在数据科学项目中，情况有所不同。对于模型培训管道的完整测试，您可能需要千兆字节的数据。一些管道可能运行数小时甚至数天，并需要分布式计算集群，因此测试它们变得不切实际。由于这个原因，许多数据科学项目避免自动化测试。因此，他们遭受了意想不到的错误、连锁反应和缓慢的变更集成周期。

连锁反应是一个常见的软件工程问题，当系统中某个部分的微小变化会以一种意想不到的方式影响其他组件，从而导致错误。自动化测试是在连锁反应造成任何实际损害之前检测连锁反应的有效解决方案。

尽管困难重重，但自动化测试的好处太大了，不容忽视。事实证明，忽视测试比构建测试的成本要高得多。数据科学项目和软件项目都是如此。自动化测试的好处随着项目规模、复杂性和团队规模的增加而增加。如果您领导一个复杂的数据科学项目，可以考虑将自动化测试作为项目的强制性要求。

让我们看看如何测试数据科学项目。模型训练管道的端到端测试可能不切实际，但是测试单个管道步骤呢？除了模型训练代码，每个数据科学项目都有一些业务逻辑代码和数据处理代码。这些代码的大部分可以从分布式计算框架中抽象出来，放在易于测试的隔离类和函数中。

如果您从一开始就将测试放在心上来设计项目的代码库，那么自动化测试将会容易得多。您团队中的软件架构师和首席工程师应该将代码的可测试性作为代码评审的主要验收标准之一。如果代码被恰当地封装和抽象，测试会变得更容易。

具体来说，就拿模特培训管道来说吧。如果我们将它分成一系列具有明确定义的接口的步骤，那么我们可以将数据预处理代码与模型训练代码分开测试。如果数据预处理花费大量时间，并且需要昂贵的计算资源，那么至少可以测试管道的单个部分。即使是基本的功能级测试(单元测试)也能为你节省很多时间，从单元测试的基础过渡到完整的端到端测试要容易得多。

要从项目中的自动化测试中获益，请从以下准则开始:

*   为更好的可测试性设计你的代码。
*   从小做起；编写单元测试。
*   考虑构建集成和端到端测试。
*   坚持下去。请记住，测试可以节省时间——尤其是当您的团队不得不在新部署的代码中修复意外错误的那些夜晚。

我们已经看到了如何在数据科学项目中管理、测试和维护代码质量。接下来，让我们看看如何打包代码进行部署。

  

# 包装代码

为数据科学项目部署 Python 代码时，您有几种选择:

*   **常规 Python 脚本**:你只需将一堆 Python 脚本部署到服务器上并运行它们。这是最简单的部署形式，但是需要大量的手动准备工作:您需要安装所有必需的包，填写配置文件，等等。虽然这些操作可以通过使用 ansi ble(【https://www.ansible.com/】)之类的工具来自动化，但是除了没有长期可维护性目标的最简单的项目，不推荐使用这种形式的部署。
*   **Python 包**:使用`setup.py` 文件创建 Python 包是打包 Python 代码的一种更方便的方式。PyScaffold 等工具为 Python 包提供了现成的模板，因此您不需要花费太多时间来构建项目。在 Python 包的情况下，Ansible 仍然是自动化手动部署操作的可行选项。
*   **Docker 镜像**:**Docker(【https://www.docker.com/】T4)基于一种叫做 Linux 容器的技术。Docker 允许将您的代码打包到一个隔离的可移植环境中，该环境可以在任何 Linux 机器上轻松部署和扩展。这就像打包、运输和运行您的应用程序以及所有依赖项，包括 Python 解释器、数据文件和操作系统发行版，而无需进入重量级虚拟机的世界。Docker 的工作方式是从 **Dockerfile** *中指定的一组命令构建一个 **Docker 镜像**。*一个 Docker 镜像的运行实例被称为 **Docker 容器** *。***

现在，我们已经准备好将处理代码、数据、实验、环境、测试、打包和部署的所有工具集成到一个连贯的流程中，以交付机器学习模型。

  

# 连续模型训练

将 CI/CD 应用于数据科学项目的最终目标是拥有一个连续的学习管道，可以自动创建新的模型版本。这种程度的自动化将允许您的团队在推出更改的代码后立即检查新的实验结果。如果一切按预期运行，自动化测试完成，并且模型质量报告显示良好的结果，那么模型可以被部署到在线测试环境中。

让我们描述连续模型学习的步骤:

1.  CI:
    1.  执行静态代码分析。
    2.  启动自动化测试。
2.  连续模型学习:
    1.  获取新数据。
    2.  生成 EDA 报告。
    3.  启动数据质量测试。
    4.  执行数据处理并创建训练数据集。
    5.  训练一个新模型。
    6.  测试模型的质量。
    7.  将实验结果记录在实验日志中。

3.  光盘:
    1.  打包新的模型版本。
    2.  将源代码打包。
    3.  将模型和代码发布到目标服务器。
    4.  推出新版本的系统。

CI/CD 服务器可以自动化前面管道的所有部分。CI/CD 步骤应该易于处理，因为它们是创建 CI/CD 服务器的目的。持续的模型学习也不应该很难，只要您构建您的管道，以便它可以从命令行自动启动。像 DVC 这样的工具可以帮助你创建可重复的管道，这使得它成为持续模型学习管道的一个有吸引力的解决方案。

现在，让我们看看如何在数据科学项目中构建 ModelOps 管道。

  

# 案例研究–为预测性维护系统构建模型

Oliver 是一家名为 MannCo 的大型制造公司的数据科学项目的团队领导，该公司的工厂遍布全国多个城市。Oliver 的团队开发了一种预测性维护模型，可以帮助 MannCo 预测和防止昂贵的设备损坏，这种损坏会导致昂贵的维修和长时间的生产线停机。该模型将多个传感器的测量值作为输入，并输出可用于规划诊断和维修会话的封装概率。

这个例子包含一些技术细节。如果您不熟悉本案例研究中提到的技术，您可能希望通过链接更好地了解细节。

这种设备的每一件都有其独特之处，因为它在曼科每一家工厂的不同条件下运行。这意味着奥利弗的团队需要不断适应和重新培训不同工厂的不同模型。让我们看看他们是如何通过构建 ModelOps 管道来解决这个任务的。

团队中有几个数据科学家，所以他们需要一个工具来互相共享代码。客户要求，出于安全考虑，所有代码都应该存储在本地公司服务器上，而不是云中。奥利弗决定使用 git lab([https://about.gitlab.com/](https://about.gitlab.com/))，因为这是公司的惯例。就整个代码管理流程而言，Oliver 建议使用 git flow(【https://danielkummer.github.io/git-flow-cheatsheet/】T2)。它为每个团队成员创建新的特性、版本和补丁提供了一套通用的规则。

奥利弗知道一个可靠的项目结构将帮助他的团队正确地组织代码、笔记本、数据和文档，所以他建议他的团队使用 py scaffold([https://pyscaffold.readthedocs.io/](https://pyscaffold.readthedocs.io/))和数据科学项目插件([https://github.com/pyscaffold/pyscaffoldext-dsproject](https://github.com/pyscaffold/pyscaffoldext-dsproject))。PyScaffold 允许他们引导一个项目模板，确保以统一的方式存储和版本化数据科学项目。PyScaffold 已经提供了`environment.yml` 文件，该文件定义了一个模板 Anaconda([https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/))环境，因此团队没有忘记从项目开始就将包依赖关系锁定在一个版本化文件中。奥利弗还决定使用 DVC([https://dvc.org/](https://dvc.org/))通过公司内部的 SFTP 服务器对数据集进行版本控制。他们还为`pyscaffold`命令使用了一个`--gitlab`标志，这样当他们需要的时候就有了一个现成的 GitLab CI/CD 模板。

项目结构如下所示(摘自`pyscaffold-dsproject`文档):

```
├── AUTHORS.rst <- List of developers and maintainers.
├── CHANGELOG.rst <- Changelog to keep track of new features and fixes.
├── LICENSE.txt <- License as chosen on the command-line.
├── README.md <- The top-level README for developers.
├── configs <- Directory for configurations of model & application.
├── data
│ ├── external <- Data from third party sources.
│ ├── interim <- Intermediate data that has been transformed.
│ ├── processed <- The final, canonical data sets for modeling.
│ └── raw <- The original, immutable data dump.
├── docs <- Directory for Sphinx documentation in rst or md.
├── environment.yaml <- The conda environment file for reproducibility.
├── models <- Trained and serialized models, model predictions,
│ or model summaries.
├── notebooks <- Jupyter notebooks. Naming convention is a number (for
│ ordering), the creator's initials and a description,
│ e.g. `1.0-fw-initial-data-exploration`.
├── references <- Data dictionaries, manuals, and all other materials.
├── reports <- Generated analysis as HTML, PDF, LaTeX, etc.
│ └── figures <- Generated plots and figures for reports.
├── scripts <- Analysis and production scripts which import the
│ actual PYTHON_PKG, e.g. train_model.
├── setup.cfg <- Declarative configuration of your project.
├── setup.py <- Use `python setup.py develop` to install for development or
| or create a distribution with `python setup.py bdist_wheel`.
├── src
│ └── PYTHON_PKG <- Actual Python package where the main functionality goes.
├── tests <- Unit tests which can be run with `py.test`.
├── .coveragerc <- Configuration for coverage reports of unit tests.
├── .isort.cfg <- Configuration for git hook that sorts imports.
└── .pre-commit-config.yaml <- Configuration of pre-commit git hooks.
```

项目团队很快发现，他们需要执行和比较许多实验来为不同的制造工厂构建模型。他们评估了 DVC 的度量跟踪能力。它允许使用 Git 中一个简单的版本化文本文件跟踪所有指标。虽然这个特性对于简单的项目来说很方便，但是在有多个数据集和模型的项目中使用它会很困难。最终，他们决定使用更先进的度量追踪器——ml flow([https://mlflow.org](https://mlflow.org))。它提供了一个方便的用户界面来浏览实验结果，并允许使用一个共享的数据库，这样每个团队成员都能够快速地与团队分享他们的结果。MLflow 是作为一个常规 Python 包安装和配置的，因此它很容易集成到项目的现有技术栈中:

![](Images/bcceca06-f47b-4ed5-92d5-5c4213c99aba.png)

该团队还决定利用 DVC 管道来使每个实验易于重复。该团队喜欢使用 Jupyter 笔记本制作模型原型，因此他们决定使用 paper mill([https://papermill.readthedocs.io/en/latest/](https://papermill.readthedocs.io/en/latest/))来处理笔记本，因为它们是一组参数化的 Python 脚本。Papermill 允许从命令行执行 Jupyter 笔记本，而无需启动 Jupyter 的 web 界面。该团队发现该功能与 DVC 管道一起使用非常方便，但是运行单个笔记本的命令行开始变得太长:

```
dvc run -d ../data/interim/ -o ../data/interim/01_generate_dataset -o ../reports/01_generate_dataset.ipynb papermill --progress-bar --log-output --cwd ../notebooks ../notebooks/01_generate_dataset.ipynb ../reports/01_generate_dataset.ipynb
```

为了解决这个问题，他们编写了一个 Bash 脚本来集成 DVC 和 papermill，这样团队成员就可以通过减少终端输入来创建可重复的实验:

```
#!/bin/bash
set -eu
​
if [ $# -eq 0 ]; then
 echo "Use:"
 echo "./dvc-run-notebook [data subdirectory] [notebook name] -d [your DVC dependencies]"
 echo "This script executes DVC on a notebook using papermill. Before running create ../data/[data subdirectory] if it does not exist and do not forget to specify your dependencies as multiple last arguments"
 echo "Example:"
 echo "./dvc-run-notebook interim ../notebooks/02_generate_dataset.ipynb -d ../data/interim/"
 exit 1
fi
​
NB_NAME=$(basename -- "$2")
​
CMD="dvc run ${*:3} -o ../data/$1 -o ../reports/$NB_NAME papermill --progress-bar --log-output --cwd ../notebooks ../notebooks/$NB_NAME ../reports/$NB_NAME"
​
echo "Executing the following DVC command:"
echo $CMD
$CMD 
```

当在一个项目中使用多个开源 ModelOps 工具时，您的团队可能需要花费一些时间将它们集成在一起。做好准备，并做好相应的计划。

随着时间的推移，一些代码开始在笔记本中复制。PyScaffold 模板通过将重复代码封装在项目的包目录— `src` *中，提供了解决这个问题的方法。*这样，项目团队可以在笔记本之间快速共享代码。要在本地安装项目的包，他们只需从项目的根目录使用以下命令:

```
pip install -e .
```

随着项目发布日期的临近，所有稳定的代码库都迁移到了项目的`src`和`scripts`目录中。`scripts`目录包含一个单一的入口点脚本，用于训练一个新的模型版本，该版本被输出到`models`目录，由 DVC 进行跟踪。

为了确保新的变化不会破坏任何重要的东西，团队使用`pytest`([https://docs.pytest.org/](https://docs.pytest.org/))为稳定的代码库编写了一套自动化测试。测试还在团队创建的特殊测试数据集上检查了模型质量。Oliver 修改了由 PyScaffold 生成的 GitLab CI/CD 模板，以便在 Git 存储库中推送每个新的提交时运行测试。

客户要求一个简单的模型 API，因此团队决定使用 MLflow 服务器([https://mlflow.org/docs/latest/models.html](https://mlflow.org/docs/latest/models.html))，因为 MLflow 已经集成到项目中。为了进一步自动化部署和打包过程，该团队决定将 Docker 与 GitLab CI/CD 一起使用。为此，他们遵循 GitLab 的构建 Docker 映像指南([https://docs . git lab . com/ee/ci/Docker/using _ Docker _ build . html)](https://docs.gitlab.com/ee/ci/docker/using_docker_build.html)。

该项目的整体 ModelOps 流程包括以下步骤:

1.  在代码中创建新的更改。
2.  运行代码质量和样式的提交前测试(由 PyScaffold 提供)。
3.  在 GitLab CI/CD 中运行 pytest 测试。
4.  将代码和训练好的模型打包成 GitLab CI/CD 中的 Docker 映像。
5.  将 Docker 映像推入 GitLab CI/CD 中的 Docker 注册表。
6.  在 GitLab UI 中进行手动确认后，在客户服务器上运行`update`命令。这个命令只是将新版本的 Docker 映像从注册表推送到客户的服务器，并运行它来代替旧版本。如果你想知道如何在 GitLab CI/CD 中做到这一点，看看这里:[https://docs . git lab . com/ee/CI/environments . html # configuring-manual-deployments](https://docs.gitlab.com/ee/ci/environments.html#configuring-manual-deployments)。

请注意，在实际项目中，您可能希望将部署分为至少两个不同环境的几个阶段:试运行和生产。

创建端到端的 ModelOps 管道简化了部署过程，并允许团队在投入生产之前发现错误，以便团队能够专注于构建模型，而不是执行重复的操作来测试和部署模型的新版本。

作为本章的结论，我们将查看一系列可用于构建 ModelOps 管道的工具。

  

# 为您的项目提供电源包

数据科学社区有大量开源工具可以帮助您构建 ModelOps 管道。有时，很难浏览永无止境的产品、工具和库列表，所以我认为这个工具列表会对您的项目有所帮助和益处。

对于 Python 的静态代码分析，请参见以下内容:

*   flake 8([http://flake8.pycqa.org](http://flake8.pycqa.org))—Python 代码的样式检查器
*   MyPy([http://www.mypy-lang.org](http://www.mypy-lang.org))—Python 的静态类型
*   wemake([https://github.com/wemake-services/wemake-python-styleguide](https://github.com/wemake-services/wemake-python-styleguide))——flake 8 的一组增强功能

以下是一些有用的 Python 工具:

*   py scaffold([https://pyscaffold.readthedocs.io/](https://pyscaffold.readthedocs.io/))—一个项目模板引擎。PyScaffold 可以为你建立一个项目结构。这个`dsproject`扩展([https://github.com/pyscaffold/pyscaffoldext-dsproject](https://github.com/pyscaffold/pyscaffoldext-dsproject))包含了一个很好的数据科学项目模板。
*   预提交([https://pre-commit.com](https://pre-commit.com))——一个允许你设置每次提交代码时运行的 Git 挂钩的工具。甚至在您决定使用 CI/CD 服务器之前，就可以将自动格式化、样式蛋糕、代码格式化和其他工具集成到您的构建管道中。
*   pytest([https://docs.pytest.org/](https://docs.pytest.org/))—一个 Python 测试框架，允许您使用可重用的夹具来构建测试。当测试具有许多数据依赖性的数据科学管道时，这很方便。
*   hypothesis([https://hypothesis . works](https://hypothesis.works))—一个 Python 的模糊测试框架，它基于关于函数的元数据创建自动化测试。

对于 CI/CD 服务器，请参见以下内容:

*   Jenkins([https://Jenkins . io](https://jenkins.io))—一个流行、稳定、陈旧的 CI/CD 服务器解决方案。它包含了许多功能，但与更现代的工具相比，使用起来有点麻烦。
*   git lab CI/CD([https://docs.gitlab.com/ee/ci/](https://docs.gitlab.com/ee/ci/))—是一个免费的 CI/CD 服务器，有云和内部选项。它很容易设置和使用，但迫使你生活在 GitLab 生态系统中，这可能不是一个坏决定，因为 GitLab 是最好的协作平台之一。
*   特拉维斯 CI([https://travis-ci.org](https://travis-ci.org))和圈子 CI([https://circleci.com](https://circleci.com))—云 CI/CD 解决方案。如果您在云环境中开发，这很有用。

对于实验跟踪工具，请参见以下内容:

*   ml flow([https://mlflow.org](https://mlflow.org))—可以在本地和共享的客户端-服务器设置中使用的实验跟踪框架
*   神圣的([https://github.com/IDSIA/sacred](https://github.com/IDSIA/sacred))—一个功能丰富的实验跟踪框架
*   https://dvc.org/doc/get-started/metrics DVC——使用 Git 的基于文件的度量跟踪解决方案

有关数据版本控制，请参见以下内容:

*   DVC([https://dvc.org/](https://dvc.org/))—数据科学项目的数据版本控制
*   Git lfs([https://git-lfs.github.com](https://git-lfs.github.com))—在 Git 中存储大文件的通用解决方案

对于管道工具，请参见以下内容:

*   数据科学项目的可再生管道:
*   https://dvc.org/doc/get-started/pipeline([DVC](https://dvc.org/doc/get-started/pipeline)
*   MLflow 项目([https://mlflow.org/docs/latest/projects.html](https://mlflow.org/docs/latest/projects.html))

对于代码协作平台，请参见以下内容:

*   GitHub([https://github.com/](https://github.com/))—世界上最大的开源库，最好的代码协作平台之一
*   git lab([https://about.gitlab.com](https://about.gitlab.com))—功能丰富的代码协作平台，具有云和内部部署选项
*   Atlassian bit bucket([https://bitbucket.org/](https://bitbucket.org/))—Atlassian 的代码协作解决方案，它与他们的其他产品、吉拉问题跟踪器和 Confluence wiki 很好地集成在一起

要部署您的代码，请参见以下内容:

*   docker([https://www.docker.com/](https://www.docker.com/))—一个工具，用于管理容器并将代码打包到一个隔离的可移植环境中，该环境可以在任何 Linux 机器上轻松部署和扩展
*   kubernetes([https://kubernetes.io/](https://kubernetes.io/))—一个容器编排平台，自动化容器化应用程序的部署、扩展和管理
*   ansi ble([https://www.ansible.com/](https://www.ansible.com/))—一个配置管理和自动化工具，如果您在部署设置中不使用容器，它可以方便地用于部署自动化和配置

  

# 摘要

在这一章中，我们介绍了 ModelOps——一组用于自动化数据科学项目中常见操作的实践。我们探讨了 ModelOps 与 DevOps 的关系，并描述了 ModelOps 流程中的主要步骤。我们研究了管理代码、版本控制数据以及在团队成员之间共享项目环境的策略。我们还研究了实验跟踪和自动化测试对于数据科学项目的重要性。作为结论，我们通过持续的模型训练概述了完整的 CI/CD 管道，并探索了一套可用于构建这种管道的工具。

在下一章中，我们将了解如何构建和管理数据科学技术栈。**