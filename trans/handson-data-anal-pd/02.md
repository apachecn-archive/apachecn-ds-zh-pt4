

# *第一章*:数据分析简介

在我们用`pandas`开始动手介绍数据分析之前，我们需要了解数据分析的基础知识。那些曾经看过软件库文档的人都知道，如果你不知道自己在找什么，这是多么的困难。因此，至关重要的是，我们不仅要掌握编码方面的知识，还要掌握分析数据所需的思维过程和工作流程，这将被证明对增强我们未来的技能最为有用。

与科学方法非常相似，数据科学有一些常见的工作流，当我们想要进行分析并呈现结果时，可以遵循这些工作流。这个过程的支柱是**统计**，它为我们提供了描述数据、做出预测以及得出结论的方法。由于先前的统计知识不是先决条件，本章将让我们接触到我们将在本书中使用的统计概念，以及进一步探索的领域。

在介绍了基础知识之后，我们将为本书的剩余部分设置 Python 环境。Python 是一种强大的语言，它的用途远远超出了数据科学:构建 web 应用程序、软件和 web 抓取等等。为了有效地跨项目工作，我们需要学习如何创建虚拟环境，这将隔离每个项目的依赖关系。最后，我们将学习如何使用 Jupyter 笔记本来理解课文。

本章将涵盖以下主题:

*   数据分析的基础
*   统计学基础
*   设置虚拟环境

# 章节材料

这本书的所有文件都在 GitHub 的 https://GitHub . com/stef molin/Hands-On-the-Hands-On-Data-Analysis-with-Pandas-第二版上。虽然拥有一个 GitHub 帐户并不是阅读这本书的必要条件，但创建一个是个好主意，因为它将作为任何数据/编码项目的投资组合。此外，使用 Git 将提供一个版本控制系统，并使协作变得容易。

小费

查看这篇文章来学习一些 Git 基础知识:[https://www . freecodecamp . org/news/learn-the-basics-of-Git-in-under-10 minutes-da 548267 cc 91/](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/)。

为了获得文件的本地副本，我们有几个选项(从最不有用到最有用排序):

*   下载 ZIP 文件，并将文件解压缩到本地。
*   克隆存储库而不派生它。
*   派生存储库，然后克隆它。

这本书包括每章的练习；因此，对于那些想在 GitHub 上保留其解决方案副本以及原始内容的人来说，强烈推荐**分叉**存储库和**克隆**分叉版本。当我们分叉一个存储库时，GitHub 会在我们自己的概要文件下创建一个存储库，包含原始版本的最新版本。然后，每当我们对我们的版本进行更改时，我们可以将更改推回。请注意，如果我们只是简单地克隆，我们不会获得这种好处。

启动此过程的相关按钮在下面的屏幕截图中被圈出:

![Figure 1.1 – Getting a local copy of the code for following along
](image/Figure_1.1_B16834.jpg)

图 1.1–获取代码的本地副本以便跟进

重要说明

克隆过程会将文件复制到当前工作目录中名为`Hands-On-Data-Analysis-with-Pandas-2nd-edition`的文件夹中。要创建一个文件夹来存放这个存储库，我们可以使用`mkdir my_folder && cd my_folder`。这将创建一个名为`my_folder`的新文件夹(目录),然后将当前目录更改为该文件夹，之后我们可以克隆存储库。我们可以通过在它们之间添加`&&`来将这两个命令(以及任意数量的命令)链接在一起。这可以被认为是*，然后是*(假设第一个命令成功)。

该存储库的每一章都有文件夹。本章资料可以在[https://github . com/stef molin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch _ 01](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_01)找到。虽然这一章的大部分内容不涉及任何编码，但是请随意跟随 GitHub 网站上的`introduction_to_data_analysis.ipynb`笔记本，直到我们在本章末尾设置好我们的环境。之后，我们将使用`check_your_environment.ipynb`笔记本来熟悉 Jupyter 笔记本，并进行一些检查，以确保本书其余部分的所有内容都设置正确。

因为用于生成这些笔记本内容的代码不是本章的主要焦点，所以大部分代码被分成了`visual_aids`包和`check_environment.py`文件，前者用于创建解释整本书概念的视觉效果。如果你选择考察这些档案，不要不知所措；本书将涵盖与数据科学相关的所有内容。

每章包括练习；然而，只针对这一章，有一个`exercises.ipynb`笔记本，带有生成一些初始数据的代码。完成这些练习需要具备基本的 Python 知识。对于那些想复习基础知识的人来说，一定要快速浏览一下本章材料中的`python_101.ipynb`笔记本。官方 Python 教程是一个开始更正式介绍的好地方:【https://docs.python.org/3/tutorial/index.html[。](https://docs.python.org/3/tutorial/index.html)

# 数据分析的基础知识

数据分析是一个高度迭代的过程，包括收集、准备(争论)**探索性数据分析** ( **EDA** )，以及得出结论。在分析期间，我们会经常回顾这些步骤。下图描述了一个通用的工作流程:

![Figure 1.2 – The data analysis workflow
](image/Figure_1.2_B16834.jpg)

图 1.2–数据分析工作流程

在接下来的几节中，我们将从数据收集开始，对每个步骤进行概述。实际上，这一过程严重偏向数据准备方面。调查发现，尽管数据科学家最不喜欢他们工作中的数据准备方面，但这占了他们工作的 80%([https://www . Forbes . com/sites/Gil press/2016/03/23/data-preparation-most-time-consumption-less-happy-data-science-task-survey-says/](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/))。这个数据准备步骤是`pandas`真正出彩的地方。

## 数据收集

数据收集是任何数据分析的第一步——我们不能分析我们没有的数据。事实上，我们甚至可以在获得数据之前就开始分析。当我们决定要调查或分析什么时，我们必须考虑我们可以收集哪些对我们的分析有用的数据。虽然数据可能来自任何地方，但我们将在本书中探索以下来源:

*   Web 抓取从网站的 HTML 中提取数据(通常使用 Python 包，如`selenium`、`requests`、`scrapy`和`beautifulsoup`)
*   **用于 web 服务的应用程序编程接口**(**API**)，我们可以通过 HTTP 请求收集数据(可能使用`cURL`或`requests` Python 包)
*   数据库(可以用 SQL 或其他数据库查询语言提取数据)
*   互联网提供数据下载的资源，如政府网站或雅虎！金融
*   Log files

    重要说明

    [*第二章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)*处理熊猫数据帧*，将为我们提供处理前述数据源所需的技能。 [*第十二章*](B16834_12_Final_SK_ePub.xhtml#_idTextAnchor254) ，*前路*，为寻找数据源提供了无数资源。

我们被数据包围着，所以可能性是无限的。然而，重要的是要确保我们收集的数据将有助于我们得出结论。例如，如果我们试图确定温度越低，热巧克力的销量是否越高，我们应该收集每天热巧克力销量和温度的数据。虽然看人们走多远去买热巧克力可能很有趣，但这与我们的分析无关。

在开始分析之前，不要太担心寻找完美的数据。很有可能，我们总会想要从初始数据集中添加/删除一些内容，重新格式化，与其他数据合并，或者以某种方式进行更改。这就是数据争论发挥作用的地方。

## 数据角力

**数据争论**是准备数据并将其转换成可用于分析的格式的过程。不幸的是，数据往往是脏的，这意味着在使用之前需要清理(准备)。以下是我们的数据可能会遇到的一些问题:

*   **人为错误**:数据记录(甚至采集)不正确，比如把`1000`填成`100`，或者错别字。此外，同一条目可能会记录多个版本，如`New York City`、`NYC`、`nyc`。
*   计算机错误:也许我们有一段时间没有记录条目(丢失数据)。
*   **意外值**:可能是记录数据的人决定对数字列中的缺失值使用问号，所以现在该列中的所有条目都将被视为文本而不是数值。
*   **信息不全**:想一个有选择题的调查；不是每个人都会回答它们，所以我们会有遗漏的数据，但不是由于计算机或人为错误。
*   **解决方案**:数据可能是每秒收集一次，而我们需要每小时的数据来进行分析。
*   **字段的相关性**:通常，数据是作为某个过程的产物收集或生成的，而不是明确用于我们的分析。为了让它达到可用状态，我们必须清理它。
*   **数据格式**:数据可能以不利于分析的格式记录，这将需要我们对其进行改造。
*   **数据记录过程中的错误配置**:来自错误配置的跟踪器和/或 webhooks 等来源的数据可能会丢失字段或以错误的顺序传递。

这些数据质量问题中的大部分都是可以补救的，但有些是无法补救的，例如每天收集数据，而我们需要每小时解决一次。仔细检查我们的数据并处理任何问题是我们的责任，这样我们的分析才不会被歪曲。我们将在 [*第三章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) 、*与熊猫的数据角力*和 [*第四章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*聚合熊猫数据帧*中深入探讨这一过程。

一旦我们对数据进行了初步清理，我们就为 EDA 做好了准备。注意，在 EDA 过程中，我们可能需要一些额外的数据争论:这两个步骤是高度交织在一起的。

## 探索性数据分析

在 EDA 过程中，我们使用可视化和汇总统计来更好地理解数据。由于人类的大脑擅长挑选视觉模式，数据可视化对任何分析都是必不可少的。事实上，数据的某些特征只能在图中观察到。根据我们的数据，我们可以创建图表来查看感兴趣的变量如何随着时间的推移而演变，比较属于每个类别的观察值的数量，找到异常值，查看连续变量和离散变量的分布，等等。在 [*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*用 Pandas 和 Matplotlib* 可视化数据、以及 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*用 Seaborn 和定制技术*绘图中，我们将学习如何为 EDA 和演示创建这些图。

重要说明

数据可视化非常强大；不幸的是，它们经常会误导人。一个常见的问题源于 *y* 轴的比例，因为大多数绘图工具默认情况下会放大以近距离显示图案。软件很难知道每一个可能的绘图的适当轴限制是什么；因此，我们的工作是在展示我们的结果之前适当地调整坐标轴。你可以在 https://venngage.com/blog/misleading-graphs/[的](https://venngage.com/blog/misleading-graphs/)阅读更多关于情节误导的方式。

在我们之前看到的工作流程图(*图 1.2* )中，EDA 和数据角力共用一个盒子。这是因为它们紧密相连:

*   EDA 之前需要准备数据。
*   在 EDA 过程中创建的可视化可能表明需要额外的数据清理。
*   数据争论使用汇总统计来寻找潜在的数据问题，而 EDA 使用它们来理解数据。当我们进行 EDA 时，不适当的清洁会扭曲结果。此外，还需要数据辩论技能来获得数据子集的汇总统计数据。

在计算汇总统计数据时，我们必须记住我们收集的数据类型。数据可以是**定量的**(可测量的数量)或**分类的**(描述、分组或类别)。在这些数据类别中，我们有进一步的细分，让我们知道我们可以对它们执行什么类型的操作。

例如，分类数据可以是**名义上的**，其中我们为类别的每个级别分配一个数值，比如`on = 1` / `off = 0`。注意，`on`大于`off`的事实是没有意义的，因为我们任意地选择了那些数字来代表状态`on`和`off`。当类别之间有一个等级时，它们是**序数**，这意味着我们可以对级别进行排序(例如，我们可以有`low < medium < high`)。

定量数据可以使用**区间标度**或**比率标度**。区间尺度包括诸如温度之类的东西。我们可以用摄氏度来测量温度，并比较两个城市的温度，但说一个城市比另一个城市热一倍并不意味着什么。因此，可以使用加法/减法，而不是乘法/除法，对区间标度值进行有意义的比较。那么，比率尺度就是那些可以与比率进行有意义的比较的值(使用乘法和除法)。比率尺度的例子包括价格、尺寸和数量。

当我们完成 EDA 时，我们可以通过得出结论来决定接下来的步骤。

## 得出结论

在我们收集了用于分析的数据，清理了数据，并执行了一些彻底的 EDA 之后，是时候得出结论了。这是我们总结 EDA 发现并决定后续步骤的地方:

*   在可视化数据时，我们是否注意到了任何模式或关系？
*   我们能从数据中做出准确的预测吗？转向数据建模有意义吗？
*   我们应该处理缺失的数据点吗？怎么会？
*   数据是如何分布的？
*   这些数据有助于我们回答现有的问题，或者让我们深入了解我们正在调查的问题吗？
*   我们需要收集新的或额外的数据吗？

如果我们决定对数据建模，这就属于机器学习和统计。虽然不是技术上的数据分析，但它通常是下一步，我们将在 [*第 9 章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188) 、*Python 中的机器学习入门*和 [*第 10 章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217) 、*做出更好的预测-优化模型*中介绍。此外，我们将在第 11 章 、*机器学习异常检测*中看到整个过程在实践中是如何工作的。作为参考，在*附录*中的*机器学习工作流程*部分，有一个工作流程图，描绘了从数据分析到机器学习的全过程。 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币与股市*、 [*第八章*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172) 、*基于规则的异常检测*，将专注于从数据分析中得出结论，而不是建立模型。

下一节将回顾统计数据；具有统计学知识的人可以直接跳到*设置虚拟环境*部分。

# 统计学基础

当我们想要对我们正在分析的数据进行观察时，我们经常(如果不是总是)以某种方式求助于统计学。我们拥有的数据被称为**样本**，它是从**人群**中观察到的(并且是其子集)。两大类统计是描述性统计和推断性统计。用**描述性统计**，顾名思义，我们正在寻找用*描述*的样本。**推断统计**涉及使用样本统计数据来*推断*或推断关于总体的某些信息，如基本分布。

重要说明

样本统计被用作总体参数的估计量，这意味着我们必须量化它们的偏差和方差。有许多方法可以做到这一点；一些人会对分布的形状做出假设(参数的)，而另一些人不会(非参数的)。这已经超出了本书的范围，但是知道这一点还是有好处的。

通常，分析的目标是为数据创建一个故事；不幸的是，误用统计数据是非常容易的。这是一句名言的主题:

"有三种谎言:谎言、该死的谎言和统计数字."

——本杰明·迪斯雷利

推论统计学尤其如此，它在许多科学研究和论文中被用来显示研究人员发现的重要性。这是一个更高级的主题，因为这不是一本统计学的书，我们将只简要地触及推理统计学背后的一些工具和原理，这可以进一步探讨。我们将关注描述性统计，以帮助解释我们正在分析的数据。

## 取样

在我们尝试任何分析之前，有一件重要的事情需要记住:我们的样本必须是能够代表总体的随机样本。这意味着数据必须无偏见地抽样(例如，如果我们问人们是否喜欢某个运动队，我们不能只问该队的球迷)，并且我们应该(理想地)在我们的样本中有来自所有不同群体的成员(在运动队的例子中，我们不能只问男人)。

当我们在 [*第 9 章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188) 、*Python 机器学习入门*中讨论机器学习时，我们将需要对我们的数据进行采样，这将是一个开始的样本。这叫做**重采样**。根据不同的数据，我们将不得不选择不同的采样方法。通常，我们最好的选择是一个简单的随机样本:我们使用一个随机数生成器来随机选择行。当我们在数据中有不同的组时，我们希望我们的样本是一个**分层随机样本**，这将保持数据中组的比例。在某些情况下，我们没有足够的数据用于前述的采样策略，所以我们可能会转向随机替换采样(**bootstrapping**)；这被称为**引导样本**。请注意，我们的基本样本需要是随机样本，否则我们会冒增加估计量偏差的风险(我们可以更频繁地选择某些行，因为如果是方便样本，它们会更频繁地出现在数据中，而在真实总体中，这些行并不普遍)。我们将在第 8 章 、*基于规则的异常检测*中看到一个引导示例。

重要说明

对自举背后的理论及其后果的彻底讨论已经超出了本书的范围，但是请看这个视频的初级读本:[https://www.youtube.com/watch?v=gcPIyeqymOU](https://www.youtube.com/watch?v=gcPIyeqymOU)。

你可以在[https://www . khanacademy . org/math/statistics-probability/designing-studies/sampling-methods-stats/a/sampling-methods-review](https://www.khanacademy.org/math/statistics-probability/designing-studies/sampling-methods-stats/a/sampling-methods-review)了解更多关于抽样方法及其优缺点的信息。

## 描述性统计

我们将从**单变量统计**开始我们对描述性统计的讨论；单变量仅仅意味着这些统计数据是从一个( **uni** )变量中计算出来的。本节中的所有内容都可以扩展到整个数据集，但是统计数据将针对我们记录的每个变量进行计算(这意味着，如果我们有 100 个速度和距离对的观察值，我们可以计算整个数据集的平均值，这将为我们提供平均速度和平均距离统计数据)。

描述性统计用于描述和/或总结我们正在处理的数据。我们可以从测量**集中趋势**和测量**扩散**或**分散**开始我们的数据汇总，集中趋势描述了大部分数据集中在哪里，分散程度或指示了值之间的距离。

### 集中趋势测量

集中趋势的量度描述了我们的数据分布的中心。有三种常用的统计数据被用作中心点的度量:平均值、中值和众数。每一种都有自己的优势，这取决于我们正在处理的数据。

#### 平均

也许用于汇总数据的最常见的统计数据是平均值，或者**是指**。总体均值用 *μ* 表示(希腊字母 *mu* ，样本均值写成![](image/B16834_01_001.png)(读作 *X-bar* )。通过将所有值相加并除以值的计数来计算样本平均值；例如，数字 0、1、1、2 和 9 的平均值是 2.6 ( `(0 + 1 + 1 + 2 + 9)/5`):

![](image/B16834_01_002.jpg)

我们用 *x* i 来表示变量 *X* 的*I*th*观测值。注意变量作为一个整体是如何用大写字母表示的，而具体的观察是小写的。*σ*(希腊大写字母 *sigma* )用于表示求和，在平均值的等式中，求和从 *1* 到 *n* ，即观察值的数量。*

关于平均值需要注意的一件重要事情是，它对**异常值**(由与我们的分布不同的生成过程创建的值)非常敏感。在前面的例子中，我们只处理了五个值；然而，9 比其他数字要大得多，除了 9 之外，其他数字的平均值都比 9 高。在我们怀疑数据中存在异常值的情况下，我们可能希望使用中位数作为集中趋势的度量。

#### 中位数

与平均值不同，**中值**对于异常值是稳健的。考虑一下美国的收入；最富有的 1%远高于其余人口，因此这将使平均值偏高，并扭曲对普通人收入的认知。然而，中位数将更能代表平均收入，因为它是我们数据的第 50 个百分位数；这意味着 50%的值大于中值，50%小于中值。

小费

第 *i* 百分位数是观察值的 *i* 百分比小于该值的值，因此第 99 百分位数是 *X* 中的值，其中 99%的 *x* 小于该值。

通过从有序的值列表中取中间值来计算中间值；在我们有偶数个值的情况下，我们取中间两个值的平均值。如果我们再次取数字 0，1，1，2 和 9，我们的中位数是 1。请注意，此数据集的平均值和中值是不同的；但是，根据数据的分布情况，它们可能是相同的。

#### 方式

**模式**是数据中最常见的值(如果我们再次拥有数字 0、1、1、2 和 9，那么 1 就是模式)。在实践中，在分布有两个或更多最受欢迎的值的情况下，我们经常会听到诸如*分布是双峰或多峰*(与单峰相对)。这并不一定意味着它们中的每一个都出现了相同的次数，而是说，它们比其他值更常见。如下图所示，单峰分布只有一个模式(在 **0** )，双峰分布有两个模式(在 **-2** 和 **3** )，多峰分布有多个模式(在 **-2** 、 **0.4** 和 **3** ):

![Figure 1.3 – Visualizing the mode with continuous data
](image/Figure_1.3_B16834.jpg)

图 1.3–用连续数据可视化模式

当描述连续分布时，理解模式的概念是很方便的；然而，大多数时候，当我们描述我们的连续数据时，我们将使用平均值或中值作为我们集中趋势的度量。另一方面，当处理分类数据时，我们通常使用模式。

### 传播的量度

知道分布的中心在哪里只能让我们部分地能够总结我们数据的分布——我们需要知道值如何落在中心周围以及它们相距多远。分布的度量告诉我们数据是如何分散的；这将表明我们的分布有多薄(低分散)或多宽(非常分散)。与集中趋势的度量一样，我们有几种方法来描述分布的扩散，我们选择哪一种将取决于情况和数据。

#### 范围

**范围**是最小值**最小值**和最大值**最大值**之间的距离。范围的单位将与数据的单位相同。因此，除非两种分布的数据采用相同的单位并测量相同的事物，否则我们无法比较它们的范围，也无法说一种分布比另一种分布更分散:

![](image/B16834_01_003.jpg)

仅仅从范围的定义，我们就能明白为什么它并不总是衡量数据分布的最佳方式。它给了我们数据的上限和下限；然而，如果我们的数据中有任何异常值，范围将变得无用。

范围的另一个问题是，它没有告诉我们数据是如何围绕其中心分散的；它只告诉我们整个数据集有多分散。这就把我们带到了方差。

#### 差异

**方差**描述了观察值与其平均值(平均值)之间的距离。总体方差表示为 *σ* 2(读作*σ平方*)，样本方差写为 *s* 2。其计算方式为距离平均值的平均平方距离。请注意，距离必须是平方的，以便低于平均值的距离不会抵消高于平均值的距离。

如果我们想让样本方差成为总体方差的无偏估计量，我们用 *n - 1* 而不是 *n* 除以样本均值而不是总体均值；这就是所谓的贝塞尔校正([https://en.wikipedia.org/wiki/Bessel%27s_correction](https://en.wikipedia.org/wiki/Bessel%27s_correction))。默认情况下，大多数统计工具会给出样本方差，因为我们拥有整个人口的数据是非常罕见的:

![](image/B16834_01_004.jpg)

方差为我们提供了一个单位为*平方*的统计数据。这意味着，如果我们从以美元计的收入数据开始，那么我们的方差将是美元的平方(2 美元)。当我们试图了解这是如何描述数据的时候，这并不真正有用；我们可以使用**量级**(大小)本身来查看一些东西是如何展开的(大值=大范围)，但除此之外，我们需要一个与我们的数据相同的单位的范围测量。为此，我们使用标准差。

#### 标准偏差

我们可以使用**标准差**来查看平均数据点*与平均*的距离。小的标准差意味着值接近平均值，而大的标准差意味着值分散得更广。这与我们如何想象分布曲线有关:标准差越小，曲线的峰值越细(**0.5**)；标准差越大，曲线的峰值越宽( **2** ):

![Figure 1.4 – Using standard deviation to quantify the spread of a distribution
](image/Figure_1.4_B16834.jpg)

图 1.4–使用标准偏差量化分布范围

标准偏差就是方差的平方根。通过执行这个操作，我们得到了一个可以再次理解的统计单位(对于我们的收入示例):

![](image/B16834_01_005.jpg)

注意，总体标准差表示为 *σ* ，样本标准差表示为 *s* 。

#### 变异系数

当我们从方差转移到标准差时，我们希望得到有意义的单位；然而，如果我们想要比较一个数据集与另一个数据集的分散程度，我们需要再次拥有相同的单元。一种方法是计算**变异系数** ( **CV** )，它是无单位的。CV 是标准偏差与平均值的比值:

![](image/B16834_01_006.jpg)

我们将在 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币和股票市场*中使用这个指标；由于 CV 是无单位的，我们可以用它来比较不同资产的波动性。

#### 四分位间距

到目前为止，除了范围之外，我们已经讨论了基于均值的离差度量；现在，我们将看看如何用中值作为集中趋势的度量来描述价差。如前所述，中位数是第 50 个百分位数或第 2 个四分位数(T2 和 Q2)。百分位数和四分位数都是**分位数**——将数据分成相等的组，每个组包含总数据的相同百分比。百分位数将数据分成 100 份，而四分位数则分成四份(25%、50%、75%和 100%)。

因为分位数整齐地划分了我们的数据，并且我们知道每个部分有多少数据，所以它们是帮助我们量化数据分布的完美候选。一个常用的衡量标准是**四分位数间距** ( **IQR** )，即第三个四分位数和第一个四分位数之间的距离:

![](image/B16834_01_007.jpg)

IQR 为我们提供了数据在中位数*周围的分布，而*量化了我们在分布的中间 50%处的离差。它在检查数据中的异常值时也很有用，我们将在第 8 章 、*基于规则的异常检测*中介绍这些内容。此外，IQR 可以用来计算色散的无单位测量，我们将在下面讨论。

#### 四分位数分散系数

就像我们在使用平均值作为中心趋势的度量时有了变异系数一样，我们在使用中位数作为中心趋势的度量时有了**四分位数分散系数**。该统计也是无单位的，因此可用于比较数据集。它是通过将**半四分位数范围**(IQR 的一半)除以**中点**(第一个和第三个四分位数之间的中点)计算出来的:

![](image/B16834_01_008.jpg)

我们将在 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币和股票市场*中再次看到这一指标，届时我们将评估股票波动性。现在，让我们来看看如何使用集中趋势和分散的度量来总结我们的数据。

### 汇总数据

我们已经看到许多描述性统计的例子，我们可以用它的中心和分散来总结我们的数据；在实践中，查看**的 5 位数汇总**并可视化分布被证明是有帮助的在开始研究其他前述指标之前的第一步。顾名思义，五位数摘要提供了五种描述性统计数据来总结我们的数据:

![Figure 1.5 – The 5-number summary
](image/Figure_1.5_B16834.jpg)

图 1.5–5 个数字的汇总

一个**盒状图**(或盒须图)是5 个数字汇总的可视化表示。中间值由框中的粗线表示。盒子的顶部是 Q3，盒子的底部是 Q1。线条(胡须)从长方体边界的两侧向最小值和最大值延伸。然而，根据我们绘图工具使用的惯例，它们可能只扩展到某个统计数据；超出这些统计值的任何值都被标记为异常值(使用点)。对于本书来说，总的来说，须的下界将是**Q**1**–1.5 * IQR**，上界将是 **Q** 3 **+ 1.5 * IQR** ，这被称为 **Tukey 盒图**:

![Figure 1.6 – The Tukey box plot
](image/Figure_1.6_B16834.jpg)

图 1.6–Tukey 盒图

虽然盒状图是初步理解分布的一个很好的工具，但是我们没有看到事物是如何在每个四分位数内分布的。为此，我们转向**直方图**中的**离散**变量(例如，人数或书籍数量)和**内核密度估计** ( **KDEs** )中的**连续**变量(例如，高度或时间)。没有什么可以阻止我们在离散变量上使用 kde，但是这样很容易让人迷惑。直方图适用于离散和连续变量；然而，在这两种情况下，我们必须记住，我们选择将数据划分成的仓的数量可以很容易地改变我们看到的分布的形状。

要制作直方图，需要创建一定数量的等宽条柱，然后添加高度与每个条柱中的值数量相对应的条柱。下图是一个包含 10 个柱的直方图，显示了用于生成图 1.6 中的箱形图的相同数据的三个集中趋势测量值:

![Figure 1.7 – Example histogram
](image/Figure_1.7_B16834.jpg)

图 1.7–直方图示例

重要说明

在实践中，我们需要试验箱的数量，以找到最佳值。然而，我们必须小心，因为这可能会歪曲分布的形状。

KDEs】类似于直方图，除了它们不是为数据创建仓，而是绘制一条平滑的曲线，该曲线是分布的**概率密度函数** ( **PDF** )的估计。PDF 是针对连续变量的，它告诉我们概率是如何分布在这些值上的。PDF 值越高，可能性越大:

![Figure 1.8 – KDE with marked measures of center
](image/Figure_1.8_B16834.jpg)

图 1.8–带有中心标记的 KDE

当分布开始变得有点不平衡，在一边有长尾巴时，中心的平均度量很容易被拉到那一边。不对称的分布有一些倾斜。一个**左(负)偏态分布**在左手边有一条长尾；一个**右(正)偏态分布**在右手边有一条长尾。在存在负偏斜的情况下，平均值将小于中值，而在正偏斜的情况下则相反。没有偏斜时，两者相等:

![Figure 1.9 – Visualizing skew
](image/Figure_1.9_B16834.jpg)

图 1.9–可视化偏斜

重要说明

还有另一个统计数据叫做“T2”，它比较了分布中心的密度和尾部的密度。偏斜度和峰度都可以用 SciPy 软件包计算。

我们数据中的每一列都是一个**随机变量**，因为每次我们观察它，都会根据底层分布得到一个值——它不是静态的。当我们对获得值 *x* 或更小的概率感兴趣时，我们使用**累积分布函数** ( **CDF** )，它是 PDF 的积分(曲线下的面积):

![](image/Formula_01_009.jpg)![](image/Formula_01_010.jpg)

随机变量 *X* 小于或等于特定值 *x* 的概率记为 *P(X ≤ x)* 。对于一个连续变量，得到精确的 *x* 的概率是 0。这是因为概率将是从 *x* 到 *x* 的 PDF 的积分(零宽度曲线下的面积)，即 0:

![](image/Formula_01_011.jpg)

为了让形象化，我们可以从样本中找到 CDF 的估计值，称为**经验累积分布函数** ( **ECDF** )。由于这是累积的，在 *x* 轴上的值等于 *x* 的点处， *y* 值就是 *P(X ≤ x)* 的累积概率。让我们以 **P(X ≤ 50)** 、 **P(X = 50)** 、 **P(X > 50)** 为例来形象化一下:

![Figure 1.10 – Visualizing the CDF
](image/Figure_1.10_B16834.jpg)

图 1.10–可视化 CDF

除了检查我们数据的分布，我们可能会发现需要利用概率分布进行模拟(在 [*第 8 章*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172) 、*基于规则的异常检测*中讨论)或假设检验(参见*推断统计*部分)；让我们来看看我们可能遇到的几个发行版。

### 常见分布

虽然有许多概率分布，每个都有特定的用例，但有一些我们会经常遇到。**高斯**或**正态**，看起来像一条钟形曲线，并通过其均值( *μ* )和标准差( *σ* )来参数化。**标准正常值** ( *Z* )的平均值为 0，标准差为 1。自然界的很多东西恰好遵循正态分布，比如身高。请注意，测试分布是否正常并不简单——查看*进一步阅读*部分了解更多信息。

**泊松分布**是一种离散分布，通常用于模拟到达人数。到达之间的时间可以用**指数分布**来建模。两者都由它们的平均值 lambda ( *λ* )定义。**均匀分布**对其边界内的每个值赋予相等的可能性。我们经常用这个来生成随机数。当我们生成一个随机数来模拟一个单一的成功/失败结果时，它被称为**伯努利试验**。这由成功的概率来参数化。当我们多次运行同一个实验( *n* )时，成功的总数就是一个**二项随机变量。伯努利分布和二项式分布都是离散的。**

我们可以看到离散和连续的分布；然而，离散分布给我们一个**概率质量函数** ( **PMF** )而不是一个 PDF:

![Figure 1.11 – Visualizing some commonly used distributions
](image/Figure_1.11_B16834.jpg)

图 1.11–可视化一些常用的分布

我们将在 [*第八章*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172) ，*基于规则的异常检测*中使用这些分布，当我们模拟一些登录尝试数据进行异常检测时。

### 缩放数据

为了比较来自不同分布的变量，我们需要到**缩放**数据，我们可以通过使用**最小-最大缩放**来处理范围。我们取*每个*数据点，减去数据集的最小值，然后除以范围。这**使**我们的数据正常化(将其缩放到范围[0，1]):

![](image/Formula_01_012.jpg)

这不是扩展数据的唯一方式；我们也可以使用均值和标准差。在这种情况下，我们将从每个观察值中减去平均值，然后除以标准偏差，以**标准化**数据。这给了我们所谓的 **Z 值**:

![](image/Formula_01_013.jpg)

我们留给一个均值为 0、标准差(和方差)为 1 的归一化分布。Z 分数告诉我们每个观察值与平均值的标准偏差是多少；平均值的 Z 值为 0，而低于平均值 0.5 个标准差的观察值的 Z 值为-0.5。

当然，还有其他方法来扩展我们的数据，我们最终选择哪种方法将取决于我们的数据以及我们试图用它做什么。通过记住集中趋势的度量和分散的度量，您将能够识别在您遇到的任何其他方法中数据的缩放是如何进行的。

### 量化变量之间的关系

在前面的章节中，我们处理的是单变量统计，并且只能对我们正在研究的变量说一些的话。通过多元统计，我们寻求量化变量之间的关系，并试图预测未来的行为。

**协方差**是一种统计量，通过显示一个变量相对于另一个变量如何变化来量化变量之间的关系(也称为它们的联合方差):

![](image/Formula_01_014.jpg)

重要说明

E[X] 对我们来说是一个新的符号。它被解读为*X*的期望值或者*X*的期望值，并且它是通过将 *X* 的所有可能值乘以它们的概率相加而计算的——它是 *X* 的长期平均值。

协方差的大小不容易解释，但它的符号告诉我们变量是正相关还是负相关。然而，我们也想量化变量之间的关系的*有多强*，这就引出了相关性。**相关性**告诉我们变量如何在方向(相同或相反)和幅度(关系的强度)上一起变化。为了找到相关性，我们通过将协方差除以变量的标准偏差的乘积来计算**皮尔逊相关系数**，用 *ρ* (希腊字母 *rho* 表示:

![](image/Formula_01_015.jpg)

这将使协方差标准化，并产生一个介于-1 和 1 之间的统计量，使描述相关性的方向(符号)和强度(幅度)变得容易。相关性为 1 称为完全正(线性)相关性，而相关性为-1 称为完全负相关性。接近 0 的值不相关。如果相关系数的绝对值接近 1，那么变量被称为强相关；那些接近 0.5 的被称为弱相关。

让我们看一些使用散点图的例子。在*图 1.12* ( **ρ = 0.11** )最左边的子图中，我们看到变量之间没有相关性:它们看起来是没有模式的随机噪声。下一个 **ρ = -0.52** 的图有一个微弱的负相关性:我们可以看到变量似乎随着 *x* 变量的增加而一起移动，而 *y* 变量减少，但仍然有一点随机性。在左起第三个图中( **ρ = 0.87** ，有很强的正相关性: *x* 和 *y* 一起递增。最右边的图 **ρ = -0.99** 具有近乎完美的负相关性:随着 *x* 增加， *y* 减少。我们还可以看到这些点是如何形成一条线的:

![Figure 1.12 – Comparing correlation coefficients
](image/Figure_1.12_B16834.jpg)

图 1.12–比较相关系数

为了快速观察两个变量之间关系的强度和方向(并查看是否似乎有一个变量)，我们通常会使用散点图，而不是计算精确的相关系数。这有几个原因:

*   在可视化中更容易找到模式，但是通过查看数字和表格得出相同的结论需要更多的工作。
*   我们可能会看到变量看起来相关，但它们可能不是线性相关的。查看可视化表示将很容易看出我们的数据实际上是二次函数、指数函数、对数函数还是其他非线性函数。

下面的两个图描绘了具有强正相关性的数据，但是当看散点图时，很明显这些数据不是线性的。左边的是对数，右边的是指数:

![Figure 1.13 – The correlation coefficient can be misleading
](image/Figure_1.13_B16834.jpg)

图 1.13–相关系数可能会产生误导

重要的是要记住，虽然我们可能会发现 *X* 和 *Y* 之间的相关性，但这并不意味着 *X 导致 Y* 或者 *Y 导致 X* 。可能有一些 Z*Z*实际上导致了两者；也许 *X* 引发了某种导致 *Y* 的中间事件，或者它其实只是一种巧合。请记住我们通常没有足够的信息来报告因果关系——*相关性并不意味着因果关系*。

小费

一定要看看泰勒·维根的*虚假相关性*博客([https://www.tylervigen.com/spurious-correlations](https://www.tylervigen.com/spurious-correlations))中一些有趣的相关性。

### 汇总统计的陷阱

有一个非常有趣的数据集说明了当只使用汇总统计数据和相关系数来描述我们的数据时，我们必须多么小心。这也告诉我们，绘图不是可有可无的。**安斯科姆的四重奏**是四个不同数据集的集合，它们具有相同的汇总统计数据和相关系数，但当绘制时，很明显它们并不相似:

![Figure 1.14 – Summary statistics can be misleading
](image/Figure_1.14_B16834.jpg)

图 1.14-汇总统计数据可能会产生误导

注意图 1.14 的*中的每个图都有一条相同的最佳拟合线，由等式 **y = 0.50x + 3.00** 定义。在下一节中，我们将在较高层次上讨论这条线是如何创建的以及它的含义。*

重要说明

当我们开始了解数据时，汇总统计数据非常有帮助，但要小心不要完全依赖它们。记住，统计数据可能会误导人；在得出任何结论或进行分析之前，一定要绘制出数据。你可以在[https://en.wikipedia.org/wiki/Anscombe%27s_quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)阅读更多关于安斯康贝的四重奏。此外，请务必查看位于[https://www.autodeskresearch.com/publications/samestats](https://www.autodeskresearch.com/publications/samestats)的 **Datasaurus 十二个**，这 13 个数据集也有相同的汇总统计数据。

## 预测和预报

假设我们最喜欢的冰淇淋店要求我们帮助预测他们在某一天预计能卖出多少冰淇淋。他们确信外面的温度对他们的销售有很大的影响，所以他们收集了在给定温度下销售的冰淇淋数量的数据。我们同意帮助他们，我们要做的第一件事是绘制他们收集的数据散点图:

![Figure 1.15 – Observations of ice cream sales at various temperatures
](image/Figure_1.15_B16834.jpg)

图 1.15-观察不同温度下的冰淇淋销售

我们可以在散点图中观察到一个上升趋势:温度越高，卖的冰淇淋越多。然而，为了帮助冰淇淋店，我们需要找到一种从这些数据中做出预测的方法。我们可以使用一种叫做**回归**的技术，用一个等式来模拟温度和冰淇淋销量之间的关系。使用这个等式，我们将能够**预测在给定温度下**冰淇淋的销量。

重要说明

记住相关性并不意味着因果关系。天气变暖时，人们可能会购买冰淇淋，但气温变暖并不一定会导致人们购买冰淇淋。

在 [*第九章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188) 、*Python 中的机器学习入门*中，我们将深入回顾回归，因此本次讨论将是一次高级概述。有许多类型的回归将产生不同类型的方程，例如线性(我们将在本例中使用)和逻辑。我们的第一步将是确定**因变量**，也就是我们想要预测的数量(冰淇淋销售额)，以及我们将用来预测它的变量，这些变量被称为**自变量**。虽然我们可以有许多独立变量，但我们的冰淇淋销售示例只有一个:温度。因此，我们将使用简单的线性回归将关系建模为直线:

![Figure 1.16 – Fitting a line to the ice cream sales data
](image/Figure_1.16_B16834.jpg)

图 1.16-给冰淇淋销售数据拟合一条线

之前散点图中的回归线得出以下关系式:

![](image/Formula_01_016.jpg)

假设今天的温度是 35 摄氏度——我们将把它代入等式中的*温度*。结果预测冰淇淋店将售出 24.54 个冰淇淋。这个预测是沿着前面图中的红线。请注意，冰淇淋店实际上不能出售分数冰淇淋。

在把模型交给冰淇淋店之前，讨论一下我们得到的回归线的虚线部分和实线部分之间的区别是很重要的。当我们使用直线的实线部分进行预测时，我们使用的是**插值**，这意味着我们将根据回归所建立的温度来预测冰淇淋销量。另一方面，如果我们试图预测有多少冰淇淋将在 45℃出售，这被称为**外推**(线的虚线部分)，因为我们在进行回归时没有任何这么高的温度。推断可能非常危险，因为许多趋势不会无限期地持续下去。人们可能会因为天气太热而决定不出门。这意味着他们不会卖出预期的 39.54 份冰淇淋，而是会卖出零份。

当处理时间序列时，我们的术语略有不同:我们经常根据过去的值来预测未来的值。预测是对时间序列的一种预测。然而，在我们尝试为时间序列建模之前，我们通常会使用一个叫做**时间序列分解**的过程来将时间序列分割成组件，这些组件可以以加法或乘法的方式组合起来，并可以用作模型的一部分。

**趋势**部分描述了时间序列在**长期**中的行为，不考虑的季节性或周期性影响。利用趋势，我们可以对长期的时间序列做出宽泛的陈述，例如*地球人口正在增加*或*股票价值正在停滞*。**季节性**部分解释了一个时间序列的系统性和与日历相关的运动。例如，纽约市街道上的冰淇淋车数量在夏季很高，而在冬季则下降到零；这种模式每年重复，不管每个夏天的实际数量是否相同。最后，**周期性**成分解释了时间序列中其他无法解释或不规则的东西；这可能是飓风导致冰淇淋车数量在短期内下降，因为在外面不安全。由于其不可预测的性质，这一部分很难用预测来预测。

我们可以使用 Python 将时间序列分解成趋势、季节性和 T2 噪声或 T4 残差。在噪声(随机、不可预测的数据)中捕捉到循环分量；在我们从时间序列中去除趋势和季节性之后，我们剩下的是残差:

![Figure 1.17 – An example of time series decomposition
](image/Figure_1.17_B16834.jpg)

图 1.17–时间序列分解示例

在建立模型预测时间序列时，一些常用的方法包括指数平滑和 ARIMA 族模型。**代表**自回归**(**AR**)**综合**(**I**)**移动平均线** ( **MA** )。**自回归**模型利用了在时间 *t* 的观察值与先前的观察值相关*的事实，例如，在时间 *t - 1* 的观察值。在 [*第五章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*用 Pandas 和 Matplotlib* 可视化数据中，我们会看一些判断一个时间序列是否自回归的技巧；请注意，并非所有时间序列都是如此。**集成**组件涉及**差异**数据，或*数据从一个时间到另一个时间的变化*。例如，如果我们关心的是 1 的**滞后**(时间之间的距离)，差异数据将是时间 *t* 的值减去时间 *t - 1* 的值。最后，**移动平均**组件使用滑动窗口来平均最后的 *x* 观察值，其中 *x* 是滑动窗口的长度。例如，如果我们有一个 3 期移动平均线，到时间 5 之前我们有了所有数据，我们的移动平均线计算只使用时间 3、4 和 5 来预测时间 6。我们将在 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币和股市*中建立一个 ARIMA 模型。***

 **移动平均值对计算中涉及的过去的每个时间段赋予相同的权重。实际上，这并不总是我们数据的现实预期。有时，*所有的*过去的值都是重要的，但是它们*对未来数据点的影响因*而异。对于这些情况，我们可以使用**指数平滑**，它允许我们对最近的值赋予更多的权重，而对离我们预测值较远的值赋予更少的权重。

注意，我们并不局限于预测数字；事实上，根据数据，我们的预测在本质上可以是绝对的——比如确定哪种口味的冰淇淋在某一天卖得最多，或者一封电子邮件是否是垃圾邮件。这种类型的预测将在 [*第九章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188) 、*Python 中的机器学习入门*中介绍。

## 推断统计学

如前所述，推断统计学处理的是从我们拥有的样本数据中推断出一些东西，以便对整个人口做出陈述。当我们想要陈述我们的结论时，我们必须注意我们是在进行观察研究还是实验。在一项**观察研究**中，自变量不在研究人员的控制之下，所以我们*观察*那些参与我们研究的人(想想关于吸烟的研究——我们不能强迫人们吸烟)。我们不能控制独立变量的事实意味着我们*不能*得出因果关系。

通过**实验**，我们能够直接影响独立变量，并将受试者随机分配到控制组和测试组，例如 A/B 测试(从网站重新设计到广告文案的任何事情)。注意，对照组不接受治疗；他们可以服用安慰剂(取决于研究内容)。这方面的理想设置是**双盲**，进行治疗的研究人员不知道哪种治疗是安慰剂，也不知道哪个受试者属于哪个组。

重要说明

我们经常可以找到贝叶斯推理和频率主义推理的参考。这些都是基于两种不同的接近概率的方法。频率统计侧重于事件的频率，而贝叶斯统计在确定事件的概率时使用一定程度的信任。我们会在 [*第十一章*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237) 、*机器学习异常检测*中看到贝叶斯统计的例子。你可以在[https://www . probabilistic world . com/frequentist-Bayesian-approach-interestial-statistics/](https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/)了解更多关于这些方法的不同之处。

推理统计学为我们提供了工具，将我们对样本数据的理解转化为关于总体的陈述。记住我们之前讨论的样本统计量是总体参数的估计量。我们的估计器需要**置信区间**，它提供点估计和围绕点估计的误差幅度。这是真实总体参数在某一**置信水平**下的范围。在 95%的置信水平下，从总体的随机样本计算的 95%的置信区间包含真实总体参数。虽然 90%和 99%也很常见，但通常选择 95%是为了统计中的置信水平和其他目的；置信水平越高，间隔越宽。

假设检验允许我们检验真实总体参数是否小于、大于或不等于某个**显著性水平**(称为**α**)的某个值。执行假设检验的过程从陈述我们的初始假设或**零假设**开始:例如，*真实总体均值是 0* 。我们选择一个统计显著性水平，通常是 5%，这是当零假设为真时拒绝零假设的概率。然后，我们计算检验统计的临界值，这将取决于我们拥有的数据量和我们正在检验的统计类型(如一个总体的平均值或候选人的投票比例)。将临界值与我们数据中的检验统计量进行比较，根据结果，我们拒绝或未能拒绝零假设。假设检验与置信区间密切相关。显著性水平等于 1 减去置信度水平。这意味着，如果零假设值不在置信区间内，则结果具有统计显著性。

重要说明

在选择计算置信区间或假设检验的适当检验统计量的方法时，我们必须注意许多事情。这超出了本书的范围，但是查看本章末尾的*进一步阅读*部分的链接以获得更多信息。此外，一定要看看假设检验中使用的 p 值的一些失误，比如 https://en.wikipedia.org/wiki/Misuse_of_p-values 的[的 p-hacking。](https://en.wikipedia.org/wiki/Misuse_of_p-values)

现在我们已经有了统计和数据分析的概述，我们准备开始阅读本书的 Python 部分。让我们从建立一个虚拟环境开始。

# 建立虚拟环境

这本书是用 Python 3.7.3 写的，但是代码应该适用于 Python 3.7.1+，它可以在所有主要的操作系统上使用。在这一节中，我们将回顾如何设置虚拟环境，以便跟随本书。如果您的计算机上还没有安装 Python，请先通读以下关于虚拟环境的部分，然后决定是否安装 Anaconda，因为它也会安装 Python。要安装 Python 不带 Anaconda，从[https://www.python.org/downloads/](https://www.python.org/downloads/)下载，然后按照 *venv* 段，而不是 *conda* 段。

重要说明

要检查 Python 是否已经安装，在 Windows 上从命令行运行`where python3`或者在 Linux/macOS 上从命令行运行`which python3`。如果没有返回任何结果，尝试只使用`python`(而不是`python3`)运行它。如果安装了 Python，通过运行`python3 --version`来检查版本。注意，如果`python3`有效，那么你应该在整本书中使用它(反之，如果`python3`无效，使用`python`)。

## 虚拟环境

大多数时候，当我们想在自己的电脑上安装软件时，我们只是简单地下载它，但是编程语言的本质是软件包不断更新并依赖于他人的特定版本，这意味着这会导致问题。我们可能某天正在做一个项目，需要某个版本的 Python 包(比如说 0.9.1)，但是第二天正在做一个分析，需要这个包的最新版本来访问一些更新的功能(1.1.0)。听起来没什么问题，对吧？那么，如果这个更新导致第一个项目或者我们项目中依赖于这个项目的另一个包发生重大变化，会发生什么呢？这是一个很常见的问题，已经有了一个解决方案来防止这个问题:虚拟环境。

虚拟环境允许我们为每个项目创建独立的环境。我们的每个环境都只有需要安装的包。这使得很容易与他人共享我们的环境，在我们的机器上为不同的项目安装同一个包的多个版本，而不会相互干扰，并且避免了安装更新或依赖他人的包所带来的意外副作用。为我们从事的任何项目创建一个专用的虚拟环境是一个很好的实践。

我们将讨论实现这种设置的两种常用方法，您可以决定哪种方法最合适。请注意，本节中的所有代码都将在命令行上执行。

### 文夫

Python 3 附带了`venv`模块，它将在我们选择的位置创建一个虚拟环境。建立和使用开发环境的过程如下(Python 安装后):

1.  为项目创建一个文件夹。
2.  使用`venv`在该文件夹中创建一个环境。
3.  激活环境。
4.  用`pip`在环境中安装 Python 包。
5.  完成后停用环境。

在实践中，我们将为我们工作的每个项目创建环境，所以我们的第一步将是为我们所有的项目文件创建一个目录。为此，我们可以使用`mkdir`命令。一旦创建完成，我们将使用`cd`命令将当前目录更改为新创建的目录。由于我们已经获得了项目文件(从*章节资料*部分的说明)，以下仅供参考。要创建一个新目录并移动到该目录，我们可以使用以下命令:

```
$ mkdir my_project && cd my_project
```

小费

`cd <path>`将当前目录更改为`<path>`中指定的路径，可以是**绝对**(完整)路径，也可以是**相对**(如何从当前目录到达那里)路径。

在继续之前，使用`cd`导航到包含这本书的存储库的目录。请注意，路径将取决于它被克隆/下载的位置:

```
$ cd path/to/Hands-On-Data-Analysis-with-Pandas-2nd-edition
```

由于剩余步骤的操作系统略有不同，我们将分别介绍 Windows 和 Linux/macOS。请注意，如果您同时拥有 Python 2 和 Python 3，请确保在下面的命令中使用`python3`而不是`python`。

#### Windows 操作系统

为了给这本书创建环境，我们将使用标准库中的`venv`模块。注意，我们必须为我们的环境提供一个名称(`book_env`)。记住，如果你的 Windows 设置有与 Python 3 相关联的`python`，那么在下面的命令中使用`python`而不是`python3`:

```
C:\...> python3 -m venv book_env
```

现在，我们在之前克隆/下载的存储库文件夹中有一个名为`book_env`的虚拟环境文件夹。为了使用环境，我们需要激活它:

```
C:\...> %cd%\book_env\Scripts\activate.bat
```

小费

Windows 用当前目录的路径替换`%cd%`。这使我们不必键入到`book_env`部分的完整路径。

注意，我们激活虚拟环境后，在命令行上我们的提示前面可以看到`(book_env)`；这让我们知道我们在环境中:

```
(book_env) C:\...> 
```

当我们使用完环境后，我们只需停用它:

```
(book_env) C:\...> deactivate
```

安装在环境中的任何包在环境之外都不存在。请注意，在命令行中，我们的提示符前面不再有`(book_env)`。你可以在 https://docs.python.org/3/library/venv.html[的 Python 文档中阅读更多关于`venv`的内容。](https://docs.python.org/3/library/venv.html)

既然已经创建了虚拟环境，激活它，然后进入下一步的*安装所需的 Python 包*部分。

#### Linux/macOS

为了给这本书创建环境，我们将使用标准库中的`venv`模块。注意，我们必须为我们的环境提供一个名称(`book_env`):

```
$ python3 -m venv book_env
```

现在，我们在之前克隆/下载的存储库文件夹中有一个名为`book_env`的虚拟环境文件夹。为了使用环境，我们需要激活它:

```
$ source book_env/bin/activate
```

注意，我们激活虚拟环境后，在命令行上我们的提示前面可以看到`(book_env)`；这让我们知道我们在环境中:

```
(book_env) $
```

当我们使用完环境后，我们只需停用它:

```
(book_env) $ deactivate
```

环境中安装的任何软件包在环境之外都不存在。注意在命令行中，我们的提示符前面不再有`(book_env)`。你可以在 https://docs.python.org/3/library/venv.html[的 Python 文档中阅读更多关于`venv`的内容。](https://docs.python.org/3/library/venv.html)

既然已经创建了虚拟环境，激活它，然后进入下一步的*安装所需的 Python 包*部分。

### 康达

Anaconda 提供了一种专门为数据科学建立 Python 环境的方法。它包括我们将在本书中使用的一些包，以及本书中没有涉及的任务可能需要的其他几个包(并且还处理 Python 之外的依赖项，否则可能很难安装)。Anaconda 使用`conda`作为环境和包管理器，而不是`pip`，尽管包仍然可以用`pip`安装(只要调用 Anaconda 安装的`pip`)。请注意，有些包可能没有`conda`，在这种情况下，我们将不得不使用`pip`。有关与`conda`、`pip`和`venv`一起使用的命令的比较，请参考`conda`文档中的本页:[https://conda . io/projects/conda/en/latest/commands . html # conda-vs-pip-vs-virtualenv-commands](https://conda.io/projects/conda/en/latest/commands.html#conda-vs-pip-vs-virtualenv-commands)。

重要说明

请注意，Anaconda 是一个非常大的安装(尽管 Miniconda 版本要轻得多)。那些将 Python 用于数据科学之外的目的的人可能更喜欢我们之前讨论的`venv`方法，以便对安装的内容有更多的控制。

Anaconda 还可以与 Spyder **集成开发环境** ( **IDE** )和 Jupyter 笔记本打包，我们将在后面讨论。注意，我们也可以使用带有`venv`选项的 Jupyter。

您可以在官方文档的以下页面阅读更多关于 Anaconda 以及如何安装它的内容:

*   **窗户**:【https://docs.anaconda.com/anaconda/install/windows/ 
*   **MAC OS**:【https://docs.anaconda.com/anaconda/install/mac-os/ 
*   **Linux**:【https://docs.anaconda.com/anaconda/install/linux/ 
*   **用户指南**:https://docs.anaconda.com/anaconda/user-guide/

一旦你安装了 Anaconda 或者 Miniconda，通过在命令行运行`conda -V`来确认它被正确安装，然后显示版本。注意，在 Windows 上，所有的`conda`命令都需要在 **Anaconda 提示符**下运行(与**命令提示符**相对)。

要为这本书创建一个名为`book_env`的新的`conda`环境，运行以下命令:

```
(base) $ conda create --name book_env
```

运行`conda env list`将显示系统上所有的`conda`环境，现在将包括`book_env`。当前活动的环境旁边会有一个星号(`*`)—默认情况下，`base`将处于活动状态，直到我们激活另一个环境:

```
(base) $ conda env list
# conda environments:
#
base                  *  /miniconda3
book_env                 /miniconda3/envs/book_env
```

为了激活`book_env`环境，我们运行以下命令:

```
(base) $ conda activate book_env
```

注意，我们激活虚拟环境后，在命令行上我们的提示前面可以看到`(book_env)`；这让我们知道我们在环境中:

```
(book_env) $
```

当我们使用完环境后，我们会停用它:

```
(book_env) $ conda deactivate
```

环境中安装的任何软件包在环境之外都不存在。注意我们在命令行上的提示符前面不再有`(book_env)`。你可以在[https://www . freecodecamp . org/news/why-you-need-python-environments-and-how-to-manage-them-with-conda-85 f 155 f 4353 c/](https://www.freecodecamp.org/news/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c/)了解更多关于如何使用`conda`管理虚拟环境的信息。

在下一节中，我们将随本书一起安装后续所需的 Python 包，因此请确保现在激活虚拟环境。

## 安装所需的 Python 包

我们可以用 Python 标准库做很多事情；然而，我们经常会发现需要安装并使用外部包来扩展功能。资源库中的`requirements.txt`文件包含了我们需要安装的所有软件包，以完成这本书。它将在我们当前的目录中，但也可以在[https://github . com/stef molin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/blob/master/requirements . txt](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/blob/master/requirements.txt)中找到。这个文件可以用来在对`pip3 install`的调用中一次安装一堆带有`-r`标志的包，并且具有易于共享的优点。

在安装任何东西之前，确保激活您用`venv`或`conda`创建的虚拟环境。请注意，如果在运行以下命令之前未激活环境，软件包将安装在环境之外:

```
(book_env) $ pip3 install -r requirements.txt
```

小费

如果您遇到任何问题，请在[https://github . com/stef molin/Hands-On-the-Hands-On-Data-Analysis-with-Pandas-2nd-edition/issues](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/issues)上报告。

## 为什么是熊猫？

当谈到 Python 中的数据科学时，`pandas`库几乎是无处不在的。它建立在 NumPy 库的基础上，允许我们高效地对单一类型数据的数组执行数学运算。Pandas 将此扩展为**数据帧**，可以认为是数据表。我们将在第 2 章 、*使用熊猫数据帧*中得到关于数据帧的更正式的介绍。

除了高效的操作，`pandas`还在`matplotlib`绘图库周围提供了**包装器**，使得创建各种绘图变得非常容易，而不需要编写许多行`matplotlib`代码。我们总是可以使用`matplotlib`来调整我们的绘图，但是为了快速可视化我们的数据，我们只需要`pandas`中的一行代码。我们将在 [*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*用 Pandas 和 Matplotlib 可视化数据*和 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*用 Seaborn 绘图和定制技术*中探讨此功能。

重要说明

包装函数包装了来自另一个库的代码，掩盖了它的复杂性，给我们留下了一个简单的接口来重复该功能。这是**面向对象编程** ( **OOP** )的一个核心原理，叫做**抽象**，降低了复杂性和代码的重复。在本书中，我们将创建自己的包装函数。

除了到`pandas`，这本书利用了 Jupyter 笔记本。虽然您可以自由选择不使用它们，但熟悉 Jupyter 笔记本很重要，因为它们在数据世界中非常常见。作为介绍，我们将在下一节使用 Jupyter 笔记本来验证我们的设置。

## Jupyter 笔记本

这本书的每一章都包括 Jupyter 笔记本。Jupyter 笔记本在 Python 数据科学中无处不在，因为与编写程序相比，它们使得在更多的发现环境中编写和测试代码变得非常容易。我们可以一次执行一个代码块，并将结果打印到笔记本上，直接放在生成它的代码下面。此外，我们可以使用 **Markdown** 为我们的工作添加文本解释。Jupyter 笔记本可以轻松打包并共享；它们可以被推送到 GitHub(在那里它们将被渲染)，被转换成 HTML 或 PDF，被发送给其他人，或者被呈现。

### 发射 JupyterLab

JupyterLab 是一个 IDE，它允许我们创建 Jupyter 笔记本和 Python 脚本，与终端进行交互，创建文本文档、参考文档，以及从本地机器上的一个干净的 web 界面进行更多操作。在真正成为超级用户之前，有很多键盘快捷键需要掌握，但是界面非常直观。当我们创建环境时，我们安装了运行 JupyterLab 所需的所有东西，所以让我们快速浏览一下 IDE，确保我们的环境设置正确。首先，我们激活我们的环境，然后启动 JupyterLab:

```
(book_env) $ jupyter lab
```

这将在默认浏览器中用 JupyterLab 启动一个窗口。我们将会看到左侧的**启动器**标签和**文件浏览器**面板:

![Figure 1.18 – Launching JupyterLab
](image/Figure_1.18_B16834.jpg)

图 1.18–启动 JupyterLab

使用**文件浏览器**窗格，双击 **ch_01** 文件夹，其中包含我们将用来验证我们的设置的 Jupyter 笔记本。

### 验证虚拟环境

打开 **ch_01** 文件夹中的`checking_your_setup.ipynb`笔记本，如下图所示:

![Figure 1.19 – Validating the virtual environment setup
](image/Figure_1.19_B16834.jpg)

图 1.19–验证虚拟环境设置

重要说明

**内核**是在 Jupyter 笔记本中运行和内省我们代码的进程。注意，我们并不局限于运行 Python——我们也可以运行 R、Julia、Scala 和其他语言的内核。默认情况下，我们将使用 IPython 内核运行 Python。在整本书中，我们会学到更多关于 IPython 的知识。

点击前面截图中指示的代码单元格，并通过点击 play()按钮运行它。如果所有东西都显示为绿色，则环境已经设置好了。然而，如果不是这种情况，从虚拟环境中运行以下命令，创建一个带有`book_env`虚拟环境的特殊内核，用于 Jupyter:

```
(book_env) $ ipython kernel install --user --name=book_env
```

这在**启动器**标签中增加了一个额外的选项，我们现在也可以从 Jupyter 笔记本切换到`book_env`内核:

![Figure 1.20 – Selecting a different kernel
](image/Figure_1.20_B16834.jpg)

图 1.20–选择不同的内核

重要的是要注意，Jupyter 笔记本将保留我们在内核运行时分配给变量的值，并且当我们保存文件时，将保存 **Out[#]** 单元格中的结果。关闭文件不会停止内核，关闭浏览器中的 JupyterLab 选项卡也不会。

### 关闭木星实验室

关闭带有 JupyterLab 的浏览器不会停止 JupyterLab 或它正在运行的内核(我们也不会恢复命令行界面)。要完全关闭 JupyterLab，我们需要在终端中按几次 *Ctrl* + *C* (这是一个键盘中断信号，让 JupyterLab 知道我们想要关闭它)，直到我们得到提示:

```
...
[I 17:36:53.166 LabApp] Interrupted...
[I 17:36:53.168 LabApp] Shutting down 1 kernel
[I 17:36:53.770 LabApp] Kernel shutdown: a38e1[...]b44f
(book_env) $
```

想了解更多关于 Jupyter 的信息，包括教程，请查看 http://jupyter.org/的 T2。在[https://jupyterlab.readthedocs.io/en/stable/](https://jupyterlab.readthedocs.io/en/stable/)了解更多关于木星实验室的信息。

# 总结

在本章中，我们学习了进行数据分析的主要过程:数据收集、数据争论、EDA 和得出结论。接下来，我们对描述统计学进行了概述，并学习了如何描述数据的集中趋势和分布；如何使用 5 个数字的摘要、箱线图、直方图和核密度估计值在数字上和视觉上对其进行总结；如何扩展我们的数据；以及如何量化数据集中变量之间的关系。

我们得到了预测和时间序列分析的介绍。然后，我们非常简要地概述了推断统计学中的一些核心主题，这些主题可以在掌握这本书的内容后进行探讨。请注意，尽管本章中的所有示例都是一个或两个变量，但现实生活中的数据通常是高维的。 [*第十章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217) 、*做出更好的预测——优化模型*，将会谈到一些解决这个问题的方法。最后，我们为这本书设置了虚拟环境，并学习了如何使用 Jupyter 笔记本。

现在我们已经建立了一个坚实的基础，我们将在下一章开始用 Python 处理数据。

# 练习

浏览`introduction_to_data_analysis.ipynb`笔记本以回顾本章内容，回顾`python_101.ipynb`笔记本(如果需要)，然后完成以下练习以练习使用 JupyterLab 并在 Python 中计算汇总统计数据:

1.  探索 JupyterLab 界面并查看一些可用的快捷方式。现在不要担心记住它们(最终，它们将成为你的第二天性，并为你节省大量时间)——只要习惯使用 Jupyter 笔记本就行了。
2.  所有数据都是正态分布的吗？解释为什么或为什么不。
3.  什么时候使用中间值而不是平均值来衡量中心更有意义？
4.  运行`exercises.ipynb`笔记本第一个单元格中的代码。它将为您提供一个包含 100 个值的列表，供您在本章的其余练习中使用。请务必将这些值视为总体样本。
5.  Using the data from *exercise 4*, calculate the following statistics without importing anything from the `statistics` module in the standard library ([https://docs.python.org/3/library/statistics.html](https://docs.python.org/3/library/statistics.html)), and then confirm your results match up to those that are obtained when using the `statistics` module (where possible):

    a)平均值

    b)中间值

    c) Mode(提示:在[https://docs . python . org/3/library/collections . html # collections 查看标准库的`collections`模块中的`Counter`类。计数器](https://docs.python.org/3/library/collections.html#collections.Counter)

    d)样本差异

    e)样本标准偏差

6.  Using the data from *exercise 4*, calculate the following statistics using the functions in the `statistics` module where appropriate:

    a)范围

    b)变异系数

    c)四分位间距

    d)四分位数分散系数

7.  Scale the data created in *exercise 4* using the following strategies:

    a)最小-最大缩放(标准化)

    b)标准化

8.  Using the scaled data from *exercise 7*, calculate the following:

    a)标准化和规范化数据之间的协方差

    b)标准化和规范化数据之间的皮尔逊相关系数(实际上是 1，但由于四舍五入，结果会略小)

# 延伸阅读

以下是一些资源，您可以利用它们来更加熟悉 Jupyter:

*   *Jupyter 笔记本基础知识*:[https://nb viewer . Jupyter . org/github/Jupyter/Notebook/blob/master/docs/source/examples/Notebook/Notebook % 20 Basics . ipynb](https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb)
*   *JupyterLab 简介*:[https://blog . jupyter . org/JupyterLab-is-ready-for-users-5a6f 039 b 8906](https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906)
*   *学习降价让你的 Jupyter 笔记本准备好演示*:[https://medium . com/IBM-data-science-experience/Markdown-for-Jupyter-Notebooks-cheat sheet-386 c05 aeebed](https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed)
*   *28 Jupyter 笔记本提示、窍门、快捷方式*:[https://www . data quest . io/blog/Jupyter-Notebook-Tips-Tricks-shortcut s/](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)

学习更高级的统计学概念(我们在这里不涉及)并谨慎应用它们的一些资源如下:

*   *Python 中常态测试的温和介绍*:[https://machine learning mastery . com/A-Gentle-Introduction-to-Normality-Tests-in-Python/](https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/)
*   *假设检验如何工作:置信区间和置信水平*:[https://statisticsbyjim . com/Hypothesis-testing/Hypothesis-Tests-Confidence-Intervals-Levels/](https://statisticsbyjim.com/hypothesis-testing/hypothesis-tests-confidence-intervals-levels/)
*   *关于 Udacity 的推断统计(用数据做预测)简介*:[https://www . uda city . com/course/Intro-to-interestial-Statistics-ud 201](https://www.udacity.com/course/intro-to-inferential-statistics--ud201)
*   第四课:置信区间(宾夕法尼亚州立大学基础统计学):【https://online.stat.psu.edu/stat200/lesson/4 
*   *看理论:概率统计的直观介绍*:【https://seeing-theory.brown.edu/index.html】T2
*   *统计做错了:亚历克斯·莱因哈特的《可悲的完全指南》*:【https://www.statisticsdonewrong.com/ 
*   *调查抽样方法*:【https://stattrek.com/survey-research/sampling-methods.aspx **