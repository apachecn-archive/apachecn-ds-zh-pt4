

# *第四章*:聚合熊猫数据帧

在这一章中，我们将从第三章 、*与熊猫*的数据角力开始，通过解决数据的浓缩和聚合来继续我们对数据角力的讨论。这包括基本技能，如合并数据帧、创建新列、执行窗口计算以及按组成员进行聚合。计算聚合和汇总将帮助我们对数据得出结论。

除了我们在前几章介绍的时间序列切片之外，我们还将看看`pandas`在处理时间序列数据方面的附加功能，包括我们如何通过聚合来汇总数据并根据一天中的时间来选择数据。我们将遇到的大部分数据是时间序列数据，因此能够有效地处理时间序列是至关重要的。当然，高效地执行这些操作很重要，所以我们也将回顾如何编写高效的`pandas`代码。

本章将让我们熟悉使用`DataFrame`对象进行分析。因此，与之前的内容相比，这些主题更加深入，可能需要重读几遍，因此请务必跟随笔记本学习，其中包含其他示例。

本章将涵盖以下主题:

*   对数据帧执行数据库风格的操作
*   使用数据帧操作来丰富数据
*   汇总数据
*   使用时间序列数据

# 章节材料

本章的材料可以在 GitHub 上找到，网址是[https://GitHub . com/stef molin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch _ 04](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04)。我们将学习四本笔记本，每本都根据使用时间进行编号。文字会提示你切换。我们将从`1-querying_and_merging.ipynb`笔记本开始学习查询和合并数据帧。然后，我们将继续讨论通过宁滨、窗口函数和管道等操作来丰富数据。在本节中，我们还将使用`window_calc.py` Python 文件，它包含一个使用管道执行窗口计算的函数。

小费

`understanding_window_calculations.ipynb`笔记本包含一些用于理解窗口功能的交互式可视化。这可能需要一些额外的设置，但说明在笔记本上。

接下来，在`3-aggregations.ipynb`笔记本中，我们将讨论聚合、数据透视表和交叉表。最后，我们将关注在使用`4-time_series.ipynb`笔记本中的时间序列数据时`pandas`提供的额外功能。请注意，我们不会查看`0-weather_data_collection.ipynb`笔记本；然而，对于那些感兴趣的人来说，它包含了用于从**国家环境信息中心** ( **NCEI** ) API 收集数据的代码，可以在[https://www.ncdc.noaa.gov/cdo-web/webservices/v2](https://www.ncdc.noaa.gov/cdo-web/webservices/v2)找到。

在本章中，我们将使用各种数据集，这些数据集可以在`data/`目录中找到:

![Figure 4.1 – Datasets used in this chapter
](image/Figure_4.1_B16834.jpg)

图 4.1-本章使用的数据集

请注意，`exercises/`目录包含完成本章末尾练习所需的 CSV 文件。关于这些数据集的更多信息可以在`exercises/README.md`文件中找到。

# 在数据帧上执行数据库风格的操作

`DataFrame`对象类似于数据库中的表:每个对象都有一个名称，由行组成，包含特定数据类型的列。因此，`pandas`允许我们对它们进行数据库风格的操作。传统上，数据库至少支持四种操作，称为 **CRUD** : **C** reate、 **R** ead、 **U** pdate 和 **D** elete。

一种数据库查询语言——最常见的是 **SQL** (发音为 *sequel* 或 *S-Q-L* )，代表**结构化查询语言**——被用来要求数据库执行这些操作。这本书不需要 SQL 知识；然而，我们将看看本节将要讨论的`pandas`操作的 SQL 等价物，因为它可以帮助熟悉 SQL 的人理解。许多数据专业人员对基本的 SQL 有些熟悉，所以请参考*进一步阅读*部分，以获得提供更正式介绍的资源。

对于这一部分，我们将在`1-querying_and_merging.ipynb`笔记本上工作。我们将从导入开始，读取纽约市天气数据 CSV 文件:

```
>>> import pandas as pd
>>> weather = pd.read_csv('data/nyc_weather_2018.csv')
>>> weather.head()
```

这是长格式数据——我们在 2018 年覆盖纽约市的各个站点每天都有几个不同的天气观测:

![Figure 4.2 – NYC weather data
](image/Figure_4.2_B16834.jpg)

图 4.2-纽约市天气数据

在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*使用熊猫数据帧*中，我们介绍了如何创建数据帧；这是相当于 SQL 语句的 T0。当我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*处理熊猫数据帧*和 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) 、*与熊猫数据争论*中讨论选择和过滤时，我们关注的是从数据帧中读取，这相当于`SELECT`(挑选列)和`WHERE`(按布尔标准过滤)SQL 子句。当我们在第三章*与熊猫*讨论处理丢失数据时，我们执行了更新(SQL 中的`UPDATE`和删除(SQL 中的`DELETE FROM`)操作。除了这些基本的 CRUD 操作，还存在表的**连接**或**合并**的概念。我们将在本节中讨论`pandas`的实现，以及查询`DataFrame`对象的想法。

## 查询数据帧

Pandas 提供了`query()`方法，因此我们可以轻松地编写复杂的过滤器，而不是使用布尔掩码。语法类似于 SQL 语句中的`WHERE`子句。为了说明这一点，让我们查询所有行的天气数据，在这些行中，对于站 ID 中带有`US1NY`的站，其`SNOW`列的值大于零:

```
>>> snow_data = weather.query(
...     'datatype == "SNOW" and value > 0 '
...     'and station.str.contains("US1NY")'
... ) 
>>> snow_data.head()
```

每一行都是给定日期和站点组合的降雪观测值。请注意，1 月 4 日的数值变化很大——一些站点的降雪量比其他站点多:

![Figure 4.3 – Querying the weather data for observations of snow
](image/Figure_4.3_B16834.jpg)

图 4.3-查询天气数据以观察降雪

这个查询相当于 SQL 中的下面的。注意`SELECT *`选择表中的所有列(在本例中是我们的数据帧):

```
SELECT * FROM weather
WHERE
  datatype == "SNOW" AND value > 0 AND station LIKE "%US1NY%";
```

在 [*第二章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*处理熊猫数据帧*中，我们学习了如何使用布尔掩码来获得相同的结果:

```
>>> weather[
...     (weather.datatype == 'SNOW') & (weather.value > 0)
...     & weather.station.str.contains('US1NY')
... ].equals(snow_data)
True
```

在很大程度上，我们使用哪一个是个人喜好的问题；然而，如果我们的数据帧有一个很长的名字，我们可能会更喜欢使用`query()`方法。在前面的示例中，为了使用掩码，我们必须再键入三次数据帧的名称。

小费

当使用布尔逻辑和`query()`方法时，我们可以同时使用逻辑运算符(`and`、`or`、`not`)和位运算符(`&`、`|`、`~`)。

## 合并数据帧

当我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035) 、*处理熊猫数据帧*中讨论用`pd.concat()`函数和`append()`方法将数据帧堆叠在另一个之上时，我们执行的是相当于 SQL `UNION ALL`语句的(或者只是`UNION`，如果我们也删除了重复项，就像我们在 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) 、*与熊猫的数据争论*中看到的那样)。合并数据帧涉及如何按行排列它们。

当提到数据库时，合并通常被称为**连接**。有四种类型的联接:完全联接(外部联接)、左联接、右联接和内部联接。这些连接类型让我们知道仅出现在连接一侧的值将如何影响结果。这是一个视觉上更容易理解的概念，所以让我们看一些文氏图，然后在天气数据上做一些样本连接。这里，较暗的区域表示执行连接后剩下的数据:

![Figure 4.4 – Understanding join types
](image/Figure_4.4_B16834.jpg)

图 4.4–了解连接类型

我们一直在处理来自众多气象站的数据，但除了它们的 id 之外，我们对它们一无所知。知道每个气象站的确切位置将有助于更好地理解纽约市同一天天气读数之间的差异。当我们查询降雪数据时，我们看到 1 月 4 日的读数有相当大的变化(见*图 4.3* )。这很可能是由于车站的位置。海拔较高或更北的气象站可能会记录更多的降雪。取决于他们实际上离纽约有多远，他们可能在其他地方经历了更严重的暴风雪，比如康涅狄格州或新泽西州北部。

NCEI API 的`stations`端点为我们提供了站点所需的所有信息。这在`weather_stations.csv`文件中，也在 SQLite 数据库的`stations`表中。让我们将这些数据读入数据帧:

```
>>> station_info = pd.read_csv('data/weather_stations.csv')
>>> station_info.head()
```

作为参考，纽约市的中央公园在北纬 40.7829，西经 73.9654(纬度 40.7829，经度-73.9654)，纽约市海拔 10 米。记录纽约市数据的前五个站点不在纽约。新泽西州的位于纽约市的西南部，而康涅狄格州的位于纽约市的东北部:

![Figure 4.5 – Weather stations dataset
](image/Figure_4.5_B16834.jpg)

图 4.5-气象站数据集

连接要求我们指定如何匹配数据。`weather`数据帧与`station_info`数据帧唯一相同的数据是站 ID。然而，包含这些信息的列的名称并不相同:在`weather`数据帧中，该列被称为`station`，而在`station_info`数据帧中，该列被称为`id`。在我们加入数据之前，让我们先了解一下我们有多少个不同的站以及每个数据帧中有多少个条目:

```
>>> station_info.id.describe()
count                   279
unique                  279
top       GHCND:US1NJBG0029
freq                      1
Name: id, dtype: object
>>> weather.station.describe()
count                 78780
unique                  110
top       GHCND:USW00094789
freq                   4270
Name: station, dtype: object
```

数据帧中唯一站点数量的差异告诉我们，它们不包含所有的相同站点。根据我们选择的连接类型，我们可能会丢失一些数据。因此，查看连接前后的行数非常重要。我们可以从`describe()`的输出的 **count** 条目中看到这一点，但是我们不需要仅仅为了获得行数而运行它。相反，我们可以使用`shape`属性，它给出了一个形式为`(number of rows, number of columns)`的元组。为了选择行，我们只需获取索引处的值`0`(对于列为`1`):

```
>>> station_info.shape[0], weather.shape[0] # 0=rows, 1=cols
(279, 78780)
```

因为我们会经常检查行数，所以编写一个函数来给出任意数量的数据帧的行数更有意义。`*dfs`参数将这个函数的所有输入收集到一个元组中，我们可以在 list comprehension 中迭代这个元组以获得行数:

```
>>> def get_row_count(*dfs):
...     return [df.shape[0] for df in dfs]
>>> get_row_count(station_info, weather)
[279, 78780]
```

现在我们知道我们有 78，780 行天气数据和 279 行气象站信息数据，我们可以开始查看连接的类型。我们将从内部连接开始，这将产生最少的行数(除非两个数据帧对于要连接的列具有相同的值，在这种情况下，所有连接都是等效的)。**内部连接**将从两个数据帧中返回列，它们在指定的键列上有匹配。因为我们将连接到`weather.station`列和`station_info.id`列，所以我们将只获得位于`station_info`的气象站的气象数据。

我们将使用`merge()`方法来执行连接(默认情况下是内部连接),方法是提供左侧和右侧数据帧，并指定要连接的列。由于车站 ID 列在数据帧中的命名不同，我们必须用`left_on`和`right_on`来指定名称。左边的数据帧是我们调用`merge()` on 的数据帧，而右边的数据帧是作为参数传入的数据帧:

```
>>> inner_join = weather.merge(
...     station_info, left_on='station', right_on='id'
... )
>>> inner_join.sample(5, random_state=0)
```

请注意，我们有五个额外的列，它们被添加到右侧。这些来自于`station_info`数据帧。该操作还保留了`station`和`id`列，它们是相同的:

![Figure 4.6 – Results of an inner join between the weather and stations datasets
](image/Figure_4.6_B16834.jpg)

图 4.6-天气和气象站数据集之间的内部连接结果

为了删除`station`和`id`列中的重复信息，我们可以在连接之前重命名其中一个。因此，我们只需为`on`参数提供一个值，因为这些列将共享相同的名称:

```
>>> weather.merge(
...     station_info.rename(dict(id='station'), axis=1), 
...     on='station'
... ).sample(5, random_state=0)
```

因为这些列共享了名称，所以我们在连接它们之后只能得到一个:

![Figure 4.7 – Matching the names of the joining column to prevent duplicate data in the result
](image/Figure_4.7_B16834.jpg)

图 4.7-匹配连接列的名称以防止结果中出现重复数据

小费

我们可以通过将列名列表传递给参数`on`或参数`left_on`和`right_on`来连接多个列。

请记住，我们在`station_info`数据帧中有 279 个独立的站点，但是只有 110 个独立的气象数据站点。当我们执行内部连接时，我们丢失了所有没有相关天气观测的站。如果我们不想在连接的特定一侧丢失行，我们可以执行左连接或右连接。一个**左连接**要求我们在左边列出我们想要保留的行的数据帧(即使它们不存在于其他数据帧中)，在右边列出其他数据帧；一个**右连接**是逆连接:

```
>>> left_join = station_info.merge(
...     weather, left_on='id', right_on='station', how='left'
... )
>>> right_join = weather.merge(
...     station_info, left_on='station', right_on='id',
...     how='right'
... )
>>> right_join[right_join.datatype.isna()].head() # see nulls
```

只要另一个数据帧不包含数据，我们就会得到空值。我们可能想要调查为什么我们没有与这些站相关的任何天气数据。或者，我们的分析可能涉及确定每个站的数据可用性，因此获得空值不一定是问题:

![Figure 4.8 – Null values may be introduced when not using an inner join
](image/Figure_4.8_B16834.jpg)

图 4.8–不使用内部连接时可能会引入空值

由于我们将`station_info`数据帧放在左侧用于左连接，放在右侧用于右连接，所以这里的结果是相同的。在这两种情况下，我们都选择让所有的气象站出现在`station_info`数据帧中，接受气象观测的空值。为了证明它们是等价的，我们需要将列按相同的顺序排列，重置索引，并对数据进行排序:

```
>>> left_join.sort_index(axis=1)\
...     .sort_values(['date', 'station'], ignore_index=True)\
...     .equals(right_join.sort_index(axis=1).sort_values(
...         ['date', 'station'], ignore_index=True
...     ))
True
```

请注意，我们在左右连接中有额外的行，因为我们保留了所有没有天气观测的站:

```
>>> get_row_count(inner_join, left_join, right_join)
[78780, 78949, 78949]
```

最后一种连接是**完全外部连接**，它将保留所有的值，不管它们是否存在于两个数据帧中。例如，假设我们查询了站点 ID 中带有`US1NY`的站点，因为我们认为测量纽约天气的站点必须这样标记。这意味着内部连接将导致康涅狄格州和新泽西州的观测数据丢失，而左/右连接将导致台站信息或天气数据丢失。外部连接将保留所有的数据。我们还将传入`indicator=True`,向生成的数据帧添加一个额外的列，这将表明每行来自哪个数据帧:

```
>>> outer_join = weather.merge(
...     station_info[station_info.id.str.contains('US1NY')], 
...     left_on='station', right_on='id',
...     how='outer', indicator=True
... )
# view effect of outer join
>>> pd.concat([
...     outer_join.query(f'_merge == "{kind}"')\
...         .sample(2, random_state=0)
...     for kind in outer_join._merge.unique()
... ]).sort_index()
```

索引 **23634** 和 **25742** 来自位于纽约州的电视台，比赛给了我们关于电视台的信息。索引 **60645** 和 **70764** 用于站点 ID 中没有`US1NY`的站点，导致站点信息栏为空。下面两行是纽约的气象站，不提供纽约的天气观测。这种连接保留所有数据，并且通常会引入空值，这与内部连接不同，内部连接不会:

![Figure 4.9 – An outer join keeps all the data
](image/Figure_4.9_B16834.jpg)

图 4.9–外部连接保存所有数据

前面提到的连接相当于以下形式的 SQL 语句，我们只需将`<JOIN_TYPE>`更改为`(INNER) JOIN`、`LEFT JOIN`、`RIGHT JOIN`或`FULL OUTER JOIN`以获得适当的连接:

```
SELECT *
FROM left_table
<JOIN_TYPE> right_table
ON left_table.<col> == right_table.<col>;
```

加入 dataframes 使得处理第三章*中的脏数据和熊猫*的数据争论变得更加容易。记住，我们有来自两个不同站点的数据:一个有有效的站点 id，另一个是`?`。`?`站是唯一一个记录雪的水当量的站(`WESF`)。现在我们已经知道了连接数据帧，我们可以将有效站 id 的数据连接到我们按日期遗漏的`?`站的数据。首先，我们需要读入 CSV 文件，将`date`列设置为索引。我们将删除重复项和`SNWD`列(雪的深度)，我们发现它没有提供任何信息，因为大多数值都是无穷大(有雪和没有雪时都是如此):

```
>>> dirty_data = pd.read_csv(
...     'data/dirty_data.csv', index_col='date'
... ).drop_duplicates().drop(columns='SNWD')
>>> dirty_data.head()
```

我们的起始数据看起来像这样:

![Figure 4.10 – Dirty data from the previous chapter
](image/Figure_4.10_B16834.jpg)

图 4.10–上一章的脏数据

现在，我们需要为每个站创建一个数据帧。为了减少输出，我们将删除一些额外的列:

```
>>> valid_station = dirty_data.query('station != "?"')\
...     .drop(columns=['WESF', 'station'])
>>> station_with_wesf = dirty_data.query('station == "?"')\
...     .drop(columns=['station', 'TOBS', 'TMIN', 'TMAX'])
```

这一次，我们想要连接的列(日期)实际上是索引，所以我们将传入`left_index`来指示左边数据帧中要使用的列是索引，然后传入`right_index`来指示右边数据帧的索引。我们将执行一次左连接，以确保我们不会丢失有效站中的任何行，并且在可能的情况下，用来自`?`站的观测值来扩充它们:

```
>>> valid_station.merge(
...     station_with_wesf, how='left',
...     left_index=True, right_index=True
... ).query('WESF > 0').head()
```

对于数据帧共有但不属于连接的所有列，我们现在有两个版本。来自左侧数据帧的版本在列名后添加了后缀`_x`，来自右侧数据帧的版本在后缀`_y`后添加了后缀:

![Figure 4.11 – Merging weather data from different stations
](image/Figure_4.11_B16834.jpg)

图 4.11-合并不同气象站的气象数据

我们可以用参数`suffixes`提供我们自己的后缀。让我们仅对`?`站使用后缀:

```
>>> valid_station.merge(
...     station_with_wesf, how='left',
...     left_index=True, right_index=True, 
...     suffixes=('', '_?')
... ).query('WESF > 0').head()
```

因为我们为左边的后缀指定了一个空字符串，所以来自左边数据帧的列有它们原来的名称。但是，右边的后缀`_?`被添加到来自右边数据帧的列名中:

![Figure 4.12 – Specifying the suffix for shared columns not being used in the join
](image/Figure_4.12_B16834.jpg)

图 4.12–为连接中未使用的共享列指定后缀

当我们连接索引时，更简单的方法是使用`join()`方法，而不是`merge()`。它也默认为一个内部连接，但是这个行为可以用`how`参数改变，就像用`merge()`一样。`join()`方法将总是使用左边数据帧的索引来连接，但是如果它的名称被传递给`on`参数，它可以使用右边数据帧中的列。注意，后缀现在使用`lsuffix`指定左边数据帧的后缀，使用`rsuffix`指定右边数据帧的后缀。这产生了与前一个例子相同的结果(*图 4.12* ):

```
>>> valid_station.join(
...     station_with_wesf, how='left', rsuffix='_?'
... ).query('WESF > 0').head()
```

需要记住的一件重要的事情是，连接可能是相当资源密集型的，所以在完成连接之前弄清楚行将会发生什么通常是有益的。如果我们还不知道我们想要什么类型的加入，这可以帮助我们有一个想法。我们可以在我们计划加入的索引上使用**集合操作**来解决这个问题。

记住，**集合**的数学定义是不同对象的集合。根据定义，索引是一个集合。集合运算通常用文氏图来解释:

![Figure 4.13 – Set operations
](image/Figure_4.13_B16834.jpg)

图 4.13–设置操作

重要说明

注意`set`也是标准库中可用的 Python 类型。集合的一个常见用途是从列表中删除重复项。关于 Python 中集合的更多信息可以在文档中找到，网址为[https://docs . Python . org/3/library/stdtypes . html # set-types-set-frozenset](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset)。

让我们使用`weather`和`station_info`数据帧来说明集合操作。首先，我们必须为将用于连接操作的列设置索引:

```
>>> weather.set_index('station', inplace=True)
>>> station_info.set_index('id', inplace=True)
```

为了查看内部连接将保留哪些内容，我们可以采用索引的**交叉点**，它向我们显示了重叠的站:

```
>>> weather.index.intersection(station_info.index)
Index(['GHCND:US1CTFR0039', ..., 'GHCND:USW1NYQN0029'],
      dtype='object', length=110)
```

正如我们在运行内部连接时所看到的，我们只获得了气象观测站的站信息。然而，这并没有告诉我们失去了什么；为此，我们需要找到**集合差**，它将减去集合，并给出第一个索引的值，而第二个索引中没有这个值。通过设置差异，我们可以很容易地看到，当执行内部连接时，我们没有丢失任何天气数据行，但是我们丢失了 169 个没有天气观测的站:

```
>>> weather.index.difference(station_info.index)
Index([], dtype='object')
>>> station_info.index.difference(weather.index)
Index(['GHCND:US1CTFR0022', ..., 'GHCND:USW00014786'],
      dtype='object', length=169)
```

注意，这个输出还告诉我们左右连接的结果。为了避免丢失行，我们希望将`station_info`数据帧放在连接的同一侧(对于左连接放在左边，对于右连接放在右边)。

小费

我们可以对参与连接的数据帧的索引使用`symmetric_difference()`方法，以查看双方都将丢失什么:`index_1.symmetric_difference(index_2)`。结果将是仅在一个索引中的值。笔记本里有一个例子。

最后，我们可以使用**联合**来查看如果我们运行一个完整的外部连接，我们将保留的所有值。请记住，`weather`数据帧包含始终重复的站，因为它们提供每日的测量，所以我们在取联合之前调用`unique()`方法，以查看我们将保留的站的数量:

```
>>> weather.index.unique().union(station_info.index)
Index(['GHCND:US1CTFR0022', ..., 'GHCND:USW00094789'],
      dtype='object', length=279)
```

本章末尾的*延伸阅读*部分包含了一些关于集合操作以及`pandas`与 SQL 相比如何的资源。现在，让我们继续讨论数据丰富。

# 使用数据帧操作来丰富数据

既然我们已经讨论了如何查询和合并`DataFrame`对象，让我们学习如何对它们执行复杂的操作来创建和修改列和行。对于这一部分，我们将在`2-dataframe_operations.ipynb`笔记本中使用天气数据，以及脸书股票的交易量和 2018 年每天的开盘价、最高价、最低价和收盘价。让我们导入我们需要的内容并读入数据:

```
>>> import numpy as np
>>> import pandas as pd
>>> weather = pd.read_csv(
...     'data/nyc_weather_2018.csv', parse_dates=['date']
... )
>>> fb = pd.read_csv(
...     'data/fb_2018.csv', index_col='date', parse_dates=True
... )
```

我们将从开始，在转移到宁滨之前回顾汇总整个行和列的操作，应用跨行和列的函数，以及窗口计算，该计算一次汇总一定数量的观察数据(例如移动平均值)。

## 算术与统计

Pandas 有几种方法来计算统计数据和执行数学运算，包括比较、除法和模运算。这些方法通过允许我们指定执行计算的轴(当在`DataFrame`对象上执行计算时),在如何定义计算方面给了我们更多的灵活性。默认情况下，计算将沿着列(`axis=1`或`axis='columns'`)进行，这些列通常包含单个数据类型的单个变量的观察值；然而，我们可以传入`axis=0`或`axis='index'`来代替按行执行计算。

在本节中，我们将使用其中的一些方法来创建新的列并修改我们的数据，看看我们如何使用新数据来得出一些初步的结论。请注意，完整的列表可以在[https://pandas . pydata . org/pandas-docs/stable/reference/series . html # binary-operator-functions](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions)中找到。

首先，让我们用脸书股票交易量的 Z 值创建一个列，并用它来找出 Z 值绝对值大于 3 的日子。这些值超过平均值的三个标准差，这可能是异常的(取决于数据)。还记得我们在 [*第 1 章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015) 、*数据分析简介*中对 Z 分数的讨论吗，我们是通过减去平均值并除以标准差来计算的。我们将分别使用`sub()`和`div()`方法，而不是使用数学运算符进行减法和除法:

```
>>> fb.assign(
...     abs_z_score_volume=lambda x: x.volume \
...         .sub(x.volume.mean()).div(x.volume.std()).abs()
... ).query('abs_z_score_volume > 3')
```

2018 年有五天的交易量 Z 值绝对值大于 3。这些日期尤其会在本章的其余部分经常出现，因为它们标志着脸书股价的一些麻烦点:

![Figure 4.14 – Adding a Z-score column
](image/Figure_4.14_B16834.jpg)

图 4.14–添加 Z 分数列

另外两个非常有用的方法是`rank()`和`pct_change()`，它们分别让我们对一列的值进行排序(并将它们存储在一个新列中)并计算周期之间的百分比变化。综合这些数据，我们可以看出哪五天脸书股票交易量的百分比变化最大:

```
>>> fb.assign(
...     volume_pct_change=fb.volume.pct_change(),
...     pct_change_rank=lambda x: \
...         x.volume_pct_change.abs().rank(ascending=False)
... ).nsmallest(5, 'pct_change_rank')
```

交易量百分比变化最大的一天是 2018 年 1 月 12 日，这一天恰好是 2018 年震动该股的众多脸书丑闻之一([https://www . CNBC . com/2018/11/20/face books-wells-in-2018-effect-on-stock . html](https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html))。这是当他们宣布改变新闻供稿，优先考虑用户朋友的内容，而不是他们关注的品牌。鉴于脸书收入的很大一部分来自广告(2017 年近 89%，*来源*:[https://www . investopedia . com/ask/answers/120114/how-does-Facebook-f b-make-money . ASP](https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp))，这引起了恐慌，因为许多人抛售股票，大幅推高了交易量，导致股价下跌:

![Figure 4.15 – Ranking trading days by percentage change in volume traded
](image/Figure_4.15_B16834.jpg)

图 4.15-按交易量的百分比变化排列交易日

我们可以使用切片来查看此次发布前后的变化:

```
>>> fb['2018-01-11':'2018-01-12']
```

请注意，我们是如何将前几章学到的知识结合起来，从数据中获得有趣的见解的。我们筛选了一年的股票数据，找到了对脸书股票有重大影响的几天(好的或坏的):

![Figure 4.16 – Facebook stock data before and after announcing changes to the news feed
](image/Figure_4.16_B16834.jpg)

图 4.16-宣布新闻源变更前后的脸书股票数据

最后，我们可以用聚合布尔运算来检查数据帧。例如，使用`any()`方法，我们可以看到脸书股票在 2018 年从未出现过大于 215 美元的每日最低价:

```
>>> (fb > 215).any()
open          True
high          True
low          False
close         True
volume        True
dtype: bool
```

如果我们想要查看一个列中的所有行是否都满足标准，我们可以使用`all()`方法。这告诉我们，脸书至少有一天的开盘价、最高价、最低价和收盘价小于或等于 215 美元:

```
>>> (fb > 215).all()
open      False
high      False
low       False
close     False
volume     True
dtype: bool
```

现在，让我们看看如何使用宁滨来划分我们的数据，而不是一个特定的值，例如在`any()`和`all()`示例中的 215 美元。

## 宁滨

有时候，使用类别比使用特定的值更方便。一个常见的例子是处理年龄——最有可能的是，我们不想查看每个年龄的数据，比如 25 岁对 26 岁；然而，我们很可能对 25-34 岁人群与 35-44 岁人群的对比感兴趣。这叫做**宁滨**或**离散化**(从连续到离散)；我们获取数据，并将观察结果放入与它们落入的范围相匹配的箱(或桶)中。通过这样做，我们可以大幅减少数据中不同值的数量，并使其更易于分析。

重要说明

虽然宁滨我们的数据可以使分析的某些部分更容易，但请记住，它将减少该领域的信息，因为粒度降低了。

对于交易量，我们可以做的一件有趣的事情是查看哪一天的交易量高，并在这些天寻找关于脸书的新闻或价格的大幅波动。不幸的是，任何两天的交易量都不太可能相同；事实上，我们可以确认，在数据中，没有两天的交易量相同:

```
>>> (fb.volume.value_counts() > 1).sum()
0
```

记住，`fb.volume.value_counts()`给出了`volume`的每个唯一值出现的次数。然后，我们可以为计数是否大于 1 创建一个布尔掩码，并对其求和(`True`计算为`1`，`False`计算为`0`)。或者，我们可以使用`any()`而不是`sum()`，它不是告诉我们`volume`出现多次的唯一值的数量，而是如果至少一个交易量出现多次，则给出`True`，否则给出`False`。

显然，我们需要为交易量创建一些范围，以便观察高交易量的日子，但是我们如何决定哪个范围是好的范围呢？一种方法是基于值对宁滨使用`pd.cut()`函数。首先，我们应该决定我们想要创建多少个容器——三个似乎是一个很好的分割，因为我们可以将容器标记为低、中和高。接下来，我们需要确定每个箱的宽度；`pandas`试图使这个过程尽可能无痛，所以如果我们想要同样大小的箱，我们所要做的就是指定我们想要的箱的数量(否则，我们必须以列表的形式指定每个箱的上限):

```
>>> volume_binned = pd.cut(
...     fb.volume, bins=3, labels=['low', 'med', 'high']
... )
>>> volume_binned.value_counts()
low     240
med       8
high      3
Name: volume, dtype: int64
```

小费

注意，我们在这里为每个箱子提供了标签；如果我们不这样做，每个容器将由它包括的值的区间来标记，这可能对我们有帮助，也可能没有帮助，这取决于我们的应用。如果我们既想标记这些值，又想在之后看到这些容器，我们可以在调用`pd.cut()`时传入`retbins=True`。然后，我们可以将分箱的数据作为返回的元组的第一个元素来访问，而分箱范围本身作为第二个元素。

看起来绝大多数交易日都处于低交易量阶段；请记住，这都是相对的，因为我们平均划分了最小和最大交易量之间的范围。让我们来看看三天高成交量的数据:

```
>>> fb[volume_binned == 'high']\
...     .sort_values('volume', ascending=False)
```

即使在高交易量的日子里，我们也可以看到 2018 年 7 月 26 日的交易量比 3 月份的其他两个日期高得多(交易了近 4000 万股额外股票):

![Figure 4.17 – Facebook stock data on days in the high-volume traded bucket
](image/Figure_4.17_B16834.jpg)

图 4.17–高交易量时段内的脸书股票数据

事实上，在搜索引擎上查询*脸书股价 2018 年 7 月 26 日*可以发现，脸书在 7 月 25 日收盘后宣布了他们的收益和令人失望的用户增长，随后是大量的盘后抛售。第二天早上开市时，的股票已经从 25 日收盘时的 217.50 美元跌至 26 日开市时的 174.89 美元。我们来看看这些数据:

```
>>> fb['2018-07-25':'2018-07-26']
```

不仅股价大跌，成交量也暴涨，增加了一亿多。这一切导致脸书市值损失约 1200 亿美元([https://www . market watch . com/story/Facebook-stock-crushed-after-income-user-growth-miss-2018-07-25](https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25)):

![Figure 4.18 – Facebook stock data leading up to the day with the highest volume traded in 2018
](image/Figure_4.18_B16834.jpg)

图 4.18-2018 年交易量最高的一天的脸书股票数据

如果我们看看被标记为高交易量交易日的另外两天，我们会发现过多的信息来解释为什么。对脸书来说，这两天都充满了丑闻。剑桥分析政治数据隐私丑闻于 2018 年 3 月 17 日星期六爆发，因此这些信息的交易直到 19 日星期一才开始([https://www . nytimes . com/2018/03/19/technology/Facebook-Cambridge-analytic a-explained . html](https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html)):

```
>>> fb['2018-03-16':'2018-03-20']
```

在接下来的几天里，随着关于事件严重性的更多信息的披露，事情变得更加糟糕:

![Figure 4.19 – Facebook stock data when the Cambridge Analytica scandal broke
](image/Figure_4.19_B16834.jpg)

图 4.19-剑桥分析公司丑闻爆发时的脸书股票数据

至于高交易量的第三天(2018 年 3 月 26 日)，FTC 对剑桥分析公司(Cambridge Analytica)丑闻展开调查，所以脸书的灾难还在继续([https://www . CNBC . com/2018/03/26/FTC-confirms-Facebook-data-breach-investigation . html](https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html))。

如果我们查看中等交易量组中的一些日期，我们可以看到许多日期是我们刚刚讨论的三个交易事件的一部分。这迫使我们重新审视我们最初是如何创建这些容器的。或许等宽垃圾箱不是答案？大多数日子的交易量非常接近；然而，几天的时间导致了仓位宽度相当大，这给我们留下了每个仓位天数的很大不平衡:

![Figure 4.20 – Visualizing the equal-width bins
](image/Figure_4.20_B16834.jpg)

图 4.20–可视化等宽箱

如果我们希望每个箱具有相同数量的观察值，我们可以使用`pd.qcut()`函数基于均匀间隔的分位数来分割箱。我们可以将交易量分成四分位数，将观察值平均分成不同宽度的仓位，这样我们就可以得到 **q4** 仓位中 63 个最高交易量的天数:

```
>>> volume_qbinned = pd.qcut(
...     fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4']
... )
>>> volume_qbinned.value_counts()
q1    63
q2    63
q4    63
q3    62
Name: volume, dtype: int64
```

注意仓位不再覆盖相同的交易量范围:

![Figure 4.21 – Visualizing the bins based on quartiles
](image/Figure_4.21_B16834.jpg)

图 4.21-基于四分位数的条块可视化

小费

在这两个例子中，我们让`pandas`计算容器范围；然而，`pd.cut()`和`pd.qcut()`都允许我们将每个库的上限指定为一个列表。

## 应用功能

到目前为止，我们对数据采取的大多数操作都是特定于列的。当我们想要在数据帧中的所有列上运行相同的代码时，我们可以使用方法来获得更简洁的代码。请注意，这不会就地完成。

在我们开始之前，让我们将中央公园站的天气观测数据分离出来，并对数据进行透视:

```
>>> central_park_weather = weather.query(
...     'station == "GHCND:USW00094728"'
... ).pivot(index='date', columns='datatype', values='value')
```

让我们计算一下 2018 年 10 月中央公园`TMIN`(最低气温)`TMAX`(最高气温)`PRCP`(降水量)观测值的 Z 值。很重要的一点是，我们不要试图把 Z 分数贯穿全年。纽约有四个季节，什么被认为是正常的天气将取决于我们正在看的季节。通过将我们的计算隔离到 10 月，我们可以看到 10 月是否有天气非常不同的日子:

```
>>> oct_weather_z_scores = central_park_weather\
...     .loc['2018-10', ['TMIN', 'TMAX', 'PRCP']]\
...     .apply(lambda x: x.sub(x.mean()).div(x.std()))
>>> oct_weather_z_scores.describe().T
```

`TMIN`和`TMAX`看起来与 10 月份的其他时间没有太大差别，但是`PRCP`有:

![Figure 4.22 – Calculating Z-scores for multiple columns at once
](image/Figure_4.22_B16834.jpg)

图 4.22–一次计算多列的 Z 值

我们可以使用`query()`来提取这个日期的值:

```
>>> oct_weather_z_scores.query('PRCP > 3').PRCP
date
2018-10-27    3.936167
Name: PRCP, dtype: float64
```

如果我们查看 10 月份降水的汇总统计数据，我们可以看到这一天的降水比其他日子多得多:

```
>>> central_park_weather.loc['2018-10', 'PRCP'].describe()
count    31.000000
mean      2.941935
std       7.458542
min       0.000000
25%       0.000000
50%       0.000000
75%       1.150000
max      32.300000
Name: PRCP, dtype: float64
```

`apply()`方法让我们可以一次对整个列或行运行矢量化操作。只要这些操作对数据中的所有列(或行)有效，我们几乎可以应用我们能想到的任何函数。例如，我们可以使用我们在上一节中讨论的`pd.cut()`和`pd.qcut()`宁滨函数来将每一列分成多个区间(假设我们想要相同数量的区间或值范围)。注意，如果我们想要应用的函数不是矢量化的，还有一个`applymap()`方法。或者，我们可以使用`np.vectorize()`来矢量化我们的函数，以便与`apply()`一起使用。查阅笔记本上的例子。

Pandas 确实提供了一些迭代数据帧的功能，包括`iteritems()`、`itertuples()`和`iterrows()`方法；然而，我们应该避免使用这些，除非我们绝对找不到其他解决方案。Pandas 和 NumPy 是为矢量化运算设计的，因为是用高效的 C 代码编写的，所以速度快很多；通过编写一个循环来一次迭代一个元素，由于 Python 实现整数和浮点的方式，我们使它的计算更加密集。例如，看看当使用`iteritems()`时，完成将数字`10`添加到一系列浮点值中的简单操作的时间是如何随着行数线性增长的，但是当使用向量化操作时，无论大小如何，时间都保持在零附近:

![Figure 4.23 – Vectorized versus iterative operations
](image/Figure_4.23_B16834.jpg)

图 4.23–矢量化与迭代运算

到目前为止，我们使用的所有函数和方法都涉及整行或整列；然而，有时候，我们更感兴趣的是执行窗口计算，它使用一部分数据。

## 窗口计算

Pandas 使得在一个窗口或一系列行/列上执行计算成为可能。在这一节中，我们将讨论几种构造这些窗口的方法。根据窗口的类型，我们可以看到不同的数据。

### 滚动窗户

当我们的索引是类型`DatetimeIndex`时，我们可以以天为单位指定窗口(如`2H`两小时或`3D`三天)；否则，我们可以将周期数指定为整数。假设我们感兴趣的是滚动 3 天窗口内的降雨量；用我们目前所学的知识来实现这一点是非常乏味的(并且可能是低效的)。幸运的是，我们可以使用`rolling()`方法轻松获得这些信息:

```
>>> central_park_weather.loc['2018-10'].assign(
...     rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()
... )[['PRCP', 'rolling_PRCP']].head(7).T
```

执行滚动三天总和后，每个日期将显示当天和前两天的降水量总和:

![Figure 4.24 – Rolling 3-day total precipitation
](image/Figure_4.24_B16834.jpg)

图 4.24–连续三天的总降水量

小费

如果我们想在滚动计算中使用日期，但是在索引中没有日期，我们可以在调用`rolling()`时将日期列的名称传递给`on`参数。相反，如果我们想使用行号的整数索引，我们可以简单地传入一个整数作为窗口；例如，`rolling(3)`为 3 行窗口。

要改变聚合，我们所要做的就是对`rolling()`的结果调用不同的方法；例如，`mean()`表示平均值，`max()`表示最大值。滚动计算也可以一次应用于所有列:

```
>>> central_park_weather.loc['2018-10']\
...     .rolling('3D').mean().head(7).iloc[:,:6]
```

这为我们提供了中央公园所有天气观测的三天滚动平均值:

![Figure 4.25 – Rolling 3-day average for all weather observations
](image/Figure_4.25_B16834.jpg)

图 4.25–所有天气观测的三天滚动平均值

要跨列应用不同的聚合，我们可以使用`agg()`方法；它允许我们将每列要执行的聚合指定为预定义函数或自定义函数。我们只需传入一个将列映射到聚合的字典来对它们执行操作。让我们找出连续三天的最高气温(`TMAX`)、最低气温(`TMIN`)、平均风速(`AWND`)、总降水量(`PRCP`)。然后，我们将它加入到原始数据中，以便我们可以比较结果:

```
>>> central_park_weather\
...     ['2018-10-01':'2018-10-07'].rolling('3D').agg({
...     'TMAX': 'max', 'TMIN': 'min',
...     'AWND': 'mean', 'PRCP': 'sum'
... }).join( # join with original data for comparison
...     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], 
...     lsuffix='_rolling'
... ).sort_index(axis=1) # put rolling calcs next to originals
```

使用`agg()`，我们能够为每列计算不同的滚动聚合:

![Figure 4.26 – Using different rolling calculations per column
](image/Figure_4.26_B16834.jpg)

图 4.26–每列使用不同的滚动计算

小费

我们还可以使用宽度可变的窗口，只需要一点额外的努力:我们可以创建一个`BaseIndexer`的子类，并在`get_window_bounds()`方法中提供确定窗口边界的逻辑(更多信息可以在[https://pandas . pydata . org/pandas-docs/stable/user _ guide/computation . html # custom-window-rolling](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling)中找到)，或者我们可以使用`pandas.api.indexers`模块中的一个预定义类。我们当前工作的笔记本包含一个使用`VariableOffsetWindowIndexer`类执行 3 个工作日滚动计算的示例。

使用滚动计算，我们有一个滑动窗口来计算我们的函数；然而，在某些情况下，我们更感兴趣的是某个函数在该点之前的所有数据的输出，在这种情况下，我们使用扩展窗口。

### 扩展窗口

扩展计算将给我们聚合函数的累积值。我们使用`expanding()`方法来执行带有扩展窗口的计算；像`cumsum()`和`cummax()`这样的方法使用扩展窗口进行计算。直接使用`expanding()`的优点是额外的灵活性:我们不局限于预定义的聚合，我们可以用`min_periods`参数(默认为 1)指定计算开始前的最小周期数。根据中央公园的天气数据，让我们使用`expanding()`方法来计算月平均降雨量:

```
>>> central_park_weather.loc['2018-06'].assign(
...     TOTAL_PRCP=lambda x: x.PRCP.cumsum(),
...     AVG_PRCP=lambda x: x.PRCP.expanding().mean()
... ).head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T
```

请注意，虽然没有累积平均值的方法，但我们可以使用`expanding()`方法来计算它。`AVG_PRCP`列中的值是`TOTAL_PRCP`列中的值除以处理的天数:

![Figure 4.27 – Calculating the month-to-date average precipitation
](image/Figure_4.27_B16834.jpg)

图 4.27–计算月平均降水量

正如我们对`rolling()`所做的那样，我们可以用`agg()`方法提供特定于列的聚合。让我们找出膨胀的最高温度、最低温度、平均风速和总降水量。注意，我们也可以将 NumPy 函数传递给`agg()`:

```
>>> central_park_weather\
...     ['2018-10-01':'2018-10-07'].expanding().agg({
...     'TMAX': np.max, 'TMIN': np.min, 
...     'AWND': np.mean, 'PRCP': np.sum
... }).join(
...     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], 
...     lsuffix='_expanding'
... ).sort_index(axis=1)
```

我们再次将原始数据加入到窗口计算中进行比较:

![Figure 4.28 – Performing different expanding window calculations per column               
](image/Figure_4.28_B16834.jpg)

图 4.28–对每列执行不同的扩展窗口计算

在执行计算时，滚动窗口和扩展窗口对窗口中的所有观察值进行同等加权，但有时，我们希望更多地强调最近的值。一种选择是对观察值进行指数加权。

### 指数加权移动窗口

熊猫也为指数加权移动计算提供了`ewm()`方法。正如在 [*第 1 章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015) 、*数据分析简介*中所讨论的，我们可以使用**指数加权移动平均线** ( **EWMA** )来平滑我们的数据。让我们比较一下日最高温度的 30 天滚动平均值和 30 天 EWMA。注意，我们使用`span`参数来指定用于 EWMA 计算的周期数:

```
>>> central_park_weather.assign(
...     AVG=lambda x: x.TMAX.rolling('30D').mean(),
...     EWMA=lambda x: x.TMAX.ewm(span=30).mean()
... ).loc['2018-09-29':'2018-10-08', ['TMAX', 'EWMA', 'AVG']].T
```

与滚动平均不同，EWMA 更重视最近的观测，因此 10 月 7 日气温的上升对 EWMA 的影响比滚动平均更大:

![Figure 4.29 – Smoothing the data with moving averages
](image/Figure_4.29_B16834.jpg)

图 4.29–用移动平均值平滑数据

小费

查看`understanding_window_calculations.ipynb`笔记本，它包含了一些理解窗口功能的交互式可视化。这可能需要一些额外的设置，但说明在笔记本上。

## 管道

管道有助于将期望`pandas`数据结构作为第一个参数的操作链接在一起。通过使用管道，我们可以构建复杂的工作流，而不需要编写高度嵌套和难以阅读的代码。一般来说，管道让我们把类似于`f(g(h(data), 20), x=True)`的东西变成下面这样，这样更容易阅读:

```
data.pipe(h)\ # first call h(data)
    .pipe(g, 20)\ # call g on the result with positional arg 20
    .pipe(f, x=True) # call f on result with keyword arg x=True
```

假设我们想通过调用此函数打印脸书数据帧子集的尺寸，并进行一些格式化:

```
>>> def get_info(df):
...     return '%d rows, %d cols and max closing Z-score: %d' 
...             % (*df.shape, df.close.max()) 
```

然而，在我们调用函数之前，我们想要计算所有列的 Z 分数。一种方法如下:

```
>>> get_info(fb.loc['2018-Q1']\
...            .apply(lambda x: (x - x.mean())/x.std()))
```

或者，我们可以在计算 Z 分数后将数据帧传输到此函数:

```
>>> fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std())\
...     .pipe(get_info)
```

管道还可以使编写可重用代码变得更加容易。在本书的几个代码片段中，我们看到了将一个函数传递给另一个函数的想法，比如当我们将一个 NumPy 函数传递给`apply()`时，它会在每一列上执行。我们可以使用管道将该功能扩展到`pandas`数据结构的方法:

```
>>> fb.pipe(pd.DataFrame.rolling, '20D').mean().equals(
...     fb.rolling('20D').mean()
... ) # the pipe is calling pd.DataFrame.rolling(fb, '20D')
True
```

为了说明这如何使我们受益，让我们看一个函数，它将给出我们选择的窗口计算的结果。该函数在`window_calc.py`文件中。我们将导入函数，并使用 IPython 中的`??`来查看函数定义:

```
>>> from window_calc import window_calc
>>> window_calc??
Signature: window_calc(df, func, agg_dict, *args, **kwargs)
Source:   
def window_calc(df, func, agg_dict, *args, **kwargs):
    """
    Run a window calculation of your choice on the data.
    Parameters:
        - df: The `DataFrame` object to run the calculation on.
        - func: The window calculation method that takes `df` 
          as the first argument.
        - agg_dict: Information to pass to `agg()`, could be 
          a dictionary mapping the columns to the aggregation 
          function to use, a string name for the function, 
          or the function itself.
        - args: Positional arguments to pass to `func`.
        - kwargs: Keyword arguments to pass to `func`.

    Returns:
        A new `DataFrame` object.
    """
    return df.pipe(func, *args, **kwargs).agg(agg_dict)
File:      ~/.../ch_04/window_calc.py
Type:      function
```

我们的`window_calc()`函数接受数据帧、要执行的函数(只要它将数据帧作为第一个参数)、关于如何聚合结果的信息以及任何可选的参数，并返回给我们一个包含窗口计算结果的新数据帧。让我们用这个函数来寻找脸书股票数据的扩展中值:

```
>>> window_calc(fb, pd.DataFrame.expanding, np.median).head()
```

请注意，`expanding()`方法不需要我们指定任何参数，所以我们所要做的就是传入`pd.DataFrame.expanding`(没有括号)，以及在 dataframe 上作为窗口计算执行的聚合:

![Figure 4.30 – Using pipes to perform expanding window calculations
](image/Figure_4.30_B16834.jpg)

图 4.30–使用管道执行扩展窗口计算

`window_calc()`函数也取`*args`和`**kwargs`；这些是可选参数，如果提供了这些参数，Python 会在按名称传递时将它们收集到`kwargs`中(如`span=20`)，如果没有提供，则收集到`args`中(按位置传递)。然后可以通过使用`args`的`*`和`kwargs`的`**`将它们**解包**并传递给另一个函数或方法调用。我们需要这个行为，以便使用`ewm()`方法计算脸书股票收盘价的 EWMA:

```
>>> window_calc(fb, pd.DataFrame.ewm, 'mean', span=3).head()
```

在前面的例子中，我们必须使用`**kwargs`，因为`span`参数不是`ewm()`接收的第一个参数，我们不想传递它之前的参数:

![Figure 4.31 – Using pipes to perform exponentially weighted window calculations
](image/Figure_4.31_B16834.jpg)

图 4.31–使用管道执行指数加权窗口计算

为了计算中央公园的连续三天天气聚合，我们利用了`*args`，因为我们知道窗口是`rolling()`的第一个参数:

```
>>> window_calc(
...     central_park_weather.loc['2018-10'], 
...     pd.DataFrame.rolling, 
...     {'TMAX': 'max', 'TMIN': 'min',
...      'AWND': 'mean', 'PRCP': 'sum'},
...     '3D'
... ).head()
```

我们能够以不同的方式聚合每一列，因为我们传入了一个字典而不是一个值:

![Figure 4.32 – Using pipes to perform rolling window calculations
](image/Figure_4.32_B16834.jpg)

图 4.32–使用管道执行滚动窗口计算

请注意，我们如何能够为窗口计算创建一致的 API，而调用者不需要知道在窗口函数之后要调用哪个聚合方法。这隐藏了一些实现细节，同时使它更容易使用。我们将使用这个函数作为我们将在 [*第 7 章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析-比特币和股票市场*中构建的`StockVisualizer`类中的一些功能的基础。

# 汇总数据

在上一节讨论窗口计算和管道时，我们已经对聚合有了初步了解。在这里，我们将着重于通过聚集来总结数据帧，这将改变我们的数据帧的形状(通常通过行缩减)。我们还看到了在`pandas`数据结构上利用矢量化 NumPy 函数是多么容易，尤其是在执行聚合时。这是 NumPy 最擅长的:它对数值数组执行计算效率高的数学运算。

NumPy 可以很好地与聚合数据帧配对，因为它为我们提供了一种用不同的预写函数汇总数据的简单方法；通常，在聚合时，我们只需要 NumPy 函数，因为我们想要自己编写的大部分内容之前已经构建好了。我们已经看到了一些常用于聚合的 NumPy 函数，比如`np.sum()`、`np.mean()`、`np.min()`、`np.max()`；然而，我们并不局限于数字运算——我们可以在字符串上使用像`np.unique()`这样的东西。在自己实现一个函数之前，总是要检查 NumPy 是否已经有了一个函数。

对于这一部分，我们将在`3-aggregations.ipynb`笔记本上工作。让我们导入`pandas`和`numpy`，并读入我们将使用的数据:

```
>>> import numpy as np
>>> import pandas as pd
>>> fb = pd.read_csv(
...     'data/fb_2018.csv', index_col='date', parse_dates=True
... ).assign(trading_volume=lambda x: pd.cut(
...     x.volume, bins=3, labels=['low', 'med', 'high'] 
... ))
>>> weather = pd.read_csv(
...     'data/weather_by_station.csv', 
...     index_col='date', parse_dates=True
... )
```

请注意，该部分的天气数据已经与一些气象站数据合并:

![Figure 4.33 – Merged weather and station data for this section
](image/Figure_4.33_B16834.jpg)

图 4.33-该路段的合并气象和气象站数据

在我们开始任何计算之前，让我们确保我们的数据不会以科学符号显示。我们将修改浮动的显示格式。我们将应用的格式是`.2f`，它将为浮点数提供小数点后两位数:

```
>>> pd.set_option('display.float_format', lambda x: '%.2f' % x)
```

首先，在继续按组汇总和构建数据透视表和交叉表之前，我们将看一下汇总整个数据集。

## 汇总数据帧

当我们讨论窗口计算时，我们看到我们可以对`rolling()`、`expanding()`或`ewm()`的结果运行`agg()`方法；然而，我们也可以以同样的方式在数据帧上直接调用它。唯一的区别是，以这种方式完成的聚合将在所有数据上执行，这意味着我们将只获得包含整体结果的序列。让我们用与窗口计算相同的方法来合计脸书股票数据。注意，我们不会从`trading_volume`列得到任何东西，该列包含来自`pd.cut()`的交易量仓位；这是因为我们没有指定要在该列上运行的聚合:

```
>>> fb.agg({
...     'open': np.mean, 'high': np.max, 'low': np.min, 
...     'close': np.mean, 'volume': np.sum
... })
open            171.45
high            218.62
low             123.02
close           171.51
volume   6949682394.00
dtype: float64
```

我们可以使用聚合轻松找到中央公园 2018 年的总降雪量和总降水量。在这种情况下，由于我们将对两者进行求和，我们可以使用`agg('sum')`或直接调用`sum()`:

```
>>> weather.query('station == "GHCND:USW00094728"')\
...     .pivot(columns='datatype', values='value')\
...     [['SNOW', 'PRCP']].sum()
datatype
SNOW   1007.00
PRCP   1665.30
dtype: float64
```

此外，我们可以提供多个函数，在我们想要聚合的每个列上运行。正如我们已经看到的，当每一列都有一个聚合时，我们得到一个`Series`对象。为了在每列多个 1 的情况下区分聚合，`pandas`将返回一个`DataFrame`对象。此数据帧的索引将告诉我们正在为哪一列计算哪个指标:

```
>>> fb.agg({
...     'open': 'mean', 
...     'high': ['min', 'max'],
...     'low': ['min', 'max'], 
...     'close': 'mean'
... })
```

这将产生一个数据帧，其中的行表示应用于数据列的聚合函数。请注意，对于我们没有明确要求的聚合和列的任何组合，我们都会得到空值:

![Figure 4.34 – Performing multiple aggregations per column
](image/Figure_4.34_B16834.jpg)

图 4.34–对每列执行多个聚合

到目前为止，我们已经了解了如何在特定窗口和整个数据帧上进行聚合；然而，真正的力量来自于通过组成员聚合的能力。这让我们可以计算我们创建的每个交易箱的每月总降水量、每个站点的总降水量和平均 OHLC 股票价格。

## 按组聚集

为了计算每个组的聚合，我们必须首先调用 dataframe 上的`groupby()`方法，并提供我们想要用来确定不同组的列。让我们看看用`pd.cut()`创建的每个交易量仓位的股票数据点的平均值；记住，这是三个等宽的箱子:

```
>>> fb.groupby('trading_volume').mean()
```

交易量越大，OHLC 平均价格越低，这是意料之中的，因为交易量大的仓位中有三个日期出现抛售:

![Figure 4.35 – Aggregating by group
](image/Figure_4.35_B16834.jpg)

图 4.35–按组汇总

运行`groupby()`之后，我们还可以选择特定的列进行聚合:

```
>>> fb.groupby('trading_volume')\
...     ['close'].agg(['min', 'max', 'mean'])
```

这为我们提供了每个交易量时段的收盘价汇总:

![Figure 4.36 – Aggregating a specific column per group
](image/Figure_4.36_B16834.jpg)

图 4.36–聚合每个组的特定列

如果我们需要更好的来控制每一列如何聚合，我们再次使用`agg()`方法和一个将列映射到它们的聚合函数的字典。正如我们之前所做的，我们可以为每个列提供功能列表；然而，结果看起来会有些不同:

```
>>> fb_agg = fb.groupby('trading_volume').agg({
...     'open': 'mean', 'high': ['min', 'max'],
...     'low': ['min', 'max'], 'close': 'mean'
... })
>>> fb_agg
```

我们现在在列中有一个层次索引。记住，这意味着如果我们想为中等交易量的桶选择最低低价，我们需要使用`fb_agg.loc['med', 'low']['min']`:

![Figure 4.37 – Performing multiple aggregations per column with groups
](image/Figure_4.37_B16834.jpg)

图 4.37–使用组对每列执行多个聚合

列存储在一个`MultiIndex`对象中:

```
>>> fb_agg.columns
MultiIndex([( 'open', 'mean'),
            ( 'high',  'min'),
            ( 'high',  'max'),
            (  'low',  'min'),
            (  'low',  'max'),
            ('close', 'mean')],
           )
```

我们可以使用列表理解来移除这个层次结构，取而代之的是以`<column>_<agg>`的形式给出我们的列名。在每次迭代中，我们将从`MultiIndex`对象中获得一个级别元组，我们可以将它组合成一个单独的字符串来删除层次结构:

```
>>> fb_agg.columns = ['_'.join(col_agg) 
...                   for col_agg in fb_agg.columns]
>>> fb_agg.head()
```

这将用一个级别替换列中的层次结构:

![Figure 4.38 – Flattening out the hierarchical index
](image/Figure_4.38_B16834.jpg)

图 4.38–展平层次索引

假设我们想要查看所有站点每天的平均观测降雨量。我们需要按日期分组，但它在索引中。在这种情况下，我们有几个选择:

*   重采样，我们将在本章后面的*处理时间序列数据*一节中介绍。
*   重置索引并使用从索引创建的日期列。
*   将`level=0`传递给`groupby()`以指示应该在索引的最外层执行分组。
*   使用一个`Grouper`对象。

这里，我们将传递`level=0`到`groupby()`，但是注意我们也可以传递`level='date'`，因为我们的索引是命名的。这给了我们各个站点的平均降雨量观测值，这可能比简单地选择一个站点来观察更能让我们了解天气。由于结果是一个单列的`DataFrame`对象，我们调用`squeeze()`将其转换成一个`Series`对象:

```
>>> weather.loc['2018-10'].query('datatype == "PRCP"')\ 
...     .groupby(level=0).mean().head().squeeze()
date
2018-10-01    0.01
2018-10-02    2.23
2018-10-03   19.69
2018-10-04    0.32
2018-10-05    0.96
Name: value, dtype: float64
```

我们也可以通过一次分组很多类别。让我们找出每个站记录的季度总降水量。这里，我们不需要将`level=0`传递给`groupby()`，而是需要使用一个`Grouper`对象来聚集从每天到每季度的频率。因为这将创建一个多级索引，所以在执行聚合后，我们还将使用`unstack()`沿着列放置内部级别(季度):

```
>>> weather.query('datatype == "PRCP"').groupby(
...     ['station_name', pd.Grouper(freq='Q')]
... ).sum().unstack().sample(5, random_state=1)
```

这个结果有很多可能的后续。我们可以看看哪个站的降雨量最多/最少。我们可以回到每个气象站的位置和海拔信息，看看这是否会影响降水。我们还可以看到各个站点中哪个季度的降雨量最多/最少:

![Figure 4.39 – Aggregating by a column with dates in the index
](image/Figure_4.39_B16834.jpg)

图 4.39–通过索引中的日期列进行聚合

小费

由`groupby()`方法返回的`DataFrameGroupBy`对象有一个`filter()`方法，它允许我们过滤组。我们可以利用这一点从聚合中排除某些组。只需传递一个函数，为数据帧的每个组的子集返回一个布尔值(`True`表示包含该组，`False`表示不包含该组)。笔记本里有一个例子。

我们来看看哪个月降水最多。首先，我们需要按日分组，并平均各站的降雨量。然后，我们可以按月分组，并对所得的降水量求和。最后我们会用`nlargest()`得出降水量最多的五个月:

```
>>> weather.query('datatype == "PRCP"')\
...     .groupby(level=0).mean()\
...     .groupby(pd.Grouper(freq='M')).sum().value.nlargest()
date
2018-11-30   210.59
2018-09-30   193.09
2018-08-31   192.45
2018-07-31   160.98
2018-02-28   158.11
Name: value, dtype: float64
```

也许之前的结果令人惊讶。俗话说*四月的阵雨带来五月的花*；然而，四月并不在前五名之列(就此而言，五月也不在前)。降雪可以算作降水，但这并不能解释为什么夏季的降雨量高于四月份。让我们寻找在给定月份中占大部分降水的日子，看看四月是否出现在那里。

为此，我们需要计算各站的日平均降水量，然后求出每月的总量；这将是分母。然而，为了将每天的数值除以当月的总数，我们需要一个等维的`Series`对象。这意味着我们将需要使用`transform()`方法，该方法将对数据执行指定的计算，同时总是返回一个与我们开始时的维度相等的对象。因此，我们可以在一个`Series`对象上调用它，并且总是得到一个`Series`对象，而不管聚合函数本身会返回什么:

```
>>> weather.query('datatype == "PRCP"')\
...     .rename(dict(value='prcp'), axis=1)\
...     .groupby(level=0).mean()\
...     .groupby(pd.Grouper(freq='M'))\
...     .transform(np.sum)['2018-01-28':'2018-02-03']
```

请注意，我们不是为一月和二月分别得到一个和，而是为一月的条目重复相同的值，为二月的条目重复不同的值。请注意，二月的值是我们在前面的结果中找到的值:

![Figure 4.40 – The denominator for calculating the percentage of monthly precipitation 
](image/Figure_4.40_B16834.jpg)

图 4.40–计算月降水量百分比的分母

我们可以在我们的数据框架中把它作为一个列，以便于计算每天发生的月降水量的百分比。然后，我们可以用`nlargest()`拉出最大值:

```
>>> weather.query('datatype == "PRCP"')\
...     .rename(dict(value='prcp'), axis=1)\
...     .groupby(level=0).mean()\
...     .assign(
...         total_prcp_in_month=lambda x: x.groupby(
...             pd.Grouper(freq='M')).transform(np.sum),
...         pct_monthly_prcp=lambda x: \
...             x.prcp.div(x.total_prcp_in_month)
...     ).nlargest(5, 'pct_monthly_prcp')
```

就月降水量而言，排在第四位和第五位的天总共占了四月降雨量的 50%以上。它们也是连续的几天:

![Figure 4.41 – Calculating the percentage of monthly precipitation that occurred each day
](image/Figure_4.41_B16834.jpg)

图 4.41–计算每天发生的月降水量的百分比

重要说明

`transform()`方法也作用于`DataFrame`对象，在这种情况下，它将返回一个`DataFrame`对象。我们可以使用它轻松地一次性标准化所有列。笔记本里有一个例子。

## 数据透视表和交叉表

为了总结这一节，我们将讨论一些`pandas`函数，这些函数将我们的数据聚合成一些常见的格式。我们之前讨论的聚合方法将为我们提供最高级别的定制；但是，`pandas`为提供了一些函数，可以快速生成一个通用格式的透视表和交叉表。

为了生成数据透视表，我们必须指定要分组的内容，以及可选地，我们想要聚集的列子集和/或如何聚集(默认情况下是平均)。让我们创建一个每交易量仓位的脸书平均 OHLC 数据的数据透视表:

```
>>> fb.pivot_table(columns='trading_volume')
```

因为我们传入了`columns='trading_volume'`，`trading_volume`列中的不同值沿着列放置。然后，原始数据帧中的列进入索引。注意这些列的指数有一个名称( **trading_volume** ):

![Figure 4.42 – Pivot table of column averages per volume traded bin
](image/Figure_4.42_B16834.jpg)

图 4.42–每个交易量仓位的列平均值的数据透视表

小费

如果我们改为将`trading_volume`作为`index`参数传递，我们会得到*图 4.42* 的转置，这也与我们使用`groupby()`时*图 4.35* 的输出完全相同。

使用`pivot()`方法，我们不能处理多级索引或具有重复值的索引。由于这个原因，我们还不能把天气数据放在宽格式中。`pivot_table()`方法解决了这个问题。为了做到这一点，我们需要将`date`和`station`信息放入索引中，并将`datatype`列的不同值放入列中。值将来自`value`列。我们将使用中位数来汇总任何重叠组合(如果有):

```
>>> weather.reset_index().pivot_table(
...     index=['date', 'station', 'station_name'], 
...     columns='datatype', 
...     values='value', 
...     aggfunc='median'
... ).reset_index().tail()
```

重置索引后，我们有了宽格式的数据。最后一步是重命名索引:

![Figure 4.43 – Pivot table with median values per datatype, station, and date
](image/Figure_4.43_B16834.jpg)

图 4.43-包含每个数据类型、工作站和日期的中值的数据透视表

我们可以使用`pd.crosstab()`函数创建一个频率表。例如，如果我们想知道脸书股票每个月有多少个低交易量、中等交易量和高交易量的交易日，我们可以使用交叉表。语法非常简单；我们将行标签和列标签分别传递给参数`index`和`columns`。默认情况下，单元格中的值将是计数:

```
>>> pd.crosstab(
...     index=fb.trading_volume, columns=fb.index.month,
...     colnames=['month'] # name the columns index
... )
```

这就很容易看出脸书股票交易量高的月份:

![Figure 4.44 – Crosstab showing the number of trading days per month, per volume traded bin
](image/Figure_4.44_B16834.jpg)

图 4.44-交叉表显示每个交易仓位每月的交易天数

小费

我们可以通过传入`normalize='rows'` / `normalize='columns'`将输出标准化为行/列总数的百分比。笔记本里有一个例子。

要更改聚合函数，我们可以向`values`提供一个参数，然后指定`aggfunc`。为了说明这一点，让我们找出每个交易量桶每月的平均收盘价，而不是上一个例子中的计数:

```
>>> pd.crosstab(
...     index=fb.trading_volume, columns=fb.index.month,
...     colnames=['month'], values=fb.close, aggfunc=np.mean
... )
```

我们现在获得每个交易量仓位每月的平均收盘价，当数据中不存在该组合时，该值为空值:

![Figure 4.45 – Crosstab using averages instead of counts
](image/Figure_4.45_B16834.jpg)

图 4.45-使用平均值而不是计数的交叉表

我们也可以用`margins`参数得到行和列的小计。让我们计算一下每个气象站每月记录降雪的次数，并包括小计:

```
>>> snow_data = weather.query('datatype == "SNOW"')
>>> pd.crosstab(
...     index=snow_data.station_name,
...     columns=snow_data.index.month, 
...     colnames=['month'],
...     values=snow_data.value,
...     aggfunc=lambda x: (x > 0).sum(),
...     margins=True, # show row and column subtotals
...     margins_name='total observations of snow' # subtotals
... )
```

沿着底部的行，我们有每个月的总降雪量观测值，而最右边的列，我们有每个站 2018 年的总降雪量观测值:

![Figure 4.46 – Crosstab counting the number of days with snow per month, per station
](image/Figure_4.46_B16834.jpg)

图 4.46-交叉表计算每个站每月下雪的天数

只要看看几个气象站，我们就会发现，尽管它们都为纽约市提供天气信息，但它们并没有分享天气的方方面面。根据我们选择观看的站点，我们可以从纽约市真实发生的事情中为添加/减去雪。

# 处理时间序列数据

对于时间系列数据，我们可以使用一些额外的操作，从选择、过滤到聚合。我们将在`4-time_series.ipynb`笔记本中探索其中的一些功能。让我们从阅读前面章节中的脸书数据开始:

```
>>> import numpy as np
>>> import pandas as pd
>>> fb = pd.read_csv(
...     'data/fb_2018.csv', index_col='date', parse_dates=True
... ).assign(trading_volume=lambda x: pd.cut( 
...     x.volume, bins=3, labels=['low', 'med', 'high']     
... ))
```

我们将从讨论时间序列数据的选择和过滤开始，然后讨论移位、差分、重采样以及最终基于时间的合并。请注意，将索引设置为 date(或 datetime)列很重要，这将允许我们利用我们将讨论的附加功能。有些操作可能不这样做也能工作，但是为了整个分析过程的顺利进行，建议使用类型为`DatetimeIndex`的索引。

## 基于时间的选择和过滤

让我们从快速回顾一下日期时间切片和索引开始。我们可以通过索引:`fb.loc['2018']`很容易地分离出当年的数据。对于我们的股票数据，将返回完整的数据框架，因为我们只有 2018 年的数据；但是，我们可以过滤到一个月(`fb.loc['2018-10']`)或一个日期范围。注意，使用`loc[]`对于范围是可选的:

```
>>> fb['2018-10-11':'2018-10-15']
```

我们只有三天的时间，因为股市周末休市:

![Figure 4.47 – Selecting data based on a date range
](image/Figure_4.47_B16834.jpg)

图 4.47–基于日期范围选择数据

请记住，也可以使用其他频率来提供日期范围，例如月份或一年中的季度:

```
>>> fb.loc['2018-q1'].equals(fb['2018-01':'2018-03'])
True
```

当以日期范围的开始或结束为目标时，`pandas`有一些额外的方法来选择指定时间单位内的第一行或最后一行。我们可以使用`first()`方法和`1W`偏移量选择 2018 年第一周的股票价格:

```
>>> fb.first('1W')
```

2018 年 1 月 1 日是假日，意味着市场关闭。那天也是星期一，所以这里的一周只有四天:

![Figure 4.48 – Facebook stock data during the first week of trading in 2018
](image/Figure_4.48_B16834.jpg)

图 4.48-2018 年第一周交易期间的脸书股票数据

我们也可以对最近的日期执行类似的操作。选择数据中的最后一周就像用`last()`方法切换`first()`方法一样简单:

```
>>> fb.last('1W')
```

由于 2018 年 12 月 31 日是星期一，最后一周只有一天:

![Figure 4.49 – Facebook stock data during the last week of trading in 2018
](image/Figure_4.49_B16834.jpg)

图 4.49-2018 年最后一周交易期间的脸书股票数据

当处理每日股票数据时，我们只有股票市场开市日期的数据。假设我们重新索引了数据，以包含一年中每一天的行:

```
>>> fb_reindexed = fb.reindex(
...     pd.date_range('2018-01-01', '2018-12-31', freq='D')
... )
```

对于 1 月 1 日和任何其他休市的日子，重新编制指数的数据将全部为空。我们可以结合`first()`、`isna()`和`all()`的方法来证实这一点。这里，我们还将使用`squeeze()`方法将调用`first('1D').isna()`产生的单行`DataFrame`对象转换为`Series`对象，这样调用`all()`将产生一个值:

```
>>> fb_reindexed.first('1D').isna().squeeze().all()
True
```

我们可以使用`first_valid_index()`方法获得我们数据中第一个非空条目的索引，这将是数据中交易的第一天。要获得最后一天的交易，我们可以使用`last_valid_index()`方法。2018 年第一季度，第一个交易日为 1 月 2 日，最后一个交易日为 3 月 29 日:

```
>>> fb_reindexed.loc['2018-Q1'].first_valid_index()
Timestamp('2018-01-02 00:00:00', freq='D')
>>> fb_reindexed.loc['2018-Q1'].last_valid_index()
Timestamp('2018-03-29 00:00:00', freq='D')
```

如果我们想知道脸书截至 2018 年 3 月 31 日的股价是什么样的，我们最初的想法可能是使用索引来检索它。然而，如果我们试图用`loc[]` ( `fb_reindexed.loc['2018-03-31']`)来做这件事，我们将得到空值，因为那天股市没有开市。如果我们使用`asof()`方法，它将给出我们要求的日期之前最接近的非空数据，在本例中是 3 月 29 日。因此，如果我们想了解脸书在每个月最后一天的表现，我们可以使用`asof()`，而不必先检查当天市场是否开放:

```
>>> fb_reindexed.asof('2018-03-31')
open                   155.15
high                   161.42
low                    154.14
close                  159.79
volume            59434293.00
trading_volume            low
Name: 2018-03-31 00:00:00, dtype: object
```

在接下来的几个例子中，除了日期，我们还需要时间信息。到目前为止，我们一直在处理的数据集缺少时间组件，因此我们将从 Nasdaq.com 切换到 2019 年 5 月 20 日至 2019 年 5 月 24 日的脸书股票数据。为了正确解析日期时间，我们需要传入一个 lambda 函数作为`date_parser`参数，因为它们不是标准格式(例如，2019 年 5 月 20 日上午 9:30 表示为`2019-05-20 09-30`)；λ函数将指定如何将`date`字段中的数据转换为日期时间:

```
>>> stock_data_per_minute = pd.read_csv(
...     'data/fb_week_of_may_20_per_minute.csv', 
...     index_col='date', parse_dates=True, 
...     date_parser=lambda x: \
...         pd.to_datetime(x, format='%Y-%m-%d %H-%M')
... )
>>> stock_data_per_minute.head()
```

我们有每分钟的 OHLC 数据，以及每分钟的交易量:

![Figure 4.50 – Facebook stock data by the minute
](image/Figure_4.50_B16834.jpg)

图 4.50-每分钟的脸书股票数据

重要说明

为了正确解析非标准格式的日期时间，我们需要指定它的格式。有关可用代码的参考，请参考位于[https://docs . Python . org/3/library/datetime . html # strftime-strptime-behavior](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)的 Python 文档。

我们可以将`first()`和`last()`与`agg()`结合使用，将这些数据提升到每日粒度。为了获得真正的开放价值，我们需要每天进行第一次观察；相反，对于真实的收盘值，我们需要每天进行最后一次观察。高点和低点将是每天各自列的最大值和最小值。交易量将是每日总和:

```
>>> stock_data_per_minute.groupby(pd.Grouper(freq='1D')).agg({
...     'open': 'first', 
...     'high': 'max', 
...     'low': 'min', 
...     'close': 'last', 
...     'volume': 'sum'
... })
```

这会将数据累计到每日频率:

![Figure 4.51 – Rolling up the data from the minute level to the daily level
](image/Figure_4.51_B16834.jpg)

图 4.51–将数据从分钟级别汇总到每日级别

接下来我们将讨论的两种方法帮助我们根据日期时间的时间部分选择数据。`at_time()`方法允许我们隔离日期时间的时间部分是我们指定的时间的行。通过运行`at_time('9:30')`，可以抓取所有市场开盘价(股市上午 9:30 开市):

```
>>> stock_data_per_minute.at_time('9:30')
```

这告诉我们每天开盘时的股票数据:

![Figure 4.52 – Grabbing the stock data at market open each day
](image/Figure_4.52_B16834.jpg)

图 4.52-每天开市时获取股票数据

我们可以使用`between_time()`方法获取日期时间的时间部分在两个时间之间的所有行(默认情况下包括端点)。如果我们想日复一日地查看某个时间范围内的数据，这个方法会非常有用。让我们抓住每天交易的最后两分钟(15:59 - 16:00)内的所有行:

```
>>> stock_data_per_minute.between_time('15:59', '16:00')
```

看起来最后一分钟(16:00)的交易量比前一分钟(15:59)多得多。也许人们在收盘前急于交易:

![Figure 4.53 – Stock data in the last two minutes of trading per day
](image/Figure_4.53_B16834.jpg)

图 4.53-每天交易最后两分钟的股票数据

我们可能想知道这是否也发生在前两分钟。人们会在头天晚上交易，然后在开市时执行吗？改之前的代码回答那个问题是小事一桩。相反，让我们来看看，平均而言，在交易的前 30 分钟或最后 30 分钟，是否有更多的股票被交易。我们可以结合`between_time()`和`groupby()`来回答这个问题。此外，我们需要使用`filter()`从聚合中排除组。排除的组是不在我们想要的时间范围内的时间:

```
>>> shares_traded_in_first_30_min = stock_data_per_minute\
...     .between_time('9:30', '10:00')\
...     .groupby(pd.Grouper(freq='1D'))\
...     .filter(lambda x: (x.volume > 0).all())\
...     .volume.mean()
>>> shares_traded_in_last_30_min = stock_data_per_minute\
...     .between_time('15:30', '16:00')\
...     .groupby(pd.Grouper(freq='1D'))\
...     .filter(lambda x: (x.volume > 0).all())\
...     .volume.mean()
```

在这一周，开盘时间比收盘时间平均多 18，593 笔交易:

```
>>> shares_traded_in_first_30_min \
... - shares_traded_in_last_30_min
18592.967741935485
```

小费

我们可以对`DatetimeIndex`对象使用`normalize()`方法，或者在第一次访问`Series`对象的`dt`属性之后，将所有日期时间规范化为午夜。当时间没有给我们的数据增加价值时，这是很有帮助的。笔记本里有这样的例子。

有了股票数据，我们就有了每一分钟或每一天的价格快照(取决于粒度)，但是我们可能有兴趣将时间段之间的变化看作一个时间序列，而不是聚合数据。为此，我们需要学习如何创建滞后数据。

## 滞后数据的移位

我们可以使用`shift()`方法来创建滞后数据。默认情况下，移位一个周期，但这可以是任何整数(正数或负数)。让我们使用`shift()`来创建一个新的列，为每日脸书股票数据显示前一天的收盘价。从这个新列中，我们可以计算由于盘后交易(在一天闭市后到第二天开市)引起的价格变化:

```
>>> fb.assign(
...     prior_close=lambda x: x.close.shift(),
...     after_hours_change_in_price=lambda x: \
...         x.open - x.prior_close,
...     abs_change=lambda x: \
...         x.after_hours_change_in_price.abs()
... ).nlargest(5, 'abs_change')
```

这给了我们受盘后交易影响最大的日子:

![Figure 4.54 – Using lagged data to calculate after-hours changes in stock price
](image/Figure_4.54_B16834.jpg)

图 4.54-使用滞后数据计算股价的盘后变化

小费

要从索引中的 datetimes 加上/减去时间，可以考虑使用`Timedelta`对象。笔记本里有这样的例子。

在前面的例子中，我们使用移位的数据来计算跨列的变化。然而，如果我们对脸书股票价格每天的变化感兴趣，而不是盘后交易，我们会计算收盘价和移动后的收盘价之间的差异。熊猫让事情变得简单一些，我们接下来会看到。

## 差异数据

我们已经讨论过用`shift()`方法创建滞后数据。然而，通常我们感兴趣的是值如何从一个时间段变化到下一个时间段。为此，`pandas`有了`diff()`的方法。默认情况下，这将计算从时间段 *t-1* 到时间段 *t* 的变化:

![](image/Formula_04_001.jpg)

注意，这相当于从原始数据中减去`shift()`的结果:

```
>>> (fb.drop(columns='trading_volume') 
...  - fb.drop(columns='trading_volume').shift()
... ).equals(fb.drop(columns='trading_volume').diff())
True
```

我们可以使用`diff()`轻松地计算脸书股票数据的每日变化:

```
>>> fb.drop(columns='trading_volume').diff().head()
```

在今年的前几个交易日，我们可以看到股价上涨，交易量每天都在下降:

![Figure 4.55 – Calculating day-over-day changes
](image/Figure_4.55_B16834.jpg)

图 4.55–计算每日变化

小费

要指定用于差值的周期数，只需向`diff()`传递一个整数。请注意，这个数字可以是负数。笔记本里就有这样的例子。

## 重采样

有时，数据的粒度不利于我们的分析。考虑这样的情况，我们有 2018 年全年每分钟的数据。数据的粒度级别和性质可能会导致绘图无用。因此，我们需要将数据聚合到更低的频率:

![Figure 4.56 – Resampling can be used to roll up granular data
](image/Figure_4.56_B16834.jpg)

图 4.56–重采样可用于汇总粒度数据

假设我们有*图 4.50* 中的全年数据(脸书股票分钟数据)。这个粒度级别可能超出了我们的有用范围，在这种情况下，我们可以使用`resample()`方法将时间序列数据聚合到不同的粒度。要使用`resample()`，我们所要做的就是说出我们想要如何汇总数据，并添加一个对聚合方法的可选调用。例如，我们可以将每分钟的数据重新采样为每日频率，并指定如何合计每列:

```
>>> stock_data_per_minute.resample('1D').agg({
...     'open': 'first', 
...     'high': 'max', 
...     'low': 'min', 
...     'close': 'last', 
...     'volume': 'sum'
... })
```

这相当于我们在*基于时间的选择和过滤*部分得到的结果(*图 4.51* ):

![Figure 4.57 – Per-minute data resampled into daily data
](image/Figure_4.57_B16834.jpg)

图 4.57-每分钟数据重新采样为每日数据

我们可以对`pandas`支持的任何频率进行重采样(更多信息可以在[http://pandas . pydata . org/pandas-docs/stable/user _ guide/time series . html](http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)的文档中找到)。让我们将每日脸书股票数据重新采样为季度平均值:

```
>>> fb.resample('Q').mean()
```

这给了我们股票的平均季度表现。2018 年第四季度显然很麻烦:

![Figure 4.58 – Resampling to quarterly averages
](image/Figure_4.58_B16834.jpg)

图 4.58–重采样至季度平均值

为了进一步了解这一点，我们可以使用`apply()`方法来查看季度开始和结束时的差异。我们还需要来自*基于时间的选择和过滤*部分的`first()`和`last()`方法:

```
>>> fb.drop(columns='trading_volume').resample('Q').apply(
...     lambda x: x.last('1D').values - x.first('1D').values
... )
```

除了第二季度，脸书的股价都在下跌:

![Figure 4.59 – Summarizing Facebook stock's performance per quarter in 2018
](image/Figure_4.59_B16834.jpg)

图 4.59-总结脸书证券 2018 年每个季度的表现

考虑`melted_stock_data.csv`中融化的每分钟股票数据:

```
>>> melted_stock_data = pd.read_csv(
...     'data/melted_stock_data.csv', 
...     index_col='date', parse_dates=True
... )
>>> melted_stock_data.head()
```

OHLC 格式使得分析股票数据变得容易，但是一个单独的列就比较棘手了:

![Figure 4.60 – Stock prices by the minute
](image/Figure_4.60_B16834.jpg)

图 4.60-每分钟的股票价格

我们在调用`resample()`后得到的`Resampler`对象有一个`ohlc()`方法，我们可以用它来检索我们习惯看到的 OHLC 数据:

```
>>> melted_stock_data.resample('1D').ohlc()['price']
```

由于原始数据中的列名为`price`，我们在调用`ohlc()`之后选择它，这是我们的数据透视。否则，我们将在列中有一个分层索引:

![Figure 4.61 – Resampling the stock prices per minute to form daily OHLC data
](image/Figure_4.61_B16834.jpg)

图 4.61-对每分钟的股票价格进行重新采样，形成每日 OHLC 数据

在前面的例子中，我们**降采样**来降低数据的粒度；然而，我们也可以**上采样**来增加数据的粒度。我们甚至可以在之后调用`asfreq()`来不聚合结果:

```
>>> fb.resample('6H').asfreq().head()
```

请注意，当我们以比现有数据更精细的粒度在重新采样时，将会引入`NaN`值:

![Figure 4.62 – Upsampling increases the granularity of the data and will introduce null values
](image/Figure_4.62_B16834.jpg)

图 4.62-上采样增加了数据的粒度，并将引入空值

以下是我们处理`NaN`值的几种方法。为了简洁起见，笔记本中列出了这些示例:

*   使用`resample()`后的`pad()`向前填充。
*   在`resample()`之后调用`fillna()`，就像我们在 [*第三章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) 、*与熊猫*的数据角力，处理缺失值的时候看到的。
*   使用`asfreq()`后接`assign()`分别处理每一列。

到目前为止，我们一直在处理存储在单个`DataFrame`对象中的时间序列数据，但是我们可能想要组合时间序列。虽然在*合并数据帧*一节中讨论的技术将适用于时间序列，但是`pandas`为合并时间序列提供了额外的功能，这样我们可以在接近匹配时进行合并，而不需要精确匹配。我们接下来将讨论这些。

## 合并时间序列

时间序列通常会到秒，甚至更细粒度，这意味着如果条目没有相同的日期时间，就很难合并。Pandas 用两个额外的合并函数解决了这个问题。当我们想要将时间上接近的观察值配对时，我们可以使用`pd.merge_asof()`来匹配附近的键，而不是相等的键，就像我们对连接所做的那样。另一方面，如果我们想要匹配相等的键，并交错不匹配的键，我们可以使用`pd.merge_ordered()`。

为了说明这些是如何工作的，我们将使用`stocks.db` SQLite 数据库中的`fb_prices`和`aapl_prices`表。它们分别包含脸书和苹果股票的价格，以及记录价格的时间戳。注意，苹果的数据是在 2020 年 8 月股票分割之前收集的([https://www . market watch . com/story/3-things-to-know-about-Apple-stock-split-2020-08-28](https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28))。让我们从数据库中读取这些表:

```
>>> import sqlite3
>>> with sqlite3.connect('data/stocks.db') as connection:
...     fb_prices = pd.read_sql(
...         'SELECT * FROM fb_prices', connection, 
...         index_col='date', parse_dates=['date']
...     )
...     aapl_prices = pd.read_sql(
...         'SELECT * FROM aapl_prices', connection, 
...         index_col='date', parse_dates=['date']
...     )
```

脸书数据处于分钟粒度；然而，对于苹果数据，我们有(虚构的)秒:

```
>>> fb_prices.index.second.unique()
Int64Index([0], dtype='int64', name='date')
>>> aapl_prices.index.second.unique()
Int64Index([ 0, 52, ..., 37, 28], dtype='int64', name='date')
```

如果我们使用`merge()`或`join()`，当苹果价格在一分钟的顶部时，我们将只有苹果和脸书的值。相反，为了尝试排列这些，我们可以从合并开始执行*。为了处理不匹配，我们将指定与最近的分钟(`direction='nearest'`)合并，并要求匹配只能发生在彼此相差 30 秒以内的时间之间(`tolerance`)。这将把苹果数据与最接近的分钟放在一起，因此`9:31:52`将使用`9:32`而`9:37:07`将使用`9:37`。因为时间在索引上，所以我们传递`left_index`和`right_index`，就像我们传递`merge()`一样:*

```
>>> pd.merge_asof(
...     fb_prices, aapl_prices, 
...     left_index=True, right_index=True,
...     # merge with nearest minute
...     direction='nearest',
...     tolerance=pd.Timedelta(30, unit='s')
... ).head()
```

这类似于左连接；但是，在匹配键时，我们会更加宽松。请注意，在 Apple 数据中的多个条目匹配同一分钟的情况下，此功能将只保留最接近的一个条目。我们得到一个空值`9:31`,因为苹果在`9:31`的条目是`9:31:52`,当使用`nearest`时，它被放置在`9:32`:

![Figure 4.63 – Merging time series data with a 30-second tolerance
](image/Figure_4.63_B16834.jpg)

图 4.63–以 30 秒的公差合并时间序列数据

如果我们不想要左连接的行为，我们可以使用`pd.merge_ordered()`函数来代替。这将允许我们指定我们的连接类型，默认情况下是`'outer'`。但是，我们必须重置我们的索引，以便能够在日期时间加入:

```
>>> pd.merge_ordered(
...     fb_prices.reset_index(), aapl_prices.reset_index()
... ).set_index('date').head()
```

每当时间不完全匹配时，这种策略都会给出空值，但它至少会为我们排序:

![Figure 4.64 – Performing a strict merge on time series data and ordering it
](image/Figure_4.64_B16834.jpg)

图 4.64–对时间序列数据执行严格合并并排序

小费

我们可以将`fill_method='ffill'`传递给`pd.merge_ordered()`来前向填充一个值后的第一个`NaN`，但是它不会传播到其他值之后；或者，我们可以将一个调用链接到`fillna()`。笔记本里有这样的例子。

`pd.merge_ordered()`函数还可以执行分组合并，所以请务必查看文档以获取更多信息。

# 总结

在本章中，我们讨论了如何连接数据帧，如何使用集合运算确定每种连接类型将丢失的数据，以及如何像查询数据库一样查询数据帧。然后，我们讨论了专栏中一些更复杂的转换，比如宁滨和排名，以及如何使用`apply()`方法有效地完成这些转换。我们还学习了向量化操作在编写高效的`pandas`代码中的重要性。然后，我们探索了窗口计算和使用管道来获得更干净的代码。我们对窗口计算的讨论为跨整个数据框架和按组聚合提供了基础。我们还学习了如何生成数据透视表和交叉表。最后，我们在`pandas`中查看了一些特定于时间序列的功能，从选择和聚集到合并。

在下一章中，我们将介绍可视化，它是由`pandas`通过在`matplotlib`周围提供一个包装器来实现的。数据争论将在为我们的数据可视化做准备的过程中发挥关键作用，因此在继续之前，请务必完成下一节中提供的练习。

# 练习

使用`exercises/`文件夹中的 CSV 文件和我们在本书中学到的知识，完成以下练习:

1.  使用`earthquakes.csv`文件，使用`mb`震级类型选择日本所有 4.9 级以上的地震。
2.  使用`ml`震级类型为每个完整的地震震级创建条柱(例如，第一个条柱为(0，1)，第二个条柱为(1，2)，依此类推)，并计算每个条柱中有多少条条柱。
3.  Using the `faang.csv` file, group by the ticker and resample to monthly frequency. Make the following aggregations:

    a)开盘价的平均值

    b)最高价格

    c)最低价格

    d)收盘价的平均值

    e)交易量的总和

4.  用地震数据在`tsunami`列和`magType`列之间构建一个交叉表。显示每个组合观察到的最大幅度，而不是显示频率计数。将量级类型放在列上。
5.  为 FAANG 数据按 ticker 计算 OHLC 数据的 60 天滚动聚合。使用与练习 *3* 相同的聚合。
6.  创建一个比较股票的 FAANG 数据的数据透视表。将报价器放在横线上，显示 OHLC 和交易量数据的平均值。
7.  使用`apply()`计算 2018 年第四季度亚马逊数据(`ticker`为 AMZN)每个数值列的 Z 值。
8.  Add event descriptions:

    a)创建一个包含以下三列的数据帧:`ticker`、`date`和`event`。这些列应该具有以下值:

    i) `ticker` : `'FB'`

    ii) `date` : `['2018-07-25', '2018-03-19', '2018-03-20']`

    三)`event` : `['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']`

    b)将索引设置为`['date', 'ticker']`。

    c)使用外部连接将该数据与 FAANG 数据合并。

9.  对 FAANG 数据使用`transform()`方法来表示数据中第一个日期的所有值。为此，将每个股票代码的所有值除以该股票代码数据中第一个日期的值。这被称为一个**指数**，第一次约会的数据是**基数**([https://EC . Europa . eu/Eurostat/statistics-explained/Index . PHP/初学者:Statistical _ concept _-_ Index _ and _ base _ year](https://ec.europa.eu/eurostat/statistics-explained/index.php/Beginners:Statistical_concept_-_Index_and_base_year))。当数据采用这种格式时，我们可以很容易地看到数据随时间的增长。提示:`transform()`可以带函数名。
10.  The **European Centre for Disease Prevention and Control** (**ECDC**) provides an open dataset on COVID-19 cases called *daily number of new reported cases of COVID-19 by country worldwide* ([https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide)). This dataset is updated daily, but we will use a snapshot that contains data through September 18, 2020\. Complete the following tasks to practice the skills you've learned up to this point in the book:

    a)准备数据:

    I)读入`covid19_cases.csv`文件中的数据。

    ii)通过将`dateRep`列解析为日期时间来创建`date`列。

    iii)将`date`列设置为索引。

    iv)使用`replace()`方法将所有出现的`United_States_of_America`和`United_Kingdom`分别更新为`USA`和`UK`。

    v)对索引进行排序。

    b)对于病例最多(累计)的五个国家，找出病例数最多的那一天。

    c)在病例最多的五个国家的数据中找出上周新冠肺炎病例的 7 天平均变化。

    d)找出除中国以外的每个国家首次出现病例的日期。

    e)使用百分位数按累积案例对国家进行排名。

# 延伸阅读

请查阅以下资源，了解本章所涵盖主题的更多信息:

*   *SQL 简介:查询和管理数据*:[https://www . khanacademy . org/computing/computer-programming/SQL](https://www.khanacademy.org/computing/computer-programming/sql)
*   *(熊猫)与 SQL 的比较*:[https://Pandas . pydata . org/Pandas-docs/stable/getting _ started/Comparison/Comparison _ with _ SQL . html](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)
*   *Set Operations*:[https://www . probabilitycourse . com/chapter 1/1 _ 2 _ 2 _ Set _ Operations . PHP](https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php)
*   ** Python 中的 args 和**kwargs 解释*:[https://Python tips . com/2013/08/04/args-and-kwargs-in-Python-explained/](https://yasoob.me/2013/08/04/args-and-kwargs-in-python-explained/)