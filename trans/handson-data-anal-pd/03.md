

# 二、处理Pandas数据帧

我们开始宇宙之旅的时候到了。这一章将让我们在使用`pandas`进行数据分析时，能够轻松地使用一些基本但强大的操作。

我们将从介绍在使用`pandas`时会遇到的主要**数据结构**开始。数据结构为我们提供了一种组织、管理和存储数据的格式。当遇到故障排除或查找如何对数据执行操作时，数据结构的知识将被证明是非常有用的。请记住，这些数据结构不同于标准 Python 数据结构是有原因的:它们是为特定的分析任务而创建的。我们必须记住，一个给定的方法可能只适用于某个特定的数据结构，所以我们需要能够识别出我们要解决的问题的最佳结构。

接下来，我们将把第一个数据集引入 Python。我们将学习如何从 API 收集数据，从 Python 中的其他数据结构创建`DataFrame`对象，读取文件，以及与数据库交互。最初，您可能想知道为什么我们需要从其他 Python 数据结构创建一个`DataFrame`对象；然而，如果我们想要快速测试一些东西，创建我们自己的数据，从 API 中提取数据，或者从另一个项目中重新利用 Python 代码，那么我们会发现这些知识是不可或缺的。最后，我们将掌握检查、描述、过滤和总结数据的方法。

本章将涵盖以下主题:

*   Pandas数据结构
*   从文件、API 请求、SQL 查询和其他 Python 对象创建 DataFrame 对象
*   检查数据帧对象并计算汇总统计信息
*   通过选择、切片、索引和过滤获取数据子集
*   添加和删除数据

# 章节材料

我们将在本章中使用的文件可以在 GitHub 资源库中找到，网址是[https://GitHub . com/stef molin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch _ 02](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02)。我们将通过使用 USGS API 和 CSV 文件来处理来自**美国地质调查局** ( **USGS** )的地震数据，这些文件可以在`data/`目录中找到。

有四个 CSV 文件和一个 SQLite 数据库文件，它们将在本章的不同地方使用。`earthquakes.csv`文件包含从 2018 年 9 月 18 日到 2018 年 10 月 13 日从 USGS API 提取的数据。对于我们对数据结构的讨论，我们将使用`example_data.csv`文件，它包含来自`earthquakes.csv`文件的五行和列的子集。`tsunamis.csv`文件是`earthquakes.csv`文件中在上述日期范围内伴随海啸的所有地震数据的子集。`quakes.db`文件包含一个 SQLite 数据库，其中有一个海啸数据表。我们将用它来学习如何用`pandas`读写数据库。最后，`parsed.csv`文件将用于本章末尾的练习，我们也将在本章中浏览它的创建。

本章随附的代码被分成六个 Jupyter 笔记本，按照使用顺序进行编号。它们包含了我们将在本章中运行的代码片段，以及任何命令的完整输出。每次我们要切换笔记本时，文本都会指示这样做。

在`1-pandas_data_structures.ipynb`笔记本中，我们将开始学习主要的`pandas`数据结构。之后，我们将讨论在`2-creating_dataframes.ipynb`笔记本中创建`DataFrame`对象的各种方法。我们关于这个话题的讨论将在`3-making_dataframes_from_api_requests.ipynb`笔记本中继续，在那里我们将探索 USGS API 来收集用于`pandas`的数据。在学习了如何收集数据之后，我们将开始学习如何在`4-inspecting_dataframes.ipynb`笔记本中进行**探索性数据分析** ( **EDA** )。然后，在`5-subsetting_data.ipynb` 笔记本中，我们将讨论选择和过滤数据的各种方法。最后，我们将学习如何在`6-adding_and_removing_data.ipynb`笔记本中添加和删除数据。让我们开始吧。

# Pandas数据结构

Python 已经有了几种数据结构，比如元组、列表和字典。Pandas 提供了两个主要结构来帮助处理数据:`Series`和`DataFrame`。`Series`和`DataFrame`数据结构各自包含另一个`pandas`数据结构`Index`，我们也必须注意到这一点。然而，为了理解这些数据结构，我们需要首先看一下 NumPy([https://numpy.org/doc/stable/](https://numpy.org/doc/stable/))，它提供了构建`pandas`的 n 维数组。

上述数据结构被实现为 Python **类**；当我们实际创建一个时，它们被称为**对象**或**实例**。这是一个重要的区别，因为，正如我们将会看到的，一些动作可以使用对象本身来执行(一个**方法**)，而另一些动作则需要我们将对象作为参数传递给某个**函数**。注意，在 Python 中，类名传统上是用`CapWords`写的，而对象是用`snake_case`写的。(更多 Python 风格指南可以在[https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/)找到。)

我们使用一个`pandas`函数将一个 CSV 文件读入到一个`DataFrame`类的对象中，但是我们使用我们的`DataFrame`对象上的方法来对它们执行操作，比如删除列或者计算汇总统计数据。使用`pandas`，我们经常想要访问我们正在处理的对象的**属性**。这不会像方法或函数那样生成动作；相反，我们将得到关于我们的`pandas`对象的信息，比如维度、列名、数据类型以及它是否为空。

重要说明

在本书的剩余部分，我们将把`DataFrame`对象称为数据帧，`Series`对象称为序列，`Index`对象称为索引，除非我们指的是类本身。

对于这一部分，我们将在`1-pandas_data_structures.ipynb`笔记本中工作。首先，我们将导入`numpy`并使用它将`example_data.csv`文件的内容读入一个`numpy.array`对象。数据来自美国地质勘探局地震 API(来源:【https://earthquake.usgs.gov/fdsnws/event/1/】T4)。请注意，这是我们唯一一次使用 NumPy 来读取文件，这样做只是为了说明的目的；重要的部分是查看用 NumPy 表示数据的方式:

```
>>> import numpy as np
>>> data = np.genfromtxt(
...     'data/example_data.csv', delimiter=';', 
...     names=True, dtype=None, encoding='UTF'
... )
>>> data
array([('2018-10-13 11:10:23.560',
 '262km NW of Ozernovskiy, Russia', 
 'mww', 6.7, 'green', 1),
 ('2018-10-13 04:34:15.580', 
 '25km E of Bitung, Indonesia', 'mww', 5.2, 'green', 0),
 ('2018-10-13 00:13:46.220', '42km WNW of Sola, Vanuatu', 
 'mww', 5.7, 'green', 0),
 ('2018-10-12 21:09:49.240', 
 '13km E of Nueva Concepcion, Guatemala',
 'mww', 5.7, 'green', 0),
 ('2018-10-12 02:52:03.620', 
 '128km SE of Kimbe, Papua New Guinea',
 'mww', 5.6, 'green', 1)],
 dtype=[('time', '<U23'), ('place', '<U37'),
 ('magType', '<U3'), ('mag', '<f8'),
 ('alert', '<U5'), ('tsunami', '<i8')])
```

我们现在将数据放在一个 NumPy 数组中。使用`shape`和`dtype`属性，我们可以分别收集关于数组的维度和它包含的数据类型的信息:

```
>>> data.shape
(5,)
>>> data.dtype
dtype([('time', '<U23'), ('place', '<U37'), ('magType', '<U3'), 
       ('mag', '<f8'), ('alert', '<U5'), ('tsunami', '<i8')])
```

数组中的每个条目都是 CSV 文件中的一行。NumPy 数组包含单一数据类型(与列表不同，列表允许混合类型)；这允许快速的矢量化操作。当我们读入数据时，我们得到了一个由`numpy.void`对象组成的数组，用来存储灵活的类型。这是因为 NumPy 必须每行存储几种不同的数据类型:四个字符串、一个浮点数和一个整数。不幸的是，这意味着我们不能利用 NumPy 为单一数据类型对象提供的性能改进。

假设我们想要找到最大星等—我们可以使用一个**列表理解**([https://www.python.org/dev/peps/pep-0202/](https://www.python.org/dev/peps/pep-0202/))来选择每一行的第三个索引，它被表示为一个`numpy.void`对象。这就形成了一个列表，意味着我们可以使用`max()`函数取最大值。我们可以使用 IPython 的`%%timeit` **魔法命令**(一个特殊命令，前面有`%`)来看看这个实现需要多长时间(时间会有所不同):

```
>>> %%timeit
>>> max([row[3] for row in data])
9.74 µs ± 177 ns per loop 
(mean ± std. dev. of 7 runs, 100000 loops each)
```

注意，每当我们写一个下面只有一行的`for`循环，或者想要对一些初始列表的成员运行操作时，我们都应该使用列表理解。这是一个相当简单的列表理解，但是我们可以通过添加`if...else`语句使它们变得更复杂。列表理解是我们武器库中非常强大的工具。更多信息可以在 Python 文档中找到，网址为 https://docs . Python . org/3/tutorial/data structures . html # list-comprehensions。

小费

**IPython**(【https://ipython.readthedocs.io/en/stable/index.html】T2)为 Python 提供了一个交互式 shell。Jupyter 笔记本是基于 IPython 构建的。虽然本书并不要求具备 IPython 知识，但是熟悉它的一些功能会很有帮助。IPython 在 https://ipython.readthedocs.io/en/stable/interactive/的文档中包含了一个教程。

如果我们为每一列创建一个 NumPy 数组，这个操作会更容易(也更有效)执行。为此，我们将使用一个**字典理解**(https://www.python.org/dev/peps/pep-0274/)来创建一个字典，其中键是列名，值是数据的 NumPy 数组。同样，这里重要的部分是现在如何使用 NumPy 表示数据:

```
>>> array_dict = {
...     col: np.array([row[i] for row in data])
...     for i, col in enumerate(data.dtype.names)
... }
>>> array_dict
{'time': array(['2018-10-13 11:10:23.560',
 '2018-10-13 04:34:15.580', '2018-10-13 00:13:46.220',
 '2018-10-12 21:09:49.240', '2018-10-12 02:52:03.620'],
 dtype='<U23'),
 'place': array(['262km NW of Ozernovskiy, Russia', 
 '25km E of Bitung, Indonesia',
 '42km WNW of Sola, Vanuatu',
 '13km E of Nueva Concepcion, Guatemala',
 '128km SE of Kimbe, Papua New Guinea'], dtype='<U37'),
 'magType': array(['mww', 'mww', 'mww', 'mww', 'mww'], 
 dtype='<U3'),
 'mag': array([6.7, 5.2, 5.7, 5.7, 5.6]),
 'alert': array(['green', 'green', 'green', 'green', 'green'], 
 dtype='<U5'),
 'tsunami': array([1, 0, 0, 0, 1])}
```

获取最大值现在只需选择`mag`键并调用 NumPy 数组上的`max()`方法。当只处理五个条目时，这几乎是 list comprehension 实现的两倍——想象一下第一次尝试在大型数据集上的表现会有多糟糕:

```
>>> %%timeit
>>> array_dict['mag'].max()
5.22 µs ± 100 ns per loop 
(mean ± std. dev. of 7 runs, 100000 loops each)
```

然而，这种表示有其他问题。假设我们想要获取最大震级地震的所有信息；我们该怎么做呢？我们需要找到最大值的索引，然后对于字典中的每个键，获取该索引。结果现在是一个 NumPy 字符串数组(我们的数值被转换了)，并且我们现在采用了我们之前看到的格式:

```
>>> np.array([
...     value[array_dict['mag'].argmax()]
...     for key, value in array_dict.items()
... ])
array(['2018-10-13 11:10:23.560',
 '262km NW of Ozernovskiy, Russia',
 'mww', '6.7', 'green', '1'], dtype='<U31')
```

考虑我们将如何按照大小从最小到最大对数据进行排序。在第一个表示中，我们必须通过检查第三个索引来对行进行排序。对于第二种表示，我们必须从`mag`列中确定索引的顺序，然后对具有相同索引的所有其他数组进行排序。显然，同时处理几个包含不同数据类型的 NumPy 数组有点麻烦；然而，`pandas`构建在 NumPy 数组之上，使这变得更容易。让我们从对`Series`数据结构的概述开始对`pandas`的探索。

## 系列

`Series`类为提供了单一类型数组的数据结构，就像 NumPy 数组一样。然而，它附带了一些额外的功能。这种一维表示可以被认为是电子表格中的一列。我们为我们的列取了一个名字，并且我们在其中保存的数据是相同类型的(因为我们测量的是相同的变量):

```
>>> import pandas as pd
>>> place = pd.Series(array_dict['place'], name='place')
>>> place
0          262km NW of Ozernovskiy, Russia
1              25km E of Bitung, Indonesia
2                42km WNW of Sola, Vanuatu
3    13km E of Nueva Concepcion, Guatemala
4      128km SE of Kimbe, Papua New Guinea
Name: place, dtype: object
```

注意结果左边的数字；这些对应于原始数据集中的行号(偏移 1，因为在 Python 中，我们从 0 开始计数)。这些行号构成了索引，我们将在下一节讨论。在行号旁边，我们有该行的实际值，在本例中，它是一个指示地震发生地点的字符串。注意我们在`Series`对象的名字旁边有`dtype: object`；这是在告诉我们`place`的数据类型是`object`。在`pandas`中，一个字符串将被归类为`object`。

为了访问`Series`对象的属性，我们使用了`<object>.<attribute_name>`形式的属性符号。以下是我们将访问的一些常见属性。注意`dtype`和`shape`是可用的，就像我们在 NumPy 数组中看到的一样:

![Figure 2.1 – Commonly used series attributes
](image/Figure_2.1_B16834.jpg)

图 2.1–常用系列属性

重要说明

在大多数情况下，`pandas`对象使用 NumPy 数组作为它们的内部数据表示。但是，对于某些数据类型，`pandas`在 NumPy 的基础上创建自己的数组(https://pandas . pydata . org/pandas-docs/stable/reference/arrays . html)。因此，根据数据类型，`values`可以返回一个`pandas.array`或者一个`numpy.array`对象。因此，如果我们需要确保获得特定的类型，建议分别使用`array`属性或`to_numpy()`方法，而不是`values`。

一定要将`pandas.Series`文档([https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas)加入书签。Series.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html))以备后用。它包含了关于如何创建一个`Series`对象的更多信息，可用的属性和方法的完整列表，以及一个到源代码的链接。有了对`Series`类的高级介绍，我们就可以继续学习`Index`类了。

## 索引

添加了`Index`类使得`Series`类比 NumPy 数组强大得多。`Index`类给了我们行标签，它支持按行选择。根据类型，我们可以提供行号、日期，甚至是字符串来选择我们的行。它在识别数据条目中起着关键作用，并用于`pandas`中的多种操作，我们将在本书中看到。我们可以通过`index`属性访问该索引:

```
>>> place_index = place.index
>>> place_index
RangeIndex(start=0, stop=5, step=1)
```

注意，这是一个`RangeIndex`对象。它的值从`0`开始，到`4`结束。步骤`1`表示所有的索引都是`1`分开的，这意味着我们拥有该范围内的所有整数。默认的索引类是`RangeIndex`；然而，我们可以改变指数，我们将在第三章 、*与Pandas*的数据争论中讨论。通常，我们要么使用行号对象，要么使用日期(时间)对象。

与`Series`对象一样，我们可以通过`values`属性访问底层数据。注意这个`Index`对象是建立在 NumPy 数组之上的:

```
>>> place_index.values
array([0, 1, 2, 3, 4], dtype=int64)
```

`Index`对象的一些有用属性包括:

![Figure 2.2 – Commonly used index attributes
](image/Figure_2.2_B16834.jpg)

图 2.2–常用的索引属性

NumPy 和`pandas`都支持算术运算，这将按元素执行。NumPy 将为此使用数组中的位置:

```
>>> np.array([1, 1, 1]) + np.array([-1, 0, 1])
array([0, 1, 2])
```

使用`pandas`，对匹配的索引值执行这种基于元素的运算。如果我们添加一个索引从`0`到`4`(存储在`x`中)的`Series`对象和另一个索引从`1`到`5`的`y`对象，我们将只得到索引对齐(`1`到`4`)的结果。在 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) 、*与Pandas*的数据争论中，我们将讨论一些改变和对齐索引的方法，以便我们可以执行这些类型的操作而不丢失数据:

```
>>> numbers = np.linspace(0, 10, num=5) # [0, 2.5, 5, 7.5, 10]
>>> x = pd.Series(numbers) # index is [0, 1, 2, 3, 4]
>>> y = pd.Series(numbers, index=pd.Index([1, 2, 3, 4, 5]))
>>> x + y
0     NaN
1     2.5
2     7.5
3    12.5
4    17.5
5     NaN
dtype: float64
```

现在我们已经对`Series`和`Index`类有了初步了解，我们准备学习`DataFrame`类。注意，关于`Index`类的更多信息可以在[https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas 的相应文档中找到。Index.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html)。

## 数据帧

使用的`Series`类，我们本质上拥有一个电子表格的列，数据都是同一类型的。`DataFrame`类建立在`Series`类的基础上，可以有许多列，每一列都有自己的数据类型；我们可以认为它代表了整个电子表格。我们可以将从示例数据构建的 NumPy 表示转换成一个`DataFrame`对象:

```
>>> df = pd.DataFrame(array_dict) 
>>> df
```

这给了我们六个系列的数据框架。注意`time`列之前的列；这是行的`Index`对象。当创建一个`DataFrame`对象时，`pandas`将所有系列对齐到同一个索引。在这种情况下，它只是行号，但我们可以很容易地为此使用`time`列，这将启用一些附加的`pandas`功能，正如我们将在 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*聚合Pandas数据帧*中看到的:

![Figure 2.3 – Our first dataframe
](image/Figure_2.3_B16834.jpg)

图 2.3–我们的第一个数据框架

我们的每一列都有单一的数据类型，但它们并不都共享相同的数据类型:

```
>>> df.dtypes
time        object
place       object
magType     object
mag        float64
alert       object
tsunami      int64
dtype: object
```

dataframe 的值看起来非常类似于我们最初的 NumPy 表示:

```
>>> df.values
array([['2018-10-13 11:10:23.560',
 '262km NW of Ozernovskiy, Russia',
 'mww', 6.7, 'green', 1],
 ['2018-10-13 04:34:15.580', 
 '25km E of Bitung, Indonesia', 'mww', 5.2, 'green', 0],
 ['2018-10-13 00:13:46.220', '42km WNW of Sola, Vanuatu', 
 'mww', 5.7, 'green', 0],
 ['2018-10-12 21:09:49.240',
 '13km E of Nueva Concepcion, Guatemala',
 'mww', 5.7, 'green', 0],
 ['2018-10-12 02:52:03.620','128 km SE of Kimbe, 
 Papua New Guinea', 'mww', 5.6, 'green', 1]], 
 dtype=object)
```

我们可以通过`columns`属性访问列名。注意，它们实际上也存储在一个`Index`对象中:

```
>>> df.columns
Index(['time', 'place', 'magType', 'mag', 'alert', 'tsunami'], 
 dtype='object')
```

以下是一些常用的数据帧属性:

![Figure 2.4 – Commonly used dataframe attributes
](image/Figure_2.4_B16834.jpg)

图 2.4–常用的数据帧属性

注意我们也可以在数据帧上执行算术运算。例如，我们可以将`df`添加到它自身，这将对数字列求和并连接字符串列:

```
>>> df + df
```

只有当索引和列匹配时，Pandas 才会执行该操作。这里，`pandas`跨数据帧连接字符串列(`time`、`place`、`magType`和`alert`)。数字列(`mag`和`tsunami`)相加:

![Figure 2.5 – Adding dataframes
](image/Figure_2.5_B16834.jpg)

图 2.5–添加数据帧

关于`DataFrame`对象的更多信息以及可以直接对其执行的所有操作可以在[的官方文档中找到，网址为 https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas。DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)；一定要把它收藏起来，以备将来参考。现在，我们准备开始学习如何从各种来源创建`DataFrame`对象。

# 创建Pandas数据框架

既然我们理解了将要使用的数据结构，我们可以讨论创建它们的不同方式。然而，在我们深入研究代码之前，了解如何从 Python 获得帮助是很重要的。如果我们发现自己不确定如何使用 Python，我们可以利用内置的`help()`函数。我们只需运行`help()`，传入我们想要阅读文档的包、模块、类、对象、方法或函数。当然，我们可以在网上查找文档；然而，在大多数情况下，与`help()`一起返回的**文档字符串**(代码中编写的文档文本)将与此等价，因为它们用于生成文档。

假设我们首先运行了`import pandas as pd`，我们可以运行`help(pd)`来显示关于`pandas`包的信息；`help(pd.DataFrame)`对于`DataFrame`对象的所有方法和属性(注意我们也可以传入一个`DataFrame`对象代替)；和`help(pd.read_csv)`来了解更多关于将 CSV 文件读入 Python 的`pandas`函数以及如何使用它。我们还可以尝试使用`dir()`函数和`__dict__`属性，这将分别给出可用内容的列表或字典；不过，这些函数可能没有`help()`函数有用。

此外，由于 IPython，我们可以使用`?`和`??`来获得帮助，这是 Jupyter 笔记本如此强大的一部分原因。与`help()`函数不同，我们可以使用问号，把它们放在我们想了解更多的任何东西之后，就好像我们在问 Python 一个问题；比如`pd.read_csv?`和`pd.read_csv??`。这三个将产生稍微不同的输出:`help()`将给出 docstring`?`将给出 docstring，加上一些附加信息，这取决于我们要查询的内容；并且`??`会给我们更多的信息，如果可能的话，还有它背后的源代码。

现在让我们转到下一个笔记本，`2-creating_dataframes.ipynb`，导入我们在接下来的例子中需要的包。我们将使用 Python 标准库中的`datetime`，以及第三方包`numpy`和`pandas`:

```
>>> import datetime as dt
>>> import numpy as np
>>> import pandas as pd
```

重要说明

我们有**别名**我们的每一个进口。这允许我们通过使用别名`pd`来引用`pandas`包，这是最常见的导入方式。事实上，我们只能将其称为`pd`，因为这是我们导入到名称空间中的内容。需要先导入包，然后才能使用它们；安装会将我们需要的文件放在我们的计算机上，但是，为了节省内存，Python 不会在我们启动它时加载每个已安装的包——只加载我们告诉它加载的包。

我们现在准备开始使用`pandas`。首先，我们将学习如何从其他 Python 对象创建`pandas`对象。然后，我们将学习如何使用平面文件、数据库中的表以及来自 API 请求的响应来做到这一点。

## 来自 Python 对象

在我们介绍从 Python 对象创建`DataFrame`对象的所有方法之前，我们应该学习如何创建`Series`对象。记住，`Series`对象本质上是`DataFrame`对象中的一列，所以，一旦我们知道了这一点，就应该很容易理解如何创建`DataFrame`对象。假设我们想要在`0`和`1`之间创建一系列五个随机数。我们可以使用 NumPy 将随机数生成为一个数组，并从中创建序列。

小费

NumPy 使得生成数字数据变得非常容易。除了生成随机数之外，我们还可以用它通过`np.linspace()`函数得到一定范围内的均匀分布的数；用`np.arange()`函数获取整数范围；用`np.random.normal()`功能从标准法线取样；使用`np.zeros()`函数轻松创建全 0 数组，使用`np.ones()`函数创建全 1 数组。我们将在本书中使用 NumPy。

为了确保结果是可重复的，我们将在这里设置种子。**种子**给出了伪随机数生成的起点。没有随机数生成算法是真正随机的——它们是确定性的，因此通过设置这个起点，每次运行代码时生成的数字将是相同的。这对于测试来说是好的，但是对于模拟来说(我们需要随机性)，我们将在第 8 章 、*基于规则的异常检测*中查看 [*。以这种方式，我们可以用任何类似列表的结构(比如 NumPy 数组)创建一个`Series`对象:*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172)

```
>>> np.random.seed(0) # set a seed for reproducibility
>>> pd.Series(np.random.rand(5), name='random')
0    0.548814
1    0.715189
2    0.602763
3    0.544883
4    0.423655
Name: random, dtype: float64
```

制作`DataFrame`对象是制作`Series`对象的扩展；它将由一个或多个系列组成，每个系列都有明确的名称。这应该提醒我们 Python 中类似于字典的结构:键是列名，值是列的内容。注意，如果我们想把一个单独的`Series`对象变成一个`DataFrame`对象，我们可以使用它的`to_frame()`方法。

小费

在计算机科学中，**构造器**是一段代码，它初始化一个类的新实例，为它们的使用做准备。Python 类用`__init__()`方法实现了这一点。当我们运行`pd.Series()`时，Python 调用`pd.Series.__init__()`，其中包含实例化一个新的`Series`对象的指令。我们将在 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146) 、*金融分析——比特币和股票市场*中了解更多关于`__init__()`的方法。

因为列可以是不同的数据类型，所以让我们来看一下这个例子。我们将创建一个包含三列的`DataFrame`对象，每列有五个观察值:

*   `random`:在`0`和`1`之间的五个随机数作为 NumPy 数组
*   `text`:五个字符串的列表或`None`
*   五个随机布尔的列表

我们还将使用`pd.date_range()`函数创建一个`DatetimeIndex`对象。该指数将包含五个日期(`periods=5`)，都相隔一天(`freq='1D'`)，以 2019 年 4 月 21 日(`end`)结束，并将被称为`date`。请注意，关于`pd.date_range()`函数接受的频率值的更多信息可以在[https://pandas . pydata . org/pandas-docs/stable/user _ guide/time series . html # offset-aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)中找到。

我们所要做的就是使用所需的列名作为键将列打包到一个字典中，并在调用`pd.DataFrame()`构造函数时将它传入。索引作为`index`参数传递:

```
>>> np.random.seed(0) # set seed so result is reproducible
>>> pd.DataFrame(
...     {
...         'random': np.random.rand(5),
...         'text': ['hot', 'warm', 'cool', 'cold', None],
...         'truth': [np.random.choice([True, False]) 
...                   for _ in range(5)]
...     }, 
...     index=pd.date_range(
...         end=dt.date(2019, 4, 21),
...         freq='1D', periods=5, name='date'
...     )
... )
```

重要说明

按照惯例，我们使用`_`在一个循环中保存我们不关心的变量。这里我们用`range()`作为计数器，它的值并不重要。关于`_`在 Python 中扮演的角色的更多信息可以在[https://hacker noon . com/understanding-the-underscript-of-Python-309 D1 a 029 EDC](https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc)找到。

索引中有个日期，可以很容易地按日期选择条目(甚至在一个日期范围内)，我们将在 [*第三章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) ，*与Pandas的数据争论*中看到:

![Figure 2.6 – Creating a dataframe from a dictionary
](image/Figure_2.6_B16834.jpg)

图 2.6–从字典创建数据帧

在数据不是字典，而是字典列表的情况下，我们仍然可以使用`pd.DataFrame()`。这种格式的数据是我们从 API 消费时所期望的。列表中的每个条目都将是一个字典，其中字典的键是列名，字典的值是该索引处该列的值:

```
>>> pd.DataFrame([
...     {'mag': 5.2, 'place': 'California'},
...     {'mag': 1.2, 'place': 'Alaska'},
...     {'mag': 0.2, 'place': 'California'},
... ])
```

这为我们提供了一个包含三行(列表中的每个条目一行)和两列(字典中的每个键一列)的数据框架:

![Figure 2.7 – Creating a dataframe from a list of dictionaries
](image/Figure_2.7_B16834.jpg)

图 2.7–从字典列表中创建数据帧

事实上，`pd.DataFrame()`也适用于元组列表。注意，我们也可以通过`columns`参数将列的名称作为一个列表传入:

```
>>> list_of_tuples = [(n, n**2, n**3) for n in range(5)]
>>> list_of_tuples
[(0, 0, 0), (1, 1, 1), (2, 4, 8), (3, 9, 27), (4, 16, 64)]
>>> pd.DataFrame(
...     list_of_tuples,
...     columns=['n', 'n_squared', 'n_cubed']
... )
```

每个元组都被视为一条记录，并成为数据帧中的一行:

![Figure 2.8 – Creating a dataframe from a list of tuples
](image/Figure_2.8_B16834.jpg)

图 2.8–从元组列表创建数据帧

我们还可以选择对 NumPy 数组使用`pd.DataFrame()`:

```
>>> pd.DataFrame(
...     np.array([
...         [0, 0, 0],
...         [1, 1, 1],
...         [2, 4, 8],
...         [3, 9, 27],
...         [4, 16, 64]
...     ]), columns=['n', 'n_squared', 'n_cubed']
... )
```

这个将具有将数组中的每个条目作为行堆叠在数据帧中的效果，给我们一个与*图 2.8* 相同的结果。

## 从文件中

我们想要分析的数据通常来自 Python 之外。在许多情况下，我们可能会从数据库或网站获得数据转储(T2 ),并将其导入 Python 进行筛选。数据转储因包含大量数据(可能是非常细粒度的数据)而得名，并且通常最初不歧视任何数据；由于这个原因，它们可能很笨重。

通常，这些数据转储将以文本文件(`.txt`)或 CSV 文件(`.csv`)的形式出现。Pandas 提供了许多读取不同类型文件的方法，所以只需查找与我们的文件格式相匹配的方法。我们的地震数据是一个 CSV 文件；因此，我们使用`pd.read_csv()`函数来读取它。但是，在试图读入文件之前，我们应该对文件进行初步检查；这将通知我们是否需要传递额外的参数，例如`sep`来指定分隔符，或者`names`在文件中缺少标题行的情况下自己提供列名。

重要说明

**Windows 用户**:根据您的设置，接下来几个代码块中的命令可能不起作用。如果你遇到问题，笔记本里有备选方案。

多亏了 IPython，我们可以直接在 Jupyter 笔记本中执行我们的尽职调查，只要我们给命令加上前缀`!`来表示它们将作为 shell 命令运行。首先，我们应该检查文件有多大，包括行数和字节数。为了检查行数，我们使用带有`–l`标志的`wc`实用程序(字数统计)来统计行数。文件中有 9，333 行:

```
>>> !wc -l data/earthquakes.csv
9333 data/earthquakes.csv
```

现在，让我们检查文件的大小。对于这个任务，我们将在`data`目录中使用`ls`。这将向我们显示该目录中的文件列表。我们可以添加`-lh`标志，以人类可读的格式获取有关文件的信息。最后，我们将这个输出发送给`grep`实用程序，它将帮助我们分离出我们想要的文件。这告诉我们这个`earthquakes.csv`文件是 3.4 MB:

```
>>> !ls -lh data | grep earthquakes.csv
-rw-r--r-- 1 stefanie stefanie 3.4M ... earthquakes.csv
```

请注意，IPython 还允许我们在 Python 变量中捕获命令的结果，因此如果我们不习惯使用管道(`|`或`grep`)，我们可以执行以下操作:

```
>>> files = !ls -lh data
>>> [file for file in files if 'earthquake' in file]
['-rw-r--r-- 1 stefanie stefanie 3.4M ... earthquakes.csv']
```

现在，让我们看看最上面的几行，看看这个文件是否带有头文件。我们将使用`head`实用程序并用`-n`标志指定行数。这告诉我们，第一行包含数据的标题，数据用逗号分隔(仅仅因为文件有`.csv`扩展名并不意味着它是逗号分隔的):

```
>>> !head -n 2 data/earthquakes.csv
alert,cdi,code,detail,dmin,felt,gap,ids,mag,magType,mmi,net,nst,place,rms,sig,sources,status,time,title,tsunami,type,types,tz,updated,url
,,37389218,https://earthquake.usgs.gov/[...],0.008693,,85.0,",ci37389218,",1.35,ml,,ci,26.0,"9km NE of Aguanga, CA",0.19,28,",ci,",automatic,1539475168010,"M 1.4 - 9km NE of Aguanga, CA",0,earthquake,",geoserve,nearby-cities,origin,phase-data,",-480.0,1539475395144,https://earthquake.usgs.gov/earthquakes/eventpage/ci37389218
```

注意，我们还应该检查底部的行，以确保没有我们需要通过使用`tail`实用程序忽略的无关数据。这个文件没问题，这里就不赘述结果了；然而，笔记本包含了结果。

最后，我们可能对查看数据中的列数感兴趣。虽然我们可以只对`head`结果的第一行中的字段进行计数，但是我们可以选择使用`awk`实用程序(用于模式扫描和处理)来对我们的列进行计数。`-F`标志允许我们指定分隔符(在本例中是逗号)。然后，我们指定对文件中的每条记录做什么。我们选择打印`NF`，它是一个预定义的变量，其值是当前记录中的字段数。在这里，我们在打印后立即说`exit`,这样我们在文件的第一行打印字段的数量；然后，我们停下来。这看起来有点复杂，但这绝不是我们需要记住的东西:

```
>>> !awk -F',' '{print NF; exit}' data/earthquakes.csv
26
```

因为我们知道文件的第一行包含标题，并且文件是用逗号分隔的，所以我们也可以通过使用`head`获取标题并使用 Python 解析它们来计算列数:

```
>>> headers = !head -n 1 data/earthquakes.csv
>>> len(headers[0].split(','))
26
```

重要说明

从我们的 Jupyter 笔记本上直接运行 shell 命令的能力极大地简化了我们的工作流程。但是，如果我们过去没有命令行的经验，最初学习这些命令可能会很复杂。IPython 在他们的文档中有一些关于运行 shell 命令的有用信息，这些文档位于[https://IPython . readthe docs . io/en/stable/interactive/reference . html # system-shell-access](https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access)。

总而言之，我们现在知道该文件大小为 3.4 MB，由 26 列和 9，333 行逗号分隔，第一行是标题。这意味着我们可以使用默认的`pd.read_csv()`函数:

```
>>> df = pd.read_csv('earthquakes.csv')
```

注意，我们并不局限于从本地机器上的文件中读取数据；文件路径也可以是 URL。举个例子，让我们从 GitHub 读入同一个 CSV 文件:

```
>>> df = pd.read_csv(
...     'https://github.com/stefmolin/'
...     'Hands-On-Data-Analysis-with-Pandas-2nd-edition'
...     '/blob/master/ch_02/data/earthquakes.csv?raw=True'
... )
```

pandas通常非常擅长根据输入数据计算出使用哪个选项，所以我们通常不需要在这个调用中添加参数；但是，如果我们需要，有许多选项可供选择，其中一些选项包括:

![Figure 2.9 – Helpful parameters when reading data from a file
](image/Figure_2.9_B16834.jpg)

图 2.9–从文件中读取数据时的有用参数

在本书中，我们将使用 CSV 文件；但是，注意我们可以使用`read_excel()`函数读入 Excel 文件，使用`read_json()`函数读入 **JSON** ( **JavaScript 对象符号**)文件，对于其他分隔的文件，比如 tab ( `\t`)，我们可以使用`read_csv()`函数，其中`sep`参数等于分隔符。

如果我们不学会如何将数据帧保存到文件中，以便与他人分享，那将是一种失职。要将我们的数据帧写入 CSV 文件，我们调用它的`to_csv()`方法。我们必须小心。如果我们的 dataframe 的索引只是行号，我们可能不想把它写到我们的文件中(这对数据的消费者来说没有任何意义)，但这是默认的。我们可以通过传入`index=False`来写入没有索引的数据:

```
>>> df.to_csv('output.csv', index=False)
```

与从文件中读取一样，`Series`和`DataFrame`对象有方法将数据写入 Excel ( `to_excel()`)和 JSON 文件(`to_json()`)。注意，当我们使用来自`pandas`的函数读入数据时，我们必须使用方法写入数据；读取函数创建我们想要使用的`pandas`对象，但是写入方法是我们使用`pandas`对象采取的动作。

小费

前面的文件读写路径是**相对于**当前目录**的**。当前目录是我们运行代码的地方。绝对路径将是文件的完整路径。例如，如果我们想要处理的文件有一个绝对路径`/home/myuser/learning/hands_on_pandas/data.csv`，我们的当前目录是`/home/myuser/learning/hands_on_pandas`，那么我们可以简单地使用相对路径`data.csv`作为文件路径。

Pandas 为我们提供了从许多其他数据源读取和写入的能力，包括数据库，我们将在下面讨论；pickle 文件(包含序列化的 Python 对象——更多信息参见*延伸阅读*部分)；和 HTML 页面。请务必查看`pandas`文档中的以下资源以获得完整的功能列表:[https://pandas . pydata . org/pandas-docs/stable/user _ guide/io . html](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)。

## 从数据库中

Pandas可以与 SQLite 数据库交互，而不需要我们安装任何额外的包；但是，需要安装 SQLAlchemy 包，以便与其他数据库风格进行交互。通过使用 Python 标准库中的`sqlite3`模块打开到数据库的连接，然后使用`pd.read_sql()`函数查询数据库或使用`DataFrame`对象上的`to_sql()`方法将其写入数据库，可以实现与 SQLite 数据库的交互。

在我们从数据库中读取之前，让我们先写入一个数据库。我们只需在 dataframe 上调用`to_sql()`,告诉它要写入哪个表，使用哪个数据库连接，以及如何处理已经存在的表。在本书的 GitHub 资源库中，这一章的文件夹中已经有一个 SQLite 数据库:`data/quakes.db`。注意，要创建一个新的数据库，我们可以将`'data/quakes.db'`改为新数据库文件的路径。让我们将海啸数据从`data/tsunamis.csv`文件写入数据库中一个名为`tsunamis`的表，替换已经存在的表:

```
>>> import sqlite3
>>> with sqlite3.connect('data/quakes.db') as connection:
...     pd.read_csv('data/tsunamis.csv').to_sql(
...         'tsunamis', connection, index=False,
...         if_exists='replace'
...     )
```

查询数据库就像写入数据库一样简单。注意，这需要了解**结构化查询语言** ( **SQL** )。虽然这不是本书所要求的，但我们将使用一些简单的 SQL 语句来说明某些概念。参见*补充阅读*部分，了解`pandas`如何与 SQL 进行比较，以及 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 、*聚合Pandas数据帧*，了解`pandas`操作如何与 SQL 语句相关的一些示例。

让我们在数据库中查询完整的`tsunamis`表。当我们编写一个 SQL 查询时，我们首先声明我们想要选择的列，在我们的例子中是所有的列，所以我们编写`"SELECT *"`。接下来，我们陈述从中选择数据的表，对我们来说是`tsunamis`，所以我们添加了`"FROM tsunamis"`。这是我们现在的完整查询(当然，它可以比这复杂得多)。为了实际查询数据库，我们使用`pd.read_sql()`，传入我们的查询和数据库连接:

```
>>> import sqlite3
>>> with sqlite3.connect('data/quakes.db') as connection:
...     tsunamis = \
...         pd.read_sql('SELECT * FROM tsunamis', connection)
>>> tsunamis.head()
```

我们现在有了数据框架中的海啸数据:

![Figure 2.10 – Reading data from a database
](image/Figure_2.10_B16834.jpg)

图 2.10-从数据库中读取数据

重要说明

我们在两个代码块中创建的`connection`对象是一个**上下文管理器**的例子，当与`with`语句一起使用时，它会在代码块中的代码执行后自动处理清理(在本例中是关闭连接)。这使得清理工作变得容易，并确保我们不会留下任何遗留问题。确保使用`with`语句和上下文管理器从实用程序标准库中检查出`contextlib`。文件在[https://docs.python.org/3/library/contextlib.html](https://docs.python.org/3/library/contextlib.html)。

## 来自 API

我们现在可以很容易地从 Python 中的数据或者从我们获得的文件中创建`Series`和`DataFrame`对象，但是我们如何从在线资源，比如 API 中获得数据呢？不能保证每个数据源都以相同的格式提供数据，因此我们必须保持方法的灵活性，并且能够轻松地检查数据源以找到合适的导入方法。在本节中，我们将从 USGS API 请求一些地震数据，并看看我们如何从结果中制作一个数据框架。在 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)*与Pandas*的数据争论中，我们将使用另一个 API 来收集天气数据。

对于这一部分，我们将在`3-making_dataframes_from_api_requests.ipynb`笔记本中工作，所以我们必须再次导入我们需要的包。和前面的笔记本一样，我们需要`pandas`和`datetime`，但是我们还需要`requests`包来发出 API 请求:

```
>>> import datetime as dt
>>> import pandas as pd
>>> import requests
```

接下来，我们将通过指定`geojson`的格式向 USGS API 发出一个`GET`请求，请求 JSON 有效载荷(一个类似字典的响应，包含随请求或响应一起发送的数据)。我们将要求过去 30 天的地震数据(我们可以使用`dt.timedelta`对`datetime`对象执行运算)。请注意，我们使用`yesterday`作为日期范围的终点，因为 API 还没有今天的完整信息:

```
>>> yesterday = dt.date.today() - dt.timedelta(days=1)
>>> api = 'https://earthquake.usgs.gov/fdsnws/event/1/query'
>>> payload = {
...     'format': 'geojson',
...     'starttime': yesterday - dt.timedelta(days=30),
...     'endtime': yesterday
... }
>>> response = requests.get(api, params=payload)
```

重要说明

`GET`是一个 HTTP 方法。这个动作告诉服务器我们想要读取一些数据。不同的 API 可能要求我们使用不同的方法来获取数据；有些将需要一个`POST`请求，在这里我们向服务器认证。你可以在[https://nordicapis . com/ultimate-guide-to-all-9-standard-HTTP-methods/](https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/)阅读更多关于 API 请求和 HTTP 方法的内容。

在我们尝试创建数据帧之前，我们应该确保我们的请求是成功的。我们可以通过检查`response`对象的`status_code`属性来做到这一点。在[https://en.wikipedia.org/wiki/List_of_HTTP_status_codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)可以找到状态代码及其含义的列表。一个`200`响应将表明一切正常:

```
>>> response.status_code
200
```

我们的请求成功了，所以让我们看看我们得到的数据是什么样的。我们向 API 请求一个 JSON 有效负载，它本质上是一个字典，所以我们可以对它使用字典方法来获得更多关于它的结构的信息。这将会是大量的数据。因此，我们不想仅仅为了检查它而将它打印到屏幕上。我们需要从 HTTP 响应中分离出 JSON 有效负载(存储在`response`变量中),然后查看键来查看结果数据的主要部分:

```
>>> earthquake_json = response.json()
>>> earthquake_json.keys()
dict_keys(['type', 'metadata', 'features', 'bbox'])
```

我们可以检查每个键的值是什么样的数据；其中一个就是我们要找的数据。`metadata`部分告诉我们一些关于我们请求的信息。虽然这肯定是有用的，但这不是我们现在所追求的:

```
>>> earthquake_json['metadata']
{'generated': 1604267813000,
 'url': 'https://earthquake.usgs.gov/fdsnws/event/1/query?
format=geojson&starttime=2020-10-01&endtime=2020-10-31',
 'title': 'USGS Earthquakes',
 'status': 200,
 'api': '1.10.3',
 'count': 13706}
```

`features`键看起来很有希望；如果这确实包含了我们所有的数据，我们应该检查它是什么类型，这样我们就不会最终试图将所有内容都打印到屏幕上:

```
>>> type(earthquake_json['features'])
list
```

这个键包含一个列表，所以让我们看看第一个条目，看看这是不是我们想要的数据。请注意，随着更多关于地震的信息浮出水面，USGS 数据可能会被更改或添加到过去的日期中，这意味着查询相同的日期范围可能会在以后产生不同数量的结果。因此，下面的是一个条目的示例:

```
>>> earthquake_json['features'][0]
{'type': 'Feature',
 'properties': {'mag': 1,
  'place': '50 km ENE of Susitna North, Alaska',
  'time': 1604102395919, 'updated': 1604103325550, 'tz': None,
  'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/ak020dz5f85a',
  'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ak020dz5f85a&format=geojson',
  'felt': None, 'cdi': None, 'mmi': None, 'alert': None,
  'status': 'reviewed', 'tsunami': 0, 'sig': 15, 'net': 'ak',
  'code': '020dz5f85a', 'ids': ',ak020dz5f85a,',
  'sources': ',ak,', 'types': ',origin,phase-data,',
  'nst': None, 'dmin': None, 'rms': 1.36, 'gap': None,
  'magType': 'ml', 'type': 'earthquake',
  'title': 'M 1.0 - 50 km ENE of Susitna North, Alaska'},
 'geometry': {'type': 'Point', 'coordinates': [-148.9807, 62.3533, 5]},
 'id': 'ak020dz5f85a'} 
```

这肯定是我们想要的数据，但是我们真的需要所有的数据吗？仔细观察，我们只关心字典里面有什么。现在，我们有一个问题,因为我们有一个字典列表，我们只想要其中的一个特定键。我们如何把这些信息提取出来，这样我们就可以制作数据框架了？我们可以使用列表理解从`features`列表中的每个词典中分离出`properties`部分:

```
>>> earthquake_properties_data = [
...     quake['properties'] 
...     for quake in earthquake_json['features']
... ]
```

最后，我们准备创建我们的数据框架。Pandas 已经知道如何处理这种格式的数据(字典列表)，所以我们所要做的就是在调用`pd.DataFrame()`时传递数据:

```
>>> df = pd.DataFrame(earthquake_properties_data)
```

现在我们知道了如何从各种来源创建数据框架，我们可以开始学习如何使用它们。

# 检查数据帧对象

当我们读入数据时，我们应该做的第一件事是检查它；我们希望确保我们的数据帧不是空的，并且这些行看起来像我们期望的那样。我们的主要目标是验证它被正确地读入，并且所有的数据都在那里；然而，这种初步的检查也会给我们一些想法，关于我们应该把我们的数据争论的努力引向哪里。在本节中，我们将探索在`4-inspecting_dataframes.ipynb`笔记本中检查数据帧的方法。

由于这是一个新的笔记本，我们必须再次处理我们的设置。这一次，我们需要导入`pandas`和`numpy`，以及读入地震数据的 CSV 文件:

```
>>> import numpy as np
>>> import pandas as pd
>>> df = pd.read_csv('data/earthquakes.csv')
```

## 检查数据

首先，我们希望确保我们的数据帧中确实有数据。我们可以检查`empty`属性来找出:

```
>>> df.empty
False
```

到目前为止，一切顺利；我们有数据。接下来，我们应该检查我们读入了多少数据；我们想知道我们拥有的观察值(行)的数量和变量(列)的数量。对于这个任务，我们使用了`shape`属性。我们的数据包含 26 个变量的 9，332 个观察值，这与我们对文件的初始检查相匹配:

```
>>> df.shape
(9332, 26)
```

现在，让我们使用`columns`属性来查看数据集中的列名:

```
>>> df.columns
Index(['alert', 'cdi', 'code', 'detail', 'dmin', 'felt', 'gap', 
       'ids', 'mag', 'magType', 'mmi', 'net', 'nst', 'place', 
       'rms', 'sig', 'sources', 'status', 'time', 'title', 
       'tsunami', 'type', 'types', 'tz', 'updated', 'url'],
      dtype='object')
```

重要说明

拥有一个列列表并不一定意味着我们知道所有列的含义。尤其是在我们的数据来自互联网的情况下，在得出任何结论之前，一定要仔细阅读这些栏目的含义。关于`geojson`格式字段的信息，包括 JSON 有效载荷中每个字段的含义(以及一些示例值)，可以在美国地质勘探局网站[https://seismic . USGS . gov/seismics/feed/v 1.0/geo JSON . PHP](https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php)上找到。

我们知道我们数据的维度，但是它实际上看起来像什么呢？对于这个任务，我们可以使用`head()`和`tail()`方法分别查看顶行和底行。这将默认为 5 行，但是我们可以通过向该方法传递一个不同的数字来改变这一点。让我们看看前几行:

```
>>> df.head()
```

下面是我们使用`head()`得到的前五行:

![Figure 2.11 – Examining the top five rows of a dataframe
](image/Figure_2.11_B16834.jpg)

图 2.11–检查数据帧的前五行

为了获得最后两行，我们使用了`tail()`方法，并将`2`作为行数传递:

```
>>> df.tail(2)
```

以下是结果:

![Figure 2.12 – Examining the bottom two rows of a dataframe
](image/Figure_2.12_B16834.jpg)

图 2.12–检查数据帧的底部两行

小费

默认情况下，当我们在 Jupyter 笔记本中打印包含许多列的数据帧时，只会显示其中的一部分。这是因为`pandas`对它将显示的列数有限制。我们可以使用`pd.set_option('display.max_columns', <new_value>)`来修改这种行为。如需更多信息，请查阅位于[https://pandas . pydata . org/pandas-docs/stable/user _ guide/options . html](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)的文档。该笔记本还包含一些示例命令。

我们可以使用`dtypes`属性来查看列的数据类型，这使得很容易看到列何时被存储为错误的类型。(记住字符串将被存储为`object`。)在这里，`time`列被存储为一个整数，这是我们将在 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) ，*与Pandas的数据争论*中学习如何解决的问题:

```
>>> df.dtypes
alert       object
...
mag        float64
magType     object
...
time         int64
title       object
tsunami      int64
...
tz         float64
updated      int64
url         object
dtype: object
```

最后，我们可以使用`info()`方法来查看每一列有多少个非空条目并获取索引信息。 **Null** 值是缺失值，在`pandas`中，对于对象通常表示为`None`，对于`float`或`integer`列中的非数值，通常表示为`NaN` ( **不是数字**):

```
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 9332 entries, 0 to 9331
Data columns (total 26 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   alert    59 non-null     object 
 ... 
 8   mag      9331 non-null   float64
 9   magType  9331 non-null   object 
 ... 
 18  time     9332 non-null   int64  
 19  title    9332 non-null   object 
 20  tsunami  9332 non-null   int64  
 ... 
 23  tz       9331 non-null   float64
 24  updated  9332 non-null   int64  
 25  url      9332 non-null   object 
dtypes: float64(9), int64(4), object(13)
memory usage: 1.9+ MB
```

在这个初始检查之后，我们对数据的结构有了很多了解，现在可以开始尝试理解它了。

## 描述和总结数据

到目前为止，我们已经检查了从地震数据中创建的`DataFrame`对象的结构，但是除了几行看起来像什么之外，我们不知道关于数据的任何事情。下一步是计算汇总统计数据，这将帮助我们更好地了解我们的数据。Pandas 提供了几种简单的方法来做到这一点；其中一个方法是`describe()`，如果我们只对特定的列感兴趣，它也适用于`Series`对象。让我们来总结一下数据中的数字列:

```
>>> df.describe()
```

这为我们提供了 5 个数字的摘要，以及数字列的计数、平均值和标准偏差:

![Figure 2.13 – Calculating summary statistics
](image/Figure_2.13_B16834.jpg)

图 2.13–计算汇总统计数据

小费

如果我们想要不同的百分点，我们可以用`percentiles`参数传递它们。例如，如果我们只想要第 5 和第 95 个百分点，我们将运行`df.describe(percentiles=[0.05, 0.95])`。请注意，我们仍然会得到第 50 个百分点，因为这是中位数。

默认情况下，`describe()`不会给我们任何关于类型为`object`的列的信息，但是我们可以提供`include='all'`作为参数，或者为类型为`np.object`的数据单独运行它:

```
>>> df.describe(include=np.object)
```

在描述非数值数据时，我们仍然得到非空出现的计数(**计数**)；但是，我们得到的不是其他汇总统计数据，而是唯一值的数量( **unique** )、模式( **top** )以及模式被观察的次数( **freq** ):

![Figure 2.14 – Summary statistics for categorical columns
](image/Figure_2.14_B16834.jpg)

图 2.14–分类列的汇总统计数据

重要说明

`describe()`方法只给出非空值的汇总统计数据。这意味着，如果我们有 100 行，并且一半的数据为空，那么平均值将被计算为 50 个非空行的总和除以 50。

使用`describe()`方法很容易获得数据的快照，但是有时，我们只是想要一个特定的统计数据，或者是一个特定的列，或者是所有的列。Pandas也让这变得不在话下。下表包括适用于`Series`和`DataFrame`对象的方法:

![Figure 2.15 – Helpful calculation methods for series and dataframes
](image/Figure_2.15_B16834.jpg)

图 2.15–系列和数据框架的有用计算方法

小费

Python 使得计算某个东西被`True`多少次变得很容易。在引擎盖下，`True`评估为`1`，`False`评估为`0`。因此，我们可以对一系列布尔值运行`sum()`方法，并获得`True`输出的计数。

对于`Series`对象，我们有一些额外的方法来描述我们的数据:

*   `unique()`:返回列的不同值。
*   `value_counts()`:返回给定列中每个唯一值出现次数的频率表，或者每个唯一值在通过`normalize=True`时出现的百分比。
*   `mode()`:返回该列最常用的值。

参考USGS API 文档中的`alert`字段(可以在[https://seismic . USGS . gov/data/comcat/data-event terms . PHP # alert](https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert)找到)告诉我们，它可以是`'green'`、`'yellow'`、`'orange'`或`'red'`(当填充时)，并且它是来自**全球地震快速评估响应** ( **寻呼机**)地震影响的警报级别根据美国地质勘探局([https://earthquake.usgs.gov/data/pager/](https://earthquake.usgs.gov/data/pager/))，“*全球大地震发生后，传呼系统提供死亡和经济损失影响评估。”根据我们对数据的初步检查，我们知道`alert`列是一个由两个唯一值组成的字符串，最常见的值是`'green'`，其中有许多空值。然而，另一个独特的价值是什么呢？*

```
>>> df.alert.unique()
array([nan, 'green', 'red'], dtype=object)
```

既然我们已经理解了这个字段的含义以及我们数据中的值，我们希望有比`'red'`多得多的`'green'`；我们可以通过使用`value_counts()`用频率表来检查我们的直觉。请注意，我们只获得非空条目的计数:

```
>>> df.alert.value_counts()
green    58
red       1
Name: alert, dtype: int64
```

注意`Index`对象也有几个方法可以帮助我们描述和总结数据:

![Figure 2.16 – Helpful methods for the index
](image/Figure_2.16_B16834.jpg)

图 2.16–索引的有用方法

当使用`unique()`和`value_counts()`时，我们得到了如何选择数据子集的预览。现在，让我们更详细地讨论选择、切片、索引和过滤。

# 抓取数据子集

到目前为止，我们已经学会了如何从整体上处理和总结数据；然而，我们通常会对在我们的数据子集上执行操作和/或分析感兴趣。我们可以从数据中分离出许多类型的子集，例如，只选择特定的列或行作为一个整体，或者当满足特定的标准时。为了获得数据的子集，我们需要熟悉选择、切片、索引和过滤。

对于这一部分，我们将在`5-subsetting_data.ipynb`笔记本中工作。我们的设置如下:

```
>>> import pandas as pd
>>> df = pd.read_csv('data/earthquakes.csv')
```

## 选择列

在上一节中，当我们查看`alert`列中的唯一值时，我们看到了一个列选择的示例；我们将该列作为 dataframe 的一个属性来访问。请记住，一列是一个`Series`对象，因此，例如，选择地震数据中的`mag`列，我们将得到地震的震级作为一个`Series`对象:

```
>>> df.mag
0       1.35
1       1.29
2       3.42
3       0.44
4       2.16
        ... 
9327    0.62
9328    1.00
9329    2.40
9330    1.10
9331    0.66
Name: mag, Length: 9332, dtype: float64
```

Pandas 为我们提供了几种选择列的方法。使用属性符号选择列的另一种方法是使用类似字典的符号来访问它:

```
>>> df['mag']
0       1.35
1       1.29
2       3.42
3       0.44
4       2.16
        ... 
9327    0.62
9328    1.00
9329    2.40
9330    1.10
9331    0.66
Name: mag, Length: 9332, dtype: float64
```

小费

我们还可以使用`get()`方法选择列。这样做的好处是，如果列不存在，就不会引发错误，并允许我们提供一个备份值——缺省值是`None`。例如，如果我们调用`df.get('event', False)`，它将返回`False`，因为我们没有`event`列。

注意我们不局限于一次选择一列。通过将一个列表传递给字典查找，我们可以选择许多列，给我们一个`DataFrame`对象，它是我们原始数据帧的子集:

```
>>> df[['mag', 'title']]
```

这为我们提供了原始数据帧中完整的`mag`和`title`列:

![Figure 2.17 – Selecting multiple columns of a dataframe
](image/Figure_2.17_B16834.jpg)

图 2.17–选择数据帧的多列

String 方法是一种非常强大的选择列的方法。例如，如果我们想要选择所有以`mag`开头的列，以及`title`和`time`列，我们将执行以下操作:

```
>>> df[
...     ['title', 'time'] 
...     + [col for col in df.columns if col.startswith('mag')]
... ]
```

我们得到一个数据帧，它由符合我们标准的四列组成。请注意，这些列是如何按照我们请求的顺序返回的，而不是它们最初出现的顺序。这意味着，如果我们想要重新排序我们的列，我们所要做的就是按照我们希望它们出现的顺序选择它们:

![Figure 2.18 – Selecting columns based on names
](image/Figure_2.18_B16834.jpg)

图 2.18-根据名称选择列

让我们来分解一下这个例子。我们使用列表理解来遍历数据帧中的每一列，并且只保留那些名称以`mag`开头的列:

```
>>> [col for col in df.columns if col.startswith('mag')]
['mag', 'magType']
```

然后，我们将这个结果添加到我们想要保留的另外两列(`title`和`time`):

```
>>> ['title', 'time'] \
... + [col for col in df.columns if col.startswith('mag')]
['title', 'time', 'mag', 'magType']
```

最后，我们能够使用该列表在数据帧上运行实际的列选择，得到图 2.18 中的数据帧:

```
>>> df[
...     ['title', 'time'] 
...     + [col for col in df.columns if col.startswith('mag')]
... ]
```

小费

完整的字符串方法列表可以在 Python 3 文档中找到，网址为[https://docs . Python . org/3/library/stdtypes . html # string-methods](https://docs.python.org/3/library/stdtypes.html#string-methods)。

## 切片

当我们希望从我们的数据帧中提取某些行(切片)时，我们使用**切片**。`DataFrame`切片的工作方式类似于其他 Python 对象的切片，例如列表和元组的，第一个索引是包含性的，最后一个索引是排他性的:

```
>>> df[100:103]
```

当指定一个片`100:103`时，我们得到返回行`100`、`101`和`102`:

![Figure 2.19 – Slicing a dataframe to extract specific rows
](image/Figure_2.19_B16834.jpg)

图 2.19-分割数据帧以提取特定的行

我们可以通过使用所谓的**链接**来组合我们的行和列选择:

```
>>> df[['title', 'time']][100:103]
```

首先，我们为所有的行选择了`title`和`time`列，然后我们取出了带有索引`100`、`101`和`102`的行:

![Figure 2.20 – Selecting specific rows and columns with chaining
](image/Figure_2.20_B16834.jpg)

图 2.20–使用链接选择特定的行和列

在前面的示例中，我们选择了列，然后对行进行了切片，但顺序并不重要:

```
>>> df[100:103][['title', 'time']].equals(
...     df[['title', 'time']][100:103]
... )
True
```

小费

注意，我们可以对索引中的任何内容进行切片；然而，很难确定我们想要的最后一个字符串或日期之后的字符串或日期，因此使用`pandas`，对日期和字符串进行切片不同于对整数进行切片，它包含了两个端点。只要我们提供的字符串可以被解析成一个`datetime`对象，日期切片就可以工作。在 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) ，*与Pandas*的数据争论中，我们将看到一些这样的例子，也将学习如何改变我们使用的索引，从而使这种类型的切片成为可能。

如果我们决定使用链接来更新我们数据中的值，我们会发现`pandas`抱怨我们这样做不对(即使它有效)。这是为了警告我们，用顺序选择设置数据可能不会得到我们预期的结果。(更多信息可在[https://pandas . py data . org/pandas-docs/stable/user _ guide/indexing . html # return-a-view-vs-a-copy](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy)找到。)

让我们触发这个警告来更好地理解它。我们将尝试更新几个地震的`title`列中的条目，以便它们是小写的:

```
>>> df[110:113]['title'] = df[110:113]['title'].str.lower()
/.../book_env/lib/python3.7/[...]:1: SettingWithCopyWarning:  
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  """Entry point for launching an IPython kernel.
```

正如警告所指出的，要成为一个有效的`pandas`用户，仅仅知道选择和切片是不够的——我们还必须掌握**索引**。由于这只是一个警告，我们的值已经更新，但情况可能并不总是如此:

```
>>> df[110:113]['title']
110               m 1.1 - 35km s of ester, alaska
111    m 1.9 - 93km wnw of arctic village, alaska
112      m 0.9 - 20km wsw of smith valley, nevada
Name: title, dtype: object
```

现在，让我们讨论如何使用索引来正确地设置值。

## 索引

Pandas 索引操作为我们提供了一种选择我们想要的行和列的方法。我们可以使用`loc[]`和`iloc[]`分别使用基于标签或基于整数的查找来划分数据帧的子集。记住这种区别的一个好方法是把它们想象成**位置**位置和 **i** 整数**位置**位置。对于所有索引方法，我们首先提供行索引器，然后提供列索引器，用逗号分隔它们:

```
df.loc[row_indexer, column_indexer]
```

请注意，通过使用`loc[]`，如警告消息所示，我们不再从`pandas`为该操作触发任何警告。我们还将结束索引从`113`更改为`112`，因为`loc[]`包含端点:

```
>>> df.loc[110:112, 'title'] = \
...     df.loc[110:112, 'title'].str.lower()
>>> df.loc[110:112, 'title']
110               m 1.1 - 35km s of ester, alaska
111    m 1.9 - 93km wnw of arctic village, alaska
112      m 0.9 - 20km wsw of smith valley, nevada
Name: title, dtype: object
```

如果我们使用`:`作为行(列)索引器，我们可以选择所有的行(列)，就像常规的 Python 切片一样。让我们用`loc[]`抓取`title`列的所有行:

```
>>> df.loc[:,'title']
0                  M 1.4 - 9km NE of Aguanga, CA
1                  M 1.3 - 9km NE of Aguanga, CA
2                  M 3.4 - 8km NE of Aguanga, CA
3                  M 0.4 - 9km NE of Aguanga, CA
4                  M 2.2 - 10km NW of Avenal, CA
                          ...                   
9327        M 0.6 - 9km ENE of Mammoth Lakes, CA
9328                 M 1.0 - 3km W of Julian, CA
9329    M 2.4 - 35km NNE of Hatillo, Puerto Rico
9330               M 1.1 - 9km NE of Aguanga, CA
9331               M 0.7 - 9km NE of Aguanga, CA
Name: title, Length: 9332, dtype: object
```

我们可以使用`loc[]`同时选择多行和多列:

```
>>> df.loc[10:15, ['title', 'mag']]
```

这样我们只剩下第`10`行到第`15`行的`title`和`mag`列:

![Figure 2.21 – Selecting specific rows and columns with indexing
](image/Figure_2.21_B16834.jpg)

图 2.21–使用索引选择特定的行和列

正如我们所看到的，当使用`loc[]`时，我们的最终指标是包含性的。而`iloc[]`却不是这样:

```
>>> df.iloc[10:15, [19, 8]]
```

观察我们如何提供整数列表来选择相同的列；这些是列号(从`0`开始)。使用`iloc[]`，我们丢失了索引`15`处的行；这是因为`iloc[]`采用的整数切片不包括结束索引，就像 Python 切片语法一样:

![Figure 2.22 – Selecting specific rows and columns by position
](image/Figure_2.22_B16834.jpg)

图 2.22–按位置选择特定的行和列

但是，我们并不局限于对行使用切片语法；列也可以工作:

```
>>> df.iloc[10:15, 6:10]
```

通过使用切片，我们可以轻松地抓取相邻的行和列:

![Figure 2.23 – Selecting ranges of adjacent rows and columns by position
](image/Figure_2.23_B16834.jpg)

图 2.23–按位置选择相邻行和列的范围

当使用`loc[]`时，也可以对列名进行切片。这给了我们许多方法来达到同样的结果:

```
>>> df.iloc[10:15, 6:10].equals(df.loc[10:14, 'gap':'magType'])
True
```

为了查找标量值，我们使用`at[]`和`iat[]`，它们更快。让我们选择在索引`10`的行中记录的地震的震级(T2 列):

```
>>> df.at[10, 'mag']
0.5
```

magnitude 列的列索引为`8`；因此，我们也可以用`iat[]`来查星等:

```
>>> df.iat[10, 8]
0.5
```

到目前为止，我们已经看到了如何使用行/列名称和范围来获取数据的子集，但是我们如何只获取符合某些标准的数据呢？为此，我们需要学习如何过滤我们的数据。

## 过滤

Pandas 给了一些选项来过滤我们的数据，包括**布尔掩码**和一些特殊方法。使用布尔掩码，我们针对某个值测试我们的数据，并得到一个相同形状的结构，只是它用`True` / `False`值填充；`pandas`可以用这个来为我们选择合适的行/列。创建布尔掩码有无限的可能性——我们所需要的只是一些为每一行返回一个布尔值的代码。例如，我们可以看到`mag`列中哪些条目的值大于 2:

```
>>> df.mag > 2
0       False
1       False
2        True
3       False
        ...  
9328    False
9329     True
9330    False
9331    False
Name: mag, Length: 9332, dtype: bool
```

虽然我们可以在整个数据框架上运行这个，但它对我们的地震数据不会太有用，因为我们有各种数据类型的列。但是，我们可以使用此策略来获取地震震级大于或等于 7.0 的数据子集:

```
>>> df[df.mag >= 7.0]
```

我们得到的数据帧只有两行:

![Figure 2.24 – Filtering with Boolean masks
](image/Figure_2.24_B16834.jpg)

图 2.24–使用布尔掩码过滤

不过，我们收回了很多我们不需要的栏目。我们可以将列选择链接到最后一段代码的末尾；然而，`loc[]`也可以处理布尔掩码:

```
>>> df.loc[
...     df.mag >= 7.0, 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

以下数据框架已经过筛选，因此只包含相关列:

![Figure 2.25 – Indexing with Boolean masks
](image/Figure_2.25_B16834.jpg)

图 2.25–使用布尔掩码进行索引

我们也不局限于一个标准。让我们用红色警报和海啸来抓住地震。要组合掩码，我们需要用括号将每个条件括起来，并且使用**按位 and 运算符** ( `&`)要求*和*都为真:

```
>>> df.loc[
...     (df.tsunami == 1) & (df.alert == 'red'), 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

数据中只有一次地震符合我们的标准:

![Figure 2.26 – Combining filters with AND
](image/Figure_2.26_B16834.jpg)

图 2.26–用 AND 组合过滤器

相反，如果我们希望条件中的*至少有一个*为真，我们可以使用**按位 OR 运算符** ( `|`):

```
>>> df.loc[
...     (df.tsunami == 1) | (df.alert == 'red'), 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

请注意，该过滤器的限制性要小得多，因为尽管两个条件都可能为真，但我们只要求其中一个条件为:

![Figure 2.27 – Combining filters with OR
](image/Figure_2.27_B16834.jpg)

图 2.27–用 OR 组合过滤器

重要说明

在创建布尔掩码时，我们必须使用按位运算符(`&`、`|`、`~`)，而不是逻辑运算符(`and`、`or`、`not`)。记住这一点的一个好方法是，我们希望我们测试的系列中的每个项目都有一个布尔值，而不是单个布尔值。例如，对于地震数据，如果我们希望选择震级大于 1.5 的行，那么我们希望每行有一个布尔值，表明是否应该选择该行。在我们需要数据的单个值的情况下，也许是为了对其进行总结，我们可以使用`any()` / `all()`将一个布尔序列压缩成一个可以与逻辑操作符一起使用的单个布尔值。我们将使用第 4 章 、*聚合Pandas数据帧*中的`any()`和`all()`方法。

在前面的两个例子中，我们的条件涉及平等；然而，我们绝不仅限于此。让我们选择阿拉斯加的所有地震，其中`alert`列有一个非空值:

```
>>> df.loc[
...     (df.place.str.contains('Alaska')) 
...     & (df.alert.notnull()), 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

阿拉斯加所有有`alert`值的地震都是`green`，有的还伴有海啸，最高震级 5.1 级:

![Figure 2.28 – Creating Boolean masks with non-numeric columns
](image/Figure_2.28_B16834.jpg)

图 2.28–创建带有非数字列的布尔掩码

让我们来分析一下这是怎么来的。`Series`对象有一些可以通过`str`属性访问的字符串方法。使用这个，我们可以创建一个所有行的布尔掩码，其中`place`列包含单词`Alaska`:

```
df.place.str.contains('Alaska')
```

为了获得所有`alert`列不为空的行，我们使用了`Series`对象的`notnull()`方法(这也适用于`DataFrame`对象)来创建所有`alert`列不为空的行的布尔掩码:

```
df.alert.notnull()
```

小费

我们可以使用**按位求反运算符** ( `~`)，也称为**而非**，对所有的布尔值求反，这使得所有的`True`值成为`False`，反之亦然。所以，`df.alert.notnull()`和`~df.alert.isnull()`是等价的。

然后，像我们之前做的一样，我们用`&`操作符将两个条件结合起来，完成我们的遮罩:

```
(df.place.str.contains('Alaska')) & (df.alert.notnull())
```

注意，我们并不局限于检查每一行是否包含文本；我们也可以使用正则表达式。**正则表达式**(通常简称为 *regex* )非常强大，因为它们允许我们定义一个搜索模式，而不是我们想要找到的确切内容。这意味着我们可以做一些事情，比如找到一个字符串中的所有单词或数字，而不必事先知道所有的单词或数字是什么(或者一次遍历一个字符)。为此，我们只需在引号外传入一个前面有一个`r`字符的字符串；这让 Python 知道它是一个**原始字符串**，这意味着我们可以在字符串中包含反斜杠(`\`)字符，而 Python 不会认为我们试图转义紧跟其后的字符(比如当我们使用`\n`代替字母`n`来表示一个新的行字符)。这使得它非常适合与正则表达式一起使用。Python 标准库中的`re`模块([https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html))处理正则表达式操作；然而，`pandas`让我们直接使用正则表达式。

使用正则表达式，让我们选择加利福尼亚州所有震级至少为 3.8 级的地震。我们需要在`place`列中选择以`CA`或`California`结尾的条目，因为数据不一致(我们将在下一节研究如何解决这个问题)。`$`字符意味着*结束*，`'CA$'`给我们以`CA`结尾的条目，所以我们可以使用`'CA|California$'`来获得以以下任一结尾的条目:

```
>>> df.loc[
...     (df.place.str.contains(r'CA|California$'))
...     & (df.mag > 3.8),         
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

在我们研究的这段时间里，加州只有两次震级超过 3.8 的地震:

![Figure 2.29 – Filtering with regular expressions
](image/Figure_2.29_B16834.jpg)

图 2.29–使用正则表达式过滤

小费

正则表达式非常强大，但不幸的是，也很难正确使用。获取一些用于解析的示例行并使用网站来测试它们通常是有帮助的。注意正则表达式有很多种，所以一定要选择 Python。这个网站支持 Python 风格的正则表达式，同时还提供了一个不错的备忘单:https://regex101.com/.

如果我们想得到所有震级在 6.5 到 7.5 之间的地震呢？我们可以使用两个布尔掩码——一个检查大于或等于 6.5 的幅度，另一个检查小于或等于 7.5 的幅度——然后用`&`操作符将它们组合起来。谢天谢地，`pandas`为我们提供了`between()`方法，使得这种类型的面具更容易制作:

```
>>> df.loc[
...     df.mag.between(6.5, 7.5), 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

结果包含震级在[6.5，7.5]范围内的所有地震——默认情况下包括两端，但我们可以传入`inclusive=False`来更改它:

![Figure 2.30 – Filtering using a range of values
](image/Figure_2.30_B16834.jpg)

图 2.30–使用一系列值进行过滤

我们可以使用`isin()`方法为匹配一个值列表的值创建一个布尔掩码。这意味着我们不必为我们可以匹配的每个值编写一个掩码，然后使用`|`连接它们。让我们利用这一点来过滤`magType`列，它表示用于量化地震震级的测量技术。我们将看看用`mw`或`mwb`震级类型测量的地震:

```
>>> df.loc[
...     df.magType.isin(['mw', 'mwb']), 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

我们用`mwb`震级类型测量了两次地震，用`mw`震级类型测量了四次地震:

![Figure 2.31 – Filtering using membership in a list
](image/Figure_2.31_B16834.jpg)

图 2.31–使用列表中的成员进行过滤

到目前为止，我们一直在筛选特定的值，但是假设我们想要查看最低震级和最高震级地震的所有数据。我们可以要求`pandas`给出这些值出现的索引，并轻松过滤以获取完整的行，而不是先找到`mag`列的最小值和最大值，然后创建一个布尔掩码。我们可以分别使用`idxmin()`和`idxmax()`作为最小值和最大值的索引。让我们抓住最低震级和最高震级地震的行号:

```
>>> [df.mag.idxmin(), df.mag.idxmax()]
[2409, 5263]
```

我们可以使用这些索引来获取行本身:

```
>>> df.loc[
...     [df.mag.idxmin(), df.mag.idxmax()], 
...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']
... ]
```

最小震级的地震发生在阿拉斯加，最大震级的地震发生在印度尼西亚，并伴有海啸。我们将在 [*第五章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106) 、*用 Pandas 和 Matplotlib* 可视化数据、 [*第六章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125) 、*用 Seaborn 和定制技术绘图*中讨论印尼地震:

![Figure 2.32 – Filtering to isolate the rows containing the minimum and maximum of a column
](image/Figure_2.32_B16834.jpg)

图 2.32–过滤以隔离包含列的最小值和最大值的行

重要说明

请注意，`filter()`方法并不像我们在本节中所做的那样根据值过滤数据；相反，它可以用于根据行或列的名称对它们进行子集划分。关于`DataFrame`和`Series`物体的例子可以在笔记本中找到。

# 添加和删除数据

在前面的章节中，我们经常选择列的子集，但是如果列/行对我们没有用，我们应该把它们去掉。我们还经常根据`mag`列的值选择数据；然而，如果我们让成为一个新的列，为以后的选择保存布尔值，我们只需要计算一次掩码。我们很少会得到既不想添加也不想删除的数据。

在我们开始添加和删除数据之前，重要的是要理解，虽然大多数方法会返回一个新的`DataFrame`对象，但有些方法会原地不动并改变我们的数据。如果我们写一个函数，传入一个数据帧并改变它，它也会改变我们原来的数据帧。如果我们发现自己不希望更改原始数据，而是希望返回已修改数据的新副本，我们必须确保在进行任何更改之前复制我们的数据帧:

```
df_to_modify = df.copy()
```

重要说明

默认情况下，`df.copy()`会对数据帧进行**深度复制**，这允许我们对副本或原始数据进行修改，而不会产生任何影响。如果我们传入`deep=False`，我们可以获得一个**浅拷贝**——对浅拷贝的更改会影响原始拷贝，反之亦然。我们几乎总是想要深层副本，因为我们可以在不影响原始内容的情况下更改它。更多信息可以在 https://pandas . py data . org/pandas-docs/stable/reference/API/pandas 的文档中找到。DataFrame.copy.html。

现在，让我们翻到最后一本笔记本`6-adding_and_removing_data.ipynb`，为本章的剩余部分做好准备。我们将再次处理地震数据，但这一次，我们将只读取列的子集:

```
>>> import pandas as pd
>>> df = pd.read_csv(
...     'data/earthquakes.csv', 
...     usecols=[
...         'time', 'title', 'place', 'magType', 
...         'mag', 'alert', 'tsunami'
...     ]
... )
```

## 创建新数据

创建新列可以通过与变量赋值相同的方式实现。例如，我们可以创建一个列来表示我们数据的来源；由于我们所有的数据都来自同一个来源，我们可以利用**广播**将该列的每一行设置为相同的值:

```
>>> df['source'] = 'USGS API'
>>> df.head()
```

新列创建在原始列的右侧，每行的值为`USGS API`:

![Figure 2.33 – Adding a new column
](image/Figure_2.33_B16834.jpg)

图 2.33–添加新列

重要说明

我们不能用属性符号(`df.source`)创建列，因为 dataframe 还没有那个属性，所以我们必须使用字典符号(`df['source']`)。

我们不局限于向整个列传播一个值；我们可以让列保存布尔逻辑或数学方程的结果。例如，如果我们有距离和时间的数据，我们可以创建一个速度列，这是距离列除以时间列的结果。利用我们的地震数据，让我们创建一个列，告诉我们地震的震级是否为负值:

```
>>> df['mag_negative'] = df.mag < 0
>>> df.head()
```

请注意，新列已添加到右侧:

![Figure 2.34 – Storing a Boolean mask in a new column
](image/Figure_2.34_B16834.jpg)

图 2.34–在新列中存储布尔掩码

在上一节中，我们看到`place`列有一些数据一致性问题——我们对同一个实体有多个名称。在某些情况下，发生在加州的地震被标记为`CA`，而在其他地方则被标记为`California`。不用说，这是令人困惑的，如果我们不事先仔细检查我们的数据，很容易给我们带来问题。例如，仅仅通过选择`CA`，我们就错过了 124 次标记为`California`的地震。这也不是唯一有问题的地方(`Nevada`和`NV`也同时存在)。通过使用正则表达式提取逗号后的`place`列中的所有内容，我们可以直接看到一些问题:

```
>>> df.place.str.extract(r', (.*$)')[0].sort_values().unique()
array(['Afghanistan', 'Alaska', 'Argentina', 'Arizona',
       'Arkansas', 'Australia', 'Azerbaijan', 'B.C., MX',
       'Barbuda', 'Bolivia', ..., 'CA', 'California', 'Canada',
       'Chile', ..., 'East Timor', 'Ecuador', 'Ecuador region',
       ..., 'Mexico', 'Missouri', 'Montana', 'NV', 'Nevada', 
       ..., 'Yemen', nan], dtype=object)
```

如果我们想把国家和它们附近的任何东西当作一个单一的实体，我们还有一些额外的工作要做(见`Ecuador`和`Ecuador region`)。此外，我们通过查看逗号后的信息来解析位置的天真尝试似乎已经失败；这是因为，在某些情况下，我们没有逗号。我们需要改变我们的解析方法。

这是一个**实体识别问题**，解决起来并不简单。有了一个相对较小的唯一值列表(我们可以用`df.place.unique()`查看)，我们可以简单地浏览并推断如何正确匹配这些名称。然后，我们可以使用`replace()`方法来替换`place`列中我们认为合适的模式:

```
>>> df['parsed_place'] = df.place.str.replace(
...     r'.* of ', '', regex=True # remove <x> of <x> 
... ).str.replace(
...     'the ', '' # remove "the "
... ).str.replace(
...     r'CA$', 'California', regex=True # fix California
... ).str.replace(
...     r'NV$', 'Nevada', regex=True # fix Nevada
... ).str.replace(
...     r'MX$', 'Mexico', regex=True # fix Mexico
... ).str.replace(
...     r' region$', '', regex=True # fix " region" endings
... ).str.replace(
...     'northern ', '' # remove "northern "
... ).str.replace(
...     'Fiji Islands', 'Fiji' # line up the Fiji places
... ).str.replace( # remove anything else extraneous from start 
...     r'^.*, ', '', regex=True 
... ).str.strip() # remove any extra spaces
```

现在，我们可以检查剩下的解析过的位置。注意，对于`South Georgia and South Sandwich Islands`和`South Sandwich Islands`来说，这里可能还有更多的问题需要解决。我们可以给`replace()`打另一个电话来解决这个问题；然而，这表明实体识别可能相当具有挑战性:

```
>>> df.parsed_place.sort_values().unique()
array([..., 'California', 'Canada', 'Carlsberg Ridge', ...,
       'Dominican Republic', 'East Timor', 'Ecuador',
       'El Salvador', 'Fiji', 'Greece', ...,
       'Mexico', 'Mid-Indian Ridge', 'Missouri', 'Montana',
       'Nevada', 'New Caledonia', ...,
       'South Georgia and South Sandwich Islands', 
       'South Sandwich Islands', ..., 'Yemen'], dtype=object)
```

重要说明

在实践中，实体识别可能是一个极其困难的问题，我们可以考虑采用**自然语言处理** ( **NLP** )算法来帮助我们。虽然这远远超出了本书的范围，但可以在 https://www . kdnugges . com/2018/12/introduction-named-entity-recognition . html 上找到更多信息。

Pandas 还为我们提供了一种在一次方法调用中创建许多新列的方法。对于`assign()`方法，参数是我们想要创建(或覆盖)的列的名称，值是列的数据。让我们创建两个新列；一个会告诉我们地震是否发生在加利福尼亚，另一个会告诉我们地震是否发生在阿拉斯加。我们将使用`sample()`随机选择五行，而不是只显示前五个条目(都在加利福尼亚):

```
>>> df.assign(
...     in_ca=df.parsed_place.str.endswith('California'), 
...     in_alaska=df.parsed_place.str.endswith('Alaska')
... ).sample(5, random_state=0)
```

注意`assign()`没有改变我们的原始数据帧；相反，它返回一个添加了这些列的新的`DataFrame`对象。如果我们想用这个替换我们原来的数据帧，我们只需使用变量赋值将`assign()`的结果存储在`df`(例如，`df = df.assign(...)`)中:

![Figure 2.35 – Creating multiple new columns at once
](image/Figure_2.35_B16834.jpg)

图 2.35–一次创建多个新列

`assign()`方法也接受 **lambda 函数**(匿名函数通常定义在一行中，仅供单次使用)；`assign()`将把数据帧作为`x`传递给`lambda`函数，我们可以从那里开始工作。这使得我们可以使用我们在`assign()`中创建的列来计算其他列。例如，让我们再次创建`in_ca`和`in_alaska`列，但这一次还要创建一个新列`neither`，如果`in_ca`和`in_alaska`都是`False`，那么它就是`True`:

```
>>> df.assign(
...     in_ca=df.parsed_place == 'California', 
...     in_alaska=df.parsed_place == 'Alaska',
...     neither=lambda x: ~x.in_ca & ~x.in_alaska
... ).sample(5, random_state=0)
```

请记住，`~`是按位求反运算符，因此这允许我们创建一个每行有`NOT in_ca AND NOT in_alaska`结果的列:

![Figure 2.36 – Creating multiple new columns at once with lambda functions
](image/Figure_2.36_B16834.jpg)

图 2.36–使用 lambda 函数一次创建多个新列

小费

当使用`pandas`时，熟悉`lambda`函数是至关重要的，因为它们可以与许多可用的功能一起使用，并将极大地提高代码的质量和可读性。在本书中，我们会看到各种可以使用`lambda`函数的地方。

现在我们已经看到了如何添加新列，让我们来看看如何添加新行。假设我们正在处理两个独立的数据框架；一个是伴随海啸的地震，另一个是没有海啸的地震:

```
>>> tsunami = df[df.tsunami == 1]
>>> no_tsunami = df[df.tsunami == 0]
>>> tsunami.shape, no_tsunami.shape
((61, 10), (9271, 10))
```

如果我们想把地震作为一个整体来看待，我们会想把这些数据框架连接成一个整体。要将行追加到数据帧的底部，我们可以使用数据帧本身的`pd.concat()`或`append()`方法。`concat()`函数允许我们指定操作将沿哪个轴执行——`0`用于将行追加到数据帧的底部，而`1`用于追加到最后一列的右侧，相对于串联列表中最左边的`pandas`对象。让我们将`pd.concat()`与`0`的默认`axis`一起用于行:

```
>>> pd.concat([tsunami, no_tsunami]).shape
(9332, 10) # 61 rows + 9271 rows
```

注意，前面的结果相当于在数据帧上运行`append()`方法。这仍然返回一个新的`DataFrame`对象，但是它让我们不必记住哪个轴是哪个轴，因为`append()`实际上是一个`concat()`函数的包装器:

```
>>> tsunami.append(no_tsunami).shape
(9332, 10) # 61 rows + 9271 rows
```

到目前为止，我们已经使用了 CSV 文件中的列的子集，但是假设我们现在想要使用我们在读取数据时忽略的一些列。由于我们已经在此笔记本中添加了新列，所以我们不想再次读取文件并执行那些操作。相反，我们将沿着列(`axis=1`)连接，以添加回我们缺少的内容:

```
>>> additional_columns = pd.read_csv(
...     'data/earthquakes.csv', usecols=['tz', 'felt', 'ids']
... )
>>> pd.concat([df.head(2), additional_columns.head(2)], axis=1)
```

因为数据帧的索引是对齐的，所以附加列被放置在原始列的右侧:

![Figure 2.37 – Concatenating columns with matching indices
](image/Figure_2.37_B16834.jpg)

图 2.37–连接具有匹配索引的列

`concat()`函数使用索引来确定如何连接这些值。如果它们没有对齐，这将生成额外的行，因为`pandas`不知道如何对齐它们。假设我们忘记了我们的原始数据帧有行号作为索引，我们通过将`time`列设置为索引来读入额外的列:

```
>>> additional_columns = pd.read_csv(
...     'data/earthquakes.csv',
...     usecols=['tz', 'felt', 'ids', 'time'], 
...     index_col='time'
... )
>>> pd.concat([df.head(2), additional_columns.head(2)], axis=1)
```

尽管额外的列包含前两行的数据，但由于索引不匹配，`pandas`为它们创建了一个新行。在 [*第三章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061) ，*与Pandas的数据角力*中，我们将看到如何重置索引和设置索引，这两者都可以解决这个问题:

![Figure 2.38 – Concatenating columns with mismatching indices
](image/Figure_2.38_B16834.jpg)

图 2.38–连接索引不匹配的列

重要说明

在 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) ，*聚合Pandas数据帧*中，我们将讨论合并，这也将在我们扩充数据帧中的列时处理其中一些问题。通常，我们会使用`concat()`或`append()`来添加行，但是使用`merge()`或`join()`来添加列。

假设我们想要连接`tsunami`和`no_tsunami`数据帧，但是`no_tsunami`数据帧有一个额外的列(假设我们向它添加了一个名为`type`的新列)。`join`参数指定如何处理列名(追加到底部时)或行名(串联到右侧时)中的任何重叠。默认情况下，这是`outer`，所以我们保留一切；但是，如果我们使用`inner`，我们将只保留它们的共同点:

```
>>> pd.concat(
...     [
...         tsunami.head(2),
...         no_tsunami.head(2).assign(type='earthquake')
...     ], 
...     join='inner'
... )
```

注意，`no_tsunami`数据帧中的`type`列没有出现，因为它不在`tsunami`数据帧中。不过，看一下指数。在我们将原始数据帧划分为`tsunami`和`no_tsunami`之前，这些是原始数据帧中的行号:

![Figure 2.39 – Appending rows and keeping only shared columns
](image/Figure_2.39_B16834.jpg)

图 2.39–追加行并仅保留共享列

如果索引没有意义，我们也可以传入`ignore_index`来获得索引中的顺序值:

```
>>> pd.concat(
...     [
...         tsunami.head(2), 
...         no_tsunami.head(2).assign(type='earthquake')
...     ],
...     join='inner', ignore_index=True
... )
```

索引现在是连续的，行号不再匹配原始数据帧:

![Figure 2.40 – Appending rows and resetting the index
](image/Figure_2.40_B16834.jpg)

图 2.40–追加行并重置索引

请务必查阅`pandas`文档，以获得关于`concat()`函数和其他数据合并操作的更多信息，我们将在*第 4 章*、*聚合Pandas数据帧*中讨论:http://Pandas . pydata . org/Pandas-docs/stable/user _ guide/merging . html # concating-objects。

## 删除不需要的数据

在将该数据添加到我们的数据帧之后，我们可以看到删除不需要的数据的需要。我们需要一种方法来撤销我们的错误，并删除我们不打算使用的数据。像添加数据一样，我们可以使用字典语法删除不需要的列，就像从字典中删除键一样。`del df['<column_name>']`和`df.pop('<column_name>')`都可以，前提是确实有那个名字的列；否则，我们会得到一个`KeyError`。这里的区别在于，`del`会立即删除它，`pop()`会返回我们正在删除的列。记住，这两个操作都会改变我们的原始数据帧，所以要小心使用。

让我们使用字典符号来删除`source`列。请注意，它不再出现在`df.columns`的结果中:

```
>>> del df['source']
>>> df.columns
Index(['alert', 'mag', 'magType', 'place', 'time', 'title', 
       'tsunami', 'mag_negative', 'parsed_place'],
      dtype='object')
```

注意，如果我们不确定该列是否存在，我们应该将列删除代码放在一个`try...except`块中:

```
try:
    del df['source']
except KeyError:
    pass # handle the error here
```

前面，我们创建了用于过滤数据帧的`mag_negative`列；但是，我们不再需要这个列作为数据框架的一部分。我们可以使用`pop()`来获取`mag_negative`列的序列，稍后我们可以将它用作布尔掩码，而无需将其放在我们的数据帧中:

```
>>> mag_negative = df.pop('mag_negative')
>>> df.columns
Index(['alert', 'mag', 'magType', 'place', 'time', 'title', 
       'tsunami', 'parsed_place'],
      dtype='object')
```

我们现在在`mag_negative`变量中有一个布尔掩码，它曾经是`df`中的一列:

```
>>> mag_negative.value_counts()
False    8841
True      491
Name: mag_negative, dtype: int64
```

由于我们使用`pop()`来移除`mag_negative`系列，而不是删除它，我们仍然可以使用它来过滤我们的数据帧:

```
>>> df[mag_negative].head()
```

这就给我们留下了震级为负值的地震。因为我们也叫`head()`，我们得到前五个这样的地震:

![Figure 2.41 – Using a popped column as a Boolean mask
](image/Figure_2.41_B16834.jpg)

图 2.41–使用弹出的列作为布尔掩码

`DataFrame`对象有一个`drop()`方法，用于就地删除多行或多列(覆盖原始数据帧，无需重新分配)或返回一个新的`DataFrame`对象。为了删除行，我们传递索引列表。让我们删除前两行:

```
>>> df.drop([0, 1]).head(2)
```

请注意，索引从`2`开始，因为我们删除了`0`和`1`:

![Figure 2.42 – Dropping specific rows
](image/Figure_2.42_B16834.jpg)

图 2.42–删除特定行

默认情况下，`drop()`假设我们要删除行(`axis=0`)。如果我们想删除列，我们可以通过传递`axis=1`或者使用`columns`参数指定我们的列名列表。让我们再删除一些列:

```
>>> cols_to_drop = [
...     col for col in df.columns
...     if col not in [
...         'alert', 'mag', 'title', 'time', 'tsunami'
...     ]
... ]
>>> df.drop(columns=cols_to_drop).head()
```

这将删除不在我们想要保留的列表中的所有列:

![Figure 2.43 – Dropping specific columns
](image/Figure_2.43_B16834.jpg)

图 2.43–删除特定列

无论我们决定将`axis=1`传递给`drop()`还是使用`columns`参数，我们的结果都是相同的:

```
>>> df.drop(columns=cols_to_drop).equals(
...     df.drop(cols_to_drop, axis=1)
... )
True
```

默认情况下，`drop()`会返回一个新的`DataFrame`对象；然而，如果我们真的想从原始数据帧中删除数据，我们可以传入`inplace=True`，这将使我们不必将结果重新赋值给我们的数据帧。结果与图 2.43 中的*相同:*

```
>>> df.drop(columns=cols_to_drop, inplace=True)
>>> df.head()
```

在现场操作时一定要小心。在某些情况下，有可能撤销它们；然而，在其他情况下，可能需要从头开始并重新创建数据帧。

# 总结

在本章中，我们学习了如何使用`pandas`进行数据分析的数据收集部分，以及如何用统计数据描述我们的数据，这在我们进入得出结论阶段时会很有帮助。我们学习了`pandas`库的主要数据结构，以及我们可以对它们执行的一些操作。接下来，我们学习了如何从各种来源创建`DataFrame`对象，包括平面文件和 API 请求。使用地震数据，我们讨论了如何总结我们的数据并从中计算统计数据。随后，我们讨论了如何通过选择、切片、索引和过滤来获取数据子集。最后，我们练习了在数据帧中添加和删除列和行。

这些任务也构成了我们`pandas`工作流的主干，以及我们将在接下来的几章中讨论的关于数据争论、聚合和数据可视化的新主题的基础。在继续之前，请务必完成下一节中提供的练习。

# 练习

使用`data/parsed.csv`文件和本章的材料，完成以下练习来练习您的`pandas`技能:

1.  使用`mb`震级类型找到日本地震震级的第 95 百分位。
2.  找出印度尼西亚伴随海啸的地震的百分比。
3.  计算内华达州地震的统计摘要。
4.  添加一栏，表明地震是否发生在一个国家或美国的一个州是在火环上。使用阿拉斯加、南极洲(寻找南极)、玻利维亚、加利福尼亚、加拿大、智利、哥斯达黎加、厄瓜多尔、斐济、危地马拉、印度尼西亚、日本、克马德克群岛、墨西哥(注意不要选择新墨西哥州)、新西兰、秘鲁、菲律宾、俄罗斯、台湾、汤加和华盛顿。
5.  计算火圈位置内的地震数量以及圈外的地震数量。
6.  沿着火环找到海啸的数量。

# 延伸阅读

那些有 R 和/或 SQL 背景的人可能会发现，看看`pandas` 语法的比较会有所帮助:

*   *与 R / R 库的比较*:https://pandas . pydata . org/pandas-docs/stable/getting _ started/Comparison/Comparison _ with _ R . html
*   *与 SQL 的比较*:https://pandas . pydata . org/pandas-docs/stable/Comparison _ with _ SQL . html
*   *SQL 查询*:https://pandas . pydata . org/pandas-docs/stable/getting _ started/comparison/comparison _ with _ SQL . html

以下是一些关于使用序列化数据的资源:

*   *Python 中的 Pickle:对象序列化*:https://www . data camp . com/community/tutorials/Pickle-Python-tutorial
*   *将 RData/RDS 文件读入Pandas。DataFrame 对象(pyreader)*:https://github.com/ofajardo/pyreadr

使用 API 的其他资源如下:

*   *请求包的文件*:https://requests.readthedocs.io/en/master/
*   *HTTP 方法*:https://restfulapi.net/http-methods/
*   *HTTP 状态码*:https://restfulapi.net/http-status-codes/

要了解有关正则表达式的更多信息，请参考以下资源:

*   *掌握 Python 正则表达式作者:费利克斯·洛佩斯，维克多·罗梅罗*:https://www . packtpub . com/application-development/Mastering-Python-Regular-Expressions
*   *正则表达式教程——学习如何使用正则表达式*:https://www.regular-expressions.info/tutorial.html